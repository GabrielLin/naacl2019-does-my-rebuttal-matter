ßrankßargumentßturkID
0ß0.0ßThe table in page 5 is only an appendix and can be ignored during review.ßdummyTurker
1ß0.0ßBesides, the page limit was strictly followed.ßdummyTurker
2ß0.0ßThe supplemental material policy (http://acl2018.org/2018/02/18/supp-materials-policy/)s was only posted on Feb 18th, which is after we have submitted our paper.ßdummyTurker
3ß0.0ßWe also did not receive emails about this at the time to ensure modifications can be made.ßdummyTurker
4ß0.0ßAs mentioned in Section 5, one way to apply our method to NLP is to test the language modelsßdummyTurker
5ß0.0ßwhether they are capable of producing a text with exponent larger than 0.5.ßdummyTurker
6ß0.0ßThe only previous work of applying Taylor's law to texts isßdummyTurker
7ß0.0ßGerlach&Altmann as cited.ßdummyTurker
8ß0.0ßBut we consider this is not a good application method,ßdummyTurker
9ß0.0ßbecause of lack of theoretical background.ßdummyTurker
10ß0.0ßOther loosely related work concerns long range correlation, as mentioned in the article,ßdummyTurker
11ß0.0ßbut the methods have no capability of distinguishing different data kinds.ßdummyTurker
12ß0.0ßThis will be mentioned more clearly in the final version.ßdummyTurker
13ß0.0ß"We did not understand which part you meant ""the given proof is simply"ßdummyTurker
14ß0.0ß"wrong""."ßdummyTurker
15ß0.0ßFor 0.5's case, please consider when words appear following Poisson process in the window.ßdummyTurker
16ß0.0ßSince mean equals variance, alpha is 0.5, whenßdummyTurker
17ß0.0ßstandard deviation is considered with respect to mean.ßdummyTurker
18ß0.0ßThis alpha = 0.5ßdummyTurker
19ß0.0ßmathematically holds not only for Poisson, but for any i.i.d.ßdummyTurker
20ß0.0ßprocess.ßdummyTurker
21ß0.0ßSince this is a commonly known mathematical fact, we omitted the proof.ßdummyTurker
22ß0.0ß>Why is CHILDES database not natural language?ßdummyTurker
23ß0.0ß"We considered that CHILDES is not natural language ""written text""."ßdummyTurker
24ß0.0ßAs you indicate, we modify this wording in the final version.ßdummyTurker
25ß0.0ß>Section 3 needs checking.ßdummyTurker
26ß0.0ßThe Least squares equation is not right.ßdummyTurker
27ß0.0ß"We did not understand what exactly you mean ""not right""."ßdummyTurker
28ß0.0ßIf you mean “not right“ by (3) being divided by |W| and applying square root,ßdummyTurker
29ß0.0ßthen in the final version, we will change the wording so that the definition conformsßdummyTurker
30ß0.0ß"precisely with ""least square error"" method."ßdummyTurker
31ß0.0ßThe overall results won't change with/without |W| (except the error values).ßdummyTurker
32ß0.0ß"If you mean ""not right"" by  taking log on the points for LSM, this is one natural way to apply the"ßdummyTurker
33ß0.0ßmethod since we are considering the power law.ßdummyTurker
34ß0.0ß>The proof in line 607 is not right.ßdummyTurker
35ß0.0ßCheck how expectation of n² is mu.ßdummyTurker
36ß0.0ßThe argument is when delta t = 1.ßdummyTurker
37ß0.0ßFor this case, since every n isßdummyTurker
38ß0.0ßone or zero, n^2 = n (one or zero).ßdummyTurker
39ß0.0ßTherefore, <n^2> = <n>,  the mean.ßdummyTurker
40ß0.0ß>Is Brunner Munzel method better than Kolmogorov-Smirnov test?ßdummyTurker
41ß0.0ßAs you say, Brunner Munzel is one way of testing and we will mentionßdummyTurker
42ß0.0ßso in the final version.ßdummyTurker
43ß0.0ßWe can  consider the application ofßdummyTurker
44ß0.0ßKolmogorov-Smirnov test in the final version, but we do not think thatßdummyTurker
45ß0.0ßthis changes the overall result anyway (that Taylor exponent cannotßdummyTurker
46ß0.0ßdistinguish languages).ßdummyTurker
47ß0.0ß>Neural networks are characterized as black-box.ßdummyTurker
48ß0.0ßIs there any reference?ßdummyTurker
49ß0.0ßWe will add the references in the final version, some papers fromßdummyTurker
50ß0.0ßrecent NIPS.ßdummyTurker
51ß0.0ß>The conclusion from LSTM trained on Shakespeare is not clear.ßdummyTurker
52ß0.0ßTheßdummyTurker
53ß0.0ß>authors have to be clear.ßdummyTurker
54ß0.0ßThe claim is big and have to beßdummyTurker
55ß0.0ß>substantiated.ßdummyTurker
56ß0.0ßWe will add a further explanation for this in the final version.ßdummyTurker
57ß0.0ßWe thank the reviewer for feedback.ßdummyTurker
58ß0.0ßThe main contribution of this paper is not building the resource but learning sense-level embedding with the dictionary information.ßdummyTurker
59ß0.0ßGenerally the ontologies require word relationships (synonyms, etc.ßdummyTurker
60ß0.0ß), but the dictionary can include only “definitions/descriptions” of words without considering the global relations given the vocabulary.ßdummyTurker
61ß0.0ßMore specifically, linguistic experts are required to manually label the synsets, hypernym, hyponym, and so on in order to build the semantic ontology.ßdummyTurker
62ß0.0ßEven though some other languages, such as Chinese and German, may not have associated semantic networks, it is easy to fetching dictionaries online for these languages.ßdummyTurker
63ß0.0ßThis is mentioned in the Generalization paragraph of Introduction section: “the number of words and senses may be increasing as time goes on, hence our approach is quite easy to generalize.”ßdummyTurker
64ß0.0ßHence, obtaining dictionaries should be more easy especially for the languages that do not have their associated linguistic ontologies.ßdummyTurker
65ß0.0ßThank you for your additional comments.ßdummyTurker
66ß0.0ßOur proposed model introduces the position-invariance into RNN, which is an intuitive and effective extension to RNN.ßdummyTurker
67ß0.0ßWe know that CRF is a straightforward extension to MEMM and ResNet is a special case of Highway Networks.ßdummyTurker
68ß0.0ßWe believe that a straightforward and effective extension is also a good novelty.ßdummyTurker
69ß0.0ßThanks for your review.ßdummyTurker
70ß0.0ßFasttext is actually located in a line orthogonal and complementary to our work.ßdummyTurker
71ß0.0ßThis work presents a unified representation learning framework to sub-word level information for NLP tasks.ßdummyTurker
72ß0.0ßWe focus on unsupervised segmentation method to obtain meaningful minimal linguistic units (subwords).ßdummyTurker
73ß0.0ßOur adopted framework is embedding representation independent and with the hope of improving the resulting embedding representation from an independent perspective.ßdummyTurker
74ß0.0ßFasttext represents a word by the sum of the vector representations of its n-grams which can be regarded as an improved word2vec which is far away from our focus in this paper.ßdummyTurker
75ß0.0ßN-gram in fasttext is set heuristically without a specific reasonable segmentation, which is too coarse process.ßdummyTurker
76ß0.0ßHowever, we will add the concerned method as a baseline by following your suggestion.ßdummyTurker
77ß0.0ßW2：ßdummyTurker
78ß0.0ßYes, this work focuses on only single models as you noticed, we will make this clearer to show we indeed have a fair comparison setting and we will add all concerned references later accordingly.ßdummyTurker
79ß0.0ßHowever, it’s worth mentioning that this work is supposed to evaluate subword-augmented algorithms, achieving state-of-the-art results is not our major goal though we have achieved such result in CMRC task.ßdummyTurker
80ß0.0ßFor our reproduction model (DIIN), the performance varies to some extent due to different initial values or parameter setting.ßdummyTurker
81ß0.0ßActually, quite a lot of researchers and projects (e.g.ßdummyTurker
82ß0.0ßDIIN-in-Keras in github) also encounter performance drops during reproduction.ßdummyTurker
83ß0.0ßWe only honestly report for reference and maintain the same setting in all our models for fair comparisons.ßdummyTurker
84ß0.0ßYes, the improvement on BiDAF is larger than the reported ones.ßdummyTurker
85ß0.0ßFor example, BPE-FRQ outperforms Word+Char by about 1.5-1.7% in both of the metrics.ßdummyTurker
86ß0.0ßFor more complex models, the improvement could be more costful.ßdummyTurker
87ß0.0ßThe BiDAF results are not reported due to page limit and we will add them back later.ßdummyTurker
88ß0.0ßEmpirical study shows the subword embeddings obtained from these two networks perform comparatively.ßdummyTurker
89ß0.0ßTo keep focus on evaluating subword embedding, we introduce the networks only for reproduction testbed.ßdummyTurker
90ß0.0ßThis has been stated in line 347-349.ßdummyTurker
91ß0.0ßYes, we will make it clearer to specify the literatures which have been already cited when introducing the baselines in each subtask.ßdummyTurker
92ß0.0ßSince the reading comprehension datasets are quite large, experiments take a long time.ßdummyTurker
93ß0.0ßwe can only evaluate them on SNLI in these 3 days.ßdummyTurker
94ß0.0ßTake the BPE-FRQ for example, the improvement is about +0.08 (dev)/ +0.10 (test).ßdummyTurker
95ß0.0ßThis might indicate increasing embedding features or dimension might not bring much gains and seeking natural and meaningful linguistic units for representation is a more significant way.ßdummyTurker
96ß0.0ßbigram: 86.43 (dev)/ 86.60 (test)ßdummyTurker
97ß0.0ßtrigram: 86.47 (dev)/ 86.54 (test)ßdummyTurker
98ß0.0ßCompared with our reported results in Table 1, bigram and trigram embeddings tend to outperform char embedding slightly while they still have a big gap to reach the performance of subword embeddings.ßdummyTurker
99ß0.0ßThanks for your correction.ßdummyTurker
100ß0.0ßWe will thoroughly check and proofread with the help of a professional native speaker.ßdummyTurker
101ß0.0ßThanks for pointing out the formatting issues in our paper; we will revise and clean the final version.ßdummyTurker
102ß0.0ßWe agree that overfitting on the test set is an issue in deep learning research, thus we tried to avoid overfitting on the test tasks.ßdummyTurker
103ß0.0ßAs we mentioned in Section 3.4 in our paper submission, we studied the hyperparameter settings on 3 out of 10 evaluation tasks, including SICK-R, SICK-E and STS14, and those 3 tasks can be regarded as the development set.ßdummyTurker
104ß0.0ßIn our paper we also mentioned that our first design (inspired by prior work) worked the best and later modifications didn’t provide much performance gain, thus we stuck to the very first design of our model.ßdummyTurker
105ß0.0ßMean+max pooling was inspired by Chen et al.ßdummyTurker
106ß0.0ß(2016), and we compared the mean+max pooling with global max pooling, which is suggested in Conneau et al.ßdummyTurker
107ß0.0ß(2017).ßdummyTurker
108ß0.0ßThe results in Table 1 show that mean+max pooling provides stronger transferability than global max pooling does.ßdummyTurker
109ß0.0ßTying word embeddings in the encoder with the word prediction layer in the decoder was recommended in both Inan et al.ßdummyTurker
110ß0.0ß(2016) and Press & Wolf (2017).ßdummyTurker
111ß0.0ßThe word embedding layer and word prediction layer take up a large portion of memory; we tied their representations together in order to reduce the memory usage and deal with the largest dictionary we could.ßdummyTurker
112ß0.0ßWe mentioned above that we only looked at the effect of hyperparameter tuning on 3 out of 10 tasks, and we reported our model performance on all 10 tasks.ßdummyTurker
113ß0.0ßThe detailed results of hyperparameter tuning are reported in the supplemental material, which was submitted with the main paper.ßdummyTurker
114ß0.0ßYes, we spent the following month after the deadline to make sure our paper is reproducible under different initialisation settings, and our code package is ready for release.ßdummyTurker
115ß0.0ßIn our paper, we presented 2 datasets, including BookCorpus (74M+) and Amazon BookReview (143M+), on which our model was trained.ßdummyTurker
116ß0.0ßThe possible data sources for next exploration include Wikipedia, UMBC and Common Crawl.ßdummyTurker
117ß0.0ßWe can add more results if our paper is accepted.ßdummyTurker
118ß0.0ßWe mentioned both DiscSent and DisSent in the related work in our paper, but we forgot to add the results from DisSent to the table.ßdummyTurker
119ß0.0ßWe present the comparison of our model with DisSent (the best single model) here, and we will modify the table in our paper soon.ßdummyTurker
120ß0.0ßTasks                  |  MR  |  CR  | SUBJ | MPQA | SST | TREC | SICK-R | SICK-E |ßdummyTurker
121ß0.0ßOur (RNN-CNN)                    | 80.9 | 82.2 |   94.1  |   88.8  | 83.8 |  89.2   |  0.8735 |    84.7   |ßdummyTurker
122ß0.0ßDisSent Books 5 | 83.4 | 81.8 |   93.4  |   90.0  | 82.5 |  87.0   |  0.8210 |    82.6   |ßdummyTurker
123ß0.0ßAs you mentioned, our model performs better than DisSent on most of the tasks.ßdummyTurker
124ß0.0ßIt is worth noting that the DisSent model requires the preprocessed discourse information to train the model on, while our model doesn’t require that information.ßdummyTurker
125ß0.0ßThere are 2 references in our paper about the importance of the context information in human language learning.ßdummyTurker
126ß0.0ßAlthough our paper is purely about learning sentence representations, we drew inspiration from human language learning and understanding and wanted to show our respect to language understanding researchers.ßdummyTurker
127ß0.0ßWe can take it out f if we need space to add more model related specifications.ßdummyTurker
128ß0.0ß"We would like to thank the reviewer for summarizing the strengths of the paper in such a clear and compelling way: ""1. the evaluation of explanation methods is of high"ßdummyTurker
129ß0.0ß"interest for the scientific community"", ""2. the state-of-the-art on existing explanation methods"", ""3. the paper is rather well examplified""."ßdummyTurker
130ß0.0ßGiven this summary, we feel that the reviewer's overall grade of 2 is too harsh.ßdummyTurker
131ß0.0ß"Reply to weakness 1 (""hypotheses at the basis of the paradigms are really simplistic and actually debatable""):"ßdummyTurker
132ß0.0ßIt would be helpful if the reviewer could state exactly what they find simplistic and debatable.ßdummyTurker
133ß0.0ßThe problems that we are aware of were discussed in the discussion section, any points beyond these would be very interesting to us.ßdummyTurker
134ß0.0ß"Reply to weakness 2 (""whole contribution is weak""):"ßdummyTurker
135ß0.0ßAgain, we find this blanket statement difficult to reply to.ßdummyTurker
136ß0.0ßWe are of the mindset that evaluating and comparing multiple existing methods is an important contribution that a large number of NLP researchers will benefit from.ßdummyTurker
137ß0.0ßThis is especially true because most previous evaluations of neural explanation methods in NLP are either qualitative-only, or compare against a single weak baseline.ßdummyTurker
138ß0.0ßOur study has a large delta to previous work in terms of scope (number of explanation methods).ßdummyTurker
139ß0.0ß"Reply to weakness 3 (""proposed experiments don't allow an evaluation/validation""):"ßdummyTurker
140ß0.0ßWe respectfully disagree with this view.ßdummyTurker
141ß0.0ßWe are using publicly available datasets.ßdummyTurker
142ß0.0ßWe rigorously define the explanation methods (code will be linked in the final version) and the code of the task methods will also be published.ßdummyTurker
143ß0.0ßIt will be possible to validate our results.ßdummyTurker
144ß0.0ßOur paper proposes and carries out two evaluation experiments for neural explanation methods for NLP.ßdummyTurker
145ß0.0ßThere are extensive experimental results and accompanying discussions in the paper: please refer to Table 2, Section 4 and Section 5.ßdummyTurker
146ß0.0ßSo contrary to the reviewer's statement, we present an evaluation.ßdummyTurker
147ß0.0ßThanks to the reviewer for some very helpful comments!ßdummyTurker
148ß0.0ß"Reply to weakness 1 (""paper refers too much to supplementary material""):"ßdummyTurker
149ß0.0ßThe reviewer is certainly right about this.ßdummyTurker
150ß0.0ßIn the final version, some of the material will be moved to the main paper.ßdummyTurker
151ß0.0ß"Reply to question 1 (""Is it possible to explain in more details, why 'small context tasks' are called like this?"ßdummyTurker
152ß0.0ß"""):"ßdummyTurker
153ß0.0ß"By ""small context tasks"", we mean tasks that can be solved by finding short, more or less self-contained indicators (words and phrases), and weighting them up."ßdummyTurker
154ß0.0ßThese are tasks where we expect CNNs with pooling to perform well.ßdummyTurker
155ß0.0ß"""large context tasks"" are those tasks where indicators interact with each other over long distances."ßdummyTurker
156ß0.0ß"For instance, consider deciding the number of [V] in ""the children in the big green house said that the big telescope [V]"" vs. ""the children in the big green house who broke the big telescope [V]""."ßdummyTurker
157ß0.0ß"The immediate contexts of ""children"" or ""[V]"" do not help you make this decision."ßdummyTurker
158ß0.0ßInstead, you have to consider a large context that spans the entire sentence.ßdummyTurker
159ß0.0ßWe agree that this could be made much clearer in the paper and we would do this in case of acceptance.ßdummyTurker
160ß0.0ßThanks for pointing it out!ßdummyTurker
161ß0.0ß"Reply to question 2 (""Will the code for LIMSSE be published online?"ßdummyTurker
162ß0.0ß"""):"ßdummyTurker
163ß0.0ßYes.ßdummyTurker
164ß0.0ßOur implementation of LIMSSE (and most of the other evaluated methods) is already online.ßdummyTurker
165ß0.0ßWe did not include the link for anonymity reasons, but will put it in the final version.ßdummyTurker
166ß0.0ßWe really appreciate taking your precious time to read our paper and suggesting the weakness points.ßdummyTurker
167ß0.0ßWe agree that our proposal is indeed a very simple, and its simplicity is much like Frustratingly Easy Domain Adaptation (Daumé 2007).ßdummyTurker
168ß0.0ßWe believe that simplicity is important in practice, since applying a sophisticated model to a real service will take more trial and error.ßdummyTurker
169ß0.0ßFurthermore, as we showed in the experiments, it is not easy to produce better results using other methods.ßdummyTurker
170ß0.0ßWe are sorry for our confusing description.ßdummyTurker
171ß0.0ßWe actually conducted statistical testing as shown in Line 291-294 and 298-299.ßdummyTurker
172ß0.0ßFor your information, we confirmed that the differences between our method and other baselines are all statistically significant for all the five data-size variations (5K-80K).ßdummyTurker
173ß0.0ß"We will add a mark such as ""dagger"" in Table 2 to emphasize the statistical significance."ßdummyTurker
174ß0.0ßIn addition, although the improvement looks small at a first glance, such a small improvement can yield a big impact for a real service.ßdummyTurker
175ß0.0ßFor example, even 0.01% improvement of the click-through rate (for online ad) in this big company (Yahoo!ßdummyTurker
176ß0.0ßJapan) can yield ad revenue of million dollars.ßdummyTurker
177ß0.0ßWe are very grateful for your insightful comments to make our paper clear.ßdummyTurker
178ß0.0ßAs for your question in Contribution 3, we will also publish it to allow other researchers to compare our results for testing.ßdummyTurker
179ß0.0ßWe are sorry for this confusion.ßdummyTurker
180ß0.0ßLang and SeqAE were indeed trained with the same supervision information as Dial on the basis of the same tweet-reply pairs.ßdummyTurker
181ß0.0ß"We did not use the term ""the same data"" or ""dialog data"", since we wanted to emphasize that they cannot use the dialog structure of tweet-reply pairs."ßdummyTurker
182ß0.0ßWe will clarify this point.ßdummyTurker
183ß0.0ßWe agree your comment and will add the Precision/Recall/F1 measures.ßdummyTurker
184ß0.0ßOverall, these numbers were consistent with those of accuracy.ßdummyTurker
185ß0.0ßThe broad intuition of dealing with polysemy can be divided into three parts: First, extract the most common semantic information for each word.ßdummyTurker
186ß0.0ßSecond, generate context-specific vectors for each word based on the common semantic information, which can also learn more context-specific information during training process.ßdummyTurker
187ß0.0ßThird, choose the proper CWE according to the different context information.ßdummyTurker
188ß0.0ßFollowing the intuition, we design the CDNN structure as follows:ßdummyTurker
189ß0.0ßFirst, we apply the convolution layer and max-pooling layer for extracting the common semantic information for each word.ßdummyTurker
190ß0.0ßThe width of the 1d-convolution layer is set as 1 and the pooling operation is applied over different kernels.ßdummyTurker
191ß0.0ßBoth the convolution layer and the pooling layer won’t involve any interaction operation among words.ßdummyTurker
192ß0.0ßIn other words, the max-pooling operation is to compare among kernels and to extract the most significant features with highly activate value for each single word.ßdummyTurker
193ß0.0ßIt is the only reasonable operation for extracting the most common semantic information for every single word.ßdummyTurker
194ß0.0ßSecond, generate context-specific vectors for each word using the deconvolution layer and select the proper CWE according to the context vector.ßdummyTurker
195ß0.0ßThe intuition of using the deconvolution layer for generation context-specific vectors is inspired by the usage of the deconvolution in up-sampling process.ßdummyTurker
196ß0.0ßThe deconvolution layer can extend the input information and learn different information using different deconvolution kernels.ßdummyTurker
197ß0.0ßAs we use the context representation vector for selecting the proper CWE vectors, the spatial resolution gradually increases with the deconvolution operation proceeding, and the parameters of deconvolution kernels will be trained to distinguish and update the context-specific word vectors for every single word during the selecting process.ßdummyTurker
198ß0.0ßThank you for your valuable comment.ßdummyTurker
199ß0.0ßWe are sorry for these mistakes and will carefully correct all the grammar mistakes in the camera-ready version.ßdummyTurker
200ß0.0ßTo obtain rigorous experimental results, we did actually perform experiments on LSTM models for comparison, and the results showed that our model can outperform the LSTMs by about 2%-4% among three datasets.ßdummyTurker
201ß0.0ßAs the experiment results showed that RNNs didn’t seem to work better than CNN and many recent researchers using CNN for building models correlated to word embedding (listed in the related work of our paper) didn’t compare their results with RNNs, either.ßdummyTurker
202ß0.0ßMoreover, the whole framework of our model is based on convolution and deconvolution neural network, it seems not so necessary to put these results in the paper.ßdummyTurker
203ß0.0ßThank you for your helpful comments.ßdummyTurker
204ß0.0ßWe will improve our paper accordingly.ßdummyTurker
205ß0.0ßIn fact, we only annotate one-fifth of the whole HIT-CDT data (11 thousand among 55 thousand sentences), and the cost is also reasonable, which amounts to 550 person-hours, and can be completed by 20 full-time annotators within 4 days.ßdummyTurker
206ß0.0ßThe most cost is spent on the quality control via two-independent annotation and inconsistency handling by experts.ßdummyTurker
207ß0.0ßThis is in order to obtain very high-quality data.ßdummyTurker
208ß0.0ßThe cost can be reduced to about 150 person-hours without such strict quality control.ßdummyTurker
209ß0.0ßMore importantly, the annotated data itself is a great contribution to the research community.ßdummyTurker
210ß0.0ßIt is very useful to future research on treebank conversion, parsing and other syntax-based NLP tasks.ßdummyTurker
211ß0.0ßCompared with direct treebank conversion, multi-task learning has two drawbacksßdummyTurker
212ß0.0ßDue to the lack of bi-tree aligned data, previous work on direct treebank conversion has made very limited progress (Niu et al., 2009; Zhu et al., 2011; Li et al., 2013).ßdummyTurker
213ß0.0ßBy annotating a small-scale bi-tree aligned data, we for the first time make significant progress on this research direction, and clearly demonstrate that direct treebank conversion is more effective than multi-task learning in utilizing multiple heterogeneous treebanks for boosting the target-guideline parsing accuracy, as shown in Section 5.3.ßdummyTurker
214ß0.0ßMoreover, we are annotating more bi-tree aligned data for other Chinese treebanks in Table 1, in order to convert all existing treebanks into our unified target guideline and obtain really large-scale training data for Chinese parsing.ßdummyTurker
215ß0.0ßWe expect to see a significant improvement on Chinese parsing accuracy, making Chinese parsers much more robust in real-life text analysis.ßdummyTurker
216ß0.0ßAll bi-tree aligned data will be released for future research on treebank conversion, parsing, and other syntax-based NLP tasks.ßdummyTurker
217ß0.0ßThank you very much for your detailed and helpful comments.ßdummyTurker
218ß0.0ßWe will improve our paper accordingly.ßdummyTurker
219ß0.0ßWe guess (sorry if we misunderstand) you mean we may employ multi-task learning for treebank conversion, by treating HIT-side parsing and HIT-to-ours conversion as two separate tasks.ßdummyTurker
220ß0.0ßThis can be considered as a combination of the multi-task learning approach and our proposed treebank conversion approaches.ßdummyTurker
221ß0.0ßWe will consider this direction in the future.ßdummyTurker
222ß0.0ßThank you.ßdummyTurker
223ß0.0ßThe focus of our current work, besides other contributions, is proposing two simple yet effective conversion approaches.ßdummyTurker
224ß0.0ßIn order to better utilize source-side train-HIT, we build the conversion model upon the multi-task learning model by reusing the embedding and biLSTM parameters without fine-tuning, as discussed in Line 634-639.ßdummyTurker
225ß0.0ßYes, it is most straightforward to first use rules to coarsely convert the HIT-side trees into our target guideline.ßdummyTurker
226ß0.0ßPlease notice that 81% is the overlap ratio of unlabeled dependencies.ßdummyTurker
227ß0.0ßHowever, it is difficult to design rules to map labels (14 types -> 20 types).ßdummyTurker
228ß0.0ßBy using a simple one-to-one mapping strategy, we can reach only 73.79 LAS, as discussed in Line 286-296.ßdummyTurker
229ß0.0ßIn contrast, our supervised conversion approach can achieve 82% LAS (Table 3 in Page 7).ßdummyTurker
230ß0.0ßWe will make discussions from this perspective in the next version.ßdummyTurker
231ß0.0ßThank you.ßdummyTurker
232ß0.0ßSince both converted train-HIT and train comply with our target guideline, we can directly train a parsing model (last row in Table 5).ßdummyTurker
233ß0.0ßIt seems unnecessary to employ multi-task learning on the two datasets and treat them as two different parsing tasks.ßdummyTurker
234ß0.0ßSorry if we misunderstand your question.ßdummyTurker
235ß0.0ßFor the second question, please notice that all sentences in train are also contained in train-HIT.ßdummyTurker
236ß0.0ßTherefore, the models in the last two rows in Table 5 are trained under the same training data scale.ßdummyTurker
237ß0.0ßSorry if we misunderstand your question.ßdummyTurker
238ß0.0ßYes, we adopt partial annotation to save annotation effort.ßdummyTurker
239ß0.0ßLi et al.ßdummyTurker
240ß0.0ß(2016) show that it is unnecessary to annotate the easy dependencies.ßdummyTurker
241ß0.0ßAfter our treebank conversion, all sentences are completely annotated.ßdummyTurker
242ß0.0ßWe adopt the recently proposed state-of-the-art deep biaffine neural parser, which outperforms the discrete-feature parser in Li et al.ßdummyTurker
243ß0.0ß(2016) by about 6-7%.ßdummyTurker
244ß0.0ßWe extend the original biaffine parser with a global CRF loss following Li et al.ßdummyTurker
245ß0.0ß(2016), in order to effectively learn from partial annotations, as discussed in Line 334-338 and 608-610.ßdummyTurker
246ß0.0ßOur approach would work better with fully annotated data, which yet requires more annotation effort according to Li et al.ßdummyTurker
247ß0.0ß(2016).ßdummyTurker
248ß0.0ßThanks for your comments.ßdummyTurker
249ß0.0ßWe will include more details in our future version and clarify the ambiguous places.ßdummyTurker
250ß0.0ßOur main goal is not to design a better language model, but to propose a “light-weighted” contextual representation model, and to develop a pruning algorithm without requiring extensive GPU memories.ßdummyTurker
251ß0.0ßRecent works have demonstrated the potential of those representations (Matthew E Peters, 2018).ßdummyTurker
252ß0.0ßOnce pre-trained, the representation models can be used as ``additional features`` without fine-tuning or extensive memory requirement.ßdummyTurker
253ß0.0ßTo some extent, these pre-trained models can be viewed as common resources like the pre-trained word embedding.ßdummyTurker
254ß0.0ßHowever, the overwhelming inference workload hinders existing methods from real-world applications.ßdummyTurker
255ß0.0ßTherefore, the motivation of this work is to reduce the model size (thus improving model efficiency in inference stage) while preserving comparable effectiveness.ßdummyTurker
256ß0.0ßAdditionally, we want our pruning method to be memory-friendly, i.e., not requiring re-training.ßdummyTurker
257ß0.0ßTo achieve this goal, we have introduced: (1) dense connectivity for layer-wise pruning; and (2) layer-wise dropout; and (3) novel sparse regularization.ßdummyTurker
258ß0.0ßThe resulting model demonstrates the capability of LD-Net and the effectiveness of the proposed pruning principle.ßdummyTurker
259ß0.0ßPrevious work (Peters et al., 2017) has concluded that **large** PTLMs are necessary for informative contextualized representations.ßdummyTurker
260ß0.0ßAlso, we tried to directly pre-train small language models (2-layer LSTM with 300 hidden#, of a similar size with the pruned PTLM) and it doesn’t help a lot (e.g.,  ~0.910 F1 scores on CoNLL03).ßdummyTurker
261ß0.0ßOur method extracts relevant information for the end task at hand, and condense large LMs to smaller ones.ßdummyTurker
262ß0.0ßThus it contains more task-specific information compared with a small LM.ßdummyTurker
263ß0.0ßAs stated in line-722, the ``best-performing`` model is picked based on their performance on the **development** set, instead of the test set.ßdummyTurker
264ß0.0ßWe will clarify experiment setup details in our final version to avoid confusion.ßdummyTurker
265ß0.0ßFor LD-Net, we encourage different modules to capture different information by putting them to different depth.ßdummyTurker
266ß0.0ßEven after the layer-wise dropout, deeper layers would have more abundant information, which allows deeper layers to capture information of higher levels.ßdummyTurker
267ß0.0ßThis setting is totally different from ensemble, as LD-Net wants and allows a layer rely on **a few** lower layers.ßdummyTurker
268ß0.0ßActually, we implemented a prototype, i.e., Mixture-of-Expert network (Shazeer et al., 2017) without gating network, which can be viewed as an ensemble model.ßdummyTurker
269ß0.0ßHowever, it doesn’t work well (e.g., cannot achieve >=0.91 test F1 scores on CoNLL03).ßdummyTurker
270ß0.0ßWe think this is due to the same reason as you mentioned (i.e., all models are forced to play similar roles).ßdummyTurker
271ß0.0ßWe will add it as another baseline in our experiment in the future version.ßdummyTurker
272ß0.0ßAs in Table 1, 10-layer LD-Net is pre-trained in our experiments (for each direction).ßdummyTurker
273ß0.0ßWe conduct an additional run.ßdummyTurker
274ß0.0ßOn NER, the remained layers are 9th(forward) and 6th(backward); the F1 score is 0.9194.ßdummyTurker
275ß0.0ßOn Chunking, the remained layers are 3rd, 7th, 8th, 10th (forward) and 3rd, 9th(backward); the F1 score is 0.9563.ßdummyTurker
276ß0.0ß1) In practice, the Backpropagation-Through-Time could become quite problematic for very **large** PTLMs.ßdummyTurker
277ß0.0ßWithout fine-tuning PTLMs, LD-Net only requires ~3GB memory for sequence labeling (e.g., CONLL’03), which can be easily fit in most GPUs.ßdummyTurker
278ß0.0ßMeanwhile, retraining PTLM for the end task requires much more resources.ßdummyTurker
279ß0.0ßIn our experiments, training **one** language model requires ~10GB memory, even after constraining the back-propagation to a limited number (e.g., 20) of timestamps.ßdummyTurker
280ß0.0ßMoreover, the contextualized representation needs **two** language models simultaneously, which doubles the memory requirement.ßdummyTurker
281ß0.0ßFurthermore, the current state-of-the-art practice on many NLP tasks (e.g.,  NER) conducts the back-propagation through the whole input sequence, which could contain more than 100 tokens in our experiments.ßdummyTurker
282ß0.0ßSuch GPU memory consumption motivates us to conduct this challenging task, i.e., pruning without retraining.ßdummyTurker
283ß0.0ß2) The arXiv version of Matthew E. Peters (2018) was posted after the ACL submission deadline.ßdummyTurker
284ß0.0ßAccordingly, we cited its ICLR version (as shown in line-871, referred as ELMo) in this work.ßdummyTurker
285ß0.0ßWe apologize for the confusion caused here, and will double check all references in the final version.ßdummyTurker
286ß0.0ß3) The evaluation metric in our experiments is the micro-averaged entity-level F1 score (can be checked in our submitted code).ßdummyTurker
287ß0.0ßWe will make it clear in the updated version.ßdummyTurker
288ß0.0ßBeing able to represent sentences in DRT is the first step towards creating more elaborate document-based representations.ßdummyTurker
289ß0.0ßNote that many of the features of multi-sentence DRT are evidenced in sentences.ßdummyTurker
290ß0.0ßThe DRT representation we work with follows segmented  DRT (SDRT), which decomposes sentences into several segments and then assigns rhetorical and co-reference relations between segments.ßdummyTurker
291ß0.0ßSentences in GMB contain multiple segments, and intra-segment dependencies.ßdummyTurker
292ß0.0ßWe will clarify this point and provide more analysis (e.g., how many co-reference relations are identified correctly within a sentence).ßdummyTurker
293ß0.0ßWe will integrate figures 2b and 3c, some of these exposition decisions were due to lack of space.ßdummyTurker
294ß0.0ßWe will also tidy up notation as best as we can.ßdummyTurker
295ß0.0ßWe only used embeddings for words and their lemmas in sentences.ßdummyTurker
296ß0.0ßFollowing initialization practice in neural syntactic parsing, we tuned random embeddings and fixed pretrained embeddings during training to make the pretrained embeddings consistent.ßdummyTurker
297ß0.0ßFor example, if we tune the pretrained embeddings,  words appearing in both the training data and pretrained lookup table will be trained, but  other words  appearing in the pretrained lookup table will remain the same.ßdummyTurker
298ß0.0ßAnd as a result, pretrained word embeddings that appear in training data will not be comparable to  pretrained word embeddings that do not appear in the training data.ßdummyTurker
299ß0.0ßThanks for the review.ßdummyTurker
300ß0.0ßIn our view, the contribution is not limited to adding a smoothing parameter to a previously proposed measure.ßdummyTurker
301ß0.0ß"In the literature, several authors have already failed in adopting APSyn with dense vectors, as they tried to use its basic form - as it was initially proposed for count-based vectors (an example is in ""Extracting common sense Knowledge via triple ranking using supervised and unsupervised distributional models"" of Jebbara et al."ßdummyTurker
302ß0.0ß(2018)).ßdummyTurker
303ß0.0ßThis paper is meant to fill this gap, making the measure available for such uses (we did provide also code and data).ßdummyTurker
304ß0.0ßOn top of it, in dense vectors it is possible to drop all parameters (the smoothing parameter is shown to be a constant), consequently removing one of the major limitations of APSyn.ßdummyTurker
305ß0.0ßFinally, we believe that our experiments prove how suitable rank-based measures are for tasks that require a discriminative notion of similarity, such as the outlier identification task.ßdummyTurker
306ß0.0ßWe have more qualitative analysis in this respect that we would be happy to add in the extra page, upon acceptance.ßdummyTurker
307ß0.0ßFirst of all, thank you very much for the positive feedback and appreciation for our work.ßdummyTurker
308ß0.0ßOur response to the questions.ßdummyTurker
309ß0.0ßSeveral criteria such as perplexity and modified KL divergence were actually improved and homogeneity of latent actions was evaluated, but it is unsure how the improvement will have the impact on performance of dialog systemsßdummyTurker
310ß0.0ßThis is a very good point.ßdummyTurker
311ß0.0ßThe performance of dialog systems depends on many factors, e.g.ßdummyTurker
312ß0.0ßthe performance of context encoder, the decoder at etc.ßdummyTurker
313ß0.0ßThe focus of this work is to enable the ability interpretable neural dialog generation, which can benefit us for improving our systems.ßdummyTurker
314ß0.0ßIndeed, the end-to-end dialog performance is the ultimate goal, and our next research step is to explore 1. how well our LAED performs at dialog level 2. how can we improve dialog performance using novel methods because of the presence of latent actions.ßdummyTurker
315ß0.0ßWe originally prepared to do error analysis.ßdummyTurker
316ß0.0ßHowever, we removed this part due to limited space.ßdummyTurker
317ß0.0ßWe will add such analysis in the revision.ßdummyTurker
318ß0.0ßWe will report the significance analysis results based on t-test between model #5 and #6 and between #11 and #12 in the revised manuscript.ßdummyTurker
319ß0.0ßThis paper reports our preliminary results of the proposed method on CoNLL 2003 NER.ßdummyTurker
320ß0.0ßTo apply our proposed method to more languages and more tasks will be our future work.ßdummyTurker
321ß0.0ßThose hyperparameters were selected according to the two baseline methods used in our paper(Ma and Povy (2016) and Liu at al.ßdummyTurker
322ß0.0ß(2018)).ßdummyTurker
323ß0.0ßIn our experiments, the models based on CNN-BLSTM (model #1-#6) adopted the same hyperparameters as the ones used in Ma and Povy (2016).ßdummyTurker
324ß0.0ßAnd the models based on LM-BLSTM (models #7-#12) adopted the same hyperparameters as the ones used in Liu et al.ßdummyTurker
325ß0.0ß(2018).ßdummyTurker
326ß0.0ßBesides, we chose the same optimizer as Liu et al.ßdummyTurker
327ß0.0ß(2018).ßdummyTurker
328ß0.0ß"In the hyperparameters of ""word-level BLSTM"", 200 and 300 combined two directions, i.e., the number of hidden unit for each direction was 100 and 150 respectively."ßdummyTurker
329ß0.0ß"And in the hyperparameters of ""character-level LSTM"", 300 referred to a single direction, which was the same as the configuration in Liu et al."ßdummyTurker
330ß0.0ß(2018).ßdummyTurker
331ß0.0ßThe motivation is that we hypothesize that the difference between representations of two words which are in the same named entity should be much smaller than the difference of those not in a named entity.ßdummyTurker
332ß0.0ß"For example, in a sentence ""Barack Obama was the president"", when we calculate the score of segment (Barack Obama), we employ the features of w(Obama) – w(Barack), where w(*) is the output of BLSTM."ßdummyTurker
333ß0.0ßAnd when we calculate the score of segment (Barack Obama was), we employ the features of w(was) – w(Barack).ßdummyTurker
334ß0.0ß"Obviously, the difference of the previous one should be much smaller than the latter one (""Barack"" is closer to ""Obama"" than a verb ""was""), which is helpful to recognize a segment."ßdummyTurker
335ß0.0ßAnd we will add this explanation in the revision.ßdummyTurker
336ß0.0ßWe will modify them accordingly in the revision.ßdummyTurker
337ß0.0ßThe novelty of this work is that we are the first to provide a general framework that exploits document-level training examples for improving the performance of aspect-level sentiment classification.ßdummyTurker
338ß0.0ßThis idea is valuable as aspect-level annotations are expensive to produce while plentiful document-level labeled samples are accessible online.ßdummyTurker
339ß0.0ßAlso, the proposed approaches are general and easily applicable to other more advanced base models.ßdummyTurker
340ß0.0ß“LSTM only” is a weaker baseline compared to LSTM+ATT.ßdummyTurker
341ß0.0ßIts (avg_acc, avg_macrof1) over 5 runs on the 4 datasets are as follows: (75.23, 64.21), (66.79, 64.02), (75.28, 54.10), (81.94, 58.11).ßdummyTurker
342ß0.0ßDue to space limitation, we did not include its results in the submitted draft, will add it in the camera-ready version.ßdummyTurker
343ß0.0ßTable 3 presents the results of PRET with different layers been transferred.ßdummyTurker
344ß0.0ßFor example, you can see that in general,  transfer of “Output layer only” achieves lower results compared to “LSTM only” and “Embeddings only”, while “Without output layer” achieves higher results than “Without LSTM / embeddings”.ßdummyTurker
345ß0.0ßThis indicates that the performance of PRET is less affected by the transfer of output layer.ßdummyTurker
346ß0.0ßPlease refer to reviewer#3 question 1 for more discussions on Table 3.ßdummyTurker
347ß0.0ßIntuitively, PRET+MULT should be able to encode all information that LSTM+ATT does, as the main component of PRET+MULT is LSTM+ATT.ßdummyTurker
348ß0.0ßGiven the results in Table 2 where PRET+MULT significantly outperforms LSTM+ATT ,  we think it is more interesting to understand why the proposed model is better with limited space.ßdummyTurker
349ß0.0ßFurther analysis on cases that LSTM+ATT performs better can be added in camera-ready version.ßdummyTurker
350ß0.0ßThe focus of this work is aspect-level sentiment classification.ßdummyTurker
351ß0.0ßWe have discussed our motivation to explore document-level data in the introduction and shown the knowledge from documents is beneficial in 4 aspects with examples.ßdummyTurker
352ß0.0ßMore intuitions and practical insights can be added when more space is available.ßdummyTurker
353ß0.0ßThanks for your comments.ßdummyTurker
354ß0.0ßWe recruited three native graduate students in Computer Science in our university to label the data, with each label payed by 20 cents.ßdummyTurker
355ß0.0ßWe will include these details in our revision.ßdummyTurker
356ß0.0ßThe idea that general/trivial responses may quickly lead to an end of conversation has been taken as a common motivation in (Li et al.ßdummyTurker
357ß0.0ßNAACL 2016; Xing et al.ßdummyTurker
358ß0.0ßAAAI 2017)[1,2].ßdummyTurker
359ß0.0ßHowever, as you mentioned, it is still worth to have an end-to-end evaluation on this assumption.ßdummyTurker
360ß0.0ßThis may need to conduct some online experiments and we will try it in our future work.ßdummyTurker
361ß0.0ß"[1] Li et al., ""A diversity-promoting objective function for neural conversation models""."ßdummyTurker
362ß0.0ßNAACL 2016.ßdummyTurker
363ß0.0ß"[2] Xing et al., ""Topic aware neural response generation""."ßdummyTurker
364ß0.0ßAAAI 2017.ßdummyTurker
365ß0.0ßThanks for your comments.ßdummyTurker
366ß0.0ßThanks for pointing out this, and we will cite and discuss the related work in our revision.ßdummyTurker
367ß0.0ßWe employ distant labels to guide the model to generate specific or general responses.ßdummyTurker
368ß0.0ßSo it is expected that the distant label (i.e, NIWF) could reflect (or be correlated with) the specificity of the response, which in turn somehow correlated with the evaluation metrics (like distinct).ßdummyTurker
369ß0.0ßIf not so, the training signal might not be effective, like NIRF.ßdummyTurker
370ß0.0ßThe basic idea is that given the specificity control variable, we want the word usage representation to regress to it through certain mapping function (sigmoid in our work).ßdummyTurker
371ß0.0ßWhen we do this under a probabilistic framework, this is equivalent to define a Gaussian Kernel layer as in Equation (4).ßdummyTurker
372ß0.0ßSimilar to the reply to the [Weakness argument 3].ßdummyTurker
373ß0.0ßBesides, we have not realized that our idea is related to the Gaussian Embeddings since they are quite different in motivation.ßdummyTurker
374ß0.0ßBut it is interesting to have some further investigation.ßdummyTurker
375ß0.0ßThanks, this is an interesting observation.ßdummyTurker
376ß0.0ßWhen we look at those cases (label 0 given s=1), we find that some conversations may not need specific replies, while the model is forced to generate the most specific reply so that it is likely to generate <UNK> words in this case.ßdummyTurker
377ß0.0ßThis means that it is not always the best to fix s=1, and that is why s=0.5 achieves the best in terms of BLEU, average and extrema metrics in automatic evaluation.ßdummyTurker
378ß0.0ßIn our work, we actually factorize the underlying intention or communication purpose into two parts, one is the semantic part and the other is the specificity part.ßdummyTurker
379ß0.0ßThe first part decides what to say next given the input, while the latter part decides how specific we should reply.ßdummyTurker
380ß0.0ßAs we have mentioned in our paper, this latter part might be affected by many latent factors, like personal mood or knowledge status, which are difficult to be enumerated one by one.ßdummyTurker
381ß0.0ßSo we summarize them into one variable.ßdummyTurker
382ß0.0ßWe use the final backward hidden state following the paper “neural machine translation by jointly learning to align and translate, ICLR 2015”.ßdummyTurker
383ß0.0ßThe concatenation from both sides could be an alternative and we will try it in our future work.ßdummyTurker
384ß0.0ßWe will cite those related work.ßdummyTurker
385ß0.0ßAs compared with Shen et al.’s work, our work conveys the following major differencesßdummyTurker
386ß0.0ßThere have been work related to learning different word embeddings, but the usage representation plus Gaussian kernel layer is new so far as we know.ßdummyTurker
387ß0.0ßThe reply to the NIWF comment is the same as that for the [Weakness argument 3].ßdummyTurker
388ß0.0ßThank you for your valuable review.ßdummyTurker
389ß0.0ßWe will cite this paper but we would like to point outßdummyTurker
390ß0.0ßThere is not much space and we would add “Conclusion”, “Future Work” and “Acknowledgement” sections once given five pages after acceptance.ßdummyTurker
391ß0.0ßAs mentioned in Line 87-90 and 198-202, we assume word vectors of different languages could be drawn from a same latent variable space that captures the semantic meaning and we want our model to learn the latent variables.ßdummyTurker
392ß0.0ßMost of your concerns (weakness 2, weakness 3, weakness 4 (ii), question 2, remark 2, remark 7) are about “the method uses annotated event triggers in testing phase”.ßdummyTurker
393ß0.0ßHowever, it seems that you have misunderstood our approach and also the task of Event Detection (ED).ßdummyTurker
394ß0.0ß(1) Our approach *NEVER* resort to annotated event triggers in the testing phase.ßdummyTurker
395ß0.0ß(2) ED, in fact, requires a model to output both the *locations of event triggers* and the event types they evoke.ßdummyTurker
396ß0.0ßLocating event triggers is *part of the goal of ED task*.ßdummyTurker
397ß0.0ß(See Section 2.1)ßdummyTurker
398ß0.0ß"Details are in ""Reply to weakness argument 2""."ßdummyTurker
399ß0.0ßWe believe that the most convincing proof of the effectiveness of adversarial training strategy is to make the following comparison:ßdummyTurker
400ß0.0ßWithout adversarial training, the “student” achieves only a performance of 68.8% on F1 (ANN-Con in Table 2); By competing with the discriminator, the adversarially enhanced “student” achieves a performance of 71.2% on F1.ßdummyTurker
401ß0.0ß(Our in Table 3)ßdummyTurker
402ß0.0ßNote that the inputs of the original “student” and the adversarially enhanced “student” are same --- just raw texts, so the comparison makes sense.ßdummyTurker
403ß0.0ßThe improvement led by adversarial training strategy is indeed considerable (2.4% on F1).ßdummyTurker
404ß0.0ßBesides, Figure 4 visualizes the learning procedure of our method; Table 4 studies different discrimination strategies.ßdummyTurker
405ß0.0ßWe admit stressing this comparison might make the conclusion salient, and we will add this comparison to Table 3 in the future version.ßdummyTurker
406ß0.0ßThe fact is we NEVER resort to annotated event triggers in the testing phase.ßdummyTurker
407ß0.0ßOur approach works as follows:ßdummyTurker
408ß0.0ßWe first enumerate each token in the document as “candidate trigger”.ßdummyTurker
409ß0.0ß“candidate trigger” indicates the word get focused and to be predicted whether it is an event trigger, and if so the model also needs to predict what event type it evokes.ßdummyTurker
410ß0.0ßIn ED task, a common setting is to assign a ‘Non-Event’ type to a non-trigger word, which unifies the two prediction phases.ßdummyTurker
411ß0.0ßWe next adopt attention computation in our approach, the *attention source is the “candidate trigger”* and the *attention targets are the surrounding words* in the sentence.ßdummyTurker
412ß0.0ßFigure 6 shows the process of treating “fired” as the “candidate trigger”.ßdummyTurker
413ß0.0ßThe embedding of “fired” is attention source vector, and the embeddings of other words are attention targets vectors.ßdummyTurker
414ß0.0ßAfter this prediction, the next word, namely “on” in this example, will be processed.ßdummyTurker
415ß0.0ßThe embedding of “on” will server as attention source vector.ßdummyTurker
416ß0.0ßAs illustrated, our approach NEVER involves with annotated event triggers in the testing phase.ßdummyTurker
417ß0.0ßWe think you might have misunderstood the meaning of “candidate trigger”, illustrated in Section 3.2 of our paper.ßdummyTurker
418ß0.0ßWe hope our explanations make sense and provide enough evidence for the correct evaluation of our work.ßdummyTurker
419ß0.0ßReply to weakness argument 3, weakness argument 4 (ii), question 2:ßdummyTurker
420ß0.0ßWe *NEVER* resort to annotated event triggers in the testing phase.ßdummyTurker
421ß0.0ßSee above.ßdummyTurker
422ß0.0ß(i) Thanks for this constructive suggestion.ßdummyTurker
423ß0.0ßIn fact, we have conducted an experiment of similar setting.ßdummyTurker
424ß0.0ßNamely, we adopt “teacher” in testing phase along with the predicted tags as model input.ßdummyTurker
425ß0.0ßThis setting only achieves 69.8% on the F1 score.ßdummyTurker
426ß0.0ßBesides, we have implemented a more devised model, namely ArgATT (proposed in Liu et.al (2017)).ßdummyTurker
427ß0.0ßAdopting predicted tags for testing, it achieves a performance of 70.7 %, studied in Table 3.ßdummyTurker
428ß0.0ßWe compare our overall approach with the better model.ßdummyTurker
429ß0.0ßOne of our concern is that comparing the adversarially enhanced “student” with the “teacher” using predicted tags as input complicates the interpretation of the results.ßdummyTurker
430ß0.0ßThe best illustration of the effectiveness of the strategy of adversarial training is described in “Replay to weakness argument 1”.ßdummyTurker
431ß0.0ßNevertheless, we will add the comparison you suggested in Table 3 for a comprehensive analysis.ßdummyTurker
432ß0.0ßThe significant test is performed on the total 17754 tokens (in the test set).ßdummyTurker
433ß0.0ßIn ED task, it should identify event triggers out from the entire corpus and assign correct event types to them.ßdummyTurker
434ß0.0ßSo the count of instances is in fact 17754, other than 418.ßdummyTurker
435ß0.0ßThis is the standard experimental settings of current works approaching ED.ßdummyTurker
436ß0.0ß2) and 7) We NEVER resort to annotated event triggers in the testing phase.ßdummyTurker
437ß0.0ßSee “Reply to weakness argument 2”.ßdummyTurker
438ß0.0ß3) The method you suggested requires additional NLP tools (LDA tools) in the testing phase, which suffers the risk of error propagation.ßdummyTurker
439ß0.0ßOn the contrary, in our adversarial training framework, the distilling of “topic information” from raw text has been incorporated into feature extraction phase, by adversarial training.ßdummyTurker
440ß0.0ß4) We assign a real vector to represent each type of entities and event-arguments.ßdummyTurker
441ß0.0ßThe vector is concatenated with word embedding as model input.ßdummyTurker
442ß0.0ß5) See reply to weakness argument 4 (i).ßdummyTurker
443ß0.0ß6) The optimal value of λ is obtained by grid searching on the dev set.ßdummyTurker
444ß0.0ßThanks for the insightful comments.ßdummyTurker
445ß0.0ßDo you randomly change the characters in corpus generation?]ßdummyTurker
446ß0.0ßWe do not replace random occurrences of a character C in the confusion set.ßdummyTurker
447ß0.0ßInstead, we keep frequent words containing C, and replace C with a random confusion candidate only when a frequent word is observed.ßdummyTurker
448ß0.0ßTo summarize, our replacement is context-aware.ßdummyTurker
449ß0.0ßMore details could be found in Section 2.2.ßdummyTurker
450ß0.0ßSen1: 王震真是一个好人ßdummyTurker
451ß0.0ßSen2: 这事真的很令人震惊ßdummyTurker
452ß0.0ßBoth sentences contain the character “震”.ßdummyTurker
453ß0.0ßIf the strategy is to randomly select characters to replace, Sen1 may be changed into “王振真是一个好人”, where 王震 is changed to 王振.ßdummyTurker
454ß0.0ßHowever, this change is useless because 王振 is also a valid expression in this sentence.ßdummyTurker
455ß0.0ßInstead, in our paper, we consider the context in which a character change may affect the meaning of words.ßdummyTurker
456ß0.0ßAs a result, we will only replace the word match 震惊 in Sen2 to 振惊.ßdummyTurker
457ß0.0ßWhy not use usal evaluation metrics in CSC?]ßdummyTurker
458ß0.0ßIndeed, usual evaluation metrics for CSC involve detection and correction.ßdummyTurker
459ß0.0ßHowever, the key contribution of this paper is the corpus rather than the CSC models.ßdummyTurker
460ß0.0ßIn experiments, we report the detection results to demonstrate the quality of our corpus.ßdummyTurker
461ß0.0ßOur corpus might also be useful in training correction models.ßdummyTurker
462ß0.0ßWe will explore such potential benefit of our corpus in future work.ßdummyTurker
463ß0.0ßThe licensing of the corpus?]ßdummyTurker
464ß0.0ßLicence will be given in the revised version.ßdummyTurker
465ß0.0ßThe models trained with simplified Chinese and tested with traditional Chinese?]ßdummyTurker
466ß0.0ßSince our original clean corpora (People’s Daily8 1998 and Chinese Treebank 8.0) are written in simplified characters, for consistency, we construct our corpus and do CSC on simplified Chinese.ßdummyTurker
467ß0.0ßIn our work, to eliminate the effect of code difference, we have converted all traditional characters to their corresponding simplified characters using OpenCC.ßdummyTurker
468ß0.0ßWe will add the description in the revised paper.ßdummyTurker
469ß0.0ßWill you release the corpus and the code?]ßdummyTurker
470ß0.0ßBoth the corpus and the code will be released in the revised version.ßdummyTurker
471ß0.0ß[Question 2]ßdummyTurker
472ß0.0ßPlease refer to our reply for weakness argument 4.ßdummyTurker
473ß0.0ßHow different proportions of P-style and V-style errors affect the quality of the corpus?]ßdummyTurker
474ß0.0ßIn Liu et al.ßdummyTurker
475ß0.0ß(2010), the authors observe that V-style and P-style error rates are 83% and 48% in naturally distributed datasets, respectively.ßdummyTurker
476ß0.0ßIn our work, we follow their observation and adopt the same error rates to make the generated corpus as close to real ones as possible.ßdummyTurker
477ß0.0ßIndeed, adjusting the rates may have some effects on the task of CSC.ßdummyTurker
478ß0.0ßWe have made this flexible in our codes.ßdummyTurker
479ß0.0ßThis facilitates more choices for future research to explore the impacts of varying error rates.ßdummyTurker
480ß0.0ßChao-Lin Liu, Min-Hua Lai, Yi-Hsuan Chuang, Chia-Ying Lee: Visually and Phonologically Similar Characters in Incorrect Simplified Chinese Words.ßdummyTurker
481ß0.0ßCOLING (Posters) 2010: 739-747ßdummyTurker
482ß0.0ßWe completely agree that a paper title usually only includes the problem, task and method, but not detailed information such as data set and experiment results.ßdummyTurker
483ß0.0ßIn the future we will explore such additional input including key findings, results, data sets, future work section in the author’s previous papers.ßdummyTurker
484ß0.0ßWe are not aiming to use our current system to replace human on writing veridical paper abstracts or producing new scientific facts and results.ßdummyTurker
485ß0.0ßBut we hope this approach can serve as an assistive technology for human to write paper abstracts more efficiently and professionally, by generating an initial draft for human’s further editing, correction and enrichment.ßdummyTurker
486ß0.0ßTo the best of our knowledge there is no existing work on the same task – generating paper abstract from a paper title.ßdummyTurker
487ß0.0ßWe compared with two state-of-the-art approaches for text generation: LSTM Language Model and Sequence-to-sequence with attention.ßdummyTurker
488ß0.0ßOur framework significantly outperforms these two baseline methods, as shown in Table 2.ßdummyTurker
489ß0.0ßPlease see our response to the weakness argument 3 of Review #1 above.ßdummyTurker
490ß0.0ßThank you for your comments.ßdummyTurker
491ß0.0ßWe are sorry to hear that.ßdummyTurker
492ß0.0ßIt was not an intention.ßdummyTurker
493ß0.0ßWe used so called closed approaches, which are currently prevailing in psycholinguistics.ßdummyTurker
494ß0.0ßThey are based on counting the frequency of predefined words that are contained in a corpus (e.g.ßdummyTurker
495ß0.0ßa database of morphological categories of words associated with emotions or social words; see Park et al., 2015).ßdummyTurker
496ß0.0ßThe tables link directly to the text.ßdummyTurker
497ß0.0ßThank you for your constructive and useful feedback.ßdummyTurker
498ß0.0ßHere is our response to your comments.ßdummyTurker
499ß0.0ß* We would like to point out that in Overnight there is still substantial work for validating data and handling the difficulties of interacting with the crowd, which is obviated by our approach.ßdummyTurker
500ß0.0ßIn addition, while in Overnight the amount of collected data grows with the number of KB constants, our approach is not affected by this and scales well.ßdummyTurker
501ß0.0ß* We agree that Overnight is a valid methodology for obtaining data, however it is orthogonal to zero-shot parsing.ßdummyTurker
502ß0.0ßIn practice, both methods produce an imperfect parser (Wang et al., 2015 report a drop of accuracy from 90% to 56.4% when utilizing the overnight grammar to parse the Geo880 dataset) after which some interactive training with users can be employed to achieve high accuracy parsing.ßdummyTurker
503ß0.0ß* In addition, our approach is orthogonal to other approaches in the sense that they could be possibly combined.ßdummyTurker
504ß0.0ßFor instance, as KBs are dynamic, a semantic parser could be re-trained from time to time from newly collected target domain training data, while having zero-shot abilities (using our framework) in the meanwhile to handle KB constants unseen in the training data.ßdummyTurker
505ß0.0ßWe are aware that data collected using the Overnight approach might differ from natural language that is not conditioned on canonical utterances.ßdummyTurker
506ß0.0ßWe plan to add an additional analysis where we apply our approach to an external domain (Geoquery or SCHOLAR), and add it to the camera ready version.ßdummyTurker
507ß0.0ß* We will sample examples from the development set to evaluate the correctness of the delexicalizer in the camera-ready version.ßdummyTurker
508ß0.0ß* Thank you for your interesting proposal for the delexicalizer.ßdummyTurker
509ß0.0ßCurrently, we developed our models such that they would be domain independent (i.e., would not encounter out-of-domain tokens in their inputs).ßdummyTurker
510ß0.0ßFor this reason we performed the rule based delexicalization on all inputs.ßdummyTurker
511ß0.0ß* A quantitive evaluation of the inference algorithm appears in lines 688-694 and includes the average number of steps in case of success (3.65) and the percentage in which the step threshold is reached (30%).ßdummyTurker
512ß0.0ßWe will add details such as the variance, and be happy to accommodate other analyses as suggested by the reviewer for the camera ready version.ßdummyTurker
513ß0.0ßThe primary goal of our paper is to study how multitask learning can help to learn universal sentence representations.ßdummyTurker
514ß0.0ßTo this end, the transfer tasks are used as proxies to evaluate the sentence representation and to compare our model with the state-of-the-art universal sentence encoder (i.e., Conneau et.ßdummyTurker
515ß0.0ßal, 17), and we are not competing on the performances of the transfer tasks.ßdummyTurker
516ß0.0ßIn our setting, the sentence representations are given as input and a model is trained to predict the output label, and we evaluate the sentence embeddings with a generic NN architecture across different transfer tasks.ßdummyTurker
517ß0.0ßDespite some end-to-end approaches with complex architecture can achieve higher performances on these tasks, they either use task specific information or they model interactions between words in different sentences as opposed to using sentence vectors; therefore, we cannot incorporate and compare them in our experimental setting.ßdummyTurker
518ß0.0ßLearning sentence representation is an active research topic.ßdummyTurker
519ß0.0ßIn this work, we further extend the idea of (Conneau et.ßdummyTurker
520ß0.0ßal, 2017) by leveraging information from multiple tasks via multitask learning.ßdummyTurker
521ß0.0ßThis setting has a real impact in the case where the annotated data is not enough to train complex end-to-end model well.ßdummyTurker
522ß0.0ßFor example, to start working in a new domain such as biomedical, if one wants to quickly compute the similarities between genes by referring to their textual definition, it can be benefited by using pre-trained sentence encoder since the training data is hard to collect (need highly-specialized knowledge).ßdummyTurker
523ß0.0ßSo, having generic sentence encoders will be useful in a broad range of transfer tasks across different domains since they capture some syntactic and semantic information of sentences as we showed in our paper.ßdummyTurker
524ß0.0ßAlthough the improvements are not huge in some tasks, they are consistent across many tasks covering various aspects.ßdummyTurker
525ß0.0ßTherefore, we consider this as encouraging success.ßdummyTurker
526ß0.0ßMoreover, in MR, CR, SUBJ, and TREC, we can see the improvements are as much as 1.9%, 1%, 1.2% and 3.4% (see table 2, row 4.1-4.2).ßdummyTurker
527ß0.0ßThis is considered a large improvement in the transfer learning literature.ßdummyTurker
528ß0.0ßIn this work, our goal is to learn universal sentence representations, which is different from the goal of learning contextualized word vectors.ßdummyTurker
529ß0.0ßUnlike the contextualized word representations, our approach forces the encoder to encapsulate the meaning and syntactic properties of a sentence into a single vector, which can be more applicable than contextualized vectors in some situations where sentence embedding is required.ßdummyTurker
530ß0.0ßFor example, in information retrieval applications, sometimes we need a single dense representation of short texts (query, document title etc.)ßdummyTurker
531ß0.0ßsuch that the similarity between these representations can be efficiently computed.ßdummyTurker
532ß0.0ßAlthough we can compute a summary vector by pooling the contextual vectors but the summary vector may not contain the right features since the encoder was not trained to embed a sentence.ßdummyTurker
533ß0.0ßThank you for pointing us to the references.ßdummyTurker
534ß0.0ßWe will include and discuss them in the next version.ßdummyTurker
535ß0.0ßThank you for the suggestion.ßdummyTurker
536ß0.0ßIntuitively, transfer learning is more useful when the transfer tasks have limited amount of data.ßdummyTurker
537ß0.0ßWe will use the additional page to add a plot and discussion to verify such intuition.ßdummyTurker
538ß0.0ßI am sorry for the appearance of typos and grammar mistakes because of the hurry preparation, and these errors will be revised.ßdummyTurker
539ß0.0ßa)	Ten-fold cross validation here actually means partitioning the whole dataset into three subsets with a proportion 7ßdummyTurker
540ß0.0ßb) As mentioned in line 519-522, reviews without summary are filtered out and the training dataset is formed by review-summary pairs.ßdummyTurker
541ß0.0ßc) Considering that testing a sort of reviews on a model trained by other sorts of data may lead to an inaccurate result, we train the regression module separately on different datasets to prove that short reviews and summaries behave better than long reviews and obtain a conclusion that summarization can help improve the performance.ßdummyTurker
542ß0.0ßd) Experiments showed in Table 2 and Table 3 are design to verify the necessity of summarization operation, so the summarization module is not applied in this part yet.ßdummyTurker
543ß0.0ße) As mentioned in line 174, MOSM means Metadata Oriented Summarization Model proposed by us.ßdummyTurker
544ß0.0ßf) The result in Table 2 is obtained without summarization and result in Table 5 is obtained by complete model.ßdummyTurker
545ß0.0ßThese two sets of result can be compared to prove the effectiveness of summarization component, so we do not list a comparison specially.ßdummyTurker
546ß0.0ßg) As mentioned in line 681, we implement LSTM unit as a contrast to evaluate the efficiency of introducing parallelism.ßdummyTurker
547ß0.0ßThe training time is to show the efficiency difference.ßdummyTurker
548ß0.0ßI think our model still has potential to improve performance by tuning network parameters and increasing network depth.ßdummyTurker
549ß0.0ßThe strength of our method is showing a better balance in general perspective, while other baseline methods may only outperform in some specified data categories.ßdummyTurker
550ß0.0ßThe idea of conducting statistical significance test is meaningful, we can realize this in future experiments.ßdummyTurker
551ß0.0ßWe thank the reviewer for their supporting feedback.ßdummyTurker
552ß0.0ßGiven the opportunity we will add pointers to the main code/toolkits/software for statistical significance analysis in NLP (both those that were used and those that we recommend).ßdummyTurker
553ß0.0ßThank you very much for spending your precious time to read our draft.ßdummyTurker
554ß0.0ßThank you for the valuable information and the kind explanation.ßdummyTurker
555ß0.0ßWe will update the draft to reflect your fruitful suggestion.ßdummyTurker
556ß0.0ßMajor differences between our work and (Boye and Wiren, 2007) are the following.ßdummyTurker
557ß0.0ß* We used the confidence score derived from the neural model to determine when to ask questions, while Boye and Wiren checked the presence of the “unknown slots.”ßdummyTurker
558ß0.0ß* We explored collecting data set from human-system conversations.ßdummyTurker
559ß0.0ßWe are happy to see that the reviewer also considers this as a strong point.ßdummyTurker
560ß0.0ß* Handling of short queries is crucial in our problem.ßdummyTurker
561ß0.0ßWe investigated learning word embeddings and neural model for the classification of short queries.ßdummyTurker
562ß0.0ßThe proposed methods statistically significantly improved the non-trivial baseline (L646-649), and none of the reviewers have suggested using stronger baselines.ßdummyTurker
563ß0.0ßWe appreciate if the reviewer could take this into consideration to assess the evaluation part.ßdummyTurker
564ß0.0ß1143 unique users were involved in the data collection.ßdummyTurker
565ß0.0ßThey are (a part of) *real* users of a commercial intelligent assistant app, Yahoo!ßdummyTurker
566ß0.0ßVoice Assist  (https://v-assist.yahoo.co.jp).ßdummyTurker
567ß0.0ßWe want to emphasize that they are NOT recruited subjects such as crowd workers.ßdummyTurker
568ß0.0ßWe are not aware of their identities because we have access to only anonymized log data.ßdummyTurker
569ß0.0ßNo specific tasks were given to the users.ßdummyTurker
570ß0.0ßThey just used Yahoo!ßdummyTurker
571ß0.0ßVoice Assist as always.ßdummyTurker
572ß0.0ßOur data set was collected from the conversations that naturally occurred between Yahoo!ßdummyTurker
573ß0.0ßVoice Assist and its real users.ßdummyTurker
574ß0.0ßReleasing conversation data collected from users in the wild is one of our contribution.ßdummyTurker
575ß0.0ßThe system output is the search engine result page.ßdummyTurker
576ß0.0ßWhen WEB domain is selected, the output is specifically a list of Web page links (accompanied with text snippets) and does not include any domain-specific search results.ßdummyTurker
577ß0.0ßHowever, it sometimes (not always) includes page links to Wikipedia articles or YouTube pages, and click logs are considered useful in such cases.ßdummyTurker
578ß0.0ßAlthough click logs are currently unavailable to us for practical reasons and we did not check clicks, we agree in that using click logs is worth exploring idea.ßdummyTurker
579ß0.0ßWe want to discuss the possibility of using click logs in the paper.ßdummyTurker
580ß0.0ßThank you for the interesting suggestion.ßdummyTurker
581ß0.0ßThank you for pointing out that our approach for jointly learning the embeddings is simple and reasonable.ßdummyTurker
582ß0.0ßAlso, thank you for pointing out the WWW2017 paper which we were not aware of.ßdummyTurker
583ß0.0ßUnfortunately, a fair and direct comparison against that paper is not practical and, arguably, impossible.ßdummyTurker
584ß0.0ßThat method relies on type information, unlike ours.ßdummyTurker
585ß0.0ßSuch information is not available in the NYT dataset itself and obtained in that method through preprocessing.ßdummyTurker
586ß0.0ßThe process to obtain such information is not at all trivial, rendering their method significantly different than ours and the other methods we cite as most related to ours, based on embeddings.ßdummyTurker
587ß0.0ßAnother difference between that method and ours is that their evaluation is based on 24 relations, which is only a subset of all relations in the dataset.ßdummyTurker
588ß0.0ßWe, on the other hand, do not apply any filtering and consider all 55 relations in the corpus, which is the protocol followed by the vast majority of papers in this sub-area.ßdummyTurker
589ß0.0ßIt goes without saying that no paper can cite and evaluate against all previous work, especially in the intersection of very active areas (relation extraction and knowledge base embeddings).ßdummyTurker
590ß0.0ßNevertheless, our comparison is fairly comprehensive and based on publicly available data on the established benchmark for the sub-area, including the seminal and most related previous work attempting to incorporate KB information for relation extraction, that is, Weston et al 2013.ßdummyTurker
591ß0.0ßAs it turns out, triggered by your comments, we performed another literature search and found an accepted AAAI2018 paper by Han et al (Neural Knowledge Acquisition via Mutual Attention between Knowledge Graph and Text), which came out one month before the ACL deadline, and is also very relevant to our work.ßdummyTurker
592ß0.0ßSince their evaluation follows the established practice, we will compare our method against theirs in our paper.ßdummyTurker
593ß0.0ßWe believe that it is easy to expand our model to generate nested SQL queries or JOIN clauses, which could also be easily trained with back-propagation if enough training instances of these SQL types are available.ßdummyTurker
594ß0.0ßFor example, we could incorporate a hierarchical “value” channel to handle nest queries.ßdummyTurker
595ß0.0ßLet us suppose our decoder works horizontally that next generated token is at the right hand of the current token.ßdummyTurker
596ß0.0ßInspired by Chunk-based Decoder for Neural Machine Translation (Ishiwatari et al.ßdummyTurker
597ß0.0ß2017, ACL), we could increase the depth of the “value” channel to generates tokens of a nested WHERE value along the vertical axis.ßdummyTurker
598ß0.0ßDuring inference, an addition gating function might be necessary to determine whether to generate a nested query, followed by the generation of WHERE value.ßdummyTurker
599ß0.0ßAn intuitive way that extends our model to handle JOIN clauses is to add the 4th channel, which predicts a table from a collection of tables.ßdummyTurker
600ß0.0ßTherefore, the decoder should learn to select one of the four channels at each time step.ßdummyTurker
601ß0.0ßAccordingly, we need to add “from” as a new SQL keyword in order to generate SQL queries including “from xxxTable”.ßdummyTurker
602ß0.0ßThe reason that \alpha_0 tends to be smaller in larger corpora may be that compared with small corpora, the positive pairs in larger corpora cover more information.ßdummyTurker
603ß0.0ßThe information contained by negative pairs accounts for a relatively smaller part.ßdummyTurker
604ß0.0ßThis can also help to explain why the performance of GloVe gradually becomes better in larger corpora.ßdummyTurker
605ß0.0ßThere are indeed some literatures arguing that the difference between models becomes smaller in large corpora.ßdummyTurker
606ß0.0ßHence, the performance in smaller corpora seems more convincing to demonstrate the advantages by incorporating all negative examples.ßdummyTurker
607ß0.0ßIn fact, Wiki dataset is already a big corpus in this field.ßdummyTurker
608ß0.0ßDespite this, we still observe that by incorporating all negative examples, the model can achieve more fidelity and performs further better.ßdummyTurker
609ß0.0ßGuided by literature and own experiments, it was demonstrated that two hidden layers architecture is optimal for similar NLP tasks -segmentation and POS tagging-.ßdummyTurker
610ß0.0ßWe explored different approaches for optimization including stochastic gradient descent with momentum, AdaDelta (Zeiler, 2012), and RMSProp (Dauphin et al., 2015) but did not lead to improvements over the results achieved using Adam (Kingma and Ba, 2014).ßdummyTurker
611ß0.0ßFatha (a), shadda (~), sukun (o) constitute the bulk of the errors both in mono-dialectal and also in joint.ßdummyTurker
612ß0.0ßEven though, the joint helped reduce the ratio of these errors -The break-down before table 5.- We speculate this is partially due to the number of examples and patterns seen during the training.ßdummyTurker
613ß0.0ßWe think that expanding the data would reduce further these errors.ßdummyTurker
614ß0.0ßBoth Moroccan and Tunisian version of the New Testaments crawled online from sources disclosed in the paper.ßdummyTurker
615ß0.0ßBoth of them are fully diacritized.ßdummyTurker
616ß0.0ßWe selected common 8,200 verses from the online sources.ßdummyTurker
617ß0.0ßThese data was used as the ground truth.ßdummyTurker
618ß0.0ßWe split the data into 5 folds for cross validation, where training splits were further split 70/10ßdummyTurker
619ß0.0ßfor training/validation.ßdummyTurker
620ß0.0ßAdditional layers would slow down the training.ßdummyTurker
621ß0.0ßWe explored different sizes of embeddings that we will add to the paper.ßdummyTurker
622ß0.0ßThe optimal architecture and the hyper parameters were the provided in paper.ßdummyTurker
623ß0.0ßThe training data was originally diacritized.ßdummyTurker
624ß0.0ßIt is extremely rare to find such data and it is very laborious to build such resource in addition the other issues of consistency and accuracy.ßdummyTurker
625ß0.0ßThank you for your comments.ßdummyTurker
626ß0.0ßUsing a single embedding with k times the dimensions of the senses (so the number of parameters is equal in both models), as you are suggesting, still returns accuracy around 3%-4% *lower* than that of the multi-sense model.ßdummyTurker
627ß0.0ßThis analysis has already been done, but was not included in the submitted version due to space reasons.ßdummyTurker
628ß0.0ßWe are planning to include this information, along additional data analysis in the final version, taking advantage of the extra page.ßdummyTurker
629ß0.0ßWe completely agree that this information is important for the evaluation of the paper---and we now realize that we should have included at least a footnote about this in the submitted version.ßdummyTurker
630ß0.0ßWe believe we provide details for the most important hyper-parameters of the systemßdummyTurker
631ß0.0ßWe fully agree on the importance of releasing the code --- this is in fact our funder requirement to promote replicability --- and this will be done in parallel with the publication of the paper, as it is common practice.ßdummyTurker
632ß0.0ßReleasing the code *now* would be against the ACL anonymity rules.ßdummyTurker
633ß0.0ßThank you for the thoughtful comments.ßdummyTurker
634ß0.0ßWe would like to clarify a few points, which we are unsure if were fully understood.ßdummyTurker
635ß0.0ßOur TF-IDF baseline is not vanilla TF-IDF.ßdummyTurker
636ß0.0ßThe document frequency is computed only in the context of the current document which makes it suitable for finding a paragraph in a document as in our setup.ßdummyTurker
637ß0.0ßThis is an exact re-implementation of the IR component from DocQA (https://arxiv.org/abs/1710.10723), which obtains 64% EM score, comparable to state-of-the-art 66.6%, and thus we considered it to be a good IR approach for this task.ßdummyTurker
638ß0.0ßWe will compare our approach to vanilla TF-IDF for the camera-ready version.ßdummyTurker
639ß0.0ßIndeed in our current approach, TF-IDF slightly outperforms our approach during search and is comparable at finding answers (see Tables 3,4).ßdummyTurker
640ß0.0ßHowever, at a higher level we argue that our approach is more flexible and useful as the NLP community moves to datasets that require reasoning.ßdummyTurker
641ß0.0ßFor example, our neural approach can handle cases where there is no lexical overlap between a local context and the question, and also can interleave answering with search (line 179 in the paper).ßdummyTurker
642ß0.0ßThus, we believe the novelty of the framework and its flexibility are of interest to the research community and can benefit researchers working on reading comprehension.ßdummyTurker
643ß0.0ßAs mentioned our TF-IDF approach is not vanilla TF-IDF and is used in comparable to state-of-the-art systems for TriviaQA.ßdummyTurker
644ß0.0ßWe would like to clarify that given a question, finding a reasonable set of documents does not require reading the entire corpus.ßdummyTurker
645ß0.0ßFor example, the way TriviaQA was formed was by matching entities to the document title and the document was not read at all.ßdummyTurker
646ß0.0ßThen our approach can be used to choose from the small set of documents.ßdummyTurker
647ß0.0ßWe did not fully understand what the reviewer means by IR methods not processing the entire corpus.ßdummyTurker
648ß0.0ßHowever, we would like to point out the scenario where documents are streamed with a question, in which case our model will be able to search through the document without processing it.ßdummyTurker
649ß0.0ßWe would like to thank you for your comments and suggestions and we modified the spelling mistakes and corrected the tenses.ßdummyTurker
650ß0.0ßFor “there is repeated content as seen in section 3 and 4”, Section 3 is a preliminary of vanilla dynamic memory networks and section 4 mainly introduces our method and how each module is calculated.ßdummyTurker
651ß0.0ßAlthough there exist some grammatical errors in our paper, it's worth to highlighting some technical contributions we have made in our paper.ßdummyTurker
652ß0.0ßTo the best of our knowledge, it is the first time that we use dynamic memory networks (DMN) for the task of dialogue act classification.ßdummyTurker
653ß0.0ßFurthermore, in the DMN, we replace the Bi-GRU with a pyramid encoder, which can boost the computational efficiency and increase the accuracy.ßdummyTurker
654ß0.0ßIn addition, we also propose a unified framework of DMN and adversarial learning for Dialogue Act Classification, which would give some insights to other researchers who are conducting similar tasks such multiclass classification task.ßdummyTurker
655ß0.0ßThank you for your comments.ßdummyTurker
656ß0.0ß* See, for example, the first couple paragraphs of “Learning to Forget: Continual Prediction with LSTM” (Gers, Schmidhuber, Cummins; 1999).ßdummyTurker
657ß0.0ßLSTMs are presented as a mechanism for controlling the backpropagated error, not for modeling.ßdummyTurker
658ß0.0ßAlso, if the original motivation for LSTMs was to use the memory as the modeling component, why did the content layer contain a vanilla RNN to begin with?ßdummyTurker
659ß0.0ß* Yes, we had actually written a related work section containing these papers and more (e.g.ßdummyTurker
660ß0.0ßQRNNs and SRUs), but could not fit it into the 4-page limit without removing experiments.ßdummyTurker
661ß0.0ßOne other interesting connection that we draw is to the transformer model (Attention Is All You Need).ßdummyTurker
662ß0.0ßIf the paper is accepted, we will add it to the camera-ready.ßdummyTurker
663ß0.0ß* Yes, we can produce those results across all tasks and add them to the paper.ßdummyTurker
664ß0.0ßWe ran some language modeling experiments with context-insensitive gates, and found that they incur a small decrease in performance with the given hyperparameters.ßdummyTurker
665ß0.0ßHowever, it is difficult to say whether these models are indeed less powerful than LSTMs because they could be an artifact of the hyperparameter setting; other work (e.g.ßdummyTurker
666ß0.0ßQRNNs and SRUs) show that adding layers, for example, can make up for this performance loss.ßdummyTurker
667ß0.0ß* These differences are very small, and typically within range of random seeds and hyperparameter tuning.ßdummyTurker
668ß0.0ß* Equation 9 is meant to be an abstraction of the class of LSTM-like simplifications that we tried.ßdummyTurker
669ß0.0ßCONTENT and OUTPUT can be any function, but in this specific case, CONTENT is a linear transformation and OUTPUT is a tanh.ßdummyTurker
670ß0.0ßWe will clarify this in the paper.ßdummyTurker
671ß0.0ßThank you for your comments.ßdummyTurker
672ß0.0ßLSTMs have been applied with great success to virtually every NLP task, and therefore advancing our understanding of LSTMs is of particular interest to the CL community.ßdummyTurker
673ß0.0ßThanks for the reviewing our paper.ßdummyTurker
674ß0.0ßThere is a typo in the paper.ßdummyTurker
675ß0.0ßUryupina (2007) in lines 670 and 671 is actually Uryupina and Moschitti (2015), i.e.ßdummyTurker
676ß0.0ß"lines 669-686 and ""+JIM"" in Table3 are about the pattern mining approach of U&M(2015)."ßdummyTurker
677ß0.0ßTherefore, Table3 compares the results of using EPM vs. U&M (2015)'s pattern mining approach.ßdummyTurker
678ß0.0ßOverall, similar to U&M 2015 we use a pattern mining approach.ßdummyTurker
679ß0.0ßHowever, they use it for extracting feature templates, e.g.ßdummyTurker
680ß0.0ßdistance+anaphor_dependency_relation is a good combinatorial feature, while we use it for extracting informative values of features, e.g.ßdummyTurker
681ß0.0ßanaphor_dependency_relation=subject is informative.ßdummyTurker
682ß0.0ßNevertheless, we use their pattern miner in the same way as ours, i.e.ßdummyTurker
683ß0.0ß+JIM in Table3.ßdummyTurker
684ß0.0ßDo you mean U&M (2015) instead of 2017?ßdummyTurker
685ß0.0ßTheir 2017 paper is about combining the output of different coreference resolvers.ßdummyTurker
686ß0.0ßIt isn't a related work of our paper.ßdummyTurker
687ß0.0ßHowever, their 2015 paper is very related to which we compared.ßdummyTurker
688ß0.0ßWe can add U&M 2017 as a sample ensemble method in line 781.ßdummyTurker
689ß0.0ßWe use a discriminative pattern mining approach for selecting a subset of feature-values that are informative for coreference.ßdummyTurker
690ß0.0ßFeature selection methods choose a feature with all its values, e.g.ßdummyTurker
691ß0.0ßdependency relation of the anaphor is a useful feature, while for each feature we only select a subset of its values, e.g.ßdummyTurker
692ß0.0ßonly anaphor_dependency_relation=subject and anaphor_dependency_relation=object are informative.ßdummyTurker
693ß0.0ßFrom this perspective, one can consider each feature-value as a feature and then use a feature selection.ßdummyTurker
694ß0.0ßThis would result in a much larger search space (and also a probability of overfitting to the training data) because there would be no frequency condition.ßdummyTurker
695ß0.0ßThe community becomes aware of the generalization problem in DL methods that have been applied to NLP tasks, e.g.ßdummyTurker
696ß0.0ßhttps://newgeneralization.github.io.ßdummyTurker
697ß0.0ßFor instance e2e-coref single model and reinforce  in Table3 both outperform deep-coref's ranking model.ßdummyTurker
698ß0.0ßHowever, on WikiCoref, both of these systems perform worse than deep-coref ranking [Table5].ßdummyTurker
699ß0.0ßOn the other hand, incorporating linguistic features is considered as a potential solution for building more generalizable NLP systems, e.g.ßdummyTurker
700ß0.0ßhttps://sites.google.com/view/relsnnlp.ßdummyTurker
701ß0.0ßIn this paper, we show that incorporating linguistic features indeed improves the generalization.ßdummyTurker
702ß0.0ßHowever only if we incorporate informative feature-values instead of all features.ßdummyTurker
703ß0.0ßWe propose EPM as an efficient and open source solution [code upon the publication] to the community for selecting informative feature-values.ßdummyTurker
704ß0.0ßHowever, we do not claim that EPM is the only method by which we can obtain the set of informative feature-values.ßdummyTurker
705ß0.0ßAny other approach that examines all combinations of feature-values and select the informative ones, is an alternative.ßdummyTurker
706ß0.0ßIf the paper gets accepted, we also add a brief overview of feature selection algorithms in the additionally provided page.ßdummyTurker
707ß0.0ßAn example of FP-Tree construction method is provided in the Supplementary material.ßdummyTurker
708ß0.0ßIf the paper gets accepted, we would provide more examples on the additional page.ßdummyTurker
709ß0.0ß"In practice,  ""percentage of meaning"" can be explained by "" what proportion of words are translated from source sentence to target sentence  and vice-versa."""ßdummyTurker
710ß0.0ßThe average of the two percentages are used.ßdummyTurker
711ß0.0ßThe statistical analysis was not carried out.ßdummyTurker
712ß0.0ßWe randomly sampled and manually compared some real low quality parallel corpus  and artificial parallel corpus, and we found that the artificial corpus and real corpus has some similarities, but the artificial corpus is not perfect yet.ßdummyTurker
713ß0.0ßIn fact, here, MAE = 1.0 - accuracy.ßdummyTurker
714ß0.0ß(In the beginning, the task was defined as a regression problem, every sentence will be scored from 0 to 5.)ßdummyTurker
715ß0.0ßWe repeated each experiment at least 5 times, but we didn't provided the whole results records.ßdummyTurker
716ß0.0ßThe goal in this paper is to achieve extremely high-quality sentence pairs.ßdummyTurker
717ß0.0ßHowever, 'high-quality' is hard to define.ßdummyTurker
718ß0.0ßWe believed that if averaged 90% f words in the source and target can be translated in a sentence pair, it is what we want.ßdummyTurker
719ß0.0ßWe would improve writing to make it understandable for readers from different research background.ßdummyTurker
720ß0.0ßThe implementation details is located in Section 6.2 Training Details.ßdummyTurker
721ß0.0ßWe recruited employees in our company and some interns from colleges.ßdummyTurker
722ß0.0ßThey both have a basic knowledge of task oriented dialog system.ßdummyTurker
723ß0.0ßThe interface is http://gd.xxx.ai/static/chat.html, based on anonymity of ACL18.ßdummyTurker
724ß0.0ßAnd we will add it in further version for testing.ßdummyTurker
725ß0.0ßOur architecture is based on a hierarchical recurrent neural networks.ßdummyTurker
726ß0.0ßWe model intentions based on dialog hidden states and current utterance representations from user.ßdummyTurker
727ß0.0ßFor dialog encoder, we apply bi-lstm to encode user's utterance.ßdummyTurker
728ß0.0ßWe use uni-lstm to encode dialog history.ßdummyTurker
729ß0.0ßThe intention is based on current user's utterance embedding and dialog history.ßdummyTurker
730ß0.0ßAt last, the dialog history and intentions is feed to attention decoder to generate response to user.ßdummyTurker
731ß0.0ßFirst, we provide likelihood-based objective function(Equ 6) and use importance sampling(Equ 10) to approximate normalized constant when calculating gradients(Equ 8, Equ 9).ßdummyTurker
732ß0.0ßSecond, latent intent is included in the dialogue state update.ßdummyTurker
733ß0.0ßSince dialog history propagates to next turn, latent intention is based on current utterance's embedding as well as the dialog history(Equ 3).ßdummyTurker
734ß0.0ßThirdly, we apply REINFORCE, which is an on-policy RL algorithm, to fine-tune our unsupervised trained model off-line.ßdummyTurker
735ß0.0ßWe provide an anonymized link to the full dataset of 1.5 million comments in the bibliography of the paper, which reviewers are encouraged to download.ßdummyTurker
736ß0.0ßRietveld, the system that facilitates code review in Chromium is licensed under Apache v2.0.ßdummyTurker
737ß0.0ßThe linguistic data we have collected from Rietveld and released in our datasets is freely available to anyone with an internet connection.ßdummyTurker
738ß0.0ßIn the datasets we are releasing, we have de-identified all usernames and email addresses.ßdummyTurker
739ß0.0ßWe will license our datasets under the Creative Commons ShareAlike license (CC-BY-SA: https://creativecommons.org/licenses/by-sa/4.0/)ßdummyTurker
740ß0.0ßCode reviews are indeed conversations that involve a back-and-forth exchange of ideas, as you correctly note and as we describe in the paper.ßdummyTurker
741ß0.0ßIn the larger dataset, the data is ordered such that the conversation structure among comments is apparent and easily extracted.ßdummyTurker
742ß0.0ßThe smaller dataset, which we use to determine whether the reviewer's feedback is actionable, contains only comments by reviewers, and thus, contains no inherent conversation structure.ßdummyTurker
743ß0.0ßIt is true that we have not controlled for non-linguistic factors, such as bug complexity.ßdummyTurker
744ß0.0ßSince the goal of this particular paper was to help developers improve their feedback, we restricted ourselves to studying the relationship between linguistic cues in a reviewer comment and whether that comment was acted upon.ßdummyTurker
745ß0.0ßIn our future work, we look forward to exploring other data available in the full code review repository, such as reviewer experience, implementation time, complexity of the bug identified by the reviewer, and potentially social factors (personality traits, etc.)ßdummyTurker
746ß0.0ßThanks for pointing out this.ßdummyTurker
747ß0.0ßActually the differentiation of internal/external meanings are handled at several places, in addition to the Wiki popularity feature: (1) In Section 4.4, additional context words are extracted from Wiki only for external meanings; (2) In Section 5.2&5.3, a confidence estimation step and a final selection step are applied to properly compensate external meanings.ßdummyTurker
748ß0.0ßThe effectiveness of these two steps are justified by experiments (Figure 5).ßdummyTurker
749ß0.0ßIt is a good idea to do a distribution analysis of internal/external acronyms, we will include it in the revision.ßdummyTurker
750ß0.0ß- Context similarity is indeed a very useful signal for acronym disambiguation, which is proved in our ablation study as well (Figure 4).ßdummyTurker
751ß0.0ßWe also mentioned in line 690 that our the ablation P+C naturally corresponds to the existing approaches relying on context similarity.ßdummyTurker
752ß0.0ßFigure 4&5 show that the proposed system can significantly outperform those context similarity based approaches.ßdummyTurker
753ß0.0ß- For the comparison with AIDA, as mentioned in Section 6.1.2 (line 591-603), we only considered test cases where the true meaning is a public meaning, since internal meanings are unknown to AIDA.ßdummyTurker
754ß0.0ßAnother thing worth mentioning is that since AIDA does not index any meanings for two-letter acronyms, the number of test cases in the JoinA dataset is much smaller and the disambiguation is much easier (the average number of candidates per acronym is only 4.5, while the numbers in other datasets are all over 15).ßdummyTurker
755ß0.0ßSo this evaluation doesn’t indicate that most acronyms can be covered by external knowledge sources.ßdummyTurker
756ß0.0ßSection 6.5 explained why our system can outperform AIDA even on public meanings.ßdummyTurker
757ß0.0ßWe will add the performance split across external/internal meanings as well as some statistics of the evaluation datasets in the revision.ßdummyTurker
758ß0.0ßThanks for the good suggestion.ßdummyTurker
759ß0.0ßWe will try our system on TREC enterprise corpus and report the experiment results in the revision.ßdummyTurker
760ß0.0ßThis is a very good suggestion.ßdummyTurker
761ß0.0ßIt is possible that some context are not captured due to more frequent acronym usage than the full meanings.ßdummyTurker
762ß0.0ßWe actually tried to iteratively enrich context by adding context of confidently-disambiguated acronyms (e.g., disambiguation score above 0.9) and we saw an overall F1 increase of ~0.02.ßdummyTurker
763ß0.0ßWe will include this experiment result as well as some statistics on the occurrence frequency of acronyms and meanings in the revision.ßdummyTurker
764ß0.0ßThis is a very good question.ßdummyTurker
765ß0.0ßIn this paper we utilized a rule-based acronym detection module to detect acronyms.ßdummyTurker
766ß0.0ß"See more details at the ""Reply to Question 1 for Reviewer #1""."ßdummyTurker
767ß0.0ß"(1) The concept ""cosine distance"" used in our paper actually indicates ""cosine similarity"", we just follow the term introduced in [1] (listed below)."ßdummyTurker
768ß0.0ßWe are sorry for any confusion caused.ßdummyTurker
769ß0.0ß(2) Lower vector dimensions (200) and smaller training corpus (500 million tokens) are probably the reasons for lower experiment results in our paper compared with some other work (e.g., in [2] the vector dimensions are 600 and the corpus contains 1.5 billion tokens).ßdummyTurker
770ß0.0ßTable 4 in [1] has demonstrated that vector dimensionality and word amount have an influence on the results of Semantic-Syntactic test.ßdummyTurker
771ß0.0ßBesides, we will also perform further experiments to double check our methodology.ßdummyTurker
772ß0.0ßIn [3] (subsection 3.3), Bhatia et al.ßdummyTurker
773ß0.0ß"compare their model with [4] based on their LSTM architecture rather than the original architecture used in [4], and we also want to focus on the ""critical difference"" (morphemes vs. the latent meanings of morphemes) between ours and other ""explicit"" models, so that we implement EMM based on our LMM-A."ßdummyTurker
774ß0.0ßHere, EMM is supposed to represent a kind of models (like those in [4][5]) that directly/explicitly incorporate morpheme vectors into word embeddings (e.g., v'(incredible) = v(incredible) + v(in-) + v(-cred-) + v(-ible)).ßdummyTurker
775ß0.0ßTheoretically, our models can be built on both CBOW and Skip-gram, since the architectures of them don't affect the construction and updating of the modified embedding(s) \hat{v}_{t_i} (as expressed in equation (3)).ßdummyTurker
776ß0.0ßHowever, according to [1], the training complexity of Skip-gram is higher than that of CBOW.ßdummyTurker
777ß0.0ßBesides, for the CBOW-like architecture, there always involve more latent meanings of morphemes of words at the input layer compared with the Skip-gram-like architecture whose input layer contains only one word, which can help to speed up the shrinking of embeddings of latent meanings during training.ßdummyTurker
778ß0.0ßIn terms of the content of subsection 2.2 in [6], our models together with CBOW, Skip-gram, Glove and EMM, which are trained on word co-occurrence counts, have the ability to capture semantic word similarity, and they can be evaluated on the word similarity task.ßdummyTurker
779ß0.0ßThank you for your helpful suggestion, and we will conduct some downstream experiments like text identification to further evaluate the performance of our models.ßdummyTurker
780ß0.0ßWe will double-check our analogy methodology, and we will add the dimension of word embeddings and evaluate the performance of the models with different vector dimensions on the Syntactic Analogy task and/or other tasks.ßdummyTurker
781ß0.0ßThank you very much, and we will correct and compensate all the content mentioned.ßdummyTurker
782ß0.0ß[1] Mikolov Tomas, et al.ßdummyTurker
783ß0.0ßEfficient estimation of word representations in vector space.ßdummyTurker
784ß0.0ßarXiv2013.ßdummyTurker
785ß0.0ß[2] Levy Omer, et al.ßdummyTurker
786ß0.0ßLinguistic regularities in sparse and explicit word representations.ßdummyTurker
787ß0.0ßCONLL2014.ßdummyTurker
788ß0.0ß[3] Parminder Bhatia, et al.ßdummyTurker
789ß0.0ßMorphological priors for probabilistic neural word embeddings.ßdummyTurker
790ß0.0ßEMNLP2016.ßdummyTurker
791ß0.0ß[4] Jan Botha, et al.ßdummyTurker
792ß0.0ßCompositional morphology for word representations and language modelling.ßdummyTurker
793ß0.0ßICML2014.ßdummyTurker
794ß0.0ß[5] Qiu Siyu, et al.ßdummyTurker
795ß0.0ßCo-learning of Word Representations and Morpheme Representations.ßdummyTurker
796ß0.0ßCOLING 2014.ßdummyTurker
797ß0.0ß[6] Faruqui Manaal, et al.ßdummyTurker
798ß0.0ßProblems with evaluation of word embeddings using word similarity tasks.ßdummyTurker
799ß0.0ßACL2016.ßdummyTurker
800ß0.0ßWe acknowledge that the accuracy improvements are limited, but the additional computation required by the CoNLL loss training is manageable (approximately 8 hours vs 4 hours of the cross entropy).ßdummyTurker
801ß0.0ß"A justification for the better performance of the ""double"" hinge loss is that it teaches the model to stay away from two bad predictions, not just one."ßdummyTurker
802ß0.0ß"To probe it, we have run a comparison over only the sentences in the English test set with more than 4 entities each (""many"" per sentence)."ßdummyTurker
803ß0.0ßIn this case, the performance of the double hinge becomes the highest (vs hinge-Hamming: 0.21 pp; hinge-CoNLL: 0.46 pp; cross entropy: 0.31 pp).ßdummyTurker
804ß0.0ßThe fact that the accuracy is higher than both the hinge-Hamming and hinge-CoNLL separately confirms the usefulness of training the model to stay away from two undesirable labelings.ßdummyTurker
805ß0.0ßWe are working around the clock to follow the reviewer's suggestion and include results for POS tagging and chunking in the extra page allowed by the cameraready.ßdummyTurker
806ß0.0ßThe dynamic programming method can be immediately extended to another well-known surrogate loss, the ramp loss:ßdummyTurker
807ß0.0ßThe ramp loss can be expressed exactly as the difference of two hinge losses, the first hinged at the usual margin, the other hinged at 0.ßdummyTurker
808ß0.0ßTherefore, the loss-augmented procedure can just be invoked twice in the loss function, at the given hinge points.ßdummyTurker
809ß0.0ßOther training approaches based on the loss-augmented inference such as the direct loss minimization (see McAllester et al., NIPS 2010; Song et al., ICML 2016; Adi and Keshet, JMLR 2016) could also be implemented using the proposed algorithm.ßdummyTurker
810ß0.0ßWang et al.ßdummyTurker
811ß0.0ßwas released on February 2nd, 2018, while the ACL deadline is February 14th.ßdummyTurker
812ß0.0ßAccording to ACL policy of Citation and Comparison, “Papers (whether refereed or not) appearing less than 3 months before the submission deadline are considered contemporaneous to your submission, and you are therefore not obliged to make detailed comparisons that require additional experimentation and/or in-depth analysis”.ßdummyTurker
813ß0.0ßTherefore, Wang et al.ßdummyTurker
814ß0.0ßis seen as contemporaneous to our work, but we still cited and discussed it.ßdummyTurker
815ß0.0ßBesides, we used the original evaluation setting as in Yu et al.ßdummyTurker
816ß0.0ßbut we are skeptical whether Wang et al.ßdummyTurker
817ß0.0ßfollows the same setting because their scores are much lower than ours, e.g., BLEU-4: 5.16 vs 14.0 (ours), METEOR: 12.32 vs 35.0 (ours).ßdummyTurker
818ß0.0ßThus the reported results are not directly comparable unless they release their code or quantitative results.ßdummyTurker
819ß0.0ßAnyway, we are glad to re-implement their model for comparison in the revision if necessary.ßdummyTurker
820ß0.0ßOur method is inspired by the existing IRL methods.ßdummyTurker
821ß0.0ßFirst, we follow Finn et al.ßdummyTurker
822ß0.0ßto formulate the reward function as an EBM, and then use it to approach the real data distribution.ßdummyTurker
823ß0.0ßSecond, our algorithm is aligned to Fu et al.ßdummyTurker
824ß0.0ßand we alternately update the reward function and the policy.ßdummyTurker
825ß0.0ßBut unlike the existing IRL algorithms, AREL bypasses the computation of the intractable partition function and thus is a more practical IRL realization for large-scale scenarios, we do plan to investigate its applications in other NLP tasks.ßdummyTurker
826ß0.0ß(1)The objective of our AREL method is to directly train a reward function by optimizing EBM towards real data distribution, which leads to a more human-like reward to better assist reinforcement learning.ßdummyTurker
827ß0.0ßIts motivation is different from GAN which aims at diversifying outputs.ßdummyTurker
828ß0.0ß(2)AREL can be seen as a GAN variation, but its objective function tackles the gradient vanishing and gradient unstable problems in traditional GAN.ßdummyTurker
829ß0.0ßPlease refer to http://yanran.li/files/pre/TacklingGAN.pdf for more details.ßdummyTurker
830ß0.0ßWe will detail this in the revision.ßdummyTurker
831ß0.0ßPlease see our reply to your Weakness #1.ßdummyTurker
832ß0.0ßThanks!ßdummyTurker
833ß0.0ßThanks for reviewer’s suggestion.ßdummyTurker
834ß0.0ßDue to the page limitation, we forsook some related work.ßdummyTurker
835ß0.0ßWe will fill this part in revised version.ßdummyTurker
836ß0.0ß1.ßdummyTurker
837ß0.0ßAs discussed in Section 2, most of professional analysis reports include “target price”.ßdummyTurker
838ß0.0ßIn order to compare the results of crowd and professional analyst, “target” is one of the important aspects.ßdummyTurker
839ß0.0ß“support and resistance” can help us capture the view of investors.ßdummyTurker
840ß0.0ß“buying” and “selling” can help us evaluate the performance of individual investors.ßdummyTurker
841ß0.0ßWe set this four aspect for different propose, and had discussed in Section 2.ßdummyTurker
842ß0.0ß2.ßdummyTurker
843ß0.0ß“target” can be seen as the conclusion of analysis.ßdummyTurker
844ß0.0ßThus, almost every professional analysis report write price target in the front part.ßdummyTurker
845ß0.0ßThe schema is for all financial social media, and is not tailored for certain dataset.ßdummyTurker
846ß0.0ß3.ßdummyTurker
847ß0.0ßOur dataset is considered as substantial agreement in (Landis and Koch, 1977).ßdummyTurker
848ß0.0ß(Line 238-240) Thanks for reviewer’s suggestion.ßdummyTurker
849ß0.0ßAs the last sentence in (Artstein and Poesio, 2008), “The decision whether a corpus is good enough for publication should be based on more than the agreement score—specifically, an important consideration is an independent evaluation of the results that are based on the corpus.” We will discuss more details of annotated results in revised version.ßdummyTurker
850ß0.0ßEvery research have some issues need to be solved, for example, the meaning of word embeddings is explanandum.ßdummyTurker
851ß0.0ßWe will try to solve the proposed issues in the future.ßdummyTurker
852ß0.0ß1.ßdummyTurker
853ß0.0ßAs discussed in Section 2, most of professional analysis reports include “target price”.ßdummyTurker
854ß0.0ßIn order to compare the results of crowd and professional analyst, “target” is one of the important aspects.ßdummyTurker
855ß0.0ß“support and resistance” can help us capture the view of investors.ßdummyTurker
856ß0.0ß“buying” and “selling” can help us evaluate the performance of individual investors.ßdummyTurker
857ß0.0ßWe set this four aspect for different propose, and had discussed in Section 2.ßdummyTurker
858ß0.0ß“target” can be seen as the conclusion of analysis.ßdummyTurker
859ß0.0ßThus, almost every professional analysis report write price target in the front part.ßdummyTurker
860ß0.0ßThe schema is not tailored for certain dataset, but for all financial social media data.ßdummyTurker
861ß0.0ß2.ßdummyTurker
862ß0.0ßThanks for reviewer’s suggestion.ßdummyTurker
863ß0.0ßhttps://nlp.stanford.edu/pubs/lrec2014-stock.pdf will be added into our related work in revised version.ßdummyTurker
864ß0.0ß#hashtag and $cashtag can be efficiently identified by the sign.ßdummyTurker
865ß0.0ßThank you for your helpful comments.ßdummyTurker
866ß0.0ßSennrich et al.ßdummyTurker
867ß0.0ß(2016) claimed the training of a decoder.ßdummyTurker
868ß0.0ßOn top of it, our proposal also enhances the encoder and attention by introducing multiple synthetic sentences into the ordinary training procedure.ßdummyTurker
869ß0.0ßWe apologize for using only in-house data.ßdummyTurker
870ß0.0ßThe evaluation on the public data is our future work.ßdummyTurker
871ß0.0ßWe think that the same tendency can be observed when we use some of the other corpora.ßdummyTurker
872ß0.0ßSince we did not find a method to directly quantify the diversity, upon the paper submission, we demonstrated the effects indirectly.<br>ßdummyTurker
873ß0.0ß1) The BLEU scores improved while increasing the number of synthetic source sentences.ßdummyTurker
874ß0.0ßThe number of sentences could be an indicator of the diversity.<br>ßdummyTurker
875ß0.0ß2) As for the generation method, the BLEU scores of the systems based on sampling were much higher than those of the n-best generation.ßdummyTurker
876ß0.0ßWe think sampling produces more diverse synthetic source sentences, because it can generate low-likelihood sentences, as shown in Table 1.ßdummyTurker
877ß0.0ßWe agree with you.ßdummyTurker
878ß0.0ßWe will attempt another approach.ßdummyTurker
879ß0.0ß"""N-best Generation"" in Figure 2 shows the results."ßdummyTurker
880ß0.0ßThe lines in Figure 2, especially (b), show that the scores of the n-best generation are significantly worse than those of the proposed methods (three filtering methods), showing that the improvements come from the difference of generation methods.ßdummyTurker
881ß0.0ßThank you for your helpful comments.ßdummyTurker
882ß0.0ßPlease refer to our reply to question 1.ßdummyTurker
883ß0.0ßGiven an MT framework, expansion of bilingual corpora has been a strategy of improving the quality of translation.ßdummyTurker
884ß0.0ßSennrich's work and our proposal present another complementarily choice that uses monolingual corpora, and enable us to select the strategy with consideration of cost and quality.<br>ßdummyTurker
885ß0.0ßWe have showed only that it works with a monolingual corpus from different domain.ßdummyTurker
886ß0.0ßFurther experiments are necessary to clarify under which circumstances the proposed method is effective.ßdummyTurker
887ß0.0ßOur proposed method is an extension of Sennrich et al.ßdummyTurker
888ß0.0ß(2016), but there were substantial improvements as demonstrated.ßdummyTurker
889ß0.0ßWe apologize for our unclear description.ßdummyTurker
890ß0.0ßThe result of Sennrich et al.ßdummyTurker
891ß0.0ß(2016) corresponds to the n-best generation with one synthetic source sentence (i.e., 1-best generation).ßdummyTurker
892ß0.0ßBLEU scores were 28.61 and 39.46 for En->Ja and Zh->Ja tasks, respectively.ßdummyTurker
893ß0.0ßAs shown in Table 3, our proposed methods (the likelihood, confidence, random filtering) achieved significant improvements.ßdummyTurker
894ß0.0ßThe table will be revised as follows.ßdummyTurker
895ß0.0ß<pre>ßdummyTurker
896ß0.0ß-----------------------------------------------------ßdummyTurker
897ß0.0ßMethod                  En-Ja           Zh-JaßdummyTurker
898ß0.0ß-----------------------------------------------------ßdummyTurker
899ß0.0ßBase Corpus Only        26.19           37.08ßdummyTurker
900ß0.0ßSennrich et al.ßdummyTurker
901ß0.0ß(2016)  28.61 (+2.42)   39.46 (+2.38)ßdummyTurker
902ß0.0ß--ßdummyTurker
903ß0.0ßLikelihood Filtering    30.41 (+4.22)   42.14 (+5.06)ßdummyTurker
904ß0.0ßConfidence Filtering    30.59 (+4.40)   42.22 (+5.14)ßdummyTurker
905ß0.0ßRandom Filtering        30.27 (+4.08)   42.09 (+5.01)ßdummyTurker
906ß0.0ßN-best Generation       29.62 (+3.43)   40.77 (+3.69)ßdummyTurker
907ß0.0ß--ßdummyTurker
908ß0.0ßManual Back-Translation 31.05 (+4.86)   42.37 (+5.29)ßdummyTurker
909ß0.0ß-----------------------------------------------------ßdummyTurker
910ß0.0ß</pre>ßdummyTurker
911ß0.0ßThanks for your helpful comments.ßdummyTurker
912ß0.0ßAuthor, SEU, word embeddings, and discourse relations are our major contributions in this paper.ßdummyTurker
913ß0.0ßAs you suggest, we will add baselines using individual contributions to further analyze the improvement of each contribution.ßdummyTurker
914ß0.0ßWe use two metrics for topic coherence evaluation: NPMI and W2V.ßdummyTurker
915ß0.0ßNPMI is based on word co-occurrence information and is a widely used metric for topic coherence evaluation.ßdummyTurker
916ß0.0ßIt is not specifically biased to any model.ßdummyTurker
917ß0.0ßTo provide additional insight, we included results based on W2V.ßdummyTurker
918ß0.0ßW2V is based on word semantic similarity using word embeddings and it is a newly proposed approach.ßdummyTurker
919ß0.0ßFor both metrics, we demonstrate that our model outperforms baselines by a large margin.ßdummyTurker
920ß0.0ßWe will add a note regarding this point in the revised paper.ßdummyTurker
921ß0.0ßOur sentiment classification results are document level.ßdummyTurker
922ß0.0ßDocument level sentiments are calculated based on probabilities of aspects and sentiments at the level of SEUs, which are sentences and phrases.ßdummyTurker
923ß0.0ßWe compare Limbic with the state-of-the-art unsupervised aspect sentiment discovery models, including ASUM, JST, and AATS.ßdummyTurker
924ß0.0ßWe will include additional state-of-the-art models based on your suggestion.ßdummyTurker
925ß0.0ßWe will add an error analysis for sentiment classification to demonstrate cases where discourse relations and aspects and sentiments of SEUs help on supporting predicting document level sentiments.ßdummyTurker
926ß0.0ßWe do not intend to claim the novelty of using discourse relation for sentiment analysis.ßdummyTurker
927ß0.0ßOur novelty lies in using a Markov Random Field to incorporate discourse relations in an unsupervised topic model.ßdummyTurker
928ß0.0ßWe will add the two papers in related work, though our goal is different from these two papers.ßdummyTurker
929ß0.0ßWe investigate discovering and modeling author, aspect, and sentiment in an unsupervised fashion.ßdummyTurker
930ß0.0ßThey study document level sentiment classification in a supervised fashion.ßdummyTurker
931ß0.0ßAlso, they do not study the problem of aspect and sentiment discovery.ßdummyTurker
932ß0.0ßTo the best of our knowledge, Limbic is the first topic model that incorporates author, aspect, sentiment, discourse relation, and word embeddings.ßdummyTurker
933ß0.0ßWe demonstrate the improvements of Limbic over the state-of-the-art unsupervised aspect sentiment discovery models.ßdummyTurker
934ß0.0ß"We will improve the ""A Generalized Polya Urn Process"" and ""sentiment alignment"" sections."ßdummyTurker
935ß0.0ßWe believe the reviewer missed some important points of the paper when he or she claims that “the paper does not add any new insights”, and would like to ask the reviewer to re-evaluate based on our response.ßdummyTurker
936ß0.0ßThe reviewer felt the method of the paper is a simple application of known RL methods with a new metric.ßdummyTurker
937ß0.0ßWe believe this claim is incorrect, and the reviewer missed the fact that this paper introduced a new method for RL with parameterized reward function trained using a discriminator — this is different from traditional RL methods that require fixed reward functions.ßdummyTurker
938ß0.0ßWe may view the idea of discriminator training in our method as an extension of GAN that can deal with general RL-rewards (such as CDR metric), and the benefit of our method is that it smoothes the original CDR metric, making RL-learning much more robust (Table 1 and 2).ßdummyTurker
939ß0.0ßWe will add additional references, which do not affect the contributions of our paper.ßdummyTurker
940ß0.0ßThe reviewer felt the results are not convincing because “it cannot outperform the approach by Wu et al.ßdummyTurker
941ß0.0ß2017.” We believe this is incorrect.ßdummyTurker
942ß0.0ßGiven that the code of (Wu et al.ßdummyTurker
943ß0.0ß2016) is not publicly available, we can only compare our methods with our implementation of their approach, see “+D_{CNN}” and “+D_{RNN}” in Tables 1 and 2.ßdummyTurker
944ß0.0ßOur methods with the additional adequacy-oriented metric consistently improves translation performances across language pairs.ßdummyTurker
945ß0.0ßIn addition, we compared our methods to stronger baseline (e.g., MRT and the coverage model).ßdummyTurker
946ß0.0ßThe reviewer felt using the CDR metric as objective in RL training is similar to using coverage feature or penalty in non-RL training.ßdummyTurker
947ß0.0ßWe believe that this claim is incorrect.ßdummyTurker
948ß0.0ßWe have shown in the paper (last two rows of Tables 1 and 2) that the additional coverage-based objective is complementary to the coverage model (Tu et al.ßdummyTurker
949ß0.0ß2016), and combining them can further improve translation performances across language pairs.ßdummyTurker
950ß0.0ßWe also implemented the coverage penalty in decoding (GNMT, Wu et al.ßdummyTurker
951ß0.0ß2016), which improves the BLEU score from 36.76 to 37.11 for the Zh-En task .ßdummyTurker
952ß0.0ßGiven that the score is lower than that of the coverage model (i.e., 39.13), we use the latter as our stronger baseline with coverage features.ßdummyTurker
953ß0.0ßThanks for your comments.ßdummyTurker
954ß0.0ßWe agree that our model is not an autoencoder technically.ßdummyTurker
955ß0.0ßWe started from the variational lower bound of data likelihood to develop our model mathematically.ßdummyTurker
956ß0.0ßThe factorization of joint distribution p(a,x) is a modeling choice; and in dependency parsing it is factorised into a sequence of words and dependency arcs.ßdummyTurker
957ß0.0ßWe will improve the equations to use q_\omega(a|x) directly (Section 3.3)ßdummyTurker
958ß0.0ßThe performance of transition systems in unsupervised dependency parsing have long been outperformed by graph-based approaches, especially extensions of DMV.ßdummyTurker
959ß0.0ßDMV and its extensions have special structure constraints as well as complex initialization which involve a substantial amount of handcrafting.ßdummyTurker
960ß0.0ßAnd they also conduct global search rather than greedy local search compared with transition systems, so they will have higher time complexity both in training and testing.ßdummyTurker
961ß0.0ßWe admit that during training, it is expensive to draw samples and perform REINFORCE into stack LSTM.ßdummyTurker
962ß0.0ßBut during prediction (at test time), our approach is much faster than graph-based methods, because we only draw one sample at each timestep---which means our prediction is linear in time complexity.ßdummyTurker
963ß0.0ßBased on our experimental results, we agree the Cheng et al.ßdummyTurker
964ß0.0ßmodel doesn't perform well for unsupervised learning without posterior regularization.ßdummyTurker
965ß0.0ßThat is also the motivation for introducing posterior regularization to our model.ßdummyTurker
966ß0.0ßEnhanced by PR, our model obtains competitive results.ßdummyTurker
967ß0.0ßWe agree that previous approaches with carefully chosen independence assumptions are successful.ßdummyTurker
968ß0.0ßBut we hope to explore an alternative research direction---grammar induction based on neural reinforcement learning with prior assumptions.ßdummyTurker
969ß0.0ßAs you discussed, our model can utilize a large amount of unlabeled data in a semi-supervised setting.ßdummyTurker
970ß0.0ßAnd unlike prior approaches, our model is end-to-end and can be optimized via stochastic gradient descent based algorithms.ßdummyTurker
971ß0.0ßWe use the encoder directly for parsing.ßdummyTurker
972ß0.0ßWe have tried to use the decoder to rerank samples, but it made little difference.ßdummyTurker
973ß0.0ßIn the final draft, we will rephrase the title and improve the explanation in  section 3.ßdummyTurker
974ß0.0ßWe will also cite the missing papers and distinguish the results of L-VAEP-PR-Exp in Table 2.ßdummyTurker
975ß0.0ßWe use the same value for -b for all models.ßdummyTurker
976ß0.0ßAnd we think it’s allowed to use dev set for tuning this parameter.ßdummyTurker
977ß0.0ßNote that gold trees are available for both dev and test sets in the PASCAL Challenge on Grammar Induction [1].ßdummyTurker
978ß0.0ßWe use 20 samples for MC simulation.ßdummyTurker
979ß0.0ß[1] Gelling, Douwe, et al.ßdummyTurker
980ß0.0ß"""The pascal challenge on grammar induction."""ßdummyTurker
981ß0.0ßProceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure.ßdummyTurker
982ß0.0ßAssociation for Computational Linguistics, 2012.ßdummyTurker
983ß0.0ßAs we see it, the main contributions of our paper are: (a) Generalize chart constraints to arbitrary grammar formalisms that can be expressed with parsing schemata or IRTGs; (b) show that chart constraints combine effectively with other pruning techniques (but see response to argument 4).ßdummyTurker
984ß0.0ßThe definition of allowable items for TAG is just one instance of this, which is intended to illustrate the usefulness of (a); in theory, allowable items could also be used to prune parsers for objects which are not strings (e.g.ßdummyTurker
985ß0.0ßgraphs).ßdummyTurker
986ß0.0ßThe neural chart-constraint tagger, which is more accurate than previous models, is a secondary contribution.ßdummyTurker
987ß0.0ßWe are not aware of a prior definition of chart constraints or allowable items for TAG.ßdummyTurker
988ß0.0ßWe see the fact that a simple definition is already so effective as a positive.ßdummyTurker
989ß0.0ßWe do compare the accuracy of our neural chart-constraint tagger to that of Roark et al.ßdummyTurker
990ß0.0ß's CRF model.ßdummyTurker
991ß0.0ßOur improved recall on begin constraints does seem to translate into a mildly higher speedup than in their paper.ßdummyTurker
992ß0.0ßHowever, we find a further substantial improvement when we combine chart constraints with CTF, which does not involve a neural model.ßdummyTurker
993ß0.0ßWe thank the reviewer for the suggestion to compare against a baseline in which TAG items are only unallowable if i or l (and not j or k) violate the begin and end constraints.ßdummyTurker
994ß0.0ßThis weaker version of TAG chart constraints obtains a 24x speedup over 10-best supertagging, compared to 35.8x for the TAG chart constraints in the paper.ßdummyTurker
995ß0.0ßThus checking constraints on the gap clearly helps.ßdummyTurker
996ß0.0ßWe hypothesize that this is because a TAG chart parser (not just ours) can derive a chart item for the auxiliary tree before it derives a chart item for the node at which it will be adjoined.ßdummyTurker
997ß0.0ßSuch items are pruned by our constraints, but not by a table lookup.ßdummyTurker
998ß0.0ßIf the paper is accepted, we will report complete results on the weaker TAG chart constraints and clarify the discussion.ßdummyTurker
999ß0.0ßAs explained in the paper, the PCFG experiment uses a PCFG which is read off of the PTB annotations, with no further refinement of the nonterminal symbols.ßdummyTurker
1000ß0.0ßAn f-score in the low 70s is normal for such a grammar.ßdummyTurker
1001ß0.0ßRoark et al.ßdummyTurker
1002ß0.0ßevaluate their chart constraint system on the Berkeley parser and find substantial speedups.ßdummyTurker
1003ß0.0ßWe see no reason why our system would not obtain similar speedups, but we agree that it would be interesting to check.ßdummyTurker
1004ß0.0ßThank you for the Zhang reference, which we had missed.ßdummyTurker
1005ß0.0ßIf the paper is accepted, we will qualify our claim that we are the first to show that supertagging and chart constraints can be combined effectively.ßdummyTurker
1006ß0.0ßThe generalized view on chart constraints, the application to TAG, and the combination with CTF remain novel.ßdummyTurker
1007ß0.0ßWhile we agree that neural taggers have previously been used to prune parsers, pruning constituent types is quite different from pruning constituent boundaries.ßdummyTurker
1008ß0.0ßWe will be sure to cite Kummerfeld and Klein as closely related work.ßdummyTurker
1009ß0.0ßWe did test a range of values for theta before submitting the paper.ßdummyTurker
1010ß0.0ß"For PCFG parsing, the parser becomes slower when theta is increased beyond 0.5 because fewer begin and end constraints are enforced, confirming the reviewer's intuition about line 288 (it should say ""accuracy"" instead of ""parsing speed"")."ßdummyTurker
1011ß0.0ßParsing accuracy stays essentially constant until theta=0.75 and then goes down.ßdummyTurker
1012ß0.0ßThis may be because true positive chart constraints, which helped overcome deficiencies in the PCFG model, are filtered out.ßdummyTurker
1013ß0.0ßFor TAG parsing, the main challenge is not so much accuracy as coverage.ßdummyTurker
1014ß0.0ßFalse positive chart constraints frequently prevent the parser from finding any parse at all, and thus lower values of theta strongly degrade the f-scores in Fig.ßdummyTurker
1015ß0.0ß4 (which combine coverage and accuracy).ßdummyTurker
1016ß0.0ßChoosing theta=0.95 achieves a marked speedup without losing much coverage.ßdummyTurker
1017ß0.0ßWe will discuss this in more detail if accepted.ßdummyTurker
1018ß0.0ßWe use the same background knowledge sources for all the systems, except Parasempre that generates SPARQL queries for Freebase and thus cannot operate over open KBs.ßdummyTurker
1019ß0.0ßNevertheless, we agree it will be interesting to understand how our approach works with limited knowledge (w/o nOKB and nestKB) to answer complex questions, even though it is orthogonal to our contributions.ßdummyTurker
1020ß0.0ßWe will include any interesting findings in the camera-ready version.ßdummyTurker
1021ß0.0ßElaborating our experimental setup, we investigate if OQA, designed to operate over triple-assertions, can derive correct answers to complex questions from n-tuple or nest-tuple assertions.ßdummyTurker
1022ß0.0ßFor TAQA, even though the system is not publicly available, we do not anticipate any performance benefits from adding nestKB as their query mechanism cannot handle the contextual assertions in nestKB.ßdummyTurker
1023ß0.0ßWe appreciate your effort in pointing us to the paper and apologize for not citing it.ßdummyTurker
1024ß0.0ßWe agree that paraphrasing is a key component of our system and that the system's performance is likely to improve if state-of-the-art paraphrasing techniques were used to derive a logical interpretation of a question.ßdummyTurker
1025ß0.0ßHowever, we would like to re-emphasize that paraphrasing is not our novel contribution.ßdummyTurker
1026ß0.0ßInstead, we focus on handling structural/lexical variances in representations in open KB assertions given a logical interpretation of a question.ßdummyTurker
1027ß0.0ßWe will try to revise our experiments with the new, better paraphrasing model and include them in the revision.ßdummyTurker
1028ß0.0ßWe consider the top-50 most relevant assertions.ßdummyTurker
1029ß0.0ßWe didn't add curated knowledge sources as we were mainly trying to address the problem of heterogeneity in representations in open KBs.ßdummyTurker
1030ß0.0ßAdding curated knowledge sources might improve the performance (given they are more accurate), but would make the comparison to TAQA (only openQA for complex questions) impossible since it is not publicly available.ßdummyTurker
1031ß0.0ßMany thanks for your helpful comments and constructive criticism.ßdummyTurker
1032ß0.0ßWe welcome your ideas on structural improvements, and will be happy to incorporate them into our paper.ßdummyTurker
1033ß0.0ßWe agree a diagram would be helpful; we will use it to indicate which weights are trained at which stage (training of base task, or senteval transfer evaluation).ßdummyTurker
1034ß0.0ßFor the base task, encoder and softmax are trained jointly; for transfer tasks, only logistic regression is trained.ßdummyTurker
1035ß0.0ßSRL models are designed to have efficient and expressive compositions, while text compositions are straightforward, ad hoc, and rarely a subject of study on their own.ßdummyTurker
1036ß0.0ßWe show that they are identical in certain aspects - but we feel composition of text embeddings has more to learn from SRL than vice versa, since textual compositions already use old SRL models.ßdummyTurker
1037ß0.0ßWe will add appropriate symbols to the table to indicate significance level.ßdummyTurker
1038ß0.0ßThe translational composition (s=t) is the one presented in equation (7).ßdummyTurker
1039ß0.0ßs represents the subtractive composition used when training our base task.ßdummyTurker
1040ß0.0ßWe incorporated the most important results for significance in the description of the results (section 6.4, page 7).ßdummyTurker
1041ß0.0ßThere is a slight difference since we removed the MLP (which had linear activations) - although initialization is indeed the main source of variance.ßdummyTurker
1042ß0.0ßWe believe that the contribution of our work is not so much about the specific way in which we have tweaked the GloVe objective, but about demonstrating the fact that high-quality relation vectors can be learned in an unsupervised way, similarly to how word vectors are learned.ßdummyTurker
1043ß0.0ßWe believe that this insight will have a significant impact in NLP, as it suggests, for instance, that we can use relation vectors to enrich the input to neural network models.ßdummyTurker
1044ß0.0ßAs a simple example, in a question answering system, we could imagine “annotating” mentions of entities with relation vectors encoding their relationship to the different words from the question.ßdummyTurker
1045ß0.0ßAs another example, we could imagine that recommendation systems might be able to take advantage of vectors expressing the relationship between items that have been bought (or viewed) by a customer and other items from the catalogue.ßdummyTurker
1046ß0.0ßFinally, the relation vectors should also be very useful for knowledge completion, especially in cases where few training examples per relation type are given (meaning that neural network models could not be used) and where relations cannot be predicted from the already available knowledge (meaning that knowledge graph embedding methods could not be used, or are at least not sufficient).ßdummyTurker
1047ß0.0ßThe significance of using (the three-argument extensions to) PMI is that it gives us a principled way of determining the importance of each context word.ßdummyTurker
1048ß0.0ßIn particular, we need to be able to distinguish between context words that describe one of the two target words from words that describe the relationship itself.ßdummyTurker
1049ß0.0ßFor instance, to describe the relationship between Paris and France, the words “Eiffel” and “tower” are irrelevant, even though they frequently occur in sentences containing these two target words.ßdummyTurker
1050ß0.0ßThe PMI variants allow us to determine whether the number of occurrences of a given context word is unusual, given that the two target words appear in the sentence.ßdummyTurker
1051ß0.0ßWe are well aware of the literature on knowledge graph embedding, and we agree that it would be useful to point out the differences with our setting in the discussion of related work.ßdummyTurker
1052ß0.0ßWe want to clarify, however, that even beyond the fact that they start from a structured knowledge graph, there is a crucial difference with our work, in that relations in these approaches are modelled in the “entity embedding” itself (e.g.ßdummyTurker
1053ß0.0ßas translations, linear maps, combinations of projections and translations, etc).ßdummyTurker
1054ß0.0ßThe counterpart to our setting would be to model the relationship between two words based on their word vectors.ßdummyTurker
1055ß0.0ßOur argument, however, is that relation vectors can encode knowledge that is complementary to what is encoded in the word vectors, i.e.ßdummyTurker
1056ß0.0ßthat word vectors are inherently limited in the kinds of relational knowledge that they can encode, and that they therefore need to be supplemented with relation vectors in applications where we need to properly represent relational knowledge.ßdummyTurker
1057ß0.0ßThe (extended) PMI metrics allow us to differentiate between relevant and irrelevant stopwords.ßdummyTurker
1058ß0.0ßIf the number of occurrences of a given stopword is in line with what we would expect, its “SI value” will be close to 0.ßdummyTurker
1059ß0.0ßIn such a case, the impact on the relation vector will be small.ßdummyTurker
1060ß0.0ßIntuitively, we can think of the context words with a non-zero SI value as pulling the relation vector away from the mean relation vector, which means that only such words will really influence the relation vector.ßdummyTurker
1061ß0.0ßOf course, we could imagine weighting functions, similar to the one used in GloVe but based on the SI value, to further reduce the impact of irrelevant stopwords, but this is something we did not try.ßdummyTurker
1062ß0.0ßThank you for your constructive comments and suggestions.ßdummyTurker
1063ß0.0ßWe politely disagree with the claim that the paper is very incremental, especially as compared to Zeng et al.ßdummyTurker
1064ß0.0ß[1].ßdummyTurker
1065ß0.0ßZeng et al.ßdummyTurker
1066ß0.0ß[1] and our work are working on two completely different aspects.ßdummyTurker
1067ß0.0ßZeng et al.ßdummyTurker
1068ß0.0ß[1] propose piecewise convolutional neural network (PCNN) as a novel sentence encoder for relation extraction (RE), while our work focus on tackling wrong labeling issue in distant supervision via hierarchical attention mechanism.ßdummyTurker
1069ß0.0ßAs shown in our experiments, we can use PCNN as the sentence encoder and then apply our hierarchical attention scheme at the instance level.ßdummyTurker
1070ß0.0ßOur method is simple and can generalize well to other settings.ßdummyTurker
1071ß0.0ßIn experiments, our method works well when using different sentence encoders such as PCNN and CNN.ßdummyTurker
1072ß0.0ßIn fact, as stated in (Line 305-313), our method can also be easily adapted to GRU and LSTM.ßdummyTurker
1073ß0.0ßWe will provide more detailed experiments and analysis in our revision.ßdummyTurker
1074ß0.0ßMoreover, the relation hierarchies are ubiquitous in many KGs such as Wikidata, YAGO, and Freebase.ßdummyTurker
1075ß0.0ßIn the future, we will explore the effectiveness of our work on various KGs.ßdummyTurker
1076ß0.0ßThanks for the suggestion.ßdummyTurker
1077ß0.0ßIn this paper we cite (Mikolov et al.ßdummyTurker
1078ß0.0ß2013) (Line 288-290) to indicate our pre-trained word embeddings are obtained using word2vec, not indicating Mikolov et al.ßdummyTurker
1079ß0.0ßfirstly proposed word embeddings.ßdummyTurker
1080ß0.0ßIn the revision, we would like to acknowledge Hinton 1986’s pioneering work as suggested by the review.ßdummyTurker
1081ß0.0ßWe thank the suggestion of referring to previous works on hierarchical prediction.ßdummyTurker
1082ß0.0ßWe will carefully survey these previous works, especially about adopting hierarchical information, to make our work self-complete.ßdummyTurker
1083ß0.0ßWe politely disagree with the claim that the evaluation setting in our paper is incorrect.ßdummyTurker
1084ß0.0ßAs shown in [2] and its codes [5], in RE the micro-average precision is actually defined as the mean of precisions over various recalls, thus an approximation to the AUC of the PR curve.ßdummyTurker
1085ß0.0ßWe will clarify this part in our revision to avoid the confusion.ßdummyTurker
1086ß0.0ßP@N is an evaluation metric used in the previous works [3,4] to make more complete and direct comparison among different sentence selective mechanisms.ßdummyTurker
1087ß0.0ßHence, we also report P@N (100,200,300) in this paper.ßdummyTurker
1088ß0.0ßRE often focuses on the performance of those top-ranked extracted facts with high confidence.ßdummyTurker
1089ß0.0ßFor example, in practice, we generally set a confidence threshold and extract all facts with higher confidence higher than the threshold, or set a number threshold N and extract the top-N scored triples.ßdummyTurker
1090ß0.0ßSince valid relational facts are usually sparse as compared to all possible triples, N is generally not that big compared to the size of the dataset.ßdummyTurker
1091ß0.0ßHence, in this benchmark dataset, N is empirically set as 100, 200 and 300.ßdummyTurker
1092ß0.0ßWe really appreciate the valuable suggestion!ßdummyTurker
1093ß0.0ßDue to the space limitation, we have to concisely explain the background information and follow empirical notations used in the previous works.ßdummyTurker
1094ß0.0ßWe will further revise our notations and present more background details in our future revision to avoid ambiguity.ßdummyTurker
1095ß0.0ß[1] Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.ßdummyTurker
1096ß0.0ßDistant supervision for relation extraction via piecewise convolutional neural networks.ßdummyTurker
1097ß0.0ßEMNLP2015.ßdummyTurker
1098ß0.0ß[2] Yi Wu, David Bamman, and Stuart Russell.ßdummyTurker
1099ß0.0ßAdversarial training for relation extraction.ßdummyTurker
1100ß0.0ßEMNLP2017.ßdummyTurker
1101ß0.0ß[3] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun.ßdummyTurker
1102ß0.0ßNeural relation extraction with selective attention over instances.ßdummyTurker
1103ß0.0ßACL2016.ßdummyTurker
1104ß0.0ß[4] Tianyu Liu, Baobao Chang, and Zhifang Sui.ßdummyTurker
1105ß0.0ßA soft-label method for noise-tolerant distantly supervised relation extraction.ßdummyTurker
1106ß0.0ßEMNLP2017.ßdummyTurker
1107ß0.0ß[5] AtNRE.ßdummyTurker
1108ß0.0ßhttps://github.com/jxwuyi/AtNREßdummyTurker
1109ß0.0ß"This paper is intended first and foremost as a theoretical contribution: our references to practical applications are intended as suggestions for future work, and are phrased as such (line 620 ""this poses a *potential* solution"", line 709, ""this *suggests* a method..."", emphasis added)."ßdummyTurker
1110ß0.0ßNote also that, although we make reference to potential applications which may interest readers, the core results of the paper as summarised in the abstract and conclusion are entirely formal-language theoretic.ßdummyTurker
1111ß0.0ßThat this is considered a weakness implies the underlying assumption that theoretical papers must contain empirical results; we do not agree with this belief.ßdummyTurker
1112ß0.0ßMany of the works which we cite and build upon are also theory papers with few or no empirical contributions (eg Gildea & Satta 2016, Kuhlmann & Satta 2012, Rogers 1994, Shieber 1994, Schabes & Waters 1993, Rosenkrantz 1967).ßdummyTurker
1113ß0.0ßSee also our answer to question 1 re.ßdummyTurker
1114ß0.0ßongoing empirical evaluations.ßdummyTurker
1115ß0.0ßOur claim re.ßdummyTurker
1116ß0.0ßrank-k PL-RSTAG vs rank-k SCFG is only speculative, as stated in the paper, and on line 796 we explicitly note that this requires further investigation.ßdummyTurker
1117ß0.0ßIf accepted, we will include an explicit proof that rank-3 PL-RSTAG is more powerful than rank-3 SCFG, including a reference to Wu (1997) who shows that the reordering on line 614 is not generated by ITG (which has the same generative power as rank-3 SCFG).ßdummyTurker
1118ß0.0ßSee our answer to question 1.ßdummyTurker
1119ß0.0ßIf accepted we will add an example showing some concrete rules from an SCFG and their corresponding trees in an intermediate and final grammar; this will help to show how links are preserved across each step of the construction, and will complement Figure 4 which shows the final grammar but does not illustrate how links are treated in the intermediate grammars.ßdummyTurker
1120ß0.0ßThank you for catching these typos.ßdummyTurker
1121ß0.0ßThe box on line 278 is the QED symbol which marks the end of the proof of theorem 1.ßdummyTurker
1122ß0.0ßWe already cite Wu 1995, but thank you for the reminder that there is a more recent version of that paper.ßdummyTurker
1123ß0.0ßThis is a typo referring to experiments which were removed from the final version of the paper for space.ßdummyTurker
1124ß0.0ßThese experiments are ongoing and some of the results have been obtained since the submission deadline, so given the author response guidelines it is probably not appropriate to comment further.ßdummyTurker
1125ß0.0ß"This line should read ""which is a grammar of the sort that has previously been used for word alignment tasks (cf Zhang & Gildea 2005)""."ßdummyTurker
1126ß0.0ßIf you are referring again to line 674, the ITG was induced from the Hansard Fr/En corpus.ßdummyTurker
1127ß0.0ßThe focus of this paper is on a theoretical contribution which we consider to be interesting in itself.ßdummyTurker
1128ß0.0ßWe chose not to discuss more empirical results under the belief that they would distract from the main contributions of the paper, and given the space constraints would hamper our ability to present these contributions completely and correctly.ßdummyTurker
1129ß0.0ßThe strengths listed in review 1 offer a compelling reason for acceptance, while the weakness arguments (apart from being a theoretical paper, for which we do not apologise) are minor points which are either already correct in the paper or typos that can be easily fixed, as explained above.ßdummyTurker
1130ß0.0ßReply to weakness argument 1 (detailed representations of pinyin and wubi):ßdummyTurker
1131ß0.0ßAs described in Sec 2.3 and Sec 2.4, firstly, we use official rule tables to obtain pinyin and wubi codes; secondly, word2vec tool is utilized to obtain vectors; finally, the vectors would be fed into network directly.ßdummyTurker
1132ß0.0ßThanks for the suggestion.ßdummyTurker
1133ß0.0ßWe conducted these experiments which are helpful for understanding the effectiveness of pinyin and wubi.ßdummyTurker
1134ß0.0ßTake CityU for example, feeding extra pinyin and wubi embeddings separately leads to 0.7% and 0.9% improvement respectively.ßdummyTurker
1135ß0.0ßThe results and related analysis will be reported in the final version.ßdummyTurker
1136ß0.0ßReply to weakness argument 3 (why pinyin and wubi works):ßdummyTurker
1137ß0.0ßPinyin makes sense because it represents the pronunciation of Chinese.ßdummyTurker
1138ß0.0ßAs Review 2 says, 1) Depending on the semantic meaning, a Chinese character could be pronounced differently.ßdummyTurker
1139ß0.0ßFor instance, '弹' has two pronunciations: 'tan' and 'dan', when it is pronounced as 'tan', there is a great chance that it is a verb; when it is pronounced as 'dan', definitely it is a noun.ßdummyTurker
1140ß0.0ß2) Character to pinyin conversion tool has the ability to predict the correct pronunciation based on the context.ßdummyTurker
1141ß0.0ßSo it adds additional information on top of the surface form of Chinese characters.ßdummyTurker
1142ß0.0ßBased on pinyin, we can decide if this character forms a word individually or this character need other characters to form a word.ßdummyTurker
1143ß0.0ßWubi makes sense because it is capable of capturing the character structures.ßdummyTurker
1144ß0.0ßMeanwhile, the meaning of a character is closely connected to its structure (drawing).ßdummyTurker
1145ß0.0ßMore specifically, 1) radical (偏旁) effect (90% characters) [Sun et al.ßdummyTurker
1146ß0.0ß2014] is completed captured by wubi codes, which means wubi reflects the semantic meanings.ßdummyTurker
1147ß0.0ß2) characters with same or samilar structures often form a word.ßdummyTurker
1148ß0.0ßFor example, '打'(hit) and '扰'(disturb).ßdummyTurker
1149ß0.0ßThese two characters are both left-right structure and have the same radical.ßdummyTurker
1150ß0.0ßSo when these two characters appear together, definitely they form the word '打扰'(disturb) together and should be segmented as a word.ßdummyTurker
1151ß0.0ßIn the final version, we will give more intuitions to illustrate why the two proposed features are useful and describe the methods of character presentations more clearly.ßdummyTurker
1152ß0.0ßReply to weakness argument 4 (why not standard multi-layer LSTMs):ßdummyTurker
1153ß0.0ßActually, we have tried standard multi-layer LSTMs, which leads to poor performance (0.5% lower) so we choose stacked LSTMs.ßdummyTurker
1154ß0.0ßTo the best of our knowledge, previous collocation error detection approaches do not type the errors they detect.ßdummyTurker
1155ß0.0ß"Although we agree that it might be useful to mention their performance, our task is very different from binary classification ""error vs. no-error""."ßdummyTurker
1156ß0.0ßTo provide a clearer picture on the performance of our approach, we will add a run on binary error detection in the final version of the paper.ßdummyTurker
1157ß0.0ßApologies if the paper does not make clear enough that we followed the methodology of other works on artificial error creation.ßdummyTurker
1158ß0.0ßCf.ßdummyTurker
1159ß0.0ß"lines 162ff: ""To generate the synthetic collocation error corpus we follow the same strategy as, e.g., Rozovskaya and Roth (2010); Felice and Yuan (2014) in the context of grammatical error detection and correction."""ßdummyTurker
1160ß0.0ßWe did not elaborate on the creation of the corpus because of space constraints.ßdummyTurker
1161ß0.0ßIn the final version, we will use the extra page for presenting the methodology.ßdummyTurker
1162ß0.0ßWe applied our model, trained on artificial data, to CEDEL2, a learner corpus.ßdummyTurker
1163ß0.0ßFelice and Yuan (2014) (in the bibliography), which is considered to be state of the art in grammatical error corpus creation, achieve with their models on real data, trained on an artificial corpus they created, a precision between 0.1396 and 0.3440.ßdummyTurker
1164ß0.0ßWe achieve on CEDEL2 a precision between 0.29 and 0.77.ßdummyTurker
1165ß0.0ßWagner et al.ßdummyTurker
1166ß0.0ß(2009) test on both artificial and real data, showing that, when tested on real data, the performance considerably decreases.ßdummyTurker
1167ß0.0ßTherefore, we consider that our results are competitive, although, obviously, improvable.ßdummyTurker
1168ß0.0ßJoachim Wagner, Jennifer Foster, and Josef van Genabith.ßdummyTurker
1169ß0.0ßJudging Grammaticality: Experiments in Sentence Classification.ßdummyTurker
1170ß0.0ßCALICO Journal, 26(3):474-490, 2009.ßdummyTurker
1171ß0.0ßThanks for your detailed comments.ßdummyTurker
1172ß0.0ßWe disagree that lack of comparison with GCN undermines the value of our work given that our contribution is to show that a graph encoder outperforms a sequence encoder for AMR to text.ßdummyTurker
1173ß0.0ßTo this end, *any* graph encoder serves a proof of concept.ßdummyTurker
1174ß0.0ßOur LSTM-based encoder is structurally more similar to the LSTM encoder baseline for sequence to sequence, and hence we built this model, which is also novel in the sense that it has not been used for NMT.ßdummyTurker
1175ß0.0ßWe will give detailed comparison with GCN once more pages are given.ßdummyTurker
1176ß0.0ßBoth our graph LSTM and GCN update node states by exchanging information between neighboring nodes.ßdummyTurker
1177ß0.0ßHowever, our graph-state LSTM adopts gated operations to make the updates, while GCN uses linear transformation with activation.ßdummyTurker
1178ß0.0ßWe will fix the minor issues according to your suggestion.ßdummyTurker
1179ß0.0ßAll our ROUGE scores are reported by the official ROUGE script and have a 95% confidence interval with at most ±0.33 in table 1 and ±0.24 in table 2.ßdummyTurker
1180ß0.0ßThe sentence attention proposed in Nallapati et al.ßdummyTurker
1181ß0.0ß(2016b) is dynamic, which means it will be different for each generated word.ßdummyTurker
1182ß0.0ßWhereas our sentence attention is fixed for all generated words.ßdummyTurker
1183ß0.0ßInspired by the high performance of extractive summarization, we proposed to use fix sentence attention.ßdummyTurker
1184ß0.0ßWe will show the results of Nallapati et al.ßdummyTurker
1185ß0.0ß(2016b) in the final paper.ßdummyTurker
1186ß0.0ßThough they do the experiment on anonymized CNN/Daily Mail dataset which is slightly different from us, we has much higher ROUGE scores (+7.9 on ROUGE-1, +5.7 on ROUGE-2, +8.1 on ROUGE-L) that significantly exceed the difference between anonymized and non-anonymized dataset (non-anonymized results may have +1~2 ROUGE scores according to See et al.ßdummyTurker
1187ß0.0ß(2017)).ßdummyTurker
1188ß0.0ßOur end-to-end model has a large capacity.ßdummyTurker
1189ß0.0ßHowever, finding a generalizable local minimal of the model becomes harder.ßdummyTurker
1190ß0.0ßFor instance, one degenerate solution is to output all sentence-level attention to probability 1 and only rely on the pointer-generation model to generate final results.ßdummyTurker
1191ß0.0ßHence, we introduce two auxiliary losses: 1) extractive summarization loss, and 2) inconsistency loss to help us regularize the training of our end-to-end model.ßdummyTurker
1192ß0.0ß1.ßdummyTurker
1193ß0.0ßOur combined attention using Eq.ßdummyTurker
1194ß0.0ß1 is a passive way to combine extractive and abstractive summarization.ßdummyTurker
1195ß0.0ßThough it slightly improves the abstractive results, we found that the extractor and abstracter didn’t cooperate with each other well.ßdummyTurker
1196ß0.0ßTable 3 shows that there are about 20% decoded words have low sentence attention.ßdummyTurker
1197ß0.0ßHence, we design the inconsistency loss to directly improve the cooperation between the extractor and abstracter.ßdummyTurker
1198ß0.0ßThe inconsistency loss can help converge to a better model and decrease the inconsistency rate to 4.2%.ßdummyTurker
1199ß0.0ß2.ßdummyTurker
1200ß0.0ßAbout the human evaluation, we show the article and 6 summaries (reference summary, 4 generated summaries and a random summary) to each human evaluator.ßdummyTurker
1201ß0.0ßThe random summary is a reference summary randomly picked from other articles and is used as a trap.ßdummyTurker
1202ß0.0ßWe reject all the evaluations that score the informativity of the random summary as 3, 4 and 5.ßdummyTurker
1203ß0.0ßBy using this trap mechanism, we can ensure a much better quality of our human evaluation.ßdummyTurker
1204ß0.0ßFor each example, we first ask 5 human evaluators to evaluate.ßdummyTurker
1205ß0.0ßHowever, for those articles that are too long, which are always skipped by the evaluators, it is hard to collect 5 reliable evaluations.ßdummyTurker
1206ß0.0ßHence, we collect at least 3 evaluations for every example.ßdummyTurker
1207ß0.0ßFor each summary, we average the scores over different human evaluators.ßdummyTurker
1208ß0.0ß3.ßdummyTurker
1209ß0.0ßFor the K words used in the inconsistency loss, we choose the words with top K attention.ßdummyTurker
1210ß0.0ßWe choose this hyper-parameter (K) based on the validation set.ßdummyTurker
1211ß0.0ß4.ßdummyTurker
1212ß0.0ßWe didn’t add hyper-parameters for the other losses since they converge normally as expected when end-to-end training.ßdummyTurker
1213ß0.0ßThank you for your review.ßdummyTurker
1214ß0.0ßWe agree that there are minor writing issues with the paper which canßdummyTurker
1215ß0.0ßbe solved in the camera ready version.ßdummyTurker
1216ß0.0ßWe believe that the contribution that the difficult BUCC task can beßdummyTurker
1217ß0.0ßsolved without using parallel sentences in a very simple way isßdummyTurker
1218ß0.0ßappropriate for an ACL short paper.ßdummyTurker
1219ß0.0ßAdditionally, we use a more realistic training corpus (for theßdummyTurker
1220ß0.0ßmonolingual non-parallel data results) than was used at BUCC.ßdummyTurker
1221ß0.0ßWe would look to remind the reviewers that the standards for short andßdummyTurker
1222ß0.0ßlong papers at ACL are different.ßdummyTurker
1223ß0.0ß"Short papers can be a ""a small"ßdummyTurker
1224ß0.0ß"focused contribution"" or ""an interesting application nugget"" according"ßdummyTurker
1225ß0.0ßto the call for papers.ßdummyTurker
1226ß0.0ßWe disagree that the work is not novel.ßdummyTurker
1227ß0.0ßAs you correctly suggest inßdummyTurker
1228ß0.0ßyour review, BUCC systems are built in a supervised fashion.ßdummyTurker
1229ß0.0ßForßdummyTurker
1230ß0.0ßlanguage pairs without available parallel data (i.e., the vastßdummyTurker
1231ß0.0ßmajority of the roughly 7000*6999 pairs of current languages), ourßdummyTurker
1232ß0.0ßapproach is the only realistic way to solve the problem that we areßdummyTurker
1233ß0.0ßaware of, which makes it important.ßdummyTurker
1234ß0.0ßIt is also important that theßdummyTurker
1235ß0.0ßfield consider unsupervised approaches to the BUCC task, which is whyßdummyTurker
1236ß0.0ßour paper should be published.ßdummyTurker
1237ß0.0ßPlease either raise your score, or explain how solving a task that isßdummyTurker
1238ß0.0ßcurrently solved using parallel sentence supervision without parallelßdummyTurker
1239ß0.0ßsentence supervision is not novel.ßdummyTurker
1240ß0.0ßWe did not use the test set to look at names and numbers.ßdummyTurker
1241ß0.0ßNo, we didßdummyTurker
1242ß0.0ßnot work on French, but our results clearly show that our systemßdummyTurker
1243ß0.0ßworks.ßdummyTurker
1244ß0.0ßIt will clearly not work as well as the previously publishedßdummyTurker
1245ß0.0ßsupervised systems, we don't believe that comparing the exact numbersßdummyTurker
1246ß0.0ßis very interesting.ßdummyTurker
1247ß0.0ßThe direct use of the embeddings does not work,ßdummyTurker
1248ß0.0ßwe will fix the writing to make this clear.ßdummyTurker
1249ß0.0ßWe are sorry for theßdummyTurker
1250ß0.0ßomission of the details on the seed lexicon, this will be provided inßdummyTurker
1251ß0.0ßthe camera ready version.ßdummyTurker
1252ß0.0ßThank you for pointing this out.ßdummyTurker
1253ß0.0ßWe used parse sub-tree introduction/removal during experiments, but we missed to mention that in 622-625.ßdummyTurker
1254ß0.0ßWhat we found is that it is much easier to generate adversarial examples by replacing single words.ßdummyTurker
1255ß0.0ßAn explanation is that this strategy allows generating surface patterns that are captured by models trained on SNLI [1].ßdummyTurker
1256ß0.0ßWe will correct this in the final version of the paper, and provide an ablation study on the generation process.ßdummyTurker
1257ß0.0ß[1]  Annotation Artifacts in Natural Language Inference Data - NAACL’18 - https://arxiv.org/abs/1803.02324ßdummyTurker
1258ß0.0ßWe devoted probably too much space to discussing the continuous space method, mainly because it is very related to how adversarial examples are generated in computer vision.ßdummyTurker
1259ß0.0ßIn the final version, we will shorten the corresponding text in the paper, and discuss our experimental results more thoroughly.ßdummyTurker
1260ß0.0ßNo: the adversarial test sets are generated from the SNLI Test set.ßdummyTurker
1261ß0.0ßThere is no overlap at all between training and test data.ßdummyTurker
1262ß0.0ßWe will mention this more explicitly.ßdummyTurker
1263ß0.0ßWe show in Fig.ßdummyTurker
1264ß0.0ß1 that for DAM and ESIM, there was no significant improvement on SNLI: the prediction accuracy stays nearly the same for proper \lambda values.ßdummyTurker
1265ß0.0ßFor cBiLSTM, however, there was a sensible improvement: from 79% to 81% accuracy.ßdummyTurker
1266ß0.0ßAn explanation for this phenomenon is that models like DAM and ESIM learn to capture annotation artifacts in the SNLI data, as shown in [1].ßdummyTurker
1267ß0.0ßAs a consequence, a better NLI model is not guaranteed to yield better results on SNLI.ßdummyTurker
1268ß0.0ßAnalogously, a model trained on SNLI is not guaranteed to be a good NLI model.ßdummyTurker
1269ß0.0ßFor such a reason, we created a set of adversarial datasets that test whether an NLI model is consistent with respect to a set of logic rules that every NLI model should comply with.ßdummyTurker
1270ß0.0ßIn our experiments, we show that, in our models, (1) performance does not degrade on SNLI, and (2) on adversarial datasets, it yields up to a 79% improvement in predictive accuracy.ßdummyTurker
1271ß0.0ß[1]  Annotation Artifacts in Natural Language Inference Data (NAACL 2018) - https://arxiv.org/abs/1803.02324ßdummyTurker
1272ß0.0ßThe data augmentation approach may be possible for Rule 2, but it is not trivial in the other cases.ßdummyTurker
1273ß0.0ßWe thank the reviewers comments.ßdummyTurker
1274ß0.0ßOur purpose is to parse a complex document to a predefined ontology, instead of tenses and QA.ßdummyTurker
1275ß0.0ßWe argue that representing the document by ontology graph can make semantic parsing and reasoning reliable and interpretable.ßdummyTurker
1276ß0.0ßGiven a complex document, reasoning by human have to rely on the relationship graph in brains or notebooks.ßdummyTurker
1277ß0.0ßand the machine also needs.ßdummyTurker
1278ß0.0ßTo the best of our knowledge, using the models at hand without the oo-semantics, the task to parse documents into ontology composed of objects and relationships between objects cannot be addressed in an elegant way.ßdummyTurker
1279ß0.0ßFor an open-domain problem, as a compromise between fine parsing and high labeling costs, we can predefine some general classes for a rough understanding.ßdummyTurker
1280ß0.0ßNo free lunch in the world, if the parsing demands in complicated domains are raised, lots of expertise is ineluctable no matter what the parsing model is.ßdummyTurker
1281ß0.0ß(1) we just validate its ability to parse dynamical ontology on bAbI dataset and the contribution of symbolic reasoner.ßdummyTurker
1282ß0.0ßWe will add some baselines to make experiments more solid, thanks.ßdummyTurker
1283ß0.0ß(2) The datasets are new corpus that is annotated by our annotators to fill the ontologies with pre-defined structures.ßdummyTurker
1284ß0.0ßOn average of Court Judgment Docs (Parsing Police Report), a sample has 421.38(95.24) Chinese words and the ontology has 14.89(3.35) objects, 10.69(3.47) mentions and 18.97(5.02) relationships between objects.ßdummyTurker
1285ß0.0ßWe thank the reviewers suggestion and comments.ßdummyTurker
1286ß0.0ßWe reply the argument 3 first due to its importance.ßdummyTurker
1287ß0.0ßMotivation.ßdummyTurker
1288ß0.0ßMapping a document into a structured form such as ontology graph is a canonical for document understanding.ßdummyTurker
1289ß0.0ßRepresenting the document by ontology graph can make semantic parsing and reasoning reliable and interpretable.ßdummyTurker
1290ß0.0ßGiven a complex document, reasoning by human have to rely on the relationship graph in brains or notebooks.ßdummyTurker
1291ß0.0ßWhy does not a machine need it?ßdummyTurker
1292ß0.0ßAs for the representation of entities, Entity Networks [Henaff et al., 2016] and ENTITYNLM [Ji et al., 2017] are the pioneers to model on tracking entities, but they were not considering the properties of the entities.ßdummyTurker
1293ß0.0ßIn fact, explicitly modeling the entities both with their properties and contents is important to understand a document, especially a complex document.ßdummyTurker
1294ß0.0ßFor example, if there are two persons named “Avery”, it is vital to know their gender or last name to avoid confusion.ßdummyTurker
1295ß0.0ßTo address above problem, we propose OONP to sketch objects and their relationships by building a structured graph for document parsing.ßdummyTurker
1296ß0.0ßTechniques.ßdummyTurker
1297ß0.0ßA word-level bi-directional GRU is used for the distributed representations of Inline Memory and a matrix with |C| rows of fixed length vectors is served as the representation of “new” objects and stored in Matrix Memory.ßdummyTurker
1298ß0.0ßThe similarity function is realized by a single-layer MLP to receive the representation of two objects, the hidden state of the reader and the symbolic features to perform “New-Assign” action.ßdummyTurker
1299ß0.0ß“Update2what' action is also an MLP classifier for each property of class.ßdummyTurker
1300ß0.0ßA fraction of related works are discussed in above Motivation.ßdummyTurker
1301ß0.0ßWe will add more in the paper due to word limitation of reply.ßdummyTurker
1302ß0.0ßHave described above(see Techniques).ßdummyTurker
1303ß0.0ßWe just validate its ability to parse dynamical ontology on bAbI dataset and the contribution of symbolic reasoner.ßdummyTurker
1304ß0.0ßWe will add some baselines to make experiments more solid, thanks.ßdummyTurker
1305ß0.0ß1) On average of Court Judgment Docs (Parsing Police Report), a sample has 421.38 (95.24) Chinese words and the ontology has 14.89(3.35) objects, 10.69(3.47) mentions and 18.97(5.02) relationships between objects.ßdummyTurker
1306ß0.0ß2) Rules: In OONP, It's convenient to define some prior rules to make it easy to learn, e.g., we have a rule to calculate the longest common string of two objects as a symbolic feature.ßdummyTurker
1307ß0.0ß3) Event extraction algorithm.ßdummyTurker
1308ß0.0ßTo extract event is an important part of building an ontology, but it is a relatively easy task with high performance[Yan,2017,arxiv:1710.00969].ßdummyTurker
1309ß0.0ß4) Thanks again for suggestions.ßdummyTurker
1310ß0.0ß"We will claim OONP as ""the first nn to parsing complex documents ""  after detailed comparison with related works, and we will thoroughly revise our paper according to the suggestions provided."ßdummyTurker
1311ß0.0ßThank you for your review, detailed questions and insightfulßdummyTurker
1312ß0.0ßcomments.ßdummyTurker
1313ß0.0ßWe are in particular grateful for suggestionsßdummyTurker
1314ß0.0ßregarding improved readability!ßdummyTurker
1315ß0.0ßWe will implement them inßdummyTurker
1316ß0.0ßcase of acceptance.ßdummyTurker
1317ß0.0ßYou correctly point out that more examples are needed.ßdummyTurker
1318ß0.0ßWeßdummyTurker
1319ß0.0ßwill add them in case of acceptance.ßdummyTurker
1320ß0.0ß"Weakness_1:""section_3.4,_fail_to_provide_such_details._Table1_which"ßdummyTurker
1321ß0.0ß"should_illustrate_the_results_is_very_hard_to_read_and_interpret."""ßdummyTurker
1322ß0.0ß[Assuming you refer to Table2.]ßdummyTurker
1323ß0.0ßTable2 has a standard NLP format.ßdummyTurker
1324ß0.0ßWe believe that ourßdummyTurker
1325ß0.0ßevaluation measures are either standard (F1) or definedßdummyTurker
1326ß0.0ßmathematically (roundtrip evaluation).ßdummyTurker
1327ß0.0ßPlease help us improve the paper by suggesting specificßdummyTurker
1328ß0.0ßimprovements to Table2 and Section 3.4.ßdummyTurker
1329ß0.0ß"Weakness_1:""a_'detailed_evaluation'_over_a_very_large_number_of_languages"""ßdummyTurker
1330ß0.0ßWe agree that it would be nice to give examples forßdummyTurker
1331ß0.0ßindividual languages beyond the examples in Figures 5 andßdummyTurker
1332ß0.0ß6.ßdummyTurker
1333ß0.0ßBut this is simply not possible for 1259 languages due toßdummyTurker
1334ß0.0ßspace constraints.ßdummyTurker
1335ß0.0ßOur contribution is that we evaluate two difficult tasks,ßdummyTurker
1336ß0.0ßindividually, on each of 1259 languages and that nobody hasßdummyTurker
1337ß0.0ßdone this on such a large number of languages before.ßdummyTurker
1338ß0.0ß"Weakness_2:""The_evaluation_measures_..._are_not_explained"""ßdummyTurker
1339ß0.0ßFigure 5 gives a roundtrip translation example.ßdummyTurker
1340ß0.0ßBased onßdummyTurker
1341ß0.0ßyour feedback, we will include an example calculation forßdummyTurker
1342ß0.0ßthe measures.ßdummyTurker
1343ß0.0ßA mathematical definition of the evaluation measures is onßdummyTurker
1344ß0.0ß506-538.ßdummyTurker
1345ß0.0ßPlease help us improve our explanation by makingßdummyTurker
1346ß0.0ßspecific suggestions.ßdummyTurker
1347ß0.0ß"Weakness_3:""which_are_the_pivot_languages?_Why_only_10?"""ßdummyTurker
1348ß0.0ßTable1 lists pivot languages.ßdummyTurker
1349ß0.0ßWe agree that ideally weßdummyTurker
1350ß0.0ßshould have found the optimal setting of the hyperparameterßdummyTurker
1351ß0.0ß"""number_of_pivot_languages"" experimentally, but the paper"ßdummyTurker
1352ß0.0ßalready contains a very large amount of experimentation.ßdummyTurker
1353ß0.0ßThis would simply be beyond the scope.ßdummyTurker
1354ß0.0ß"Weakness_3:""edition_has_2_roles:_a_pivot_and_a_target_language."""ßdummyTurker
1355ß0.0ßWe agree that it's potentially confusing thatßdummyTurker
1356ß0.0ßpivot-languages have two roles.ßdummyTurker
1357ß0.0ßHowever, it is actuallyßdummyTurker
1358ß0.0ßsimple.ßdummyTurker
1359ß0.0ßThere are two independent copies of the 10ßdummyTurker
1360ß0.0ßpivot-translations: one is used as pivot-edition, one asßdummyTurker
1361ß0.0ßtarget-edition.ßdummyTurker
1362ß0.0ß"Weakness_3:""why_..._report_on_..._the_'alignment-based_dictionary'_if_you_then_dismiss_it"""ßdummyTurker
1363ß0.0ßAlignment-based dictionaries (left part of Table2, labeledßdummyTurker
1364ß0.0ß"""WORD"") work great if a language can be tokenized in good"ßdummyTurker
1365ß0.0ßquality, but work poorly for hard-to-tokenizeßdummyTurker
1366ß0.0ßlanguages.ßdummyTurker
1367ß0.0ßTherefore, we also test chi2-basedßdummyTurker
1368ß0.0ßdictionaries.ßdummyTurker
1369ß0.0ß"The right part of Table2 (labeled ""CHAR"")"ßdummyTurker
1370ß0.0ßshows that chi2-based dictionaries work better overall.ßdummyTurker
1371ß0.0ß"Weakness_3:""how_did_you_learn_the_embeddings?_...__Skip-gram_model?_How"ßdummyTurker
1372ß0.0ßbig_is_your_corpus?_The_parameters_used_for_the_creation_ofßdummyTurker
1373ß0.0ß"embeddings_must_be_reported_as_well."""ßdummyTurker
1374ß0.0ßWe apologize for not specifying that we use word2vec defaultßdummyTurker
1375ß0.0ßparameters (except for exceptions stated in section 3.3) andßdummyTurker
1376ß0.0ßskip-gram.ßdummyTurker
1377ß0.0ßCorpus size: line 545.ßdummyTurker
1378ß0.0ß"Weakness_3:""The_baselines_methods_are_..._poorly_described.__..._How"""ßdummyTurker
1379ß0.0ß"""does_it_differ_from_your_method?"""ßdummyTurker
1380ß0.0ßscalability across large number of languages, applicabilityßdummyTurker
1381ß0.0ßto our dataset and strong performance.ßdummyTurker
1382ß0.0ßBOW and S-ID meetßdummyTurker
1383ß0.0ßthese criteria, are state-of-the-art, widely used and wellßdummyTurker
1384ß0.0ßknown in the community.ßdummyTurker
1385ß0.0ßWe introduce RTSIMPLE to show thatßdummyTurker
1386ß0.0ßtraining embeddings adds value compared to using theßdummyTurker
1387ß0.0ßdictionary graphs directly.ßdummyTurker
1388ß0.0ßThe novelty of our methods is that that they are, inßdummyTurker
1389ß0.0ßcontrast to these baselines, concept-based.ßdummyTurker
1390ß0.0ß"Weakness_4:""related_work...I_do_not_see_any_critical_reflections,_a"""ßdummyTurker
1391ß0.0ß"""comparison_of_the_mentioned_approaches_to_your_approach,_how"""ßdummyTurker
1392ß0.0ß"""they_differ"""ßdummyTurker
1393ß0.0ß"The first paragraph of related work concludes ""inappropriate"ßdummyTurker
1394ß0.0ßfor a low-resource setting of 150,000 tokens perßdummyTurker
1395ß0.0ß"language""."ßdummyTurker
1396ß0.0ß"The second concludes ""We use Levy et al."ßdummyTurker
1397ß0.0ß(2017)’sßdummyTurker
1398ß0.0ß"S-ID ... approach as a baseline."""ßdummyTurker
1399ß0.0ßAnd so on.ßdummyTurker
1400ß0.0ßSo we believe that we critically reflect for each area ofßdummyTurker
1401ß0.0ßprior work how it relates to our work.ßdummyTurker
1402ß0.0ßHow our methods differ from baselines: see discussion of the results in Section 3.4.ßdummyTurker
1403ß0.0ß"Question_1:""Why_..._not_use_WordNet"""ßdummyTurker
1404ß0.0ß*Monolingual* semantic equivalence is completely orthogonalßdummyTurker
1405ß0.0ßto the main content of the paper: *multilingual* semanticßdummyTurker
1406ß0.0ßequivalence.ßdummyTurker
1407ß0.0ßOur contribution is concept-based embeddingßdummyTurker
1408ß0.0ßlearning; the concepts we induce from the dictionariesßdummyTurker
1409ß0.0ßestablish multilingual semantic equivalence.ßdummyTurker
1410ß0.0ßWe agree that using wordnet is promising and would like toßdummyTurker
1411ß0.0ßuse it in future work.ßdummyTurker
1412ß0.0ßBut this is orthogonal to the mainßdummyTurker
1413ß0.0ßcontribution of the paper.ßdummyTurker
1414ß0.0ßWe are in the uncomfortable position of being unable to replicate Hill et al, even after extensive discussion with the authors.ßdummyTurker
1415ß0.0ßOur reimplementation of their BoW model, using their 500D vectors, performed worse than what they report.ßdummyTurker
1416ß0.0ßWe hint at this in footnote 3, but will state it explicitly in future revisions.ßdummyTurker
1417ß0.0ßWe even found their embeddings to lack coverage of several words in their test set (this is easily verifiable as their data is online).ßdummyTurker
1418ß0.0ßWe tried random initialisations and performance was comparable to PCA.ßdummyTurker
1419ß0.0ßWe suspect the issue is not with PCA, but with the numbers of Hill et al.ßdummyTurker
1420ß0.0ßWe are happy to report the performance of Le & Zuidema.ßdummyTurker
1421ß0.0ßWe did not mention it alongside ours as they tested on different tasks.ßdummyTurker
1422ß0.0ßFurther, a main aim of our paper was to investigate whether tree structures are helpful in NLP.ßdummyTurker
1423ß0.0ßTheir model does not induce trees, but rather mixtures of nodes corresponding to overlapping spans (as shown in their Fig6).ßdummyTurker
1424ß0.0ßAs their model cannot be used to investigate our main hypothesis, we instead chose to directly compare to SOTA.ßdummyTurker
1425ß0.0ßWhile it is true that ERPs like the P600 were first shown with garden path sentences,ßdummyTurker
1426ß0.0ßthis paper intentionally chases ecological validity using naturalistic data followingßdummyTurker
1427ß0.0ßhttp://science.sciencemag.org/content/303/5664/1634.shortßdummyTurker
1428ß0.0ßandßdummyTurker
1429ß0.0ßhttps://www.nature.com/articles/doi%3A10.1038/nature17637ßdummyTurker
1430ß0.0ßsee also the edited collectionßdummyTurker
1431ß0.0ßhttp://www.cambridge.org/gb/academic/subjects/psychology/cognition/cognitive-neuroscience-natural-language-useßdummyTurker
1432ß0.0ßSurprisals and entropies were computed over syntactic structures.ßdummyTurker
1433ß0.0ßThe cited review article, Hale 2016, writes out standardßdummyTurker
1434ß0.0ßmathematical definitions for surprisal and entropy.ßdummyTurker
1435ß0.0ßThank you, will do.ßdummyTurker
1436ß0.0ßThanks for the comments.ßdummyTurker
1437ß0.0ßOur solution for leveraging source-side monolingual data is essentially a semi-supervised method.ßdummyTurker
1438ß0.0ßIn MLE training for NMT, previous works have shown that source-side monolingual data is useful [1, 2].ßdummyTurker
1439ß0.0ßWith the help of source-side monolingual data, we can generate more pseudo bilingual data and have better generalization, as shown in [1].ßdummyTurker
1440ß0.0ßFurthermore, using source-side monolingual data, together with our proposed solution for how to use it in RL training (in subsection 4.1), we can achieve better exploration-exploitation trade-off: for a source-side sentence x, the pseudo-target reference y generated by beam search exploits the accuracy of current NMT model, and the training sentence y' generated by multinomial sampling for reward computation achieves better exploration.ßdummyTurker
1441ß0.0ßIn this way, we can better control the RL training with the help of source-side monolingual data.ßdummyTurker
1442ß0.0ßWe are aware of your concern.ßdummyTurker
1443ß0.0ßApart from the new method of leveraging source-side monolingual data, what we provide is the first comprehensive study of effective RL training strategies when faced with quite competitive deep NMT models and large-scale datasets, which is crucial for real-world translation services.ßdummyTurker
1444ß0.0ßTowards this end, we hope our experiences can be helpful to NMT community.ßdummyTurker
1445ß0.0ßWe are afraid that you misunderstood our experimental settings.ßdummyTurker
1446ß0.0ßWe report both the dev and test set performance, and we tuned all the hyper-parameter (including \alpha) based on dev set, as stated in Line 475-477.ßdummyTurker
1447ß0.0ßFurthermore, reviewer can find the corresponding best dev BLEU in each Table.ßdummyTurker
1448ß0.0ßThanks for the question.ßdummyTurker
1449ß0.0ßWe indeed performed such comparisons at the very early phase of our experiments, although not formally reported in the paper due to space limitation.ßdummyTurker
1450ß0.0ßTo demonstrate the BLEU improvement is from RL training instead of Adam restarting, we restarted the MLE model training by Adam with learning rate 0.0001 (same as used in our RL experiments), 0.001, 0.01, starting from the same warm started model (BLEU 24.29) as in RL, and continued the MLE training for more than 50k steps.ßdummyTurker
1451ß0.0ßHowever, we find there is no gain for MLE (24.31/24.30/24.27 w.r.t.ßdummyTurker
1452ß0.0ßlearning rate 0.0001/0.001/0.01).ßdummyTurker
1453ß0.0ßWe suspect that the converged MLE model is at a good minimum point and it is hard to find another better solution with restarting.ßdummyTurker
1454ß0.0ßWe didn't report this in our paper since this is a common practice, such as [3].ßdummyTurker
1455ß0.0ßWe thank the reviewer for the comments and suggestions.ßdummyTurker
1456ß0.0ßWe will fix some writing problems in our later version.ßdummyTurker
1457ß0.0ß[1].ßdummyTurker
1458ß0.0ßZhang, et al.ßdummyTurker
1459ß0.0ß"""Exploiting source-side monolingual data in neural machine translation."""ßdummyTurker
1460ß0.0ßEMNLP-2016.ßdummyTurker
1461ß0.0ß[2].ßdummyTurker
1462ß0.0ßCheng, et al.ßdummyTurker
1463ß0.0ß"""Semi-supervised learning for neural machine translation."""ßdummyTurker
1464ß0.0ßACL-2016.ßdummyTurker
1465ß0.0ß[3].ßdummyTurker
1466ß0.0ßBahdanau, et al.ßdummyTurker
1467ß0.0ß"""An actor-critic algorithm for sequence prediction."""ßdummyTurker
1468ß0.0ßICLR-2017.ßdummyTurker
1469ß0.0ßActually we had more experiments than the paper presented but we had to pick out some important results due to the limitation of paper amount.ßdummyTurker
1470ß0.0ßIf necessary we will provide all results that can support the usefulness of our model.ßdummyTurker
1471ß0.0ßIt is natural to remind that the global weight plays similar role as the TF-IDF value.ßdummyTurker
1472ß0.0ßHowever, this global weight is not the main goal of this paper but just the by-product of the whole process, which can be applicable to other research.ßdummyTurker
1473ß0.0ßBut we admit that it should have been appropriate to mention about the similarity between them.ßdummyTurker
1474ß0.0ßThanks for your considerate review.ßdummyTurker
1475ß0.0ßOn one hand, the paper is devoted to efficiently dealing with significant problems in medical data application, where data shift emerges usually and annotated data is much more costly to acquire.ßdummyTurker
1476ß0.0ßWe conduct experiments with diverse configurations on typical datasets from well-known databases, and make substantial contribution to this problem.ßdummyTurker
1477ß0.0ßOn the other hand, to our knowledge, adversarial domain adaptation has been scarcely explored on phrase matching tasks with multiple matching definitions, where features of pairwise relation are transferred.ßdummyTurker
1478ß0.0ßMore interesting research can be conducted on more complicated relation transferring.ßdummyTurker
1479ß0.0ßIn our further exploration, we compete with an instantiation of Deep Domain Confusion (DDC) [1], which is a classical domain adaptation scheme aiming at minimizing the distance between the representation distributions of the source and target domains.ßdummyTurker
1480ß0.0ßAn adaptation layer based on Maximum Mean Discrepancy [2] is applied after the phrase matcher to implement DDC.ßdummyTurker
1481ß0.0ßTo be more convincing, we implement DDC and DA-BiMPM in the unsupervised setting by considering only source annotations in the matching loss.ßdummyTurker
1482ß0.0ßFinally, the accuracy of the two approaches are 92.39 and 96.12, respectively.ßdummyTurker
1483ß0.0ßIn other words, the unsupervised DA-BiMPM outperforms DDC-based BiMPM considerably and prove the effectiveness of our model.ßdummyTurker
1484ß0.0ßWe will include this in the final version.ßdummyTurker
1485ß0.0ßWe will add the following discussions and references to the introduction section.ßdummyTurker
1486ß0.0ßIn recent years, several domain adaptation methods are proposed, e.g., mDSA [3], DDC [1], DAN [4], JAN [5], and DANN [6].ßdummyTurker
1487ß0.0ßInspired by DANN’s applications on text classification [7], relation extraction [8], and paraphrase identification [9], we consider applying it to BiMPM to exploit source-domain annotations.ßdummyTurker
1488ß0.0ß[1] Tzeng, Eric et al.ßdummyTurker
1489ß0.0ß2014.ßdummyTurker
1490ß0.0ßDeep domain confusion: Maximizing for domain invariance.ßdummyTurker
1491ß0.0ß[2] K. M. Borgwardt et al.ßdummyTurker
1492ß0.0ß2006.ßdummyTurker
1493ß0.0ßIntegrating structured biological data by kernel maximum mean discrepancy.ßdummyTurker
1494ß0.0ß[3] Minmin Chen et al.ßdummyTurker
1495ß0.0ß2012.ßdummyTurker
1496ß0.0ßMarginalized denoising autoencoders for domain adaptation.ßdummyTurker
1497ß0.0ß[4] Long, Mingsheng, et al.ßdummyTurker
1498ß0.0ß"""Learning transferable features with deep adaptation networks."""ßdummyTurker
1499ß0.0ßarXiv preprint arXiv:1502.02791(2015).ßdummyTurker
1500ß0.0ß[5] Long, Mingsheng, et al.ßdummyTurker
1501ß0.0ß"""Deep transfer learning with joint adaptation networks."""ßdummyTurker
1502ß0.0ßarXiv preprint arXiv:1605.06636 (2016).ßdummyTurker
1503ß0.0ß[6] Ganin, Yaroslav, et al.ßdummyTurker
1504ß0.0ß"""Domain-adversarial training of neural networks."""ßdummyTurker
1505ß0.0ßThe Journal of Machine Learning Research 17.1 (2016): 2096-2030.ßdummyTurker
1506ß0.0ß[7] Pengfei Liu et al.ßdummyTurker
1507ß0.0ß2017.ßdummyTurker
1508ß0.0ßAdversarial multi-task learning for text classification.ßdummyTurker
1509ß0.0ß[8] Lisheng Fu et al.ßdummyTurker
1510ß0.0ß2017.ßdummyTurker
1511ß0.0ßDomain adaptation for relation extraction with domain adversarial neural network.ßdummyTurker
1512ß0.0ß[9] Jianfei Yu et al.ßdummyTurker
1513ß0.0ß2018.ßdummyTurker
1514ß0.0ßModelling domain relationships for transfer learning on retrieval-based question answering systems in E-commerce.ßdummyTurker
1515ß0.0ßWe have conducted the requested experiment and found that the domain specific features alone do not perform wellßdummyTurker
1516ß0.0ßUnfortunately, the journalists were not available for a longitudinal annotation study, since the extra time spent on every article was significant.ßdummyTurker
1517ß0.0ßWe'd argue that style models will not be as strongly affected by covering longer time intervals than the topic baseline would; in news, topics change drastically, style does not.ßdummyTurker
1518ß0.0ßNevertheless, the outlined limitations apply, and we will add remark to that effect.ßdummyTurker
1519ß0.0ßAs for to the partisanship-judgment of a site, we consider it reliable, since such publishers hardly ever change their attitudes, let alone into the diametrically opposed stance.ßdummyTurker
1520ß0.0ßTo correct for potential publisher-bias, we made sure that publishers were not simultaneously in the training set and the test set.ßdummyTurker
1521ß0.0ßThis way our results reflect partisan style rather than publisher style.ßdummyTurker
1522ß0.0ßSee answer to weakness argument #1.ßdummyTurker
1523ß0.0ßThe training time of our systems is about twice of the baseline.ßdummyTurker
1524ß0.0ßThe training time of the tree-based system is about 1.5 times of the baseline.ßdummyTurker
1525ß0.0ßWe will report them in the final version.ßdummyTurker
1526ß0.0ßWe will also report the information about the machine and GPUs we used.ßdummyTurker
1527ß0.0ßThanks for pointing them out.ßdummyTurker
1528ß0.0ßWe will reflect your comments.ßdummyTurker
1529ß0.0ßThanks a lot for the insightful comments.ßdummyTurker
1530ß0.0ßPlease see our replies below:ßdummyTurker
1531ß0.0ßWe noticed that information retrieval was included in the document analysis area specifically (Detailed descriptions of areas, httpßdummyTurker
1532ß0.0ßWe only used pre-trained word embeddings for initialization.ßdummyTurker
1533ß0.0ßFor entities, we utilized three kinds of semantics to enhance entity representations are all associated with linguistic manners closely.ßdummyTurker
1534ß0.0ßEnd-to-end neural models have strong ability to catch linguistic/semantic information from knowledge graphs such as entity types.ßdummyTurker
1535ß0.0ßThe scope of linguistic nowadays is also much larger than years ago; we would argue that end-to-end learning with neural networks upon texts is also capturing certain types of linguistic information, though perhaps implicitly.ßdummyTurker
1536ß0.0ßHow to better understand the linguistic aspects of such end-to-end models is a crucial challenge for our community but perhaps would require a large amount of future work.ßdummyTurker
1537ß0.0ßWe have done case studies in the paper.ßdummyTurker
1538ß0.0ßWe did not put them in this paper because of the limitation of space.ßdummyTurker
1539ß0.0ßWe also did some experiments such as ablation studies to explore the semantic effectiveness from knowledge graphs, which is associated with the linguistic analysis.ßdummyTurker
1540ß0.0ßOur related work introduced neural ranking models and knowledge-based ranking models.ßdummyTurker
1541ß0.0ßInteraction-based ranking models, as a kind of neural ranking models, aim to establish an interaction matrix between the query and document for ranking.ßdummyTurker
1542ß0.0ßKnowledge-based ranking models typically leverage manual feature engineering to bridge queries and documents through entities.ßdummyTurker
1543ß0.0ßIn this work, we for the first time incorporated knowledge into interaction-based ranking models without manual features and demonstrated its effectiveness is much better compared with state-of-the-art models.ßdummyTurker
1544ß0.0ßWe appreciate the insightful suggestion.ßdummyTurker
1545ß0.0ßWe will do further analysis with success and failure case studies and present them in the revision.ßdummyTurker
1546ß0.0ßEDRM showed greater improvement for short queries which have little information.ßdummyTurker
1547ß0.0ßIt indicated that semantics could provide necessary information for neural ranking models.ßdummyTurker
1548ß0.0ßMoreover, evaluating EDRM in different scenarios especially length helps us to know why knowledge can improve the ranking performance.ßdummyTurker
1549ß0.0ßCNNs with slide windows are used for sentence encoding in our experiments.ßdummyTurker
1550ß0.0ßCNNs with fewer parameters can summarize important parts in the descriptions for the match.ßdummyTurker
1551ß0.0ßTherefore we choose CNNs to encode entity descriptions.ßdummyTurker
1552ß0.0ßWe will perform careful proofreading and check grammatical errors in our paper.ßdummyTurker
1553ß0.0ßThanks for your comments.ßdummyTurker
1554ß0.0ßHowever, we believe that the motivation and model description is pretty clear.ßdummyTurker
1555ß0.0ßf is a mapping function defined in Section 3.2, and theta_2 is the parameter in the mapping function.ßdummyTurker
1556ß0.0ßThe overall training loss is (5)+(6)+(9).ßdummyTurker
1557ß0.0ßThanks for the suggestion.ßdummyTurker
1558ß0.0ßDue to space limit, we neglect the description of word similarity and relatedness task.ßdummyTurker
1559ß0.0ßWe will supplement the material in the next version.ßdummyTurker
1560ß0.0ßSorry to make you misunderstand the statements.ßdummyTurker
1561ß0.0ßFor the “existing models … cannot handle words without perceptual information”, we mean most of the existing multimodal word representation models.ßdummyTurker
1562ß0.0ßMore importantly, all existing models did not model all three characters listed in the introduction.ßdummyTurker
1563ß0.0ßIn this paper, we are dealing with a totally different task from Aytar et al.ßdummyTurker
1564ß0.0ßThanks for your suggestion.ßdummyTurker
1565ß0.0ßHowever, the above papers solve totally different tasks and are only related in some way like how to obtain auditory representations which is not the focus of our paper.ßdummyTurker
1566ß0.0ßWe aim to learn a better multimodal word representation model by effectively fusing textual, visual and auditory inputs.ßdummyTurker
1567ß0.0ßWe are comparing to Chen’s result specifically when the “gold” document is known.ßdummyTurker
1568ß0.0ßThey report this number in their paper in the end of Section 5, “Given the correct document (but not the paragraph) we can achieve 49.4…”.ßdummyTurker
1569ß0.0ßThus the methods are on equal footing, both are being tested when the correct document but not the correct paragraph, is known.ßdummyTurker
1570ß0.0ßIn this setting Chen’s model does not do any retrieval, it just runs the neural model over each paragraph in the document.ßdummyTurker
1571ß0.0ßWe don’t compare to their work in the fully open setting since our method is built for the case when a small number of relevant documents have already been selected for the question.ßdummyTurker
1572ß0.0ßWe consider the “no answer” prediction to be a baseline.ßdummyTurker
1573ß0.0ßIn the camera ready version we will certainly cite https://arxiv.org/pdf/1706.04115.pdf.ßdummyTurker
1574ß0.0ßThe SQuAD result is from the test set, we will clarify that in the camera ready version.ßdummyTurker
1575ß0.0ßThe comment “the results were particularly bad for SQuAD” refers to the performance on document-level SQuAD when the correct paragraph, but not the correct document, is not known, we will clarify this as well.ßdummyTurker
1576ß0.0ßSee our reply to weakness 1 for an explanation of the comparison to Chen's system.ßdummyTurker
1577ß0.0ßOur model is tested on the unmodified TriviaQA data, so it is certainly using the same data everyone else is using.ßdummyTurker
1578ß0.0ßAll models used for TriviaQA take entire documents as input, and many of them have a two stage pipeline like our model does.ßdummyTurker
1579ß0.0ßTriviaQA is intended to be used to evaluate pipelined systems like these in an end-end manner, just as we have done here.ßdummyTurker
1580ß0.0ßWe consider our observation that using a more sophisticated first stage of the pipeline was important to be a minor contribution of this work.ßdummyTurker
1581ß0.0ßThe ablation study indicates how important doing this was.ßdummyTurker
1582ß0.0ßAll the plots are from the dev set, but thanks for pointing out that this was not specified in the captions.ßdummyTurker
1583ß0.0ßWe will amend this in the camera ready version.ßdummyTurker
1584ß0.0ßThere are few results on leaderboard that are ahead of ours, but they were released after our results (the date we made the comparison is in the paper) and were unpublished when we wrote the paper.ßdummyTurker
1585ß0.0ßIn the camera ready version we can mention (or cite, if they do get published) these results as well.ßdummyTurker
1586ß0.0ßThe scores are log factors, they cannot be interpreted as probabilities until they are exponentiated and normalized.ßdummyTurker
1587ß0.0ßWe did find using the normalized version to perform slightly worse, possibly because the normalization factor will be higher for larger paragraphs than shorter ones, so using normalized scores biases the model towards short paragraphs.ßdummyTurker
1588ß0.0ßThis is exactly the same approach Chen et al.ßdummyTurker
1589ß0.0ßused.ßdummyTurker
1590ß0.0ßNote that in our loss function the scores are normalized as expected.ßdummyTurker
1591ß0.0ßWe thank the reviewer for the time and effort taken to review our paper.ßdummyTurker
1592ß0.0ßIn general, the weaknesses and questions raised suggest that we may have not conveyed our main points clearly enough.ßdummyTurker
1593ß0.0ßWe will clarify all these points in the camera-ready version.ßdummyTurker
1594ß0.0ßThe only model to date that incorporates lexical knowledge is Qian Chen et al.ßdummyTurker
1595ß0.0ß(2017) which was only recently published on arXiv.ßdummyTurker
1596ß0.0ßWhile we agree that it would be very interesting to include their model in the analysis, they didn't make their code available yet.ßdummyTurker
1597ß0.0ßWe hope that additional work on incorporating external knowledge would follow this paper.ßdummyTurker
1598ß0.0ßWith the current stage of research on NLI, we believe it’s still important to point out that models that don’t employ external knowledge fail to learn this knowledge from the data.ßdummyTurker
1599ß0.0ßWe also think that the paper of Qian Chen et al.ßdummyTurker
1600ß0.0ß(2017) is not sufficient to demonstrate this point, given the minor performance improvement (stemming from the nature of SNLI).ßdummyTurker
1601ß0.0ßWe chose replacement words that appear both in the training data and in pretrained embeddings, such that it would be possible for the model to learn the required lexical inferences.ßdummyTurker
1602ß0.0ßWe show that even so, the model fails to make many inferences, and mainly learns the ones that are frequent in the training data.ßdummyTurker
1603ß0.0ßThis is, in fact, exactly the point we were trying to convey: in a real-world scenario, it is very likely to encounter word-pairs which are not contained in (a sufficient number of) training examples.ßdummyTurker
1604ß0.0ßThe goal of NLI models isn't to perfectly memorize the training data but to be able to generalize to unseen examples, and we demonstrate that they currently fail to do so.ßdummyTurker
1605ß0.0ßWe agree that SNLI is suboptimal and we don’t claim that our new test set is an alternative to SNLI or a good NLI test set in general; it's only valuable to evaluate a model on a specific ability: whether it learned to perform lexical inferences in context.ßdummyTurker
1606ß0.0ßAs stated in the paper, the Decomopsable Attention model does not update the embeddings, while the ESIM and Residual Encoder models do update the embeddings.ßdummyTurker
1607ß0.0ßWe will add more elaborate model descriptions.ßdummyTurker
1608ß0.0ßWe will include the replacement words’ frequency information in the camera-ready version.ßdummyTurker
1609ß0.0ß1) advantage of adaptive knowledge prediction: Thank you for your comments.ßdummyTurker
1610ß0.0ßFor issue in Section 3.3 The previous methods we referred to are those requiring user-specified parameter K, e.g., Hit@K. The main disadvantage of such methods is that if there are more/less than K real entities in the test set, then some entities will be missed/unnecessarily included.ßdummyTurker
1611ß0.0ßIn contrast, the proposed method uses mean score of true entities as threshold to adaptively decide the number of entities, which makes it superior.ßdummyTurker
1612ß0.0ßCertainly, we will revise Section 3.3 of our manuscript accordingly.ßdummyTurker
1613ß0.0ßFor issue in Section 4.2, Our method is applied to both baselines and our models to ensure a fair comparison in experiment.ßdummyTurker
1614ß0.0ß2) issue in Section 2.3: We acknowledge the reading difficulty caused by the English writing issue.ßdummyTurker
1615ß0.0ßIn response to your concern on user/entity vectors, we explain as follows.ßdummyTurker
1616ß0.0ßUser, entity and relation are all encoded as vectors.ßdummyTurker
1617ß0.0ßDepending on the embedding method used (e.g., TransR), the user/entity vectors are connected by a function of relation vector, e.g., Eq 3.ßdummyTurker
1618ß0.0ßFor ease of reference, we will add a notation table in revision with detailed descriptions for each notation.ßdummyTurker
1619ß0.0ß3) issue in Section 3.2: Regarding the two examples: Fig.ßdummyTurker
1620ß0.0ß2 shows an example of using Conceptual Taxonomy to complete the Factual Knowledge Graph.ßdummyTurker
1621ß0.0ßThe solid back lines indicate known relations, while the dashed red lines indicate the knowledge we wish to predict.ßdummyTurker
1622ß0.0ßThe prediction is made by utilizing the three concepts shared by Google and Facebook.ßdummyTurker
1623ß0.0ßFig.ßdummyTurker
1624ß0.0ß3 shows an example of using information from Factual Knowledge Graph to complete the Conceptual Taxonomy.ßdummyTurker
1625ß0.0ßFor example, we would like to know if “iPhone is an Apple Device” is true or not.ßdummyTurker
1626ß0.0ßThe prediction is made by the three predictor objects pairs extracted from Factual Knowledge Graph.ßdummyTurker
1627ß0.0ß4) train and test sets: We realised the confusion was caused by the terms we used.ßdummyTurker
1628ß0.0ßIn revision we will change the term from “real set” to “test set”.ßdummyTurker
1629ß0.0ßAs usual, the full dataset is random shuffled and then split into training set and test set (9:1 ratio).ßdummyTurker
1630ß0.0ßWe will revise our manuscript accordingly.ßdummyTurker
1631ß0.0ßThe predicted entities could be either shared or existed in only one knowledge base, there is no explicit association.ßdummyTurker
1632ß0.0ßHere we briefly list some key figures.ßdummyTurker
1633ß0.0ßIn training set, 276,574 entities are from Probase and 4,905,374 entities from DBpedia.ßdummyTurker
1634ß0.0ßAmong them, 37,614 entities are shared.ßdummyTurker
1635ß0.0ßIn test set, there are 27,723 entities from Probase and 1,235,481 entities from DBpedia.ßdummyTurker
1636ß0.0ßAmong them, 5,021 entities are shared.ßdummyTurker
1637ß0.0ßThese statistics will be summarised in revision.ßdummyTurker
1638ß0.0ßWe truly thank detailed comments.ßdummyTurker
1639ß0.0ßWe would like to take this opportunity to address your concerns.ßdummyTurker
1640ß0.0ßThe source code of DeepMatch_{tree}  is not publicly available, and when we require the code from the authors, they just told us that it is an internal code of HuaWei Corporation so that they cannot share it.ßdummyTurker
1641ß0.0ßBecause the performance of DeepMatch_{tree} highly relies on a dependency parser and an entity list which the authors do not want to share, we cannot achieve the same performance solely with the description of their paper.ßdummyTurker
1642ß0.0ßTherefore, we test the method on other state-of-the-art methods such as LSTM and obtain expected results.ßdummyTurker
1643ß0.0ßWe expect the same trend on DeepMatch_{tree}, but unless the authors release their code, no one can reproduce their results on the same data.ßdummyTurker
1644ß0.0ßYes, we agree.ßdummyTurker
1645ß0.0ßWe do not aim to handle the ``safe response’’ problem for generation-based methods in this work.ßdummyTurker
1646ß0.0ßOur focus is to learn a better matching model.ßdummyTurker
1647ß0.0ßWhat we can conclude is that iteratively updating a matching model and a generative model does not help so much, but we agree that we cannot fully prove that there is no influence from the safe response problem to the learning of matching models, since policy-gradient is not a good way to improve seq2seq, as pointed out by Reviewer #3.ßdummyTurker
1648ß0.0ßThere are few safe responses in the retrieved candidates.ßdummyTurker
1649ß0.0ßThat might be another reason why our iterative attempt does not work.ßdummyTurker
1650ß0.0ßUsing some techniques in generation literature (e.g., learning with a new objective (Jiwei Li et al.ßdummyTurker
1651ß0.0ßNAACL’16) ) might be helpful, but that is beyond the scope of the paper.ßdummyTurker
1652ß0.0ßWe can expand the conclusion part with extra space.ßdummyTurker
1653ß0.0ßMost existing work focuses on modeling conversations with deep architectures, but little attention is paid to how to learn such architectures in a good way.ßdummyTurker
1654ß0.0ßOur unique contribution is that with the proposed learning method, existing architectures can generally get improved.ßdummyTurker
1655ß0.0ßWe will invite a native speaker to proof-read.ßdummyTurker
1656ß0.0ßWhen l=64, we first enumerate 64 differentßdummyTurker
1657ß0.0ßsegmentations from l-best segmentations both for source and targetßdummyTurker
1658ß0.0ßsentences, and sample *one* segmentation for each according to theßdummyTurker
1659ß0.0ßsegmentation probability.ßdummyTurker
1660ß0.0ßThe NMT training is then performed onlyßdummyTurker
1661ß0.0ßwith this one sampled sentence pair.ßdummyTurker
1662ß0.0ßThis sampling is executedßdummyTurker
1663ß0.0ßin every parameter update of NMT.ßdummyTurker
1664ß0.0ßThe additional process which mayßdummyTurker
1665ß0.0ßslow down the training is this sampling.ßdummyTurker
1666ß0.0ßHowever, sampling cost isßdummyTurker
1667ß0.0ßsmall enough compared to the NMT training.ßdummyTurker
1668ß0.0ßThank you for the suggestion.ßdummyTurker
1669ß0.0ßHere is the preprocessing time on CPUßdummyTurker
1670ß0.0ß(We implemented them in C++.ßdummyTurker
1671ß0.0ßPython wrapper is also available for TF/PyTorch etc.ßdummyTurker
1672ß0.0ß).ßdummyTurker
1673ß0.0ßBPE:                    43k (sentences/sec),ßdummyTurker
1674ß0.0ßUnigram w/o sampling:   42k,ßdummyTurker
1675ß0.0ßUnigram w   sampling:   22k,ßdummyTurker
1676ß0.0ßsampling is two times slower than BPE, but it is still fast enoughßdummyTurker
1677ß0.0ßcompared to NMT training.ßdummyTurker
1678ß0.0ßSubword sampling can be executed in parallelßdummyTurker
1679ß0.0ßon CPU(s) so we could not see any visible speed drop with subword samplingßdummyTurker
1680ß0.0ßin our experiments.ßdummyTurker
1681ß0.0ßWe tried several parameters and algorithms, but we found that they areßdummyTurker
1682ß0.0ßnot sensitive to the final segmentations in terms of the final unigramßdummyTurker
1683ß0.0ßlikelihood optimized with EM algorithm.ßdummyTurker
1684ß0.0ßThus, we selected them ratherßdummyTurker
1685ß0.0ßheuristically in this work.ßdummyTurker
1686ß0.0ßNo, they are tuned with dev set.ßdummyTurker
1687ß0.0ßWe started our experiments withßdummyTurker
1688ß0.0ßIWSLT15 en->vi data and found optimal parameters alpha.ßdummyTurker
1689ß0.0ßIn addition,ßdummyTurker
1690ß0.0ßwe found alpha=0.5 showed better result on KFTT, so used alpha=0.5 forßdummyTurker
1691ß0.0ßlarger data set KFTT/WMT/ASPEC.ßdummyTurker
1692ß0.0ß"Yes, we generate responses by fitting our model to a corpus consisting of human-human conversations, but almost all existing research work does in that way (from the early ``A Neural Conversational Model"" by Oriol Vinyals and Quoc Le from Google to the recent works on Alexa Prize), otherwise one cannot even establish a language model for a chatbot."ßdummyTurker
1693ß0.0ßTo prevent a chatbot from saying inappropriate things, one can equip the model with some post-processing rules (e.g., a black list), but that goes beyond the scope of this research work.ßdummyTurker
1694ß0.0ßWe report Fleiss' kappa (httpsßdummyTurker
1695ß0.0ßdialogue act classification is not a trivial task, because the language of social conversation is noisy and in a very casual form, not to mention we also have to deal with sequential relationship among utterances.ßdummyTurker
1696ß0.0ßConsider the case raised by you, even though you ignore all other acts, it is not easy to achieve an accuracy of 67.5%, because that means you have to correctly classify all CM.S and CM.Q data (a binary classifier with 100% accuracy).ßdummyTurker
1697ß0.0ßIn fact, differentiating questions from statements is not trivial.ßdummyTurker
1698ß0.0ßQuestions can be very short without a question word.ßdummyTurker
1699ß0.0ß"For example, ""your name?"""ßdummyTurker
1700ß0.0ß(=what is your name?).ßdummyTurker
1701ß0.0ßPeople often omit question marks in their posts on social media.ßdummyTurker
1702ß0.0ß"For example, ""you don't know what my job is, right"" (a question for confirmation."ßdummyTurker
1703ß0.0ßMore seriously, without ``right'', it becomes a statement).ßdummyTurker
1704ß0.0ßIn that case, how can you easily and correctly classify all data with CM.S and CM.Q?ßdummyTurker
1705ß0.0ßYou can only achieve an accuracy of 55.8% by simply classifying all data to CM.S, but that does not make any sense to our work.ßdummyTurker
1706ß0.0ßIn fact, although not directly comparable, in a recent paper published on NAACL'16 (httpsßdummyTurker
1707ß0.0ßWe can improve the readability (e.g., by giving more details) with extra space.ßdummyTurker
1708ß0.0ßKnowledge-directed attention + domain specific word embeddings lead to quite an improvement (note that the numbers in Table 6 are relative to the used embeddings, so the actual improvement over the baseline is larger).ßdummyTurker
1709ß0.0ßNevertheless, it is impossible to know in advance what will help and what will not.ßdummyTurker
1710ß0.0ßGiven that MedNLI is a new dataset, we feel it is important to provide the research community with all results, regardless the size of the improvements.ßdummyTurker
1711ß0.0ßThis clearly shows what works well, what works to a less extent, and what does not work on this new dataset.ßdummyTurker
1712ß0.0ßOne of the surprising results that we got is that, in contrast to SNLI, the feature based system does not perform well on MedNLI.ßdummyTurker
1713ß0.0ßWe also observe that learnt knowledge (through domain specific embeddings) yields larger gains than explicit human curated knowledge (UMLS).ßdummyTurker
1714ß0.0ßCombination of knowledge-directed attention and domain-specific word embeddings lead to better results indeed.ßdummyTurker
1715ß0.0ßWe report it in the Table 6.ßdummyTurker
1716ß0.0ßBesides discussion of retrofitting, we actually have more error analysis that we were not able to include due to space constraints (ie handling of nagations and use of knowledge-directed attention).ßdummyTurker
1717ß0.0ßGiven an extra page, we will add additional error analysis in the final version of the paper.ßdummyTurker
1718ß0.0ßIf providing additional details requested in the review can address the concerns, can you please consider changing your assessment?ßdummyTurker
1719ß0.0ßScore 1 is assigned only in very particular cases and it needs to be strongly supported especially if it is completely not aligned to the other reviews.ßdummyTurker
1720ß0.0ß"For these reasons, we think that this review is ""almost"" unacceptable and not respectful of the work presented in this paper."ßdummyTurker
1721ß0.0ß- We subsampled the dataset only for the model training.ßdummyTurker
1722ß0.0ß- Final benchmark consists of all documents of the entire dataset(Sec.3.9), users can decide whether or not to include years before 2000.ßdummyTurker
1723ß0.0ß- Experiments with the full-dataset and further analysis showed, that the dataset`s skewed distribution (years before 2000 have less than 1000 documents per year) can cause corrupted results in our approach (linear regression for slope computation).ßdummyTurker
1724ß0.0ß- 10,000 documents was the lowest common number of documents per year after the year 2000.ßdummyTurker
1725ß0.0ß(Sec.3.1)ßdummyTurker
1726ß0.0ß- This work's main focus is construction of a reliable benchmark for (anti-)trend detection, not a system by itself.ßdummyTurker
1727ß0.0ß- Experiments showed that a number of clusters(1000) of a couple magnitudes lower than the sample size(160,000) yielded coherent clusters, which aren`t too detailed while also not too general.ßdummyTurker
1728ß0.0ß- We conducted experiments on determining the best number of top clusters(150).ßdummyTurker
1729ß0.0ßDue to the paper`s space constraints , we excluded a detailed description.ßdummyTurker
1730ß0.0ßAccording to experiments, after 150top clusters the number of newly founded trends didn`t increase a lot/stayed stable(dependent on the abstract or full-text collection).ßdummyTurker
1731ß0.0ß- According to Sec.3.5/Sec.3.6, annotators were asked to annotate the 217(anti-)trend clusters.ßdummyTurker
1732ß0.0ßThey were provided with ten randomly picked document-titles representing a cluster and 15 most frequent (for the given cluster) key-words (key-words were provided by the authors;after the abstract in original papers).ßdummyTurker
1733ß0.0ßThey were also provided with a list of all (anti-)trend labels.ßdummyTurker
1734ß0.0ß- Task: To pick from the list with labels an item that matches the cluster best.ßdummyTurker
1735ß0.0ß- Only few clusters didn`t achieved a majority after three annotations.ßdummyTurker
1736ß0.0ßAfter the only one reevaluation, the majority was achieved.ßdummyTurker
1737ß0.0ßWe didn`t observe any influence of this reevaluation on the entire annotation process.ßdummyTurker
1738ß0.0ßTo ensure the quality of the annotation we computed an inter-annotator agreement with: Krippendorf`s alpha & CrowdFlower`s agreement rate(Sec.3.7).ßdummyTurker
1739ß0.0ßBoth approaches confirmed a moderate to high annotation quality.ßdummyTurker
1740ß0.0ßWe agree with the reviewer and will make this change should the paper be accepted.ßdummyTurker
1741ß0.0ßThere are 3 types of users in our setting.ßdummyTurker
1742ß0.0ßThe same user can play more than one role.ßdummyTurker
1743ß0.0ß1.ßdummyTurker
1744ß0.0ßThe submission author - this user (a.k.a OP) posts a topic for discussion (these users are not the focus of our research)ßdummyTurker
1745ß0.0ß2.ßdummyTurker
1746ß0.0ßThe argument author - this user writes arguments in order to change the view of other usersßdummyTurker
1747ß0.0ß3.ßdummyTurker
1748ß0.0ßThe delta award reader - this user marks an argument with a delta award, indicating that the user has changed his view because of the argumentßdummyTurker
1749ß0.0ßIn our setting, we are looking on pairs of <argument author> - <delta reader>, and the interaction between their personalities.ßdummyTurker
1750ß0.0ßWe started with data that contains 50K topics (each topic is a thread of arguments), produced by 22K unique submission authors.ßdummyTurker
1751ß0.0ßThe 50K threads have a total amount of 480K arguments, produced by 130K unique argument authors, out of which 56K has a profile (i.e., wrote enough arguments so we can extract a profile -  the Personality Insights service requires around 3000 words of input (https://console.bluemix.net/docs/services/personality-insights/input.html#sufficient).ßdummyTurker
1752ß0.0ß"As we are looking on threads that a ""direct argument"" (directly replying to the submission) is awarded a delta, we basically have a subset of the data which contains around 17K submissions, produced by 11K submission authors."ßdummyTurker
1753ß0.0ßThe 17K threads have a total of 27K direct arguments produced by 10K unique argument authors, and include 14K arguments that are awarded with delta, by 7.5K unique delta award readers.ßdummyTurker
1754ß0.0ßFor the negative examples, we randomly select a user from the argument authors that have a profile (i.e., they created enough content (arguments) so we can build a profile for them).ßdummyTurker
1755ß0.0ßThe argument authors might also be submission authors and/or delta awarded readers.ßdummyTurker
1756ß0.0ßThe random arguments are all direct arguments that didn't receive a delta award.ßdummyTurker
1757ß0.0ßThis way we have a balanced dataset for training.ßdummyTurker
1758ß0.0ßThe issue of interaction between personalities is well known in the domain of call center, where companies like https://www.mattersight.com/ match between the customer personality to the service representative agent.ßdummyTurker
1759ß0.0ßIn our paper we wanted to study this interaction for the new domain of persuasive messages.ßdummyTurker
1760ß0.0ßNovelty.ßdummyTurker
1761ß0.0ßThere are several things that are novel about the paper:ßdummyTurker
1762ß0.0ßa.ßdummyTurker
1763ß0.0ßThis is first of its kind multi-dialectal Arabic POS tagger.ßdummyTurker
1764ß0.0ßSuch a combined system has eluded folks who worked on Arabic for such a long time that the only existing dialectal Arabic taggers are mono-dialectal.ßdummyTurker
1765ß0.0ßb.ßdummyTurker
1766ß0.0ßThough individual features may not be novel, actually figuring out which features work and in what combination is novel.ßdummyTurker
1767ß0.0ßFor example, the use of automatic dialect ID as a feature is non-obvious particularly that reported dialect ID system accuracies in the system are rather low.ßdummyTurker
1768ß0.0ßc.     We also show how neural nets can benefit from additional layers of information, such as Brown Clustering, metatypes and stem templates.ßdummyTurker
1769ß0.0ßComparison to other systems.ßdummyTurker
1770ß0.0ßExisting dialectal Arabic POS tagging is very limited.ßdummyTurker
1771ß0.0ßThus we are comparing to what is available.ßdummyTurker
1772ß0.0ßMoreover, we are creating new dataset and reporting benchmark results.ßdummyTurker
1773ß0.0ßBoth training and testing were performed on the original texts without any correction with pre-processing of URLs, hashtags, etc.ßdummyTurker
1774ß0.0ßIn fact the results are not too poor, given that we are using a harsh metric for evaluation, that is word-level (not token level) accuracy.ßdummyTurker
1775ß0.0ßA word in Arabic typically consists of more than one token.ßdummyTurker
1776ß0.0ßAddressed above in Weakness argument 1ßdummyTurker
1777ß0.0ßThe data is too small.ßdummyTurker
1778ß0.0ßPart of the contribution of the paper is that we show that you can develop a very reasonable POS tagger with limited data.ßdummyTurker
1779ß0.0ßFor low-resourced languages and language varieties, this technique can be helpful in bootstrapping new data and accelerate the annotation process when needed.ßdummyTurker
1780ß0.0ßThe variations within the dialects.ßdummyTurker
1781ß0.0ßWe agree with the reviewer that there exists considerable variations within the dialect group.ßdummyTurker
1782ß0.0ßHowever, we assume that regional dialects share some characteristics.ßdummyTurker
1783ß0.0ßA major part of the contribution of the paper is that the tagger can handle different dialects without the need to develop dialect specific taggers and that dialects can learn from each other.ßdummyTurker
1784ß0.0ßNonetheless, It will be worthwhile investigating the difference between neighboring dialects.ßdummyTurker
1785ß0.0ßIn addition,ßdummyTurker
1786ß0.0ßWe also show how neural nets can benefit from additional layers of information, such as Brown Clustering, metatypes and stem templates.ßdummyTurker
1787ß0.0ßThis is first of its kind multi-dialectal Arabic POS tagger,  existing dialectal Arabic taggers are mono-dialectal.ßdummyTurker
1788ß0.0ßhough individual features may not be novel, actually figuring out which features work and in what combination is novel.ßdummyTurker
1789ß0.0ßFor example, the use of automatic dialect ID as a feature is non-obvious particularly that reported dialect ID system accuracies in the system are rather low.ßdummyTurker
1790ß0.0ßNEG_PART and PROG_PART.ßdummyTurker
1791ß0.0ßWe beg to differ with reviewer.ßdummyTurker
1792ß0.0ßThese tags appear in multiple dialects of Arabic.ßdummyTurker
1793ß0.0ßFor example, PROG_PART is expressed as “b” in Egyptian and Levantine, as “k” in Maghrebi, and as “d” in Iraqi.ßdummyTurker
1794ß0.0ßThe same thing goes for NEG_PART.ßdummyTurker
1795ß0.0ßWe used 350 tweets per dialect because we don’t have a larger annotated corpus.ßdummyTurker
1796ß0.0ßTo properly tag a mere 350 tweets for 5 dialects required roughly 3 person-months.ßdummyTurker
1797ß0.0ßFor low-resourced languages and language varieties, this technique can be helpful in bootstrapping new data and accelerate the annotation process when needed.ßdummyTurker
1798ß0.0ßThe tweets we used to train the Brown Clusters, they were unannotated.ßdummyTurker
1799ß0.0ßJust to confirm, we looked at all the dialects.ßdummyTurker
1800ß0.0ßAll the tags that we used were used in multiple dialects.ßdummyTurker
1801ß0.0ßWe used a small set of MSA data was to primarily compare MSA to other dialects.ßdummyTurker
1802ß0.0ßIt is not possible for us to retrain MADAMIRA.ßdummyTurker
1803ß0.0ßWe thank the reviewer for the encouraging review.ßdummyTurker
1804ß0.0ßThank you so much.ßdummyTurker
1805ß0.0ßConsider the rather large size of test set, the improvement at the level of 1% is stable and reliable, compared to improvements reported in many previous work.ßdummyTurker
1806ß0.0ßOther than that, statistical significance is provided additionally in our paper.ßdummyTurker
1807ß0.0ßWe agree with the review.ßdummyTurker
1808ß0.0ßOne main reason is that the supplementary annotations provided by MultiNLI dataset (Williams et al, 2017) only have 495 samples for both in-domain and out-domain set.ßdummyTurker
1809ß0.0ßIt is a small subset of development set (about 1/20 of the entire development set).ßdummyTurker
1810ß0.0ßWe agree that it is better to be used for qualitative analysis and rendering insights during analysis.ßdummyTurker
1811ß0.0ßWe provide the results here as if future systems want to compare to our results on these categories, they have the reference.ßdummyTurker
1812ß0.0ßAgain, KIM out-performs ESIM on the overall accuracies on both in-domain and cross-domain subset of development set.ßdummyTurker
1813ß0.0ßWe will take advantage of the one extra page in revision to improve the clarity that the reviewer kindly pointed out, including the 5 detailed suggestions.ßdummyTurker
1814ß0.0ßThank you!ßdummyTurker
1815ß0.0ß“In 3.3, the…co-attention level?”ßdummyTurker
1816ß0.0ßWe did add a two-layer MLP as universal function approximator to learn the underlying combination function but did not observe further improvement over the best performance.ßdummyTurker
1817ß0.0ßWe will include the results in revision with a short discussion.ßdummyTurker
1818ß0.0ß“In Section 3.4…me much.”ßdummyTurker
1819ß0.0ßYes, your understanding is correct.ßdummyTurker
1820ß0.0ßa_i^m, b_i^m are used to collect local inference information (i.e., word-level inference).ßdummyTurker
1821ß0.0ßTo achieve this, we need to align the words between premise and hypothesis.ßdummyTurker
1822ß0.0ßThus, a^s_i is individual word representation in premise and a^c_i is context representation from hypothesis which is align to word a^s_i.ßdummyTurker
1823ß0.0ßThrough concatenating a^s_i and a^c_i, in addition to their difference, product, and relation for external knowledge, we can obtain word-level inference information for each words.ßdummyTurker
1824ß0.0ßWe will make this clearer in revision.ßdummyTurker
1825ß0.0ß“Also the…are relating to?”ßdummyTurker
1826ß0.0ßWe will only discuss a_i^m in equation 7.ßdummyTurker
1827ß0.0ßThe last term in equation 7 is used to obtain word-level inference information from external knowledge.ßdummyTurker
1828ß0.0ßr_ij is the relation between i-th word in premise and j-th word in hypothesis, but we only care about relation between aligned word pairs between premise and hypothesis.ßdummyTurker
1829ß0.0ßThus, we use a soft-aligned version through soft-alignment weight \alpha_ij.ßdummyTurker
1830ß0.0ßFor i-th word in premise, the last term is a word-level inference information based on external knowledge between i-th word and the aligned word.ßdummyTurker
1831ß0.0ßWordNet is among the most widely used resource that naturally encodes entailment and contradiction information (e.g., “is-a” and “sibling” relation between word pairs can help resolve entailment and contradiction, respectively).ßdummyTurker
1832ß0.0ßParticularly, considering how NLI data is constructed (e.g., SNLI relies on annotators’ common sense to write entailment and contradiction sentences), we focus on WordNet in understanding how to enhance on three typical NN-based NLI components.ßdummyTurker
1833ß0.0ßFurthermore, we indeed incorporated WikiData (Freebase) and it did not improve model performance (it is not surprising as most of WikiData is about entities and relations (e.g.ßdummyTurker
1834ß0.0ßBill Gates and Microsoft) which do not correspond to common entailment/contradiction relation (e.g., red/yellow is contradicting).ßdummyTurker
1835ß0.0ßThank you for the comment.ßdummyTurker
1836ß0.0ßWe will add discussion on this in our revision.ßdummyTurker
1837ß0.0ßThe reviewer is right on that the focus of this paper is not on proposing a striking new machine learning architecture.ßdummyTurker
1838ß0.0ßWe are seriously caring about a basic questionßdummyTurker
1839ß0.0ßThank you so much.ßdummyTurker
1840ß0.0ßWe will reorganize the suggested content to distinguish our contributions as discussed above.ßdummyTurker
1841ß0.0ßRefer to the reply above.ßdummyTurker
1842ß0.0ßThank you so much for this very constructive comments.ßdummyTurker
1843ß0.0ßWe will cite these papers and note the unique role of inference as a very hard testbed for continuous, distributed space-based approaches.ßdummyTurker
1844ß0.0ßWeakness argument 1ßdummyTurker
1845ß0.0ßThe original DRNN model by Alvarez-Melis and Jaakkola (2017) was only applied to a limited NMT task.ßdummyTurker
1846ß0.0ßAlso the original model distinguishes between left and right children which we found does not work as well as our modification for NMT, also it operates on word level.ßdummyTurker
1847ß0.0ßAs pointed out elsewhere, our syntactic language model is tied into the DRNN and is entirely novel.ßdummyTurker
1848ß0.0ßThe original DRNN also does not have an attention mechanism.ßdummyTurker
1849ß0.0ßOur NMT model is more comprehensive, has less parameters while being NMT focused, has attention over tree structures (which is missing from previous syntactic NMT models), and has SynLM.ßdummyTurker
1850ß0.0ßWeakness argument 2ßdummyTurker
1851ß0.0ßThe IWSLT2017 proceeding is using existing highly optimised NMT toolkits with many methods to further improve the results.ßdummyTurker
1852ß0.0ßThe settings are not comparable to ours: the most common is around 1024+ for hidden states, 512+ for input, and the batch size is 128.ßdummyTurker
1853ß0.0ßPrzybysz et al., 2017 are using 3 servers with 8GPUs (1080 Ti and Tesla K80) each, while we only have a workstation with a singe 1080 Ti.ßdummyTurker
1854ß0.0ßIn an ACL paper, do the ideas matter or the size of the GPUs?ßdummyTurker
1855ß0.0ßOur Seq2DRNN+LM is using constituency information, the size of the training data alone can be 4 times larger than Seq2Seq (the amount of clauses and their types, and the data structure to store these information).ßdummyTurker
1856ß0.0ßThe best we can do with our hardware is 256 for both input and hidden dimension and mini-batch of size 64.ßdummyTurker
1857ß0.0ßWe found that the number of layers in the decoder did not make much difference for our Seq2DRNN+LM model, so we lowered the number of layers in the decoder in order to run more experiments.ßdummyTurker
1858ß0.0ßBPE splitting for constituency information would mess up the syntactic structure, since we cannot assign tags to sub-word units.ßdummyTurker
1859ß0.0ßDespite this, we ran the experiment and found the improvement to be insignificant.ßdummyTurker
1860ß0.0ßWe do not claim state-of-the-art results.ßdummyTurker
1861ß0.0ßOur goal is rather to demonstrate the effectiveness of our model.ßdummyTurker
1862ß0.0ßWeakness argument 3ßdummyTurker
1863ß0.0ßWe have conducted experiments on three extra pairs of dataset: CS-EN, RU-EN from News Commentary V8, and JP-EN from ASPEC corpus.ßdummyTurker
1864ß0.0ßOur Seq2DRNN+LM model is able to achieve better results than Seq2Seq and we beat Eriguchi et al on CS-EN and RU-EN.ßdummyTurker
1865ß0.0ßWe will add these to the final version.ßdummyTurker
1866ß0.0ß1.Thank you for writing advice to the paper.ßdummyTurker
1867ß0.0ßWe will improve the mathematical parts accordingly.ßdummyTurker
1868ß0.0ß2.Good point!ßdummyTurker
1869ß0.0ßWe empirically verified the relationship between the smoothness of the probability sequence and the BLEU in the paper, which is also the motivation of this work.ßdummyTurker
1870ß0.0ßWe leave theoretical analysis to future work as mentioned in §6.ßdummyTurker
1871ß0.0ß1.Beam search is independent to smoothness.ßdummyTurker
1872ß0.0ßIt could lead to either smooth sequences or non-smooth sequences.ßdummyTurker
1873ß0.0ßFor beam size 2, consider 3 partial sequences with log probabilities P1=[-0.1, -0.8, -0.5], P2=[-0.1, -0.9, -0.1], and P3=[-0.1, -0.9, -0.2].ßdummyTurker
1874ß0.0ßClearly, the last two sequences will be selected by beam search, but are not as smooth as the first sequence.ßdummyTurker
1875ß0.0ß2.The corpus-BLEU is computed in the same way as in §4 by using moses script.ßdummyTurker
1876ß0.0ßWe separate sentences by MLE loss to clearly demonstrate the relationship between the smoothness and the BLEU, since the comparison of probability variance is meaningless if sentences' MLE loss (equal to the average of negative log(p)) aren’t in similar value.ßdummyTurker
1877ß0.0ß3.We tuned lambda and k according to the validation set.ßdummyTurker
1878ß0.0ßIn Table 4, we show the results of different lambda's.ßdummyTurker
1879ß0.0ßBecause of the limitation of space, we exclude the results of different k, and would add it in new version.ßdummyTurker
1880ß0.0ßTwo Principles:We will make all statement about the two principles clearer: Generally speaking, if we focus on optimizing the “easy” tokens during training, it will bring little gain during inference as the “hard” tokens are ignored to be optimized and thus the translation result can be bad.ßdummyTurker
1881ß0.0ßThis is the motivation about the two principles.ßdummyTurker
1882ß0.0ßSome curriculum methods share similar motivations.ßdummyTurker
1883ß0.0ßMLE proof:We will add the proof about MLE in the next version.ßdummyTurker
1884ß0.0ßTerminology:(1)The definition of smoothness obeys the definition in §3.2.ßdummyTurker
1885ß0.0ßIn §1, in order to show the relationship between smoothness and translation quality easily understandable for the readers in the beginning of the paper, we simply use variance to replace smoothness.ßdummyTurker
1886ß0.0ßWe will footnote the difference in the next version.ßdummyTurker
1887ß0.0ß(2)We have typos for sentence “the smoothness of a loss function” in §6.ßdummyTurker
1888ß0.0ßIt should be “the smoothness of token probabilities”.ßdummyTurker
1889ß0.0ßWe will correct it in later version.ßdummyTurker
1890ß0.0ßSignificance:For Table 3, actually we’ve done the significant test and the improvements are significant where all p-value<0.05.ßdummyTurker
1891ß0.0ßWe will add significance test results in new version.ßdummyTurker
1892ß0.0ßTraining set:In Figure 2 and Table 3, we aim to show that our defined loss indeed leads to smoother token probabilities after training.ßdummyTurker
1893ß0.0ßThanks for the suggestion!ßdummyTurker
1894ß0.0ßWe will add the results in test set in later version.ßdummyTurker
1895ß0.0ßLow-probability tokens:We have empirical results that the majority of the low-probability tokens are rare words, polysemous words and function words.ßdummyTurker
1896ß0.0ßIt's hard to generate these low-probability tokens during inference for MLE, which is also found in previous works [1].ßdummyTurker
1897ß0.0ßFor example, the ratio of rare words on the reference sentence on IWSLT2014 German-to-English test set is 2.0%.ßdummyTurker
1898ß0.0ß(We define rare words as the 75% tokens with lower frequency in training set vocabulary) However, for the sentence generated by the MLE baseline, the ratio is just 0.8%.ßdummyTurker
1899ß0.0ßOur method can increase the ratio to 1.1%.ßdummyTurker
1900ß0.0ßAs the result is similar to that in case study, we exclude these analyses now and would add it in later version.ßdummyTurker
1901ß0.0ßLabel smoothing:We will move label smoothing to the experiment section.ßdummyTurker
1902ß0.0ß[1] Ott M, Auli M, Granger D, Ranzato MA.ßdummyTurker
1903ß0.0ßAnalyzing Uncertainty in Neural Machine Translation.ßdummyTurker
1904ß0.0ßarXiv:1803.00047.ßdummyTurker
1905ß0.0ß"Reviewer 1 rightly expressed a concern that the task might ""encourage subjects to choose something versus nothing, perhaps leading to unnatural sounding sentences""."ßdummyTurker
1906ß0.0ß"With respect to ""choosing something versus nothing"",  we gave subjects the option of choosing no conjunction ""NONE""), an option that some subjects chose for some passages."ßdummyTurker
1907ß0.0ß"With respect to ""unnatural sounding sentences"", in a previous experiment, we  attempted to separate out the conjunction insertion task (as an index of the participant's inferred relation) from a separate would-you-say-it-this-way question (to reduce the risk that participants' choices would reflect what they thought sounded best on the surface rather than what relation they were inferring)."ßdummyTurker
1908ß0.0ßWe have mentioned both issues in previous publications, but if there is space, we can mention some of the participants' more interesting behaviors here.ßdummyTurker
1909ß0.0ßThe reviewer is correct that we omitted a Related Work section and also that other researchers have noted the need for multiple discourrse relations.ßdummyTurker
1910ß0.0ßWe will add a Related Work section, and use it to detail the several quite different things that researchers have meant when they talk about multiple relations.ßdummyTurker
1911ß0.0ß(For example, other research has advocated allowing multiple sense labels in cases where one label is more specific than another, or where the relation associated with one sense label operates at an event level and another at a rhetorical level.)ßdummyTurker
1912ß0.0ßWe will also add references to the Penn Discourse TreeBank, (PDTB) which has been annotated with multiple concurrent relations [Prasad et al, 2008; Prasad et al, 2012; Webber et al, 2016].ßdummyTurker
1913ß0.0ßWe will comment on potential impacts for discourse parsing, as well on a recent attempt to take concurrent relations into account in discourse parsing [Pyatkin, 2016].ßdummyTurker
1914ß0.0ß(Other work on discourse parsing based on the PDTB has ignored its annotation of multiple concurrent relations in both training and testing.)ßdummyTurker
1915ß0.0ßWe will include statistical tests as to the significance of our results, even thoughßdummyTurker
1916ß0.0ßwe hope it is clear that the patterns in the bar charts are very unlikely to have arisen by chance.ßdummyTurker
1917ß0.0ß"As for our choices of conjunctions and adverbials, we included all coordinating conjunctions in English, plus the subordinating conjunction ""because"" since EXPLANATION relations hold so commonly in discourse."ßdummyTurker
1918ß0.0ßAs for adverbials, we chose ones that our earlier studies had shownßdummyTurker
1919ß0.0ßsubjects to split on in interesting ways.ßdummyTurker
1920ß0.0ß"(This is in contrast to adverbials like ""therefore""  and ""nevertheless"", where subjects mainly split between more specific ""so"" and less specific  ""and"" (with ""therefore"") and more specific ""but"" and less specific ""and"" (with ""nevertheless"").)"ßdummyTurker
1921ß0.0ßSee Reviewer1/#1,2ßdummyTurker
1922ß0.0ßSee Reviewer1/#3,4ßdummyTurker
1923ß0.0ßConsider it this way: if you were dropped into a community of speakers of an unknown language, and had access to only one manuscript of their language and one bilingual annotator who could annotate morphological analyses for only N amounts of it, given your goal was to build the best inflection system under these constraints, should you choose a rule-based or neural or semi-supervised system?ßdummyTurker
1924ß0.0ßThis is the basis of our experimental comparison.ßdummyTurker
1925ß0.0ßBut FST is type-level, and “NN” is somewhat in-between: because it ignores the hallucinations of LM on unlabeled data (the contextually important part of the model), it is equivalent to a type-level model where types are sampled according to their relative frequency in the corpus.ßdummyTurker
1926ß0.0ßIt is also approximately state-of-the-art in this task at type-level, as evaluated on the CoNLL 2009 shared.ßdummyTurker
1927ß0.0ßNote that the factorization of the model (Eq 1.)ßdummyTurker
1928ß0.0ßyields an inflector that depends only on a morph-tag and lemma pair at time i, conditionally independent of what happens elsewhere in the sequence.ßdummyTurker
1929ß0.0ßThis means that for evaluating how well our model learns the inflection task, we can reduce a token-level resource, here the UD Treebank data, to type-level by taking unique <form, lemma, tag> triples as we describe in 7.3 (the context thus matters only for training).ßdummyTurker
1930ß0.0ßThat does interestingly open the door for token-level inflection evaluation, which our model can naturally perform, but we don’t know of any adequate baselines to compare against.ßdummyTurker
1931ß0.0ßCoNLL shared task data was mentioned by a reviewer, to clarify: we use the baseline system from that task, but not the data.ßdummyTurker
1932ß0.0ßThe data set is entirely partitions of UD, as stated at the start of Sec.ßdummyTurker
1933ß0.0ß7.2.ßdummyTurker
1934ß0.0ßThank you for the helpful review.ßdummyTurker
1935ß0.0ßWe are confident that we can address your comments in a final version if the paper gets accepted - in particular as the additional page can be exploited for the final version.ßdummyTurker
1936ß0.0ßWe hope we can convince you with our replies to think about increasing the overall score and to vote for accepting the paper.ßdummyTurker
1937ß0.0ßWe performed evaluations on existing NED corpora but did not report the results due to space constraints and because standard NED corpora are all very time homogeneous, so that improvements are limited.ßdummyTurker
1938ß0.0ßNevertheless, we achieve slight improvements over the baselines.ßdummyTurker
1939ß0.0ßWe would be happy to report the results on the additional page that is given to address the reviewers' comments in the final version.ßdummyTurker
1940ß0.0ßThis is correct.ßdummyTurker
1941ß0.0ßThe motivation of using a decay function and not a symmetric function is that entities must have existed at the point in time of the temporal context of a mention and we assume that the temporal KB information (i.e., the entities signature) starts (at the latest) with the existence of the entity (sometimes even earlier for popular entities which report, in the case of persons, on parents etc., and pre-entity history).ßdummyTurker
1942ß0.0ßWe tried to cover most important recent works on NED.ßdummyTurker
1943ß0.0ßUsing the additional space for the final version, we would be happy to add a further paragraph with more details as suggested in the review.ßdummyTurker
1944ß0.0ßIt's not huge but, we think, large enough to demonstrate the importance of using temporal features for NED.ßdummyTurker
1945ß0.0ßWe are planning large-scale experiments with distant supervision data in the future and mention this in the future work part.ßdummyTurker
1946ß0.0ßThe temporal context of entity mentions when training on CoNLL-AIDA were the respective document creation times (on year granularity).ßdummyTurker
1947ß0.0ßUsing normalized temporal expressions from the texts resulted in almost no further information due to the nature of news-style articles which often do not cover broad time intervals.ßdummyTurker
1948ß0.0ßsee reply to argument 1 --> we are happy to report the results in the final versionßdummyTurker
1949ß0.0ßThank you for the helpful review.ßdummyTurker
1950ß0.0ßWe are confident that we can address your comments in a final version if the paper gets accepted.ßdummyTurker
1951ß0.0ßWe hope we can convince you with our replies to think about increasing the overall score and to vote for accepting the paper.ßdummyTurker
1952ß0.0ßCreating new datasets is labor- and time-consuming.ßdummyTurker
1953ß0.0ßWe added two sources and think that the corpus is large enough to show the importance of temporal information for NED.ßdummyTurker
1954ß0.0ßWe are planning large-scale experiments with distant supervision data in the future and mention this in the future work part.ßdummyTurker
1955ß0.0ßWe are happy to use the addition page to provide further details about the annotation process in the final version.ßdummyTurker
1956ß0.0ßDue to space constraints, a detailed error analysis is almost impossible to be reported in a short paper.ßdummyTurker
1957ß0.0ßBut we hope to be able to include a paragraph about most important findings using the additional page for the final version.ßdummyTurker
1958ß0.0ßwe used Welch’s t-test and will add this info.ßdummyTurker
1959ß0.0ßOf course, the annotations will be made available.ßdummyTurker
1960ß0.0ßSorry for the trouble with accessing the link.ßdummyTurker
1961ß0.0ßWe checked the link regularly during the review phase and did not experience any issues.ßdummyTurker
1962ß0.0ßCould you please try again?ßdummyTurker
1963ß0.0ßwww.dropbox.com/s/nly47gf77igtcw9/timeNED.zip?dl=0ßdummyTurker
1964ß0.0ßWe have actually conducted an experiment using SVM for our Indonesian NER.ßdummyTurker
1965ß0.0ßSVM is the most frequently used machine learning algorithm in many Indonesian NER researches.ßdummyTurker
1966ß0.0ßMany experiments showed that a deep-learning-based Indonesian NER could outperform an SVM-based one.ßdummyTurker
1967ß0.0ßShould the manuscript be accepted, we will add our experimental result on SVM-based Indonesian NER.ßdummyTurker
1968ß0.0ßWe regret that we have not further analyzed the effect of PoS feature and output layer for each NE type.ßdummyTurker
1969ß0.0ßShould the manuscript be accepted, we will add further explanation about the effect of PoS feature and output layer for each NE type.ßdummyTurker
1970ß0.0ßWe appreciate the valuable feedback the reviewer give.ßdummyTurker
1971ß0.0ßWe could not give too much explanation about the corpus because of the page limitation.ßdummyTurker
1972ß0.0ßShould the manuscript be accepted and need revision, we are pleased to provide further explanation about the training and testing corpus.ßdummyTurker
1973ß0.0ßAs long as the writers know, our research is the first that empirically compares the softmax and conditional random field (CRF) layer.ßdummyTurker
1974ß0.0ßWe are also the first that use part-of-speech (PoS) as one of the input features for recognizing named entities (NEs).ßdummyTurker
1975ß0.0ßWe have added some sentences to the Abstract (page 1 line 27-30) to clarify those.ßdummyTurker
1976ß0.0ßWhile Lample (2016) only mentioned that CRF is better than softmax for NE tagging, we empirically prove that CRF outperform softmax albeit only a little, supporting their explanation.ßdummyTurker
1977ß0.0ßHowever, adding PoS as the input feature can even make the softmax slightly outperform the CRF.ßdummyTurker
1978ß0.0ßWe have performed the experiment using around 3000, 6000, and exactly 8400 sentences.ßdummyTurker
1979ß0.0ßWhile the performance increased significantly when the training data was increased from 3000 to 6000 sentences, the performance increase was not significant when the data was increased from 6000 to 8400 sentences.ßdummyTurker
1980ß0.0ßTherefore we assumed that 8400 sentences were sufficient for our experiment.ßdummyTurker
1981ß0.0ßWe regret that some of our data and code are intellectual property that belongs to a company.ßdummyTurker
1982ß0.0ßTherefore, we cannot release them as an open-source software.ßdummyTurker
1983ß0.0ßThe early detection experiment tested in certain intervals has been implemented and the result is shown in Figure 4.ßdummyTurker
1984ß0.0ßIn this figure, using different percentages of repost information means using information within certain time interval, because repost information is sorted by time.ßdummyTurker
1985ß0.0ßThe improvement obtained by RNN-based rumor detection is significant from this figure.ßdummyTurker
1986ß0.0ßOur early rate result shown in Table 2 is the average of the early rate for each sample in the test set, because our CED method can detect an Credible Detection Point for each test sample.ßdummyTurker
1987ß0.0ßThat is also the reason why ablation experiment cannot be implemented.ßdummyTurker
1988ß0.0ßBaseline methods cannot employ “Early Detection Point” to decide how much repost information every microblog needs to use.ßdummyTurker
1989ß0.0ßThanks for your suggestion.ßdummyTurker
1990ß0.0ßActually, only using O_{pred} and O_{time} violates our original intention to keep the prediction probabilities after Credible Detection Point stable.ßdummyTurker
1991ß0.0ßBut we will conduct ablation test to show the gain of each component and various loss functions.ßdummyTurker
1992ß0.0ßThanks for pointing it out.ßdummyTurker
1993ß0.0ßEq.ßdummyTurker
1994ß0.0ß7 is the decision function for the testing set, because we need to make a decision before the last repost.ßdummyTurker
1995ß0.0ßFor the real-world application, we will take your suggestion and state it in revision.ßdummyTurker
1996ß0.0ßSorry for the mistake.ßdummyTurker
1997ß0.0ßActually, \beta is not trainable.ßdummyTurker
1998ß0.0ßIt is determined by \alpha in our model.ßdummyTurker
1999ß0.0ßWe will correct this mistake in revision.ßdummyTurker
2000ß0.0ßThanks for this instructive idea.ßdummyTurker
2001ß0.0ßWe will follow it to test the gain of RNN.ßdummyTurker
2002ß0.0ßResults will be shown in the next version.ßdummyTurker
2003ß0.0ßIn addition to false positive cases, we also considered false negative cases, which are detected as non-rumors but finally judged as rumors.ßdummyTurker
2004ß0.0ßThe number of such inversion cases in Weibo testing set is 121(11.52%).The case study shown in section 4.7 is one of inversion cases.ßdummyTurker
2005ß0.0ßThe fact that these examples are finally detected correctly reflects the necessity and rationality of the Critical Detection Point.ßdummyTurker
2006ß0.0ßAs you suggested, we will give an example in revision.ßdummyTurker
2007ß0.0ßYes, it can scale to large number of entities.ßdummyTurker
2008ß0.0ßActually, the size of entities mainly influences the vocabulary size, since the number of entities that appear simultaneously in a sentence is usually small.ßdummyTurker
2009ß0.0ßFor example, in the dstc2 task, 4 entities appear in a single sentence at most.ßdummyTurker
2010ß0.0ßThank the reviewer for this great suggestion.ßdummyTurker
2011ß0.0ßWe will add the experimental results of CNN+CRF and CNN+Softmax to our paper to illustrate the contribution of BiLSTM to our approach.ßdummyTurker
2012ß0.0ßThank the reviewer for this comment.ßdummyTurker
2013ß0.0ßIn Fig.ßdummyTurker
2014ß0.0ß3 we illustrate the performance of our approach on test data with different lambda values.ßdummyTurker
2015ß0.0ßWe agree with the reviewer that when lambda is 0.5 our approach achieves the highest Fscore on the test data.ßdummyTurker
2016ß0.0ßIn our experiments, the hyper-parameter lambda was tuned on the training data using cross-validation, and 0.4 seems to be the most appropriate value for lambda according to the cross-validation results.ßdummyTurker
2017ß0.0ßThus, we set lambda to 0.4 in our experiments, although it is not the optimal value on the test data.ßdummyTurker
2018ß0.0ßThank the reviewer very much for recommending this related work.ßdummyTurker
2019ß0.0ßWe will cite this paper and add some discussions on it to our manuscript.ßdummyTurker
2020ß0.0ßIn addition, we will compare the results of our approach with the method proposed in that paper.ßdummyTurker
2021ß0.0ßThank the reviewer very much for pointing out these typos and spelling errors.ßdummyTurker
2022ß0.0ßWe will carefully refine the writing of our paper and correct these typos and spelling errors.ßdummyTurker
2023ß0.0ßThank the reviewer very much for this great suggestion.ßdummyTurker
2024ß0.0ßWe will explore to apply the active learning techniques to select the most informative instances for Chinese NER.ßdummyTurker
2025ß0.0ßIndeed, every layer treats its previous layer as input, so this means that each LSTM is fed into the next BiLSTM layer.ßdummyTurker
2026ß0.0ßIn Figure 1, we omit other arrows to make the model more concise.ßdummyTurker
2027ß0.0ßThanks for your review.ßdummyTurker
2028ß0.0ßYes.ßdummyTurker
2029ß0.0ßHowever, our tasks are both quite challenging especially for the reading comprehension tasks, existing model improvements have been already very marginal which can be seen from the leaderboards.ßdummyTurker
2030ß0.0ßHowever, we find a new solution in a new direction instead of stacking sophisticated attention mechanisms.ßdummyTurker
2031ß0.0ßThe introduced subword embedding could promisingly give further advances due to its meaningful linguistic augments, which has not thoroughly studied yet for the concerned tasks.ßdummyTurker
2032ß0.0ßYes, this is insightful and indeed what we are pushing on.ßdummyTurker
2033ß0.0ßBesides, as you suggested, Chinese character itself can be represented by strokes, the writing components of character, with embedding enhancement, it is hopeful to furthermore extend our current subword work.ßdummyTurker
2034ß0.0ßThe sizes were found empirically on the dev set.ßdummyTurker
2035ß0.0ßWe will clarify this in the later version.ßdummyTurker
2036ß0.0ßYes, as in W3ßdummyTurker
2037ß0.0ßYes, as in W1, the latest existing model improvements have been already very marginal which can be seen from the leaderboards.ßdummyTurker
2038ß0.0ßWe think the research community is facing a bottleneck.ßdummyTurker
2039ß0.0ßCompared with the recent advances, our improvements should be substantial.ßdummyTurker
2040ß0.0ßFollowing your suggestion, we will add necessary statistical significant test results later.ßdummyTurker
2041ß0.0ßThis reviewer totally accepted our contribution without comment.ßdummyTurker
2042ß0.0ßThank you for your comments.ßdummyTurker
2043ß0.0ßIn fact, easy-first parsing can be regarded as a non-directional version of classical Arc-Standard stack-based parsing.ßdummyTurker
2044ß0.0ßWe think that easy-first parsing is relatively intuitive and due to space limitation, we have not included too many details of it.ßdummyTurker
2045ß0.0ßWe will add more descriptions for it.ßdummyTurker
2046ß0.0ßAbout the neural model, we adopted a similar architecture to the one of (Kiperwasser and Goldberg, 2016), that is, collecting word representations from LSTM and feeding them to MLP for scoring.ßdummyTurker
2047ß0.0ßSince modeling is not our focus and space is limited, we did not provide too many descriptions for it.ßdummyTurker
2048ß0.0ßPlease notice that we will open our source code and all the details can be found there.ßdummyTurker
2049ß0.0ßAbout the formula at line 505, according to the current form, $\alpha$ should be even.ßdummyTurker
2050ß0.0ßBut we can address this by taking the absolute value of $\Delta$, and then $\alpha$ will not be restricted.ßdummyTurker
2051ß0.0ßThank you for denoting this and we will modify it correspondingly.ßdummyTurker
2052ß0.0ßThank you for the valuable feedback.ßdummyTurker
2053ß0.0ß== Comparison with contemporary techniques on the same dataset ==ßdummyTurker
2054ß0.0ßOur original idea was to conduct fair and direct comparison such that the effectiveness of proposed algorithms is clearly evaluated.ßdummyTurker
2055ß0.0ßTo avoid repeating the same information, please refer to our response to Reviewer 1 to see the comparison with contemporary techniques.ßdummyTurker
2056ß0.0ßFor machine translation, our implementation is clearly more efficient than previous methods (better MLE results with smaller/same architecture).ßdummyTurker
2057ß0.0ßMore importantly, the proposed ERAC consistently outperforms AC with or without the input feeding technique.ßdummyTurker
2058ß0.0ßFor image captioning, our proposed method achieves comparable performance with self-critic training with a smaller architecture.ßdummyTurker
2059ß0.0ß== Human evaluation ==ßdummyTurker
2060ß0.0ßWe do wish we had the resource and time to conduct a reliable human evaluation.ßdummyTurker
2061ß0.0ßHowever, due to the lack of resources, we follow previous works in developing algorithms for sequence prediction in NLP and rely on the automatic evaluation.ßdummyTurker
2062ß0.0ß== Metric for image captioning ==ßdummyTurker
2063ß0.0ßWe present BLEU in the paper because it is the metric used for training.ßdummyTurker
2064ß0.0ßActually, we have the results for other metrics, which are summarized below.ßdummyTurker
2065ß0.0ß<table>ßdummyTurker
2066ß0.0ß<tbody>ßdummyTurker
2067ß0.0ß<tr><td>Methods</td><td>| BLEU4</td><td>| ROUGE-L</td><td>| Meteor</td><td>| CIDEr</td></tr>ßdummyTurker
2068ß0.0ß<tr><td></td></tr>ßdummyTurker
2069ß0.0ß<tr><td>RAML</td><td>| 29.83 +/- 0.21</td><td>| 51.66 +/- 0.11</td><td>| 23.84 +/- 0.11</td><td>| 88.88 +/- 0.62</td></tr>ßdummyTurker
2070ß0.0ß<tr><td>VAML</td><td>| 29.93 +/- 0.22</td><td>| 51.72 +/- 0.14</td><td>| 23.84 +/- 0.11</td><td>| 88.84 +/- 0.58</td></tr>ßdummyTurker
2071ß0.0ß<tr><td>AC</td><td>| 30.90 +/- 0.20</td><td>| 52.34 +/- 0.13</td><td>| 23.59 +/- 0.10</td><td>| 89.01 +/- 0.61</td></tr>ßdummyTurker
2072ß0.0ß<tr><td>ERAC</td><td>| 31.44 +/- 0.22</td><td>| 52.66 +/- 0.07</td><td>| 23.78 +/- 0.05</td><td>| 90.52 +/- 0.53</td></tr>ßdummyTurker
2073ß0.0ß</tbody>ßdummyTurker
2074ß0.0ß</table>ßdummyTurker
2075ß0.0ßThanks for your comments, we agree it is possible to further improve the performance of the pruned model.ßdummyTurker
2076ß0.0ßStill, LD-Net demonstrates the capability to effectively prune PTLMs without retraining.ßdummyTurker
2077ß0.0ßTo make our model more applicable, we try to conduct the compression without retraining.ßdummyTurker
2078ß0.0ßThis setting makes our task even more challenging.ßdummyTurker
2079ß0.0ßFor example, we’ve tried a very recent sparse regularization technique (Louizos et al., 2017), however, its performance is even worse.ßdummyTurker
2080ß0.0ßWe will add this variant as a baseline in the revised version, if it helps demonstrate the effectiveness of LD-Net.ßdummyTurker
2081ß0.0ßIn our future work, we will explore other principles for better effectiveness and efficiency.ßdummyTurker
2082ß0.0ßLouizos, Christos, Max Welling, and Diederik P. Kingma.ßdummyTurker
2083ß0.0ß"""Learning Sparse Neural Networks through L_0 Regularization."""ßdummyTurker
2084ß0.0ßInternational Conference on Learning Representations Accepted as poster.ßdummyTurker
2085ß0.0ßhttps://openreview.net/forum?id=H1Y8hhg0b.ßdummyTurker
2086ß0.0ßThanks for your advice .ßdummyTurker
2087ß0.0ßWe will correct the grammatical errors in the revised version.ßdummyTurker
2088ß0.0ßWe will append experiment setting, hyper-parameter choice and other details.ßdummyTurker
2089ß0.0ßAll the implementations are based on PyTorch http://pytorch.org/.ßdummyTurker
2090ß0.0ßWe will open source the implementation.ßdummyTurker
2091ß0.0ßTo Review#2ßdummyTurker
2092ß0.0ßThanks for your insightful consideration and comments.ßdummyTurker
2093ß0.0ßWe use self-collected NE dictionaries for both monolingual and cross-lingual NE detection task.ßdummyTurker
2094ß0.0ßResult from the deficiency of NE dictionaries in other languages, we only complete our experiment in and between English and Chinese.ßdummyTurker
2095ß0.0ßAs claimed in line 294, we state that there are undeniable differences between different language and NE types due to the fact that the quality of NE dictionaries has a major impact on the performances as well as the quality of pre-trained word embeddings.ßdummyTurker
2096ß0.0ßFurthermore, as we do not have enough human resource to verify, we randomly sampled embeddings inside the hypersphere but outside NE dictionary, finding quite a lot of embeddings were actually true NEs just due to the insufficiency of NE dictionary.ßdummyTurker
2097ß0.0ßReferring to line 171, we claim that we choose hypersphere due to it has the least definition parameters and it presents a closure surface that includes gathering NEs.ßdummyTurker
2098ß0.0ßWe will consider more mature techniques later accordingly.ßdummyTurker
2099ß0.0ßWhen we use the term Visualization, we mean that the distribution property is actually obtained from human observation by various angles of figure drawing, which is not an image processing issue actually.ßdummyTurker
2100ß0.0ßTo Review#1ßdummyTurker
2101ß0.0ßThanks for your insightful consideration and comments.ßdummyTurker
2102ß0.0ßWe manually sampled and checked embedding inside the supposed NE hypersphere but outside our NE dictionaries.ßdummyTurker
2103ß0.0ßAs we give the corresponding NE visualization which has been shown far from the proper hypersphere shape, we found nearly all checked embeddings are truly NE but not in our dictionaries which means that our NE dictionaries over these languages are extremely insufficient.ßdummyTurker
2104ß0.0ßConsidering the concerned NE dictionaries are not sufficient enough to support a meaningful evaluation, we dropped out the respective experiments based on them.ßdummyTurker
2105ß0.0ßYes, what you understand is exact, the evaluation is just be about hypersphere exactly includes all the NEs in the dictionary.ßdummyTurker
2106ß0.0ßThe performances are evaluated jointly by two factors, calculating the ratio of NE from the dictionary which is included in the hypersphere (recall), and counting those NE inside the hypersphere but outside the dictionary (this is for precision).ßdummyTurker
2107ß0.0ßF-score is then computed from the harmonic average of recall and precision.ßdummyTurker
2108ß0.0ßAll the evaluated data are NE dictionaries, which is supposed to be sufficient and accurate, though not really so.ßdummyTurker
2109ß0.0ßThe purpose of the evaluation in monolingual case is just to show how well a hypershpere model can accurately depict all NE distribution given by a good enough dictionary.ßdummyTurker
2110ß0.0ßOnce we have more space later, we will add back the required analysis and discussion.ßdummyTurker
2111ß0.0ßFor the reason why cross-lingual NE mapping is better than monolingual, it is due to that the high quality of Chinese location NE dictionary and transform matrix can additionally help the bilingual processing which right demonstrates the effectiveness of our approach and model.ßdummyTurker
2112ß0.0ßFor the monolingual case, we have too poor English location NE dictionary in the meantime.ßdummyTurker
2113ß0.0ßWe use a simple strategy to represent multi-word NEs with average vector of all member word vectors inside the corresponding NE.ßdummyTurker
2114ß0.0ßThe evaluation method is described in response to W1 for review#1.ßdummyTurker
2115ß0.0ßThis is probably because of relatively more poor quality of German NE dictionary and there is more serious mismatch between the NE dictionary and the corresponding pre-trained embedding.ßdummyTurker
2116ß0.0ßEven though German NE dictionary is much larger, the above two factors also play an important role in NE hypersphere modeling.ßdummyTurker
2117ß0.0ßThe worst-case results for the proposed method are that it cannot predict a boundary within the maximum length of sentences.ßdummyTurker
2118ß0.0ßIn that case, following the paper of Wang et al 2016, the method will predict a boundary after the position that has the highest confidence score.ßdummyTurker
2119ß0.0ßThe results on the table 6 are obtained from the 'online' version of system.ßdummyTurker
2120ß0.0ßThe words were input into the system one by one.ßdummyTurker
2121ß0.0ßIt is a great idea to report the performance both on sentence-level nuclearity and above sentence-level nuclearity and the experimental results prove your conjecture.ßdummyTurker
2122ß0.0ßWe  find out that the performance of the sentence-level nuclearity recognition is better than that of the above sentence-level as follows.ßdummyTurker
2123ß0.0ßsentence-level(F1): NA 56.3, NB 46.0, MN 84.1, Macro-F1 63.1, Micro-F1 74.7;ßdummyTurker
2124ß0.0ßabove sentence-level(F1): NA 54.2, NB 42.2, MN 75.8, Macro-F1 58.5, Micro-F1 65.0.ßdummyTurker
2125ß0.0ßWe will add a table to the revised paper and give the analysis.ßdummyTurker
2126ß0.0ßYes.ßdummyTurker
2127ß0.0ßThis method also can be used in English.ßdummyTurker
2128ß0.0ßTo compare our method with the state-of-the-art neural networks methods used in English nuclearity recognization, we re-implement the neural network model (Li et al.ßdummyTurker
2129ß0.0ß2016), i.e., Bi-LSTM(A)+T, as a baseline.ßdummyTurker
2130ß0.0ßBesides, We are preparing the experiments on the RST-DT corpus and will report the results in our revised paper if this paper is accepted.ßdummyTurker
2131ß0.0ßConvolutional-pooling LSTM has been applied to many NLP tasks.ßdummyTurker
2132ß0.0ßIn our paper, we first applied it to the nuclearity recognition task in Chinese discourses.ßdummyTurker
2133ß0.0ßWe will modify this claim in our revised paper.ßdummyTurker
2134ß0.0ßNuclearity recognition task should be performed after the construction of discourse tree.ßdummyTurker
2135ß0.0ßIn our future work, we will focus on the other tasks of the discourse parsing in CDTB.ßdummyTurker
2136ß0.0ßThe ME model, one of the baseline, is the system provided by Li et al.ßdummyTurker
2137ß0.0ß(2015), maybe we don not have a clear explanation about this in the Section Experiments.ßdummyTurker
2138ß0.0ßAnd we also run the system provided by Chu et al.ßdummyTurker
2139ß0.0ß(2015) and find out that its performance is worse than Li et al.ßdummyTurker
2140ß0.0ß(2015), so we didn't choose their model as a baseline.ßdummyTurker
2141ß0.0ßWe are preparing the experiments on the RST-DT corpus and will report the results in our revised paper if this paper is accepted.ßdummyTurker
2142ß0.0ßBesides, to compare with the methods on RST DT, we re-implement a state-of-the-art neural network model (Li et al.ßdummyTurker
2143ß0.0ß2016), i.e., Bi-LSTM(A)+T, as a baseline.ßdummyTurker
2144ß0.0ßThe Me model of Li et al.ßdummyTurker
2145ß0.0ß(2015) is the state-of-the-art in CDTB.ßdummyTurker
2146ß0.0ßThis statement is related to the characteristics of Chinese language.ßdummyTurker
2147ß0.0ßYou are right, the syntactic information is useful in many English NLP tasks.ßdummyTurker
2148ß0.0ßHowever, it is not so effective in Chinese due to the low performance of syntactic parser and the flexible structure of Chinese sentences.ßdummyTurker
2149ß0.0ßSo we think more semantic information is needed in Chinese discourse parsing tasks.ßdummyTurker
2150ß0.0ßThe choice of these hyperparameters is based more on experience or on the values used in other papers.ßdummyTurker
2151ß0.0ßWe simply the hyperparameter tuning and list several different sets of the hyperparameters.ßdummyTurker
2152ß0.0ßWe choose the one with the best performance.ßdummyTurker
2153ß0.0ßAll inputs of baselines do not include paragraph information, and it is an innovative point of our work to consider the semantic links between discourse units and paragraph in nuclearity recognition task.ßdummyTurker
2154ß0.0ßDue to the space limitation, we didn’t give these features in ME.ßdummyTurker
2155ß0.0ßThe ME model contains the context features, lexicon features and dependency tree features, which is similar with Lin et al.ßdummyTurker
2156ß0.0ß(2009) (Recognizing implict discourse relations in the penn discourse treebank, in EMNLP 2009).ßdummyTurker
2157ß0.0ßYou are right, NNs are not better than systems based on hand-crafted features.ßdummyTurker
2158ß0.0ßWe will modify the corresponding description in our paper.ßdummyTurker
2159ß0.0ßWe will simplify example 1 and give an example with 4 or 5 EDUs.ßdummyTurker
2160ß0.0ßThe discourse relation between DUs a and b is Nucleus Ahead, because DU b gives the detail information to further explain DU a.ßdummyTurker
2161ß0.0ßMany thanks for your helpful reviews.ßdummyTurker
2162ß0.0ß1.Reply to weakness argument-“I am not total …”:ßdummyTurker
2163ß0.0ßWe conducted manual evaluation of response self-consistency on 30 message pairs built on test set.ßdummyTurker
2164ß0.0ßEach pair consists of two messages sharing similar meaning, e.g.ßdummyTurker
2165ß0.0ß“I am waiting to get off work.”/“It's really boring to work.”.ßdummyTurker
2166ß0.0ßThen we run one system on the two messages in each pair and then checked if the two responses express similar personal preference or attitude.ßdummyTurker
2167ß0.0ßWe see that OGM can show self-consistency with 50% message pairs, S2S_{1m, fo} with a ratio of 33%,  S2S_{4m, fo} with a ratio of 40%, and S2S_{4m, mr} with a ratio of 30%.ßdummyTurker
2168ß0.0ßWe will update the draft with these results.ßdummyTurker
2169ß0.0ßThis system is applicable to generation of opinions with various targets, e.g.ßdummyTurker
2170ß0.0ß, products, concepts, not limited to individuals.ßdummyTurker
2171ß0.0ßMoreover, this system can also generate responses to non-fact utterances since our model does not require the input to be factual utterances.ßdummyTurker
2172ß0.0ß"""e"" refers to an opinion target."ßdummyTurker
2173ß0.0ßThe vector representations v(r) and v(m) are trained using ScoreNet.ßdummyTurker
2174ß0.0ßThanks for the insightful comments.ßdummyTurker
2175ß0.0ßThe qualitative analysis should directly measure how humans produce errors.]ßdummyTurker
2176ß0.0ßVisually and phonologically similar characters are two types of characters in spelling errors according to the analysis (Liu et al.ßdummyTurker
2177ß0.0ß2010), based on which we generate the annotated corpus.ßdummyTurker
2178ß0.0ßAs for what percentage of our produced spelling errors that humans would actually produce in their writing, it is difficult to evaluate and what we can do is to generate as many spelling errors as possible in our generated corpus in order to improve the detection performance.ßdummyTurker
2179ß0.0ßChao-Lin Liu, Min-Hua Lai, Yi-Hsuan Chuang, Chia-Ying Lee: Visually and Phonologically Similar Characters in Incorrect Simplified Chinese Words.ßdummyTurker
2180ß0.0ßCOLING (Posters) 2010: 739-747ßdummyTurker
2181ß0.0ßHow large the dataset should be for outperforming the manually annotated corpora?]ßdummyTurker
2182ß0.0ßWe conduct an additional experiment for studying the performance of models trained on different scale of A-corpus.ßdummyTurker
2183ß0.0ßThe results show that an A-corpus about four times larger than the manually annotated corpora is needed to achieve the expected performance.ßdummyTurker
2184ß0.0ßWhether the tests address error detection (= spell checking), correction, or both?]ßdummyTurker
2185ß0.0ßError detection serves as the key focus of this paper.ßdummyTurker
2186ß0.0ßF1, precision, and recall reported in the experiment results are all concerned with the detection-specific performance.ßdummyTurker
2187ß0.0ßWe will clarify this in the revised paper.ßdummyTurker
2188ß0.0ßThe current state-of-the-art results on these four datasets?]ßdummyTurker
2189ß0.0ßIn the revised paper, we will include the four state-of-the-art results reported in the original reports.ßdummyTurker
2190ß0.0ßThe experimental results show that the model trained on our generated corpus outperform the state-of-the-art results.ßdummyTurker
2191ß0.0ß"Why is the approach called ""a compound approach""?]"ßdummyTurker
2192ß0.0ßIn generating the annotated corpus, we use two different methods.ßdummyTurker
2193ß0.0ßSo the overall approach is a compound or hybrid approach.ßdummyTurker
2194ß0.0ß[Additional Comments]ßdummyTurker
2195ß0.0ßThanks for your detailed advice in further polishing our paper.ßdummyTurker
2196ß0.0ßWe will revise our paper accordingly.ßdummyTurker
2197ß0.0ß"We justify our use of UrbanDictionary by noting that is the largest compilation of online slang spanning more than 15 years which we believe reflects slang usage ""in the wild, as opposed to studying pre-compiled dictionaries in print which may not be up to date and representative of slang as used in recent years."ßdummyTurker
2198ß0.0ßWe acknowledge in the paper that the analysis of offline slang (in speech etc) is a separate research question (see Footnote 2).ßdummyTurker
2199ß0.0ßOur goal in this work is to characterize online slang in English.ßdummyTurker
2200ß0.0ßWe are quite aware that our analysis likely reflects the usage of usage of slang of likely young people (again we explicitly note this in Lines 782-786) but we believe our study sets the stage for future work and analysis of slang across cultures and demographics.ßdummyTurker
2201ß0.0ßOur argument is that slang manifests at-least as much gender stereotyping as found in News.ßdummyTurker
2202ß0.0ßNote that we even analyze this and show prevailing stereotypes in Figure 5.ßdummyTurker
2203ß0.0ßAnalysis in Section 3.2.2 was done using the data released by Mattiello.ßdummyTurker
2204ß0.0ß"Identifying a ""Front"" or ""Back"" clipping is just a matter of checking whether a prefix or a suffix of the original word is retained (as per definition)."ßdummyTurker
2205ß0.0ßWe would like to clarify that we do contextualize our results with the work of Nguen.2017 et.al and quite some other prior work.ßdummyTurker
2206ß0.0ßIn particular, see Lines 725-740ßdummyTurker
2207ß0.0ßIt is precisely for that very reason, we do two things (a) We establish the generic quality of these slang embeddings by evaluating them on standard word embedding benchmarks (see Lines 571-573) and show that they are comparable to other standards like GloVE at-least in a holistic sense.ßdummyTurker
2208ß0.0ß(b) Second, we ensure we test for differences via statistical tests to weed out false positives (see Figure 7 with 95% confidence intervals) and Lines 687-690 for justification of our claims.ßdummyTurker
2209ß0.0ßWe sample 5 SQL queries for each table in WikiSQL training data, resulting in 43.5K SQL queries.ßdummyTurker
2210ß0.0ßApplying our QG model on these SQL queries, we get 92.8K SQL-question pairs.ßdummyTurker
2211ß0.0ßAs a reference, the original WikiSQL training data consists of 61.3K SQL-question pairs.ßdummyTurker
2212ß0.0ßIn our preliminary experiment, we set the number of generated question for each table as 3, 5, to 7.ßdummyTurker
2213ß0.0ßIn the setting with 30% supervised data, the execution accuracy of our semantic parser is 73.2%, 74.0% and 72.9% respectively.ßdummyTurker
2214ß0.0ßBased on this result we use 5 in the following experiment.ßdummyTurker
2215ß0.0ßWe believe that a small number of generated questions lacks diversity while a large number of generated questions would induce noise.ßdummyTurker
2216ß0.0ßWe will give more discussion about these details.ßdummyTurker
2217ß0.0ßThanks.ßdummyTurker
2218ß0.0ßWe will polish the paper via discussing on missing related work wang-berant-liang-acl2015, and properly mention Iyer-2017 in the question generation part of related work.ßdummyTurker
2219ß0.0ßThanks for the suggestion on the structure of this paper.ßdummyTurker
2220ß0.0ßWe totally agree with you and would emphasize the strong empirical results than the logarithmic relationship.ßdummyTurker
2221ß0.0ßWe would surely polish the article to give more clarifications on section 5.1, modifying confusing symbols in equations, and fix grammar and word choice issues.ßdummyTurker
2222ß0.0ßThank you for your review!ßdummyTurker
2223ß0.0ßPlease allow us to give explanation and clarification on your misunderstanding.ßdummyTurker
2224ß0.0ßWe validate our idea on two annotations due to the tight experiment and writing schedule before the submission deadline.ßdummyTurker
2225ß0.0ßSince most of the recent work related to learning on different annotations experimented on two annotations, we think that the experiments can still demonstrate the point.ßdummyTurker
2226ß0.0ßAs you suggested, it will be better to conduct experiments on more annotations.ßdummyTurker
2227ß0.0ßThe unified learning is indeed a new way to do parameter sharing, as well as previous methodologies including multitask learning and cascaded classification.ßdummyTurker
2228ß0.0ßIn this work we argue that our method is more efficient for learning on different annotations.ßdummyTurker
2229ß0.0ßThe major difference between our method and multitask learning lies in that, the unified learning shares information across all of the embedding, encoding and predicting layers, and the topmost module in unified learning is also the same for different annotations.ßdummyTurker
2230ß0.0ßWe thank the reviewer for their valuable feedback.ßdummyTurker
2231ß0.0ßThe challenge corpus is mainly released with visual and audio information, and the test set used in the challenge was never released.ßdummyTurker
2232ß0.0ßThe challenge also did not concentrate on multimodal analysis, but mostly on video analysis, with the same methods often applied to audio without adaptation.ßdummyTurker
2233ß0.0ßWe have also included text information by sending request to the chalenge organisers to obtain the transcriptions.ßdummyTurker
2234ß0.0ßThe main aim of our research is to investigate the effect of adding different modalities in the personality recognition task, where this analysis was never performed before.ßdummyTurker
2235ß0.0ßSure, we will cite Polzehl et al.ßdummyTurker
2236ß0.0ß"""Automatically assessing personality from speech""."ßdummyTurker
2237ß0.0ßPlease note that they did not make use of deep learning methods, and also analyzed a different corpus, but part of the audio analysis is similar.ßdummyTurker
2238ß0.0ßthis may perhaps be due to a strong conclusion not being presented.ßdummyTurker
2239ß0.0ßbut the documentation of the work is completeßdummyTurker
2240ß0.0ßa syntactic tree would have made things clearer.ßdummyTurker
2241ß0.0ßwe opted for bracketed labelling due to limited space issues.ßdummyTurker
2242ß0.0ßit is a review of various NLP work (not just parsing) in healthcare, so it may have come across as confusing.ßdummyTurker
2243ß0.0ßißdummyTurker
2244ß0.0ßa file is annotated by a student linguist.ßdummyTurker
2245ß0.0ßthen it is judged by 2 - in house linguists independently.ßdummyTurker
2246ß0.0ßthe difference in judgement, which is the inter-annotator disagreement.ßdummyTurker
2247ß0.0ßthis is resolved through discussionßdummyTurker
2248ß0.0ßinter-annotator agreement is calculated via Parse Eval tool from the Stanford CoreNLP LibraryßdummyTurker
2249ß0.0ßsection 5 is only a representation of the accuracy Berkeley grammar achieved on our data.ßdummyTurker
2250ß0.0ßso we did not think it was a section that needed elaboration.ßdummyTurker
2251ß0.0ßWe have conducted a systematic investigation for this task.ßdummyTurker
2252ß0.0ßIn particular, we have studied textual diagnosis report examples written by human doctors.ßdummyTurker
2253ß0.0ßIt seems that there are no general, unified, and standard metadata definitions for diagnosis report.ßdummyTurker
2254ß0.0ßAn example of a diagnosis provided by a human doctor can look like this:ßdummyTurker
2255ß0.0ß”ßdummyTurker
2256ß0.0ß•Prominent mediastinum.ßdummyTurker
2257ß0.0ß•May be due to mediastinal fat.ßdummyTurker
2258ß0.0ß•Comparison films, if available, would be useful to determine if this is a chronic appearance.ßdummyTurker
2259ß0.0ß•Clear lungs.ßdummyTurker
2260ß0.0ß”ßdummyTurker
2261ß0.0ßThe above example shows that a diagnosis report is mainly unstructured free texts containing phrases and sentences.ßdummyTurker
2262ß0.0ßThus, this task cannot be simplified via metadata extraction.ßdummyTurker
2263ß0.0ßJing's approach and ours share the same dataset.ßdummyTurker
2264ß0.0ßIn Jing's method, they generate both findings and diagnosis by taking X-ray image as input.ßdummyTurker
2265ß0.0ßIn comparison to their work, we generate only diagnosis by using 1) findings, 2)indication, and also 3) X-ray image.ßdummyTurker
2266ß0.0ßA typical diagnosis includes both conclusions about the radiographic study and recommendations for further evaluation and patient management.ßdummyTurker
2267ß0.0ßRecommendations are based on the results of the radiologic study and the experience of the individual radiologist.ßdummyTurker
2268ß0.0ßOur method is more comparable to how the human doctors work, since the human doctors also take more information types into account for their diagnosis report.ßdummyTurker
2269ß0.0ßIn Jing’s method, only partial data in the dataset was utilized for generating diagnosis.ßdummyTurker
2270ß0.0ßBy leveraging more data and data types, we can obtain better result for the diagnosis generation task.ßdummyTurker
2271ß0.0ßThanks for pointing this out.ßdummyTurker
2272ß0.0ßMore solid error analysis will be available in the final version.ßdummyTurker
2273ß0.0ßThanks for raising this insightful question.ßdummyTurker
2274ß0.0ßIn the final version, we will add a comparison of our approach and some rule-based systems.ßdummyTurker
2275ß0.0ßRule-based systems can only work well if relevant information can be encoded into rules, namely, in an explicit way.ßdummyTurker
2276ß0.0ßHowever, medical images are complex and the relevant information encoded in the images may not be all explicitly encoded into rules.ßdummyTurker
2277ß0.0ßAt the same time, textual descriptions report facts encoded in an X-ray for a doctor, which are also mostly difficult to be explicitly represented by rules.ßdummyTurker
2278ß0.0ßOur model is an end-to-end neural network with better scalability.ßdummyTurker
2279ß0.0ßIt avoids managing a complex sequential pipeline.ßdummyTurker
2280ß0.0ßDeep learning based method is more suitable to identify implicit correlations between images and textual data.ßdummyTurker
2281ß0.0ßReply to weakness1 as question1.ßdummyTurker
2282ß0.0ßReply to weakness2 as question 2.ßdummyTurker
2283ß0.0ßComparison against a gold standard,ßdummyTurker
2284ß0.0ßData-driven evaluation,ßdummyTurker
2285ß0.0ßManual User-based evaluation,ßdummyTurker
2286ß0.0ßApplication or task-based evaluation.ßdummyTurker
2287ß0.0ßAs there is no golden standard to compare the results with, the first evaluation method cannot be performed.ßdummyTurker
2288ß0.0ßData-driven evaluation which is the process of comparing ontology against existing data about the domain is exactly the procedure that has been followed to construct the ontology.ßdummyTurker
2289ß0.0ßThe third method; manual evaluation of ontology by an expert is what we did during the ontology building lifecycle and after it.ßdummyTurker
2290ß0.0ßThe built ontology before being applied in the application was examined by an expert to see if it satisfies the competency questions, if the relations, tags, properties and hierarchies are correct.ßdummyTurker
2291ß0.0ßAny incorrect part of the ontology is fixed by the expert at this phase.ßdummyTurker
2292ß0.0ßFor evaluating the ontology via an application, we applied it in our expert finding system.ßdummyTurker
2293ß0.0ßFor each query, the first 5 expert recommendations of the expert finding system are considered.ßdummyTurker
2294ß0.0ßFor each recommended expert, the evaluator checks if the expert’s papers which leads the system to this recommendation are relevant to the query or not.ßdummyTurker
2295ß0.0ßEvaluation shows that 92.64% of ontological relations used by the system to recommend an expert were semantically true.ßdummyTurker
2296ß0.0ßIn other words %92.64 of the assessed papers are correctly associated to the query topics in the ontology.ßdummyTurker
2297ß0.0ßWithout using the ontology, the expert finding system finds experts whose papers (titles and abstracts) contain the exact keywords in the query and ignore those containing concepts related (synonym or hyponym or other relations) to the query.ßdummyTurker
2298ß0.0ßWithout the ontology although the precision is high as well, the recall is very low.ßdummyTurker
2299ß0.0ßActually the automatic evaluation we reported is not related to the application for which we constructed the ontology.ßdummyTurker
2300ß0.0ßDifferent measurements of the depth and breadth of ontology, number of concepts and relations and properties, and dispersion measurements are used to show the features of the ontology of NLP independent of the expert finding system.ßdummyTurker
2301ß0.0ßThese will be useful for further comparisons with newer versions.ßdummyTurker
2302ß0.0ßThanks very much for your advices!ßdummyTurker
2303ß0.0ßThe works of WSD are really meaningful fix-ups.ßdummyTurker
2304ß0.0ßThis do is a key element in building commercial SDS.ßdummyTurker
2305ß0.0ßFrom our perspective, most template libraries in commercial SDS are constructed manually.ßdummyTurker
2306ß0.0ßIn our paper, the template library we used is provided by a commercial SDS.ßdummyTurker
2307ß0.0ßAnd, to follow the ACL anonymous policy, we do not illustrate the details of the construction process of these templates.ßdummyTurker
2308ß0.0ßActually, we do ignore the relationships to the WSD.ßdummyTurker
2309ß0.0ßThis is really a helpful suggestion.ßdummyTurker
2310ß0.0ßThank you very much!ßdummyTurker
2311ß0.0ßReply to the works of ATIS task that use semantic structure: We will seriously compare our method with the previous works on ATIS that use the semantic structure.ßdummyTurker
2312ß0.0ßThank you for your advice.ßdummyTurker
2313ß0.0ßThank you very much for your insightful review.ßdummyTurker
2314ß0.0ßBoth the datasets are limited, but the German one is even smaller than the Italian one, hence the difference in performance with the finetuning (or the single language setup) probably not having enough data to be trained.ßdummyTurker
2315ß0.0ßIt is a good point.ßdummyTurker
2316ß0.0ßIt is indeed also true that perfoming data sampling might remove too much data to prevent any kind of multilingual correlation to be learnt.ßdummyTurker
2317ß0.0ßIn any case we are planning in a future work to analyze the amount of data needed to train an affect system in the most effective way, by collecting even more data and experimenting to see the correlation between data size and performance.ßdummyTurker
2318ß0.0ßWe did not apply any specific preprocessing, if not feeding the squared signal in a different channel and randomize the input volume to better estimate energy.ßdummyTurker
2319ß0.0ßWe have some graphics, which we could not include for space constraints (not so related with our multilingual claim), where we apply FFT to the first layer parameters and clearly show that this layer is leveraging pitch and energy features.ßdummyTurker
2320ß0.0ßWe will consider including the plots and discuss them in the extra page if the paper is accepted.ßdummyTurker
2321ß0.0ßThe agreement was generally not high, but most of the times the disagreement was between one emotion class (especially happy) and the neutral (or garbage) class, or between sadness and anxiety.ßdummyTurker
2322ß0.0ßWe did discarded all the disagreement cases leaving only the samples where at least three annotators agreed.ßdummyTurker
2323ß0.0ßThe utterances annotated once only were always obtained by a pool of graduate students consisting from the authors of this paper and their colleagues in the same research group.ßdummyTurker
2324ß0.0ßThey were very confident about the task and were instructed to discard all the samples where an unambiguous decision could not be obtained.ßdummyTurker
2325ß0.0ßThank you for the thoughtful comments.ßdummyTurker
2326ß0.0ßWe agree that experimenting with other types of document structure is an important research direction and will add a discussion on this important point in the camera-ready version.ßdummyTurker
2327ß0.0ßWe mean that each question is paired with a small set of documents, but the number of questions is large.ßdummyTurker
2328ß0.0ßWe will clarify this in the camera-ready version.ßdummyTurker
2329ß0.0ßIndeed relating our work to walkers on DOM structures is relevant, and we will add that for the camera-ready.ßdummyTurker
2330ß0.0ßWe thank the reviewer for the valuable and concise feedback.ßdummyTurker
2331ß0.0ßWe will address the concerns brought up in the review in turn:ßdummyTurker
2332ß0.0ß* Indeed we rely on improvements in BLEU alone to conclude that useful properties must have been captured by the graphs when used with Emb/CNN encoders.ßdummyTurker
2333ß0.0ßThe argument about distance indeed has more to do with the graphs being non-trivial (as opposed to those obtained with recurrent encoders) than with them capturing linguistic generalisations.ßdummyTurker
2334ß0.0ßWe will make the argument more explicit.ßdummyTurker
2335ß0.0ß* We use Google’s https://github.com/tensorflow/nmt for our baselines and trust that results from this toolkit are well regarded by the community.ßdummyTurker
2336ß0.0ßWe do not have external baselines, but our Emb/CNN/RNN baselines offer a direct contrast to our contribution, which we felt was more appropriate for a short submission.ßdummyTurker
2337ß0.0ß* Perhaps the paper is dense because some of the techniques related to deep generative models are not yet popular in the field.ßdummyTurker
2338ß0.0ßWe intend to use a good portion of the extra page allowed for publication to address this issue.ßdummyTurker
2339ß0.0ßThank you for the good suggestions.ßdummyTurker
2340ß0.0ßWe will design some stronger baselines to evaluate the performance of our method as your suggestions.ßdummyTurker
2341ß0.0ßThank you very much, we will rethink this problem and rewrite this part.ßdummyTurker
2342ß0.0ßWe will add some related work in this paper as suggested by you.ßdummyTurker
2343ß0.0ßThank you for your suggestion.ßdummyTurker
2344ß0.0ßWe will revise the paper carefully and invite a native speaker to improve the paper.ßdummyTurker
2345ß0.0ßThe MIR system used in this paper ranked 1st in Wikipedia corpus based task and 2nd in arvix corpus based task on NTCIR-12.ßdummyTurker
2346ß0.0ßWe will add more information about this part, thank you so much for your suggestion.ßdummyTurker
2347ß0.0ßIndeed, no agreement was computed, but all five law students were trained by the expert.ßdummyTurker
2348ß0.0ßAlso, the expert who validated the data manually annotates obligations and prohibitions as a core part of his/her main job based on strict guidelines of the company that provided the data.ßdummyTurker
2349ß0.0ßIndeed, it cannot be released due to confidentiality issues.ßdummyTurker
2350ß0.0ßA few indicative examples of errors can be easily added, but space does not allow us to include a more detailed error analysis.ßdummyTurker
2351ß0.0ßThe main advantages of our method areßdummyTurker
2352ß0.0ßIt might be caused by different sizes between English-Russian corpus and English-Chinese corpus that we used.ßdummyTurker
2353ß0.0ßWe randomly pick data from high-quality bilingual corpora to construct our training data in this task.ßdummyTurker
2354ß0.0ßWe have around 2 billion English-Chinese parallel sentence pairs, but 50 million in English-Russian.ßdummyTurker
2355ß0.0ßDifferent sizes might lead to different degree of richness in the corpus.ßdummyTurker
2356ß0.0ßOur model in this paper is designed for task oriented system.ßdummyTurker
2357ß0.0ßFor non-task-oriented data, it's inappropriate to model intentions by latent variables with multi-nominal distribution.ßdummyTurker
2358ß0.0ßBecause the intentions are too diversified.ßdummyTurker
2359ß0.0ßThe samples of dialogue are stored in our hard drive, we provide one example here because of space limitation:ßdummyTurker
2360ß0.0ßfirst turnßdummyTurker
2361ß0.0ßUser                              : I need to find an expensive restaurant that's in theßdummyTurker
2362ß0.0ßsouth section of the city.ßdummyTurker
2363ß0.0ßour model, +db_vector, +RL       : what food are you looking for ?ßdummyTurker
2364ß0.0ßsecond turnßdummyTurker
2365ß0.0ßUser                              : no i don't care about the type of cuisine.ßdummyTurker
2366ß0.0ßour model, +db_vector, +RL       : the good luck Chinese food takeaway serves ChineseßdummyTurker
2367ß0.0ßfood and is in the expensive pricertange.ßdummyTurker
2368ß0.0ßthird turnßdummyTurker
2369ß0.0ßUser                              : what is their address ?ßdummyTurker
2370ß0.0ßour model, +db_vector, +RL       : 82 Cherry Hinton Road Cherry Hinton .ßdummyTurker
2371ß0.0ßfirst turnßdummyTurker
2372ß0.0ßUser                              : I need to find an expensive restaurant that's in theßdummyTurker
2373ß0.0ßsouth section of the city.ßdummyTurker
2374ß0.0ßour model, +db_vector+RL         : what food are you looking for ?ßdummyTurker
2375ß0.0ßsecond turnßdummyTurker
2376ß0.0ßUser                              : no i don't care about the type of cuisine .ßdummyTurker
2377ß0.0ßour model, +db_vector+RL         : there is The Good Luck .ßdummyTurker
2378ß0.0ßwould you like theßdummyTurker
2379ß0.0ßinformation ?ßdummyTurker
2380ß0.0ßthird turnßdummyTurker
2381ß0.0ßUser                              : what is their address ?ßdummyTurker
2382ß0.0ßour model, +db_vector, +RL       : there address is 82 Cherry Hinton Road Cherry HintonßdummyTurker
2383ß0.0ß, is there anything else i can help you with ?ßdummyTurker
2384ß0.0ßlast turnßdummyTurker
2385ß0.0ßUser                              : what is the food they serve ?ßdummyTurker
2386ß0.0ßour model, +db_vector, +RL       : they serve Chinese food .ßdummyTurker
2387ß0.0ßNVIL optimized model tends to generate short sentences and slot-related tokens due to high variances from gradients calculation.ßdummyTurker
2388ß0.0ßHowever, MLE optimized model would generate more fluent and detailed response.ßdummyTurker
2389ß0.0ßWe would pay attention to typos.ßdummyTurker
2390ß0.0ß-----Reply to weaknesses-----ßdummyTurker
2391ß0.0ß--standard deviationßdummyTurker
2392ß0.0ßFigure-3 showed that joint models outperform the best runs of s2s.ßdummyTurker
2393ß0.0ßIt just happens that the best run of s2s is within one std from mean.ßdummyTurker
2394ß0.0ß--random selectionßdummyTurker
2395ß0.0ßWe used simulated situations to have more controlled experiments.ßdummyTurker
2396ß0.0ßWe also note that since test sets for IWSLT are taken from different documents than training/dev, our results are still valid.ßdummyTurker
2397ß0.0ß--data sizeßdummyTurker
2398ß0.0ßWe believe that low-resource is a relative concept -- 150K is enough for IWSLT, but low-resource for WMT.ßdummyTurker
2399ß0.0ßAs a result, we take 10K from IWSLT for more controlled experiments.ßdummyTurker
2400ß0.0ß--dropoutßdummyTurker
2401ß0.0ßWe followed Zoph et al.ßdummyTurker
2402ß0.0ß(2016) to use a big dropout rate.ßdummyTurker
2403ß0.0ß--greedy decoding in Figure-4ßdummyTurker
2404ß0.0ßGreedy decoding score on dev set is reported during training.ßdummyTurker
2405ß0.0ßWe did not save a separate copy after each epoch, making it impossible to report full decoding results after training is done.ßdummyTurker
2406ß0.0ß-----Reply to questions-----ßdummyTurker
2407ß0.0ß--tokenizationßdummyTurker
2408ß0.0ßWe used multi-bleu.pl and tokenizer.pl from Moses.ßdummyTurker
2409ß0.0ß--monolingual dataßdummyTurker
2410ß0.0ßWe believe it’s orthogonal to our investigation, and it’s useful to add them in the future.ßdummyTurker
2411ß0.0ß--high resourceßdummyTurker
2412ß0.0ßIn our preliminary experiments, our method outperforms the baselines until about one-third of the full corpus (50K).ßdummyTurker
2413ß0.0ßWe hypothesize that the model can effectively learn a peaked attention distribution given enough data, thus eliminating the need for hard attention.ßdummyTurker
2414ß0.0ßThrough the auxiliary task of dependency relation prediction, similar/identical relation enforces participating words close to each other, i.e.ßdummyTurker
2415ß0.0ßthe model clusters words with similar syntactic functionalities in different domains.ßdummyTurker
2416ß0.0ßRegarding semantics, we take pre-trained word embeddings as input to the recursive neural network.ßdummyTurker
2417ß0.0ßThe word embeddings are obtained through word2vec which captures semantic information that produces similar representations for semantically-similar words.ßdummyTurker
2418ß0.0ßBy taking them as input, together with the auxiliary task, our model encodes both semantic and syntactic information.ßdummyTurker
2419ß0.0ßYes, the word vectors mainly distinguish in syntactic structure and semantic relatedness, instead of sentiment polarity, because the focus of our model is cross-domain aspect/opinion terms extraction.ßdummyTurker
2420ß0.0ßWe appreciate your concern and will investigate the more challenging problem of transferring both target terms and their sentiment polarities in our future work.ßdummyTurker
2421ß0.0ßFor each training sentence, we built a recursive-neural-network based on the dependency tree.ßdummyTurker
2422ß0.0ßThen we obtained the hidden representation h_n for each node and relation feature r_{nm} for each path in the tree, according to Section 4.1.ßdummyTurker
2423ß0.0ßFor each path, we also obtained the ground-truth relation label y^R_{nm}.ßdummyTurker
2424ß0.0ßThe set of tuples (r_{nm}, y^R_{nm}) corresponds to the inputs and labels for D_R.ßdummyTurker
2425ß0.0ßAll meta-parameters are selected based on 3-round random-split validation on terms extraction task in the source domain and relation task over the source and target domain.ßdummyTurker
2426ß0.0ßSpecifically, for source-domain validation data, we evaluate the model's performance on both terms extraction task and relation prediction task.ßdummyTurker
2427ß0.0ßFor target-domain validation data, we evaluate the performance on relation task.ßdummyTurker
2428ß0.0ßWe understand this may not be an optimal approach to tune meta-parameters, but how to set optimal parameters for cross-domain setting is still an open issue.ßdummyTurker
2429ß0.0ßWe would like to thank the Reviewer for the very informative feedback.ßdummyTurker
2430ß0.0ßThe main result is that intrinsic metrics lack correlation with extrinsic ones.ßdummyTurker
2431ß0.0ßWe illustrate this using Figure 1ßdummyTurker
2432ß0.0ßThe issue of translation disagreement exists in cross-language word embeddings evaluation, and we will particularly highlight our mention of this issue in our paper (in Section 3, Item 1ßdummyTurker
2433ß0.0ßWe agree with the need to compare our findings to the previous ones more explicitly.ßdummyTurker
2434ß0.0ßDespite we have surveyed the most close works in “Related work” section, we should probably add more discussion about our results in the context of existing knowledgeßdummyTurker
2435ß0.0ßWe agree that verbs are truly highly relational, so they probably should not be used in cross-language benchmarks.ßdummyTurker
2436ß0.0ßTheir amount in “classic” word similarity datasets is not significant.ßdummyTurker
2437ß0.0ßDatasets that consist only of verbs should be considered separately.ßdummyTurker
2438ß0.0ßWe would like to draw reviewer’s attention to the heatmap (Figure 1)ßdummyTurker
2439ß0.0ßWe would like to thank the Reviewer for the very informative feedback.ßdummyTurker
2440ß0.0ßWe agree with the Reviewer that this distinction is not absolutely correct.ßdummyTurker
2441ß0.0ßThe main point is to compare benchmarks, not models.ßdummyTurker
2442ß0.0ßWe tried to find an answer to “could we robustly estimate extrinsic performance given some intrinsic metrics?”.ßdummyTurker
2443ß0.0ßThere are only a few papers on evaluation of cross-language word embeddings (and we have made our best citing all key papers in that particular field), but we also could add some papers from a broader field.ßdummyTurker
2444ß0.0ßWe agree that the paper lacks some details, but we tried to keep the paper concise and focused.ßdummyTurker
2445ß0.0ßThanks to this comment, we know that more details have to be added.ßdummyTurker
2446ß0.0ßWe promised to release the code and the data.ßdummyTurker
2447ß0.0ßWe decided to omit some technical details to make the paper concise and easier to read.ßdummyTurker
2448ß0.0ßNevertheless, this comment is reasonable and we will try to add more details into the paper.ßdummyTurker
2449ß0.0ßFigure 1 contains a heatmap and a dendrogram.ßdummyTurker
2450ß0.0ßThe heatmap depicts mutual correlations of models scores on different datasets.ßdummyTurker
2451ß0.0ßLighter colors correspond to stronger positive correlation.ßdummyTurker
2452ß0.0ßDendrogram shows how the datasets cluster together.ßdummyTurker
2453ß0.0ßFrom this Figure one can see that models behave very differently on different benchmarks, but there are roughly 4 clusters.ßdummyTurker
2454ß0.0ßThe limitations addressed by the Reviewer listed in Section 3; we will make a reference to this section more explicit.ßdummyTurker
2455ß0.0ßWe think that humans estimate similarity of words in different languages via similarity of concepts these words denote, and many words may correspond to multiple concepts (polysemy).ßdummyTurker
2456ß0.0ßThis may highly affect human assessments for different language pairs.ßdummyTurker
2457ß0.0ßEven within one culture this effect takes place (e.g.ßdummyTurker
2458ß0.0ßRussian word “paradnaya” denotes “hall” in Saint-Petersburg and “dress” in Moscow).ßdummyTurker
2459ß0.0ßWe should probably rewrite that statement in the paper.ßdummyTurker
2460ß0.0ßThere are papers which state that (mono-lingual) human judgments of word similarity tend to be dependent of a background knowledge of an assessorßdummyTurker
2461ß0.0ßWe used dictionaries from (Conneau, 2017), which contain only one possible translation for each word (we omitted multi-sense words).ßdummyTurker
2462ß0.0ßEnglish-Russian embeddings were trained on a comparable corpora of 16B/5B tokens, but parallel datasets are much smaller (the concatenation of EN-RU corpora available at httpßdummyTurker
2463ß0.0ßNo, there was only one translation for each sentence.ßdummyTurker
2464ß0.0ßTranslations were produced manually by human translators which were instructed to paraphrase their translations.ßdummyTurker
2465ß0.0ßWe can use a larger learning rate and batch size to train sLSTM.ßdummyTurker
2466ß0.0ßSo, we can process more examples per second for sLSTM.ßdummyTurker
2467ß0.0ßWe have not done a thorough comparison in terms of training convergence.ßdummyTurker
2468ß0.0ßWe will include bits-per-character results should the paper be accepted.ßdummyTurker
2469ß0.0ß- use as initial model to train regular RNN (as done in the paper).ßdummyTurker
2470ß0.0ß- convert grammar/syntactic rules into RNN with learnable parameters (which can later be adjusted or adapted).ßdummyTurker
2471ß0.0ß- provides an alternative way to combine FSM and RNN LMs.ßdummyTurker
2472ß0.0ß- compress a finite-state model into a smaller RNN model.ßdummyTurker
2473ß0.0ßWe are using in-house datasets (anonymized to conform to double-blind review guideline).ßdummyTurker
2474ß0.0ßMore details can be included should the paper be accepted.ßdummyTurker
2475ß0.0ßWe did not compare to other state-of-the-art models.ßdummyTurker
2476ß0.0ßOur dataset comes from a recently released dataset, which contains questions, documents and coarse-grain annotations on whether or not an entire assertion is a correct answer.ßdummyTurker
2477ß0.0ßHowever, these coarse-grain annotations could not support explicit reasoning and deep question understanding.ßdummyTurker
2478ß0.0ßTo address these issues, we make a huge effort to do fine-grained annotations, resulting in a relatively large dataset, which we plan to release to the community.ßdummyTurker
2479ß0.0ßIn our preliminary experiment, we use a rule-based approach to detect anchor and measure the coverage of correct answers through 1-hop and 2-hop paths.ßdummyTurker
2480ß0.0ßWe observe that the coverage of 1-hop candidates is 55.6% and that of 2-hop candidates is 69.6%.ßdummyTurker
2481ß0.0ßThis indicates that deep/multi-hop inference is required in this dataset.ßdummyTurker
2482ß0.0ßThe input of our task includes a question, a document and assertions automatically extracted from the document.ßdummyTurker
2483ß0.0ßThe output is a subject or an object from the assertions which could correctly answer the question.ßdummyTurker
2484ß0.0ßThe task could be viewed as a ranking problem, in which candidate answers come from subjects and objects of the assertions.ßdummyTurker
2485ß0.0ßPlease kindly refer to line 116-124 for a formal definition.ßdummyTurker
2486ß0.0ßThus, we drop the triples that are not correctly extracted.ßdummyTurker
2487ß0.0ßAssertions with pronouns are saved to call for multi-hop reasoning.ßdummyTurker
2488ß0.0ßWe will give more details about our implementation of Borders’s approach.ßdummyTurker
2489ß0.0ßWe plan to run a strong RCQA system as an additional baseline on our dataset.ßdummyTurker
2490ß0.0ßWe are very grateful for taking your precious time to read our paper and suggesting the weakness points.ßdummyTurker
2491ß0.0ßWe are really sorry for this inconvenience.ßdummyTurker
2492ß0.0ßWe forgot to refer to the supplemental material...ßdummyTurker
2493ß0.0ßPlease look at Figure 1 in the supplemental material.ßdummyTurker
2494ß0.0ß"This shows an example of the difference between ""short titles"" and headlines."ßdummyTurker
2495ß0.0ßThe left figure (a) shows the list of news topics (important news articles) in the news aggregation feature, and the right figure (b) shows the entry page of the first topic in the list.ßdummyTurker
2496ß0.0ßOur task is to generate a short title (13.5 characters in the left figure) from the headline and lead of an article (blue and gray sentences in the right figure).ßdummyTurker
2497ß0.0ßThis is almost the same way as the ordinary work of editors in this news aggregation service.ßdummyTurker
2498ß0.0ßA short title has two advantages compared to a normal headline; one is quick understandability of the content, and the other is saving a display area by using just one line.ßdummyTurker
2499ß0.0ßAs implied in the figure, there is no case where a short title is very close to or even the same as the original headline, since the professional editors always rethink it for the limit of 13.5 characters.ßdummyTurker
2500ß0.0ßWe will add such detailed explanations to clarify our task.ßdummyTurker
2501ß0.0ßYour suggestion is correct.ßdummyTurker
2502ß0.0ßThe complex data includes a style transfer.ßdummyTurker
2503ß0.0ßPlease look at Table 4 in the supplemental material, which represents a typical example.ßdummyTurker
2504ß0.0ß"In the examples of ""Ex.1"",  the ""Editor"" result consists of two short phrases, which were extracted from the first sentence in the original headline (""Input"") so as to delete a preposition word (""With"") for space saving."ßdummyTurker
2505ß0.0ß"We did not directly address this problem as a style transfer, but we regard it as a translation (from a normal language to a ""space saving"" language)."ßdummyTurker
2506ß0.0ßYour point is precisely related to why we used two information sources.ßdummyTurker
2507ß0.0ßAs you said, we know that the lead sentence often includes the most important information.ßdummyTurker
2508ß0.0ßHowever, our preliminary experiments showed a model trained with headline-title pairs performed much better than one trained with lead-title pairs.ßdummyTurker
2509ß0.0ßThis is possibly caused by different styles, as well as lengthy lead sentences.ßdummyTurker
2510ß0.0ßBecause titles have a more similar style to headlines than leads, we wanted to use two information sources: similar-style headline and informative lead.ßdummyTurker
2511ß0.0ßWe will add more explanation to clarify this point.ßdummyTurker
2512ß0.0ßWe agree that the improvement looks small at a first glance, but we believe such a small improvement can yield a big impact for a real service.ßdummyTurker
2513ß0.0ßIn fact, even 0.01\% improvement of the click-through rate (CTR) in this popular service (Yahoo!ßdummyTurker
2514ß0.0ßJapan) can yield ad revenue of million dollars.ßdummyTurker
2515ß0.0ßIn terms of the average of readability and usefulness, we confirmed that Gate+Query (3.389) performed better than OpenNMT (3.345), which means the improvement of about 1\%.ßdummyTurker
2516ß0.0ßThis is promising from a practical standpoint, since headline quality is related to the CTR of ads.ßdummyTurker
2517ß0.0ß"As for ""QueryBased"", the basic idea, where one information source can support to understand the other information source, can be applied to our task."ßdummyTurker
2518ß0.0ßIn our case, we expect that QueryBased can enhance reformulation/compression of headlines by using the informative leads.ßdummyTurker
2519ß0.0ß"W1:""novelty_aspect_of_the_submission"""ßdummyTurker
2520ß0.0ßWe formulate the problem as multiview learning using twoßdummyTurker
2521ß0.0ßmeta-views: language and representation.ßdummyTurker
2522ß0.0ßThis is novel in entity typing, and to the best of our knowledge,ßdummyTurker
2523ß0.0ßit is not used in relation extraction (which is a related task) either.ßdummyTurker
2524ß0.0ß"Also, using attention to combine multiple ""entity"" representations,"ßdummyTurker
2525ß0.0ßas well as improving singleview entity typing models is novel.ßdummyTurker
2526ß0.0ßAlso, we see our dataset as a goodßdummyTurker
2527ß0.0ßevaluation for single/cross/multi lingual word embeddings.ßdummyTurker
2528ß0.0ßWe will discuss that in the possible final version.ßdummyTurker
2529ß0.0ß"W2:""Related_work_does_not_tease_apart_the_salient_features_of_the_present_work"""ßdummyTurker
2530ß0.0ßWe agree that the related work was too short, we will expand it to make ourßdummyTurker
2531ß0.0ßcontributions/features more clear.ßdummyTurker
2532ß0.0ß"Q1:""language_specific_matrices?"""ßdummyTurker
2533ß0.0ß"This should have been ""view specific matrices""."ßdummyTurker
2534ß0.0ßEach view is presumably in different vector space and therefore we first needßdummyTurker
2535ß0.0ßto transform them into a shared space, before applying ATT, MAX or AVG.ßdummyTurker
2536ß0.0ßThese view matrices are the transformation matrices which are trained like other modelßdummyTurker
2537ß0.0ßparameters.ßdummyTurker
2538ß0.0ß"Q2:""...present_technique...superior_to...other_techniques?"""ßdummyTurker
2539ß0.0ßAttention-based multiview learning is very recent and just used in Qu et al.ßdummyTurker
2540ß0.0ß(2017), based onßdummyTurker
2541ß0.0ßour knowledge, and for a different task (node representation).ßdummyTurker
2542ß0.0ßBased on Qu et al., attention outperformed multiple strong baselines,ßdummyTurker
2543ß0.0ßsince it can use complementaryßdummyTurker
2544ß0.0ßinformation of views and also weights them based on their importance.ßdummyTurker
2545ß0.0ßOur attention model is similar, and therefore it also holds theseßdummyTurker
2546ß0.0ßfeatures and could beat baselines.ßdummyTurker
2547ß0.0ßSo it is a competent approach for multiview learning.ßdummyTurker
2548ß0.0ßText classification is different from entity typing, with longer history of workßdummyTurker
2549ß0.0ßin multiview learning.ßdummyTurker
2550ß0.0ßHowever, our multi-view modeling is novel in entity typing.ßdummyTurker
2551ß0.0ßEven inßdummyTurker
2552ß0.0ßtext classification, we are not aware of any work with multiple languagesßdummyTurker
2553ß0.0ßand multiple levels of representations, so even with respectßdummyTurker
2554ß0.0ßto text classification our method is new.ßdummyTurker
2555ß0.0ßIn addition to the ROUGE evaluation, we also conducted human evaluation on 20 random samples from CNN/DailyMail test set.ßdummyTurker
2556ß0.0ßThree participants were asked to compare the generated summaries with the human summaries, and assess each summary from four independent perspectives: (1) Informative (2) Concise (3) Coherent (4) Fluent.ßdummyTurker
2557ß0.0ßEach property is assessed with a score from 1(worst) to 5(best).ßdummyTurker
2558ß0.0ßThe average results are presented as following:ßdummyTurker
2559ß0.0ßMethod                      Informative          Concise           Coherent           FluentßdummyTurker
2560ß0.0ßLead-3                            3.48                 3.20                  3.83                  4.01ßdummyTurker
2561ß0.0ßSeq2seq-baseline          3.08                 3.01                   2.96                  3.69ßdummyTurker
2562ß0.0ßCoverage                        3.26                 3.22                   3.19                  3.74ßdummyTurker
2563ß0.0ßOur Model                      3.73                  3.49                  3.51                  3.79ßdummyTurker
2564ß0.0ßThe results show that our model consistently outperforms the previous state-of-the-art method Coverage.ßdummyTurker
2565ß0.0ßDue to the limit of words in responses, detailed analysis will be added in the revision of our paper.ßdummyTurker
2566ß0.0ßThanks for your inspiring comments about the challenge of sentence generation.ßdummyTurker
2567ß0.0ßSince this work mainly focuses on the information selection process during document summarization, we didn’t analyze it explicitly in our paper.ßdummyTurker
2568ß0.0ßAccording to the above human evaluation results, our model is able to generate more concise summaries than Coverage.ßdummyTurker
2569ß0.0ßWe will further explore this problem deeper in the following work.ßdummyTurker
2570ß0.0ßThis paper uses the non-anonymized data as See et al.ßdummyTurker
2571ß0.0ß(2017) and Paulus et al.ßdummyTurker
2572ß0.0ß(2017), which we believe is the favorable problem to solve because it requires no pre-processing.ßdummyTurker
2573ß0.0ßIn order to be directly comparable with the reported results in recent state-of-the-art methods, we use the same settings with Graph-attention (Tan et al., 2017).ßdummyTurker
2574ß0.0ßDue to the limit of total words in responses, we are sorry that we can’t reply all arguments and questions.ßdummyTurker
2575ß0.0ßThanks for your extraordinarily careful review.ßdummyTurker
2576ß0.0ß(1) We have normalized every feature by z-scores when calculating the Euclidean distance.ßdummyTurker
2577ß0.0ßWe will describe more clearly in the revised version.ßdummyTurker
2578ß0.0ß(2) Conducting a more in-depth analysis of the sentence complexity within the books will be part of our future work.ßdummyTurker
2579ß0.0ßConducting the experiment where the “simple” test is performed by first-year English major students and graduates, etc.ßdummyTurker
2580ß0.0ßis not very meaningful to the evaluation, because most of the results can be anticipated.ßdummyTurker
2581ß0.0ßNevertheless, we can not deny that it could make the evaluation more convincing.ßdummyTurker
2582ß0.0ßYour suggestion of evaluation by item analysis in testing theory is feasible.ßdummyTurker
2583ß0.0ßDue to the page limit, we cannot use all the metrics.ßdummyTurker
2584ß0.0ßThus we chose the most suitable ways of evaluation for our task.ßdummyTurker
2585ß0.0ßHowever, a more comprehensive evaluation will be part of our future work.ßdummyTurker
2586ß0.0ßWe thank the reviewer for the helpful comments!ßdummyTurker
2587ß0.0ßIn the paper we directly included the scores of the logistic regression model as reported in previous work and were not able to do a full analysis on its results.ßdummyTurker
2588ß0.0ßHowever, we will add the reviewer’s observation with appropriate discussions to the paper.ßdummyTurker
2589ß0.0ßDuring the training and testing of the model, we mask the subject entities in each example with a special SUBJ-<NER> token, where <NER> is the entity type of the subject.ßdummyTurker
2590ß0.0ßWe mask the object entities similarly.ßdummyTurker
2591ß0.0ßThe special tokens are finetuned during training as other words in the vocabulary.ßdummyTurker
2592ß0.0ßWe will make this clear in the paper.ßdummyTurker
2593ß0.0ßThe POS/NER embeddings are initialized randomly and finetuned on the training set.ßdummyTurker
2594ß0.0ßWe will make this clear.ßdummyTurker
2595ß0.0ßThanks for the comment.ßdummyTurker
2596ß0.0ßThe experiment results are run by the authors as for the experiment data, there is no exact training-test data partition in the original publication of the baseline models.ßdummyTurker
2597ß0.0ßTherefore, we re-implemented and run these models in our datasets.ßdummyTurker
2598ß0.0ßThanks for the comment.ßdummyTurker
2599ß0.0ßWe will add the explanation of the automatic evaluation metrics.ßdummyTurker
2600ß0.0ßThe 3 metrics are used to measure the semantic similarity between the generated responses and the ground truth.ßdummyTurker
2601ß0.0ßThey are verified to be more effective in conversation/dialogue generation than BLEU and Perplexity by Serban et al.ßdummyTurker
2602ß0.0ß(2016a) and Serban et al.ßdummyTurker
2603ß0.0ß(2017b).ßdummyTurker
2604ß0.0ßThanks for the comment.ßdummyTurker
2605ß0.0ßWe will add more details about the measures of human evaluation.ßdummyTurker
2606ß0.0ßDue to the space limitation, we didn’t present the descriptions in the current version.ßdummyTurker
2607ß0.0ßThanks for the comment.ßdummyTurker
2608ß0.0ßWe will modify the related papers that support our claims here and cite the relevant papers in dialogue research.ßdummyTurker
2609ß0.0ßThe word embeddings are trained on the Ubuntu and Opensubtitles data respectively.ßdummyTurker
2610ß0.0ßThe dimension of the hidden layer is a tunable parameter.ßdummyTurker
2611ß0.0ßTherefore, the parameter settings here are obtained when the corresponding models get the best performance in dev set.ßdummyTurker
2612ß0.0ßThanks for the comment.ßdummyTurker
2613ß0.0ßWe will revise the format of reference and carefully proofread the paper.ßdummyTurker
2614ß0.0ßThanks for the comment.ßdummyTurker
2615ß0.0ßTo the best of our knowledge, our bidirectional dependency tree network is the first to adopt a bidirectional propagation approach on the dependency-based parse tree.ßdummyTurker
2616ß0.0ßIt can effectively extract the dependency information between words as shown in our experiments.ßdummyTurker
2617ß0.0ßWe believe that this network has the potential to be applied to other NLP tasks as well, such as Translation and NER.ßdummyTurker
2618ß0.0ßThere are three differences between our work and the existing tree-LSTM model.ßdummyTurker
2619ß0.0ßFirstly, we use a mapping function $r_(k)$ to map the syntactic relation type to its corresponding parameter matrix instead of using the children order.ßdummyTurker
2620ß0.0ßSecondly, we encode the syntactic relation as in Eqs.ßdummyTurker
2621ß0.0ß(2-5).ßdummyTurker
2622ß0.0ßIt is proven effective in Table 4.ßdummyTurker
2623ß0.0ßFinally, our tree-LSTM model is bidirectional, which works better than one directional as shown in the ablation experiments.ßdummyTurker
2624ß0.0ß$W_{r(k)}$ and $U_{r(k)}$ are different parameters.ßdummyTurker
2625ß0.0ßThe former regards syntactic relations type as another embedding like words.ßdummyTurker
2626ß0.0ßThe latter propagates hidden representations from children nodes to their parent.ßdummyTurker
2627ß0.0ßIt is proven correct and effective in our experiments.ßdummyTurker
2628ß0.0ßOnly a brief description of CRF layer and decoding processing is given with references because of the page limit and also because we thought that CRF is fairly well known and has been used in some other deep models.ßdummyTurker
2629ß0.0ßWe will add more description to the revised paper.ßdummyTurker
2630ß0.0ßThanks for your suggestion.ßdummyTurker
2631ß0.0ßWe have performed statistical significance test using the t-test.ßdummyTurker
2632ß0.0ßThe p-value for L-14 is less than 0.01, and the p-value for R-16 is less than 0.05, which show that the improvements are significant.ßdummyTurker
2633ß0.0ßThe improvement for R-14 is not significant although we have a little higher average F1 scores based on 20 runs.ßdummyTurker
2634ß0.0ßThanks for pointing these out.ßdummyTurker
2635ß0.0ßWe will tone them down.ßdummyTurker
2636ß0.0ßTheir advantages and shortcomings will be described in the revised version.ßdummyTurker
2637ß0.0ßThose empty slots indicate that the baselines did not use the corresponding datasets in their experiments.ßdummyTurker
2638ß0.0ßWe reproduced some of them because the source codes are not available, but the results have biases.ßdummyTurker
2639ß0.0ßWe finally decided to compare with their original results, which is appropriate as the test data and the evaluation program are the same (they are both provided by SemEval).ßdummyTurker
2640ß0.0ßWe will release our source code once the paper is accepted.ßdummyTurker
2641ß0.0ßThe embedding size is a hyper-parameter, and 300 is commonly used in many embedding papers (including (Wang et al.ßdummyTurker
2642ß0.0ß2016b)) and has been shown quite appropriate.ßdummyTurker
2643ß0.0ßWe also find 300 gives good results in our case.ßdummyTurker
2644ß0.0ß300 can still be applied to other datasets.ßdummyTurker
2645ß0.0ßOf course, it may not be optimal for every dataset.ßdummyTurker
2646ß0.0ßThank you for your valuable comments!ßdummyTurker
2647ß0.0ßOur GAN based generator and data relabeling method are components which can be integrated with other neural models.ßdummyTurker
2648ß0.0ßWe will apply our method to other neural models, cross-domain datasets or even other NLP tasks in future to verify the generalization and expansibility.ßdummyTurker
2649ß0.0ßOur model without GAN based generator is a BGRU+ATT network, which achieves similar performance (PR curve=0.51) with the PCNN+ATT model on the dataset NYT.ßdummyTurker
2650ß0.0ßMany thanks for your valuable comments.ßdummyTurker
2651ß0.0ßWe acknowledge this reviewer for pointing that out, and we had compared with 3 different models on Universal Dependencies English dataset to show the importance of the ReLU function and residual connections:ßdummyTurker
2652ß0.0ß1.ßdummyTurker
2653ß0.0ßA softmax attention model with residual connection: UAS = 84.4%.ßdummyTurker
2654ß0.0ß2.ßdummyTurker
2655ß0.0ßReLU non-local feature fusion without residual connection: UAS = 85.6%.ßdummyTurker
2656ß0.0ß3.ßdummyTurker
2657ß0.0ßA softmax attention model without residual connection: UAS = 84.8%ßdummyTurker
2658ß0.0ßThe experimental results showed that the proposed method of ReLU non-local feature fusion outperforms softmax attention significantly, and the residual connection is helpful to train non-local models.ßdummyTurker
2659ß0.0ßWe will supplement this experiment results into the revised version.ßdummyTurker
2660ß0.0ßThe result of baseline system on PTB and CTB will be added in revised version.ßdummyTurker
2661ß0.0ßIn our prelimenary experiments, our baseline system achieved similar performance with Kiperwasser and Goldberg (2016).ßdummyTurker
2662ß0.0ßOur Baseline        PTB UAS = 93.2% LAS = 90.9%ßdummyTurker
2663ß0.0ßCTB UAS = 85.6% LAS = 84.4%ßdummyTurker
2664ß0.0ßKiperwasser and Goldberg (2016)           PTB UAS = 93.2% LAS=91.2%ßdummyTurker
2665ß0.0ßCTB UAS = 86.6% LAS=85.1%ßdummyTurker
2666ß0.0ßMany thanks for your valuable comments!ßdummyTurker
2667ß0.0ßWe will carefully corrected the mistakes in writting in the revised version.ßdummyTurker
2668ß0.0ßWe used the arc-standard dependency scheme in the all experiments, and train/validation/testing data is splited following the method as proposed by Chen and Manning (2014).ßdummyTurker
2669ß0.0ßThe compared models for the PTB/CTB comparison are both fusion models, which is the most powerful model we can find in the prelimenary experiments on Universal Dependencies.ßdummyTurker
2670ß0.0ßOur code will be released publicly in the future.ßdummyTurker
2671ß0.0ßWe used the same hyper-parameters setting as Kiperwasser and Goldberg (2016) except the learning rate, which was set to the default learning rate for Adam optimizer of MXNet framework.ßdummyTurker
2672ß0.0ßIn the experiments, the hyper-parameters were not carefully tuned.ßdummyTurker
2673ß0.0ßIt would be a little difficult to build this connection.ßdummyTurker
2674ß0.0ßIn fact, many models are facing with the difficulities in sparse issue when using Chinese data in Universal Dependencies for evaulation.ßdummyTurker
2675ß0.0ßAnd non-local feature fusion is not proposed to address such sparsity problem.ßdummyTurker
2676ß0.0ßThanks for your fruitful and helpful comments and suggestions, which are very important to refine the paper.ßdummyTurker
2677ß0.0ßSorry for the confusion.ßdummyTurker
2678ß0.0ßSince long sentences are more likely to suffer from the exposure bias problem, we compare our method with the baseline in terms of source sentence lengths in figure 3.ßdummyTurker
2679ß0.0ßOur method gains more improvements on longer sentences, which confirms that our method can ease the exposure bias problem.ßdummyTurker
2680ß0.0ßHowever directly measuring the improvement from exposure bias problem is difficult as the reviewer pointed, we will resort to more analysis, such as evaluating qualities of 5-word prefixes and suffixes of translations using partial BLEU [Lemao et al.ßdummyTurker
2681ß0.0ß2016] in the future version.ßdummyTurker
2682ß0.0ßWe will refine the description of the sampling produce and add more explanations in the camera-ready version.ßdummyTurker
2683ß0.0ßWe will refine these details, including footnote and Figure 2.ßdummyTurker
2684ß0.0ßWe thank the reviewers for their invaluable comments and constructive suggestions.ßdummyTurker
2685ß0.0ßWe address the issues raised in the reviews below.ßdummyTurker
2686ß0.0ßThank you for the suggestion.ßdummyTurker
2687ß0.0ßThis ambiguity was chosen deliberately for author anonymity.ßdummyTurker
2688ß0.0ßWe plan to describe the details on data upon release including the list of news media outlet, language traits, and a preprocessing methods used in Section 3.2 Dataset Generation.ßdummyTurker
2689ß0.0ßFor fair comparison, we trained each models (in Table 2) with FNC-1 dataset only.ßdummyTurker
2690ß0.0ßIn a single model case, feature-based machine learning method, the XGB, outperformed the deep learning models.ßdummyTurker
2691ß0.0ßWe conjecture this is caused by insufficient variations in the FNC-1 data that is used to train complex neural models.ßdummyTurker
2692ß0.0ßThe FNC-1 dataset is sizable (around 50,000 rows), yet it was built from only 1,683 distinct news articles.ßdummyTurker
2693ß0.0ßThe data instances were generated by modifying {headlines}, indicating that 29.7 data instances have identical {body text} on average.ßdummyTurker
2694ß0.0ßSuch redundancy likely affected the result.ßdummyTurker
2695ß0.0ßHowever, given that in real systems it is uncommon for news  articles to have identical {body text}, we consider deep learning models to perform consistently well in the wild.ßdummyTurker
2696ß0.0ßWe presented relevant materials in Appendix C.ßdummyTurker
2697ß0.0ßTable 1 intends to illustrate the idea of our proposed method to represent a word embedding using a number of attribute embeddings.ßdummyTurker
2698ß0.0ßIn our case, those latent attributes are automatically learned from the parallel training data.ßdummyTurker
2699ß0.0ßIn general, the number attributes and its values can be arbitrarily chosen only if it gives enough unique combinations to represent all words.ßdummyTurker
2700ß0.0ßFor simplicity, we chose to use only two attributes, and each of which contains \sqr(|V|) values.ßdummyTurker
2701ß0.0ßThat is, for a given vocabulary, we need at least |a_1| and |a_2| attribute values, such that |V| = |a_1| x |a_2|.ßdummyTurker
2702ß0.0ßOur method is very different from that of Morin and Bengio (2005) in several aspects.ßdummyTurker
2703ß0.0ß1) We use RNN to learn the embeddings of attribute values of a word representation which is context dependent; 2) We do not impose any prior hierarchical knowledge (as they use WordNet to obtain the word clusters) in the learning.ßdummyTurker
2704ß0.0ßThe attribute value representations of word embeddings are automatically learnt.ßdummyTurker
2705ß0.0ßOur method is ready for any language without relying on other hierarchical decomposition tools; and 3) Our method is trained to improve the translation quality by better capture the syntactic and semantic mean of a word as well as reducing the time and space complexity, while their work only focuses on the latter aspect.ßdummyTurker
2706ß0.0ßThe details of the baseline model are presented in Section 2, while the used configurations are provided in Section 4.1.ßdummyTurker
2707ß0.0ßTo conduct a comprehensive comparison against the state-of-the-art architectures.ßdummyTurker
2708ß0.0ßWe have prepared the following English-German translation results of various models, including the adapted hierarchical probabilistic model in NMT by Chen et al.ßdummyTurker
2709ß0.0ß(2016).ßdummyTurker
2710ß0.0ßIn the final paper, we will include this part of evaluations as suggested.ßdummyTurker
2711ß0.0ßModel			newstest13	newstest14	newstest15ßdummyTurker
2712ß0.0ßLuong et al.ßdummyTurker
2713ß0.0ß(2015)	-		-		22.0ßdummyTurker
2714ß0.0ßJean et al.ßdummyTurker
2715ß0.0ß(2015)	-		19.4		-ßdummyTurker
2716ß0.0ßSennrich et al.ßdummyTurker
2717ß0.0ß(2016)	-		-		22.8ßdummyTurker
2718ß0.0ßChung et al.ßdummyTurker
2719ß0.0ß(2016)	-		21.3		23.5ßdummyTurker
2720ß0.0ßChen et al.ßdummyTurker
2721ß0.0ß(2016)	21.0		19.0		21.9ßdummyTurker
2722ß0.0ßOur Model		22.09		21.55		24.07ßdummyTurker
2723ß0.0ßOur Model (Big)		23.34		22.27		25.39ßdummyTurker
2724ß0.0ßThe performance of LSTM-CRF on OntoNotes* is (1) Dev.ßdummyTurker
2725ß0.0ß: Pr(92.19), Re(92.63), F1(92.41); (2) Test: Pr(93.03), Re(92.60) F1(92.81).ßdummyTurker
2726ß0.0ßPOM scheme helps in most cases except the cases where different entities appear consecutively, like Figure 5(b); and consistent with your studies, we also find that such consecutive entities are rare.ßdummyTurker
2727ß0.0ßPOM relies on CRFs' capability of structure learning.ßdummyTurker
2728ß0.0ßGenerally, to extract a named entity, we need to identify its non-common word and determine its boundaries.ßdummyTurker
2729ß0.0ßCRFs learns the structure and determines the boundaries, but using CRFs alone may fail in the cases (especially in domain-specific entity recognition) where common text contains phrases that have similar structure as named entities.ßdummyTurker
2730ß0.0ßThe non-common word helps distinguish the named entities from the phrases with similar structures.ßdummyTurker
2731ß0.0ßThe strength of POM scheme is protects the predictive power of non-common words as well as the features that depend on words, like POS tags; this further protects CRFs' structure learning capability.ßdummyTurker
2732ß0.0ßBIO scheme instead undermines those properties.ßdummyTurker
2733ß0.0ßPower law in entity length is discovered as a linguistic phenomenon, like Zipf's law in words' rank-frequency.ßdummyTurker
2734ß0.0ßIts direct use is its mean: less than 2 words in an average entity (Line 394-398, Line 430-431).ßdummyTurker
2735ß0.0ßNER includes two sub-tasks: named entity extraction and classification.ßdummyTurker
2736ß0.0ßThe extraction is related to syntax and the classification to semantics.ßdummyTurker
2737ß0.0ßAccording to Chomsky's Syntactic Structures (1957, Page 93-94), syntax is not necessarily related to semantics and semantics does not necessarily affect syntax.ßdummyTurker
2738ß0.0ßIn Syntactic Structures, Chomsky critisizes the misunderstanding of syntax and semantics.ßdummyTurker
2739ß0.0ßDefining the named entity extraction and classification as an end-to-end task is a kind of such misunderstanding.ßdummyTurker
2740ß0.0ßEvidence from Chomsky's syntactic theory to our analysis (proper nouns are the syntactic features) and experiments (semantic features learned by LSTM-CRF for semantic classification does not help the syntactic extraction; Line 684-693, 725-729) suggests us to address the extraction and classification separately.ßdummyTurker
2741ß0.0ßChomsky's Syntactic Structures can be found at http://www.linguist.univ-paris-diderot.fr/~edunbar/ling499b_spr12/readings/syntactic_structures.pdfßdummyTurker
2742ß0.0ßIn experiments, we incorporate entity types in the labeling tags but reports only the extraction results (Line 594-497, 610-612).ßdummyTurker
2743ß0.0ßWe will give the discussion about the similarity and the difference between our method and that listed in weakness 1.ßdummyTurker
2744ß0.0ßWe will also give the discussion about the similarity and the difference between our method and other multimodal representation learning methods.ßdummyTurker
2745ß0.0ßWe will add the experimental results of the human subjective evaluation.ßdummyTurker
2746ß0.0ßPerhaps we need further the analysis of the case.ßdummyTurker
2747ß0.0ßThe reason will be added in Section 3.2.ßdummyTurker
2748ß0.0ßWe will add the performance of single MultiCap-{X,H,C,HC} in AIC-ICC dataset.ßdummyTurker
2749ß0.0ßWe thank the reviewer for their thorough work.ßdummyTurker
2750ß0.0ßWe aimed to follow the writing in the DR16 paper.ßdummyTurker
2751ß0.0ßGiven the opportunity we will improve the writing and, particularly, will use a clearer notation.ßdummyTurker
2752ß0.0ßThis is a short ACL paper submission.ßdummyTurker
2753ß0.0ßWe believe that our results - which show that a theoretically justified idea proposed in a previous paper, can actually have substantial practical impact when applied in the context of another algorithm - justify a short paper.ßdummyTurker
2754ß0.0ßWe experiment with standard, widely explored, sequence tagging tasks.ßdummyTurker
2755ß0.0ßJudging based on the number of papers published on ACL conferences, NER is a major task that still draws lots of attention.ßdummyTurker
2756ß0.0ßWe would even argue that based on SOTA results, NER might be a more difficult task than dependency parsing.ßdummyTurker
2757ß0.0ßOur initial idea was to fit the model to an existing corpus and generate semantic and coherent text.ßdummyTurker
2758ß0.0ßFuture work can be applied to image descriptions, machine translations, and other conditional text generation tasksßdummyTurker
2759ß0.0ßWe will add it in later revisionsßdummyTurker
2760ß0.0ßUnfortunately, we can't find any related research regarding echoing problem or something similar.ßdummyTurker
2761ß0.0ß"We can explain the ""novelty"" of this problem as a result of our more sophisticated evaluation process."ßdummyTurker
2762ß0.0ßMost of the prior papers evaluate the quality of their models on samples containing one positive and small (fixed) amount of negative responses (for instance see Ubuntu Dialog Corpus).ßdummyTurker
2763ß0.0ßThese negative responses are selected randomly, or extracted by some heuristics, so the chance to pick negative responses very similar to the context is not so high.ßdummyTurker
2764ß0.0ßAs for us, we compute the quality across all the given test dataset with hundreds candidate responses, so the chance to meet echo-responses is significant.ßdummyTurker
2765ß0.0ßThat's how we came across the echoing problem.ßdummyTurker
2766ß0.0ßRegarding the comparison with intent detection-based systems: it's an interesting question if these systems are exposed to the echoing problem (since usually response structure for each intent is quite fixed).ßdummyTurker
2767ß0.0ßBut that's another big idea to study, thanks for your remark.ßdummyTurker
2768ß0.0ßYes, we understand an importance of evaluation on public and widely used datasets (in particular, we already performed some experiments on Ubuntu Dialog Corpus), but due to limitation on the paper size we could not provide quality metrics, result samples and their interpretation for such datasets.ßdummyTurker
2769ß0.0ßSo we decided, on the one hand, to share our own data, on the other hand – to provide evaluation results on it.ßdummyTurker
2770ß0.0ßYes, you are right.ßdummyTurker
2771ß0.0ßThis work mainly focuses on transferring the relation classification knowledge across languages, rather than an end-to-end system which is completely language independent.ßdummyTurker
2772ß0.0ßThe reason might be that BiLSTM is fitter for encoding order information and long-range context dependency for sequence labeling problem, while CNN is suited to extracting local and position-invariant features.ßdummyTurker
2773ß0.0ßFor relation classification, the essential features are always distributed around the entity mentions, thus CNN may better utilize this information.ßdummyTurker
2774ß0.0ßOn the other hand, the work of “Yin et al., Comparative study of CNN and RNN for natural language processing” (Table 1) also demonstrated the conclusion that CNN is slightly better than LSTM for relation classification.ßdummyTurker
2775ß0.0ßLine 235: A pseudo-parallel sentence is the automatically translated text word-by-word by the MT module from English to Chinese.ßdummyTurker
2776ß0.0ßLine 486: The relation types of English dataset is the same as Chinese dataset, while the texts are not translated between English and Chinese.ßdummyTurker
2777ß0.0ßWe will clarify these issues in the revised version.ßdummyTurker
2778ß0.0ßReply-to-weakness-argument#1: ‌ßdummyTurker
2779ß0.0ßThe confidence score is a multiplicative factor for the learning rate and what we do is tuning the step-sizes dynamically in the backpropagation.ßdummyTurker
2780ß0.0ßThis is an elegant way for incorporating the confidence and better learning pace is rooted in the fact that we have a better control on the learning rate (similar to adaptive learning rate: http://proceedings.mlr.press/v28/ranganath13.pdf).ßdummyTurker
2781ß0.0ßWe will add a mathematical explanation on what we do and how this helps based on this discussion in the camera-ready.ßdummyTurker
2782ß0.0ßThe confidence network receives the weak label as the input besides the main input and aims at learning the “difference” between the weak and true label which is simpler than learning to predict the label.ßdummyTurker
2783ß0.0ßIn a way, confidence network needs to learn, kind of, residual information about the labels.ßdummyTurker
2784ß0.0ßWe will make this point more explicit in the camera-ready version.ßdummyTurker
2785ß0.0ßThis is the exact point that we mentioned as the immediate future work in our paper.ßdummyTurker
2786ß0.0ßThere are many factors involved like the |V|:|U| ratio, the quality of the weak annotation, etc.ßdummyTurker
2787ß0.0ßWe have done some experiments in which we used weak annotators with different quality and we noticed that the if the weak annotations are extremely bad, it hurts the quality of the learned representation and if it is extremely good, there is less room for improvement.ßdummyTurker
2788ß0.0ßWe are also aware that a group of researchers who employed our model on the task of intent classification in dialogue systems which is a multi-label classification in which CWS achieves good results.ßdummyTurker
2789ß0.0ßWe thank the reviewer for his/her thoughtful remarks.ßdummyTurker
2790ß0.0ßWe agree that the automatically built corpus includes noisy samples, which might affect the quality of the intrinsic evaluation.ßdummyTurker
2791ß0.0ßSuch a task is actually in need of a manually labeled corpus for accurate intrinsic evaluation.ßdummyTurker
2792ß0.0ßIn the absence of a manually labeled corpus, we would like to stress the quality of the available automatically built corpus.ßdummyTurker
2793ß0.0ßThis can be deduced from the quality of the word embeddings as shown in Table 1.ßdummyTurker
2794ß0.0ßIn addition, we have been using the science-domain word embeddings in our internal experiments for other science-related NLP projects such as question generation for science textbooks and we are seeing significant performance improvements.ßdummyTurker
2795ß0.0ßWe accept this point and will work on addressing it in the final version.ßdummyTurker
2796ß0.0ßWe have focused our examples on words that have a certain meaning in the science domain and another dominant meaning in other domains.ßdummyTurker
2797ß0.0ßThis is the basis for the examples chosen in Table 1, which we found to exhibit the most significant difference in the sample that we examined.ßdummyTurker
2798ß0.0ßOther than that, the process was not very selective but the differences were apparent.ßdummyTurker
2799ß0.0ßOur only concern in expanding to the World Wide Web is the process of automatically scraping the pages--a process that would either require expensive manual labor, or introduce significant noise.ßdummyTurker
2800ß0.0ßNevertheless, the underlying algorithm is not designed specifically to the content of Wikipedia and should be applicable to any set of text articles of multiple domains.ßdummyTurker
2801ß0.0ßThis question might be the result of a misunderstanding.ßdummyTurker
2802ß0.0ßThe use of 100 Wikipedia articles as a seed set is an artificial setup that we don’t expect the user to have.ßdummyTurker
2803ß0.0ßIn other words, if a domain in Wikipedia is only of size 100, we don’t expect the user to use any of these articles as a seed set.ßdummyTurker
2804ß0.0ßWe expect the user to have a certain set of articles pertinent to his/her application collected from somewhere other than Wikipedia, and would only use Wikipedia to augment that initial corpus.ßdummyTurker
2805ß0.0ßAddressed above in reply to weakness argument 3.ßdummyTurker
2806ß0.0ßReviewer 3 raises a good point that has also been raised by Reviewer 1.ßdummyTurker
2807ß0.0ßWe agree that the BiLSTM may have an advantage on large data tasks.ßdummyTurker
2808ß0.0ßWe should make this observation more explicit.ßdummyTurker
2809ß0.0ßWe agree with the reviewer that our discussion of the GO token could be improved.ßdummyTurker
2810ß0.0ßOur main point in this work is that a models with simple architectures can outperform complex architectures.ßdummyTurker
2811ß0.0ßThe fact that we need this additional GO token does not contradict this point.ßdummyTurker
2812ß0.0ßAlthough we agree that further investigation is necessary to fully understand its role in the decoding process.ßdummyTurker
2813ß0.0ßWe will expand Section 2.3 with more description and examples.ßdummyTurker
2814ß0.0ßWe agree that our description of the GO token is unclear, please see response given to Reviewer 2.ßdummyTurker
2815ß0.0ßThank for your useful suggestions, we address you questions as follows:ßdummyTurker
2816ß0.0ßFor weakness1: To verify the influence of pre-trained word embeddings, we use the same model to conduct experiments with random initialized word embeddings.ßdummyTurker
2817ß0.0ßThe results show that word embeddings improve performances about 0.3%~0.6% and 0.9%~1.4% on English and Spanish dataset respectively.ßdummyTurker
2818ß0.0ßThe performance of our model also demonstrated that our model can perform well without pre-trained word embeddings.ßdummyTurker
2819ß0.0ßWe will clarify the influence of word embeddings in revised version.ßdummyTurker
2820ß0.0ßFor weakness2: We will give more details later in revised version.ßdummyTurker
2821ß0.0ß"In fact, ""Neutral baseline"" is a CRF model with word embeddings as features and the ""Discrete baseline"" is a CRF model with four kinds of features such as surface features, linguistic features, cluster features and sentiment features, described in (Zhang et al., 2015)."ßdummyTurker
2822ß0.0ßFor weakness3: When obtaining the target and sentiment labeling score matrix, we use Viterbi algorithm to find the path with the highest score in the score matrix.ßdummyTurker
2823ß0.0ßThe optimal path stands for the label of each word.ßdummyTurker
2824ß0.0ßWe will add these details of finding optimal path when space is allowed.ßdummyTurker
2825ß0.0ßFor question1: Thanks for your suggestion.ßdummyTurker
2826ß0.0ßWe will add more details and citation for baselines.ßdummyTurker
2827ß0.0ßFor question2: This is a good question.ßdummyTurker
2828ß0.0ß"We have used  ""SBi-LSTM"", ""S-GRU"" and ""Bi-GRU"" to conduct experiments."ßdummyTurker
2829ß0.0ß"The results of ""SBi-LSTM"" are almost the same as SBi-GRU, but LSTM has more parameters than GRU."ßdummyTurker
2830ß0.0ßTherefore, we chose GRU rather than LSTM.ßdummyTurker
2831ß0.0ß"The performances of ""S-GRU"" are worse than bidirectional model, and the reason is that bidirectional model can capture past and future features, such as boundary features."ßdummyTurker
2832ß0.0ß"As for ""single layer"" model, many works have demonstrated the effectiveness of ""stack RNN"", e.g."ßdummyTurker
2833ß0.0ß"""Speech Recognition with Deep Recurrent Neural Networks; Graves et al."ßdummyTurker
2834ß0.0ß"(2013)"", because “stack RNN” can build up progressively higher level representations of sequence data."ßdummyTurker
2835ß0.0ß"We also find that ""SBi-GRU"" outperforms ""Bi-GRU"", so we chose 'multi-layer' model."ßdummyTurker
2836ß0.0ßBecause of limited space, we don't show the results of those variants.ßdummyTurker
2837ß0.0ßWe will add them in the extra page if this paper is accepted.ßdummyTurker
2838ß0.0ßThanks for your kind advice.ßdummyTurker
2839ß0.0ßWe have corrected the typos, grammatical mistakes and inaccuracies as you suggested.ßdummyTurker
2840ß0.0ßWe have been carefully editing the paper since the submission on Feb. 22th.ßdummyTurker
2841ß0.0ßAnd we are ready to contact ELSEVIER WebShop for further proofreading.ßdummyTurker
2842ß0.0ßCould you please refer to the newest experiment we respond to Reviewer2 due to the limitation of word number limitation.ßdummyTurker
2843ß0.0ßWe add the statistical significance of the results as you suggested.ßdummyTurker
2844ß0.0ßWe will shorten the paper within 8 pages and make sure it will not influence the soundness and readability of the paper.ßdummyTurker
2845ß0.0ßThank you for your feedback on the article.ßdummyTurker
2846ß0.0ßWe are happy to read that you appreciate the idea of the paper, and find it well-written and structured.ßdummyTurker
2847ß0.0ßAlthough it is possible to train the embeddings for the bigger corpora,ßdummyTurker
2848ß0.0ßwe believe that using a smaller dataset amplifies our centralßdummyTurker
2849ß0.0ßhypothesis: how fast can we extract information from a dataset.ßdummyTurker
2850ß0.0ßGivenßdummyTurker
2851ß0.0ßthe limited amount of positive samples, we want to distillate as fast asßdummyTurker
2852ß0.0ßpossible.ßdummyTurker
2853ß0.0ßIt is likely that in the extreme case, where all approachesßdummyTurker
2854ß0.0ßsee all the data, it would be relatively difficult to show the gainßdummyTurker
2855ß0.0ßbetween different samplers.ßdummyTurker
2856ß0.0ßThanks for your constructive and thoughtful comments!ßdummyTurker
2857ß0.0ßWe realized that we have caused a confusion by missing out some details.ßdummyTurker
2858ß0.0ßWe actually did not tune on test.ßdummyTurker
2859ß0.0ßThere are five folds in our cross validation setting, which strictly follow Peng et al., (2017).ßdummyTurker
2860ß0.0ßTheir corpus does not contains a held-out devset in each fold.ßdummyTurker
2861ß0.0ßTherefore, for the first fold, we take 200 sentences out of the training data as the devset, using the remaining for development training.ßdummyTurker
2862ß0.0ßIn the end the 200 sentences are merged back into the training set.ßdummyTurker
2863ß0.0ßThe number of iterations is decided on the devset, and used for all five folds.ßdummyTurker
2864ß0.0ßFor all the other hyper parameters, we simple followed Peng et al., (2017) and did not do tuning.ßdummyTurker
2865ß0.0ßBelow are the numbers of different transition iterations on the devset of that fold.ßdummyTurker
2866ß0.0ß#transition iter: 1  2  3  4  5ßdummyTurker
2867ß0.0ß#F1: 72.0  77.5  80.5  84.0  84.5ßdummyTurker
2868ß0.0ßWe used this fold for showing the effectiveness of iterations and single-direction information exchange, where the figure is drawn on the test portion of that fold.ßdummyTurker
2869ß0.0ßWe mentioned that ``we used 5 as the number of iterations for remaining experiments'' at the end of the section, which sounded as if we tuned on test.ßdummyTurker
2870ß0.0ßWe did not realize this confusing statement in our draft.ßdummyTurker
2871ß0.0ßWe will give all the above details in our revision!ßdummyTurker
2872ß0.0ßWe will try to play down the claims that you mentioned.ßdummyTurker
2873ß0.0ßThe title is inherited from  Peng et al., (2017) for consistency.ßdummyTurker
2874ß0.0ßThe task we try to solve pre-specifies the arguments (Section 3), while our model can actually be used for cases with flexible (with a maximum), if a mask representing the number of entities are applied to the features (input of logistic regression).ßdummyTurker
2875ß0.0ß"The possible relations {""resistance or non-response"", ""sensitivity"", ""response"", ""resistance"", ""None""} are defined in Peng et al., (2017), and we follow them to group the first four as positive and treat ""None"" as negative."ßdummyTurker
2876ß0.0ßThus the problem is a binary classification problem, and accuracies are classification precisions.ßdummyTurker
2877ß0.0ßWe will add the details to our draft.ßdummyTurker
2878ß0.0ßThe dataset is also available online.ßdummyTurker
2879ß0.0ßWe will expand the related work section as you suggest.ßdummyTurker
2880ß0.0ß[1] is an interesting paper, and we will to cite and compare with it in the next version.ßdummyTurker
2881ß0.0ßPrerequisite relations essentially can be considered as the learning dependency among concepts, which means a concept is necessary to learn before one can proceed to understand more advanced knowledge.ßdummyTurker
2882ß0.0ß"For example, we should learn ""binary tree"" before ""AVL tree"" when learning data structure."ßdummyTurker
2883ß0.0ßMOOC proposed semantic, contextual and structural features for concept pairs in Massive Open Online Courses, and then trained binary classifiers such as Naïve Bayes, Logistic Regression, SVM with linear kernel and Random Forest for the relation learning.ßdummyTurker
2884ß0.0ßCPR-Recover proposed an unsupervised optimization-based method based on causality and sparsity assumptions to learn concept-level prerequisite relations from course dependencies.ßdummyTurker
2885ß0.0ßWe will proofread the paper carefully.ßdummyTurker
2886ß0.0ßSome notations were used in a previous work.ßdummyTurker
2887ß0.0ßWe will clearly state and highlight the novel contribution of our work.ßdummyTurker
2888ß0.0ßOur approach can find more prerequisite relations among concepts with the aid of the recovery-based model, but it would also introduce a few noisy concept pairs.ßdummyTurker
2889ß0.0ßIn future, we would like to directly learn the prerequisite relations among concepts with an end-to-end learning system.ßdummyTurker
2890ß0.0ßThe novelty of this approach lies in the difficulty of the VUAMC dataset, which contains many lexicalized metaphors that are generally considered much more challenging to detect.ßdummyTurker
2891ß0.0ßThe use of a neural network for this data shows how examining different combinations of important contextual features by a NN model can improve the final results on the data.ßdummyTurker
2892ß0.0ßThe current approach is also data-driven, not requiring any hand-coded knowledge when applied to other datasets.ßdummyTurker
2893ß0.0ßThis paper demonstrates the degree of complexity that would be required in a NN model, with respect to metaphor processing, showing that recurrent models do not necessarily perform better than a regular feedforward NN.ßdummyTurker
2894ß0.0ßThe high results on classification at the sentence level demonstrates the importance of including non-local contextual information.ßdummyTurker
2895ß0.0ßThe use of NN models allows to examine different combinations of these latent contextual features, obtained from the Paragraph Vector.ßdummyTurker
2896ß0.0ßThis paper compares the use of NNs with other models, showing that they improve results from more basic methods, but do not need to be overly complex (eg.ßdummyTurker
2897ß0.0ßLSTM models).ßdummyTurker
2898ß0.0ßAlthough SVM was also very effective due to high recall, NN models were still an improvement on more basic methods such as logistic regression and outperforming other methods like SVM with the best input features (PV-DBOW).ßdummyTurker
2899ß0.0ßThanks for your meticulous comments and valuable time in making our work thorough.ßdummyTurker
2900ß0.0ßWe address reviewer-specific clarifications below.ßdummyTurker
2901ß0.0ßW1: This technique is actually required in our real QA systems since a user-friendly human-computer interface not only needs to precisely retrieve the answers but also interprets the process to users in human language.ßdummyTurker
2902ß0.0ßIn this way, the interpretation can help users better understand the correctness of returned answers.ßdummyTurker
2903ß0.0ßIn fact, we have found many potential applications of our model such as generating short news report from structured sports game data and Table-to-Text generation (Sha et al.ßdummyTurker
2904ß0.0ßAAAI 2018, Liu et al.ßdummyTurker
2905ß0.0ßAAAI 2018).ßdummyTurker
2906ß0.0ßMoreover, in existing NLP community, many approaches represent a text as either a sequence (seq-LSTM) or a tree (tree-LSTM).ßdummyTurker
2907ß0.0ßWith our graph encoder,ßdummyTurker
2908ß0.0ßa text can be represented as a more complex graph, e.g., the combination of word sequence and dependency parse tree and constituent tree.ßdummyTurker
2909ß0.0ßWe think our approach provides a solution to simultaneously learn these different structured features in a neural network model.ßdummyTurker
2910ß0.0ßW2: We tune hyper-parameters on the validation set.ßdummyTurker
2911ß0.0ßW3: In fact, we consider these logical connectors in our approach.ßdummyTurker
2912ß0.0ßSpecifically, we add nodes in the graph to represent these connectors.ßdummyTurker
2913ß0.0ßW4: We use Seq2Seq models as baselines since they have been recently proved to be able to capture some structured information, such as Tree or Table.ßdummyTurker
2914ß0.0ßWe compare our model with these models to demonstrate that these seq2seq methods may suffer information loss when imposing an order on graph structured data.ßdummyTurker
2915ß0.0ßW5: We did manually evaluate the correctness of generated questions.ßdummyTurker
2916ß0.0ßSince it is impractical to manually evaluate the correctness of all generated questions, we carefully analyzed 300 generated questions as mentioned in Error Analysis (session 5.6).ßdummyTurker
2917ß0.0ßAmong them, 224 questions (74.7%) correctly described the meanings of SQL queries.ßdummyTurker
2918ß0.0ßQ1: Thanks for pointing out this interesting direction.ßdummyTurker
2919ß0.0ßIn fact, we are wording on a seq2graph project.ßdummyTurker
2920ß0.0ßWe would like to share some observations with you.ßdummyTurker
2921ß0.0ßFrom current results we have, it seems more difficult to decode a graph than a sequence, because there is only one way to generate a sequence but may have numerous ways to generate a graph.ßdummyTurker
2922ß0.0ßThis makes it difficult to train the seq2graph model.ßdummyTurker
2923ß0.0ßWe are still working on this and hopefully, we could find some ways to address this issue.ßdummyTurker
2924ß0.0ßAdditional Comment: Thanks very much for your kind suggestion about the writing.ßdummyTurker
2925ß0.0ßWe will polish the paper accordingly.ßdummyTurker
2926ß0.0ßThanks for your meticulous comments and valuable time in making our work thorough.ßdummyTurker
2927ß0.0ßWe appreciate your detail comments and address reviewer-specific clarifications below.ßdummyTurker
2928ß0.0ßW1: This time complexity of node embedding generation algorithm is |K|*|V|_{avg} where |K| is the hop size and |V|_{avg} is the average neighbor size of nodes in the graph.ßdummyTurker
2929ß0.0ß"For large input graphs that may have large |V|_{avg}, we could use the sampling method introduced in ""FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling"" (Chen et al."ßdummyTurker
2930ß0.0ßICLR 2018) to guarantee the efficiency of this algorithm on large graphs.ßdummyTurker
2931ß0.0ßW2: Experimental results on the SQL-to-Text could demonstrate our graph encoder is better than the sequence encoder.ßdummyTurker
2932ß0.0ßWe conduct this toy experiment to verify whether our pooling-based method that generates the graph embedding still works in the seq2seq models.ßdummyTurker
2933ß0.0ßQ1: We explain graph encoding algorithm using the following simple example.ßdummyTurker
2934ß0.0ßSuppose we have three nodes in a graph whose structure is A -> B -> C. In the first iteration, the information of A is propagated to B (line 5 in Algorithm 1).ßdummyTurker
2935ß0.0ßThe embedding of B is then updated (line 7 in Algorithm 1), resulting B_new.ßdummyTurker
2936ß0.0ßIn the second iteration, the information of B_new is propagated to C (line 5).ßdummyTurker
2937ß0.0ßSince the B_new already conveys the information of A, the C thus could aggregate A's information into its embedding.ßdummyTurker
2938ß0.0ßWe hope this explanation could help you better understand how our algorithm works.ßdummyTurker
2939ß0.0ßIf some edges have valuable information, we could also add one node for each edge and still uses the proposed algorithm to generate the node embeddings.ßdummyTurker
2940ß0.0ßQ2: Here, each node has a text attribute that equals to an integer.ßdummyTurker
2941ß0.0ßThe graph is in the chain form where each node has at most one parent and one child.ßdummyTurker
2942ß0.0ßSimilar to the experiment in Sec.ßdummyTurker
2943ß0.0ß5.5, in the input graph each node has a text attribute that equals to an integer (representing its ID).ßdummyTurker
2944ß0.0ßThe expected output is a sequence of integers representing the nodes in the shortest path between START and END node.ßdummyTurker
2945ß0.0ßAdditional Comments: Thanks very much for your advice, we may attempt it in the future.ßdummyTurker
2946ß0.0ßFirst, we would like to note that some methods in the comparison are transductive, e.g.ßdummyTurker
2947ß0.0ßthe SFA approach of (Pan et al., 2010).ßdummyTurker
2948ß0.0ßAs suggest by the reviewer, we tested the Tranductive SVM (Joachims et al, 2001) approach in the multi-source sentiment classification setting.ßdummyTurker
2949ß0.0ßWe obtained the following results using K_0/1:ßdummyTurker
2950ß0.0ßDEK→B: 82.4%;ßdummyTurker
2951ß0.0ßBEK→D: 83.0%;ßdummyTurker
2952ß0.0ßBDK→E: 83.9%;ßdummyTurker
2953ß0.0ßBDE→K: 86.0%.ßdummyTurker
2954ß0.0ßAlthough SVM (see comment to Weakness3 of Reviewer3) yields performance similar to KRR, the results of TSVM are about 1-2% below our top results presented in Table 1.ßdummyTurker
2955ß0.0ßWe will include these results in the final paper.ßdummyTurker
2956ß0.0ßOur results are supported by statistical significance tests and we do mention that we make use of the unlabeled test samples.ßdummyTurker
2957ß0.0ßHowever, we can rephrase our presentation to make it more modest.ßdummyTurker
2958ß0.0ßIndeed, the first approach (transductive string kernels) does not use the training labels.ßdummyTurker
2959ß0.0ßIt can be seen as an unsupervised kind of transduction and it helps the second approach (the transductive classifier) to obtain better results.ßdummyTurker
2960ß0.0ßIf we have a normalized kernel matrix K (with values between [0, 1]) we can obtain a distance matrix D as follows:ßdummyTurker
2961ß0.0ßD = 1 - K. We use this transformation in Eq.ßdummyTurker
2962ß0.0ß(4) and (5).ßdummyTurker
2963ß0.0ßThe reason for using the RBF kernel as in Eq.ßdummyTurker
2964ß0.0ß(5) is to project the similarity values from the interval [0, 1] to the interval [0.36, 1], and to remove the values equal to 0 (two strings can have a similarity of 0 if they have NO n-grams in common).ßdummyTurker
2965ß0.0ßSince we use the scalar product of kernel matrices in Eq.ßdummyTurker
2966ß0.0ß(6), having many values equal to zero would result in a more sparse matrix.ßdummyTurker
2967ß0.0ßSince train and test documents are from different domains, it is likely that many similarities are zero in the initial kernel matrix.ßdummyTurker
2968ß0.0ßThis was due to lack of space.ßdummyTurker
2969ß0.0ßWe considered other sections to be more important.ßdummyTurker
2970ß0.0ßSince the final paper has an extra page, we will make sure to include a conclusion.ßdummyTurker
2971ß0.0ßYes indeed the results suggest that using LR would be most reasonable, we will state this explicitly.ßdummyTurker
2972ß0.0ßWith Figure 1 we were attempting to use alternative (not measured as classification accuracy) evaluation for embeddings.ßdummyTurker
2973ß0.0ßIt is true that the results do not seem to translate to downstream tasks; we will adjust discussion parts accordingly.ßdummyTurker
2974ß0.0ßWe have already prepared detailed illustration for ``bound'' and ``unbound'' context representations.ßdummyTurker
2975ß0.0ßIf the paper is accepted  we will use additional page to place this illustration.ßdummyTurker
2976ß0.0ß"Thank you for your detailed comments, we have fixed typos and covered more existing works concerning intrinsic and extrinsic tasks, particularly the following papers: ""Evaluation methods for unsupervised word embeddings"", ""Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance"", and ""Evaluating Word Embeddings Using a Representative Suite of Practical Tasks""."ßdummyTurker
2977ß0.0ßApropos the conjecture of systematic bias :  Our primary goal in the experiments was to compare NVSE to the baseline systems.ßdummyTurker
2978ß0.0ßSince NVSE is present in both groups therefore our experiment can determine the rank of NVSE in comparison to BS, BM25, W2Vec and SetExpan.ßdummyTurker
2979ß0.0ßFor example, consider a hypothetical 'extreme' version of our experiment, where we compare NVSE to each baseline individually.ßdummyTurker
2980ß0.0ßSo we are doing comparison in 4 groups.ßdummyTurker
2981ß0.0ßIf the annotators had found NVSE better than 3 baselines but worse than 1 of the baseline then we will know exactly how NVSE lies in comparison to the other systems.ßdummyTurker
2982ß0.0ßThe only information lost is that we can not distinguish between the three system that were deemed inferior, but we do not care much for the ranking amongst the baselines anyway.ßdummyTurker
2983ß0.0ßThank you for the encouraging and insightful review!ßdummyTurker
2984ß0.0ßW1.ßdummyTurker
2985ß0.0ßWe agree that comparisons to state-of-the-arts on individual tasks send a stronger message.ßdummyTurker
2986ß0.0ßAs you pointed out, our focus is the general architecture, we therefore compared mTreeLSTM to LSTM and TreeLSTM on various tasks instead of focusing on specific tasks (which typically emphasizes task-dependent setup/components).ßdummyTurker
2987ß0.0ßWe carried out such comparisons for the NLI task on a bigger data set, partly because NLI is arguably more semantically complex among the tasks we considered and hence better showcases the usefulness of mTreeLSTM.ßdummyTurker
2988ß0.0ßW2.ßdummyTurker
2989ß0.0ßi) Table 5 shows the effects of the relation embedding size.ßdummyTurker
2990ß0.0ßTreeLSTM doesn't have this parameter; its results served as a baseline.ßdummyTurker
2991ß0.0ßThe size of LSTM hidden state is fixed as in Tai et al.ßdummyTurker
2992ß0.0ß2015, and is kept the same for all models.ßdummyTurker
2993ß0.0ßii) In our experiments, training converges after 10 iterations for all models and we didn't observe better results with more iterations.ßdummyTurker
2994ß0.0ßThus we trained each model for 10 iterations, and took the model that obtained best results on the development set.ßdummyTurker
2995ß0.0ßW3.ßdummyTurker
2996ß0.0ßIn dependency and AMR trees, we can obtain explicit relation information between words, thus mTreeLSTM can be directly applied to these trees.ßdummyTurker
2997ß0.0ßIn contrast, no such relation information exists in constituency trees and, consequently, mTreeLSTM cannot be directly applied.ßdummyTurker
2998ß0.0ßW4.ßdummyTurker
2999ß0.0ßThanks for pointing it out: Line 163 should be revised.ßdummyTurker
3000ß0.0ßWhat we tried to say is that AMR can be helpful in certain complex tasks (such as for longer sentences in NLI).ßdummyTurker
3001ß0.0ßWe will comment more on these subsets of data in the final version.ßdummyTurker
3002ß0.0ßOthers.ßdummyTurker
3003ß0.0ßThanks for the detailed suggestions!ßdummyTurker
3004ß0.0ßWe will correct the style and grammar issues.ßdummyTurker
3005ß0.0ßThanks a lot for your kind comments.ßdummyTurker
3006ß0.0ßIn this paper, we use three ways to prevent the risk of overfitting limiting the scalability of this method.ßdummyTurker
3007ß0.0ß(1)The objective function is the cross-entropy loss.ßdummyTurker
3008ß0.0ßHere, we employ the L2 regularization term to avoid overfitting.ßdummyTurker
3009ß0.0ß(2)We use early stop mechanism in the training process.ßdummyTurker
3010ß0.0ß(3)The datasets of SemEval2017 task7 and Pun of the Day provide training sets and test sets.ßdummyTurker
3011ß0.0ßIn training sets, we divide the training and development sets to train the parameters.ßdummyTurker
3012ß0.0ßAfter that, we apply 5-fold cross validation to prevent overfitting.ßdummyTurker
3013ß0.0ßWe would like to express our sincere thanks to the reviewer for recognizing the novelty and providing very positive comments to our work.ßdummyTurker
3014ß0.0ßFor homographic puns recognition, more clues are provided by the collocation of candidate pun words.ßdummyTurker
3015ß0.0ßThe candidate pun words consist of nouns, verbs, adjectives and adverbs in each pun.ßdummyTurker
3016ß0.0ßFor each type of candidate pun words, we design an attention mechanism to obtain useful weights.ßdummyTurker
3017ß0.0ßThen we concatenate the four types to aggregate informative words.ßdummyTurker
3018ß0.0ßWe consider the relation of the candidate puns.ßdummyTurker
3019ß0.0ßIn Figure 5, the weight of the word [sleep] is highest in this sentence, compared to the word [deciding] and [mattress].ßdummyTurker
3020ß0.0ßWe agree, that our definition of trend is somewhat simplistic.ßdummyTurker
3021ß0.0ßHowever, to our knowledge, there is no easy way to come up with an alternative definition that addresses the potential problems the reviewer enumerates.ßdummyTurker
3022ß0.0ßOur hope would be that a large collection (we provide it as one of our contributions) contains enough clear signal that will outweight any noise due to the pointed problems.ßdummyTurker
3023ß0.0ßNote also that scientific publications don`t suffer much from redundancy: most scientific publication must contain an original core, so they aren`t redundant.ßdummyTurker
3024ß0.0ßWe agree, that trend analysis is corpus-dependent and that we didn`t address the important questions of how to create a corpus for a particular study.ßdummyTurker
3025ß0.0ßThere is a separate field in corpus linguistics on corpus creation - on the one hand a complex problem, on the other a problem that much successful research has been done on, soßdummyTurker
3026ß0.0ßwe don't feel that this should be the focus of our paper.ßdummyTurker
3027ß0.0ßIn any case, we used all documents that semantic scholar made available, except for initial years with few publications and except for non-English publications.ßdummyTurker
3028ß0.0ßSo in our case there was a clear and simple corpus creation rationale.ßdummyTurker
3029ß0.0ßThis is an excellent point.ßdummyTurker
3030ß0.0ßIf the paper is accepted, we`ll add more detailed description, analysis and discussion of the Gold Trends we identified in the meta-literature.ßdummyTurker
3031ß0.0ßThank you for your review.ßdummyTurker
3032ß0.0ßWe do cited the paper by Ji et al.ßdummyTurker
3033ß0.0ßin both Introduction and Related Work as follows:ßdummyTurker
3034ß0.0ßIn many neural network based GEC systems, words are split into character level or BPE level to avoid OOV words (Schmaltz et al., 2016; Ji et al., 2017; Chollampatt and Tou Ng, 2018).ßdummyTurker
3035ß0.0ßOn the other hand, in many GEC systems, OOV words are split in character-based or BPE-based units and fed into sequence-to-sequence models (Xie et al., 2016; Schmaltz et al., 2016; Ji et al., 2017; Chollampatt and Tou Ng, 2018).ßdummyTurker
3036ß0.0ßWe didn't make comparison to Ji et al.ßdummyTurker
3037ß0.0ßbecause they fused the spelling correction into GEC and the model must be trained with GEC data.ßdummyTurker
3038ß0.0ßThe GEC data they used is private and we couldn't reproduce the experiment.ßdummyTurker
3039ß0.0ßWe are sorry for the lack of comparision experiments.ßdummyTurker
3040ß0.0ßWe will improve this next.ßdummyTurker
3041ß0.0ßThe LSTM-Char-CNN we used is a modified one that takes both forward & backward information.ßdummyTurker
3042ß0.0ß"In the last section we pointed out that ""The last point we would emphasize is that the LSTM-Char-CNN resembles the nested RNN ex-cept that it uses a CharCNN to represent a word instead of a CharRNN."""ßdummyTurker
3043ß0.0ßActually, it should be renamed to BiLSTM-Char-CNN.ßdummyTurker
3044ß0.0ßWe are sorry for not explaining this explicitly.ßdummyTurker
3045ß0.0ßThe only parameter tuned on dev set is P_err.ßdummyTurker
3046ß0.0ßWe ran different experiments with P_err in [0.02:0.1] and found that there's not much difference in F_0.5 score for all the methods mentioned.ßdummyTurker
3047ß0.0ßThe training data is the One-Billion-Word for training which is publicly available.ßdummyTurker
3048ß0.0ßAnd the probability of perturbing a word is 0.05 as in section 5.1ßdummyTurker
3049ß0.0ßWe will fix the misprints.ßdummyTurker
3050ß0.0ßWe will take into account keyboard errors and run experiments on theta_att next.ßdummyTurker
3051ß0.0ß"This work only corrects intra-word errors and is unable to deal with errors like ""correctionwith"""ßdummyTurker
3052ß0.0ßWhen we get the chance to edit the paper we will add more examples and add a manual evaluation of a small subset.ßdummyTurker
3053ß0.0ßVroniplag contains different categories of plagiarisms.ßdummyTurker
3054ß0.0ß"We used the ""Verschleierung"" (obfuscation) type only."ßdummyTurker
3055ß0.0ßA Verschleierung is a paraphrase of a sentence without proper citation.ßdummyTurker
3056ß0.0ßMostly these are not almost identical.ßdummyTurker
3057ß0.0ßWe calculated statistics about the similarity of the plagiarisms using bag-of-words distances and deleted pairs that are too similar (bow distance < 6).ßdummyTurker
3058ß0.0ßThe classification results are different in Vroniplag and MSRPar.ßdummyTurker
3059ß0.0ßBut it is a very hard (maybe impossible) task to look inside of the classifier and find out the reason.ßdummyTurker
3060ß0.0ßWe had a theory that the classification is easier on Vroniplag and the results of the classification indicated that the theory was right.ßdummyTurker
3061ß0.0ßWe cannot prove this theory and we cannot say for sure that there are no other reasons.ßdummyTurker
3062ß0.0ßWe had the theory that paraphrase detection is easier on the Vroniplag dataset then on the MSR Paraphrase Corpus because the training example of sentence pairs not being paraphrases are semantically very different in Vroniplag and semantically more similar in MSR Paraphrase Corpus.ßdummyTurker
3063ß0.0ßTherefore we trained a classifier to support our theory.ßdummyTurker
3064ß0.0ßThe higher evaluation scores on the Vroniplag dataset does not proof our theory but it gives the results we expected so it is at least an indication that our theory was right.ßdummyTurker
3065ß0.0ßWhen we edit the paper we will state the purpose of the evaluation more clearly.ßdummyTurker
3066ß0.0ß"Maybe ""Exploration"" is a better heading then ""Evaluation"" for this part of the paper."ßdummyTurker
3067ß0.0ßWe considered tf-idf common knowledge but in our last paper we got critizised because reviewers did not know it.ßdummyTurker
3068ß0.0ßWhen we edit the paper we will add some details about the parameters of the classification algorithm.ßdummyTurker
3069ß0.0ßWe adopted the style of the MSRPar dataset to divide the dataset in a test and a train set.ßdummyTurker
3070ß0.0ßThis makes it easier to compare results between experiments because they are trained and tested on exactly the same data.ßdummyTurker
3071ß0.0ßBut I see your point.ßdummyTurker
3072ß0.0ßIt was a very tight decision between cross validation and the train / test split.ßdummyTurker
3073ß0.0ß-We absolutely didn’t claim CLEVR as a real-world dataset.ßdummyTurker
3074ß0.0ß"We mentioned in L30/L74/L793 consistently that ""real-world"" refers to language, supported by CLEVR-Humans/NLVR results."ßdummyTurker
3075ß0.0ßWe confirmed whether each dataset is synthetic in section-4.ßdummyTurker
3076ß0.0ßIt’s by no means misleading.ßdummyTurker
3077ß0.0ß-Original CLEVR-paper didn't cover CLEVER-Humans.ßdummyTurker
3078ß0.0ß-Visual reasoning has become an individual research field(L44).ßdummyTurker
3079ß0.0ßMost recent works(PG+EE-on-ICCV/RN-on-NIPS/FiLM-on-AAAI) didn’t use real-world images even in computer-vision top-conferences; NLVR was also accepted as outstanding-paper of ACL2017.ßdummyTurker
3080ß0.0ßThus, not using real-world images shouldn’t be weakness; we also show the best real-world-language-abilities in reasoning-models by testing on both NLVR&CLEVR-Humans.ßdummyTurker
3081ß0.0ß-The research field isn't newly-proposed.ßdummyTurker
3082ß0.0ßDiscussing it in introduction isn’t late.ßdummyTurker
3083ß0.0ß-Extracting features with CNN is surely included in image-processing.ßdummyTurker
3084ß0.0ßDid you even go-to-wiki before arguing this?ßdummyTurker
3085ß0.0ßSame usage:(De-Vries-2017-Introduction-paragraph-4)(L184-in-our-paper).ßdummyTurker
3086ß0.0ß-Even the Stanford Parser can parse contribution-1(appositive clause) correctly.ßdummyTurker
3087ß0.0ß"-How could ""RNN"" be confusing nowadays?"ßdummyTurker
3088ß0.0ß"(Johnson-2017-a,b/Andreas-2016) all mentioned to ""LSTM"" before/without reference."ßdummyTurker
3089ß0.0ßSee (L435) for ours.ßdummyTurker
3090ß0.0ß-We described all model details with formulas/texts.ßdummyTurker
3091ß0.0ßAll variable-definitions, vector-sizes and almost all hyper-parameters exist directly in the paper.ßdummyTurker
3092ß0.0ßWe submitted code.ßdummyTurker
3093ß0.0ßThere shouldn’t be problems on Replicability.ßdummyTurker
3094ß0.0ß-We don’t rely on (Perez-2018) to be readable.ßdummyTurker
3095ß0.0ßFiLM is just an affine-transformation; we showed the original equations.ßdummyTurker
3096ß0.0ßAll none-basic-knowledge modules, even unchanged-blocks, are fully described in the text in our paper.ßdummyTurker
3097ß0.0ß-You are ASKED TO PROVIDE EVIDENCE TO SUPPORT YOUR ARGUMENTS in the review form.ßdummyTurker
3098ß0.0ß"Which part is missing/inclear, making it ""not-possible-to-understand"" without (Perez-2018), or ""challenging-to-reproduce""?"ßdummyTurker
3099ß0.0ßAnd why not ask questions?ßdummyTurker
3100ß0.0ß"-The only shared-part of our figure-1 with Perez-2018 is ""ResBlocks+classifier""."ßdummyTurker
3101ß0.0ßThe computational graph is largely different.ßdummyTurker
3102ß0.0ßIn our figure, GRU isn't shown, and ResBlock-outputs come-to-the-left at each step.ßdummyTurker
3103ß0.0ßMoreover, our structure almost halves the error-rate of the already 97.7% score on CLEVR, and achieves 7-point promotion on NLVR over original structure(figure-3, Perez-2018).ßdummyTurker
3104ß0.0ß"How can this be ""too-close-model, small-improvement, small-contribution""?"ßdummyTurker
3105ß0.0ß"-Section-3 may have some ordering problems, but other reviewers understand the model, and even list ""clear-model-explanation/details"" in the Strengths."ßdummyTurker
3106ß0.0ßSince you’ve made mistakes in all arguments, we can doubt your effort on carefully reading the paper.ßdummyTurker
3107ß0.0ß-Thanks for your opinions, although most are wrong.ßdummyTurker
3108ß0.0ßOur Originality, Replicability and Readability are seriously under-estimated, and there can be much higher overall score.ßdummyTurker
3109ß0.0ßWe sincerely ask you to reconsider these issues carefully, and we’ll connect the Chairs because you’re ignoring rules in the reviewer form.ßdummyTurker
3110ß0.0ßWe want to express our thanks for your review.ßdummyTurker
3111ß0.0ßIt is very useful for us to understand more about the community perception of our paper from your opinions.ßdummyTurker
3112ß0.0ßIt is true that we made a very focused contribution on the empirical validation of previously designed neural architectures for text classification.ßdummyTurker
3113ß0.0ßOur thought is that this would fit to the purpose of a short paper -- a straight and focused contribution.ßdummyTurker
3114ß0.0ßOur hope that these results can help the community to move forward on what neural architecture to use for NLP in general.ßdummyTurker
3115ß0.0ßAlthough there are many datasets (14 large-scale datasets), they were previously developed by Zhang & LeCun 2017, therefore we did not put much introduction to them in this paper.ßdummyTurker
3116ß0.0ßThat said, we could make the illustration clearer and more detailed (rather than too condensed) using the extra page limit.ßdummyTurker
3117ß0.0ßWe want to express our thanks for your review.ßdummyTurker
3118ß0.0ßThe many details suggestions are very helpful for us to make the paper better.ßdummyTurker
3119ß0.0ßThe justification for adding residual and dense connections is simply because otherwise it is impossible to train the network as deep as ours (up to 96 layers) due to gradient vanishing.ßdummyTurker
3120ß0.0ßThough not from an NLP perspective, the motivation has always been that making network deeper can improve the results for a large margin.ßdummyTurker
3121ß0.0ßIt is the same justification as in He et al.ßdummyTurker
3122ß0.0ß2016 or Huang et al.ßdummyTurker
3123ß0.0ß2016, where these connections were first used for image classification.ßdummyTurker
3124ß0.0ßWith our article, the advantages of extremely deep convolutional networks were verified for language processing as well.ßdummyTurker
3125ß0.0ßWe will add a paragraph to introduce this motivation clearly in the paper.ßdummyTurker
3126ß0.0ßApart from this, we did have an architecture choice that is motivated from NLP, which is the recursive structure in the dynamic network.ßdummyTurker
3127ß0.0ßThe new models in this paper are all similarly designed and with similar depth.ßdummyTurker
3128ß0.0ßThe parameter size is up to 11.55 million (about 90 MB of storage space), which is moderate compared to other convolutional network models in literature.ßdummyTurker
3129ß0.0ßNote that a linear model could have comparable number of parameters.ßdummyTurker
3130ß0.0ßFor example, for a 7-class classification problem using word-level 5-gram features as in Zhang et al.ßdummyTurker
3131ß0.0ß2015 and Zhang & LeCun 2017, the number of parameters is 7 million because the number of n-gram features used was 1 million.ßdummyTurker
3132ß0.0ßTraining one of such models on a GPU only requires up to about 800MB of GPU memory, most of which is occupied by intermediate feature maps and gradients.ßdummyTurker
3133ß0.0ßThe training times between models in this article also do not differ that much due to their similar designs.ßdummyTurker
3134ß0.0ßThe time it would take for a typical model to go over 1,000,000 samples is around 8 hours, minus or plus 1-2 hours, on an NVIDIA Tesla K40.ßdummyTurker
3135ß0.0ßThis puts the model at a slightly slower speed than other models that were based on one-hot encoding (Zhang et al.ßdummyTurker
3136ß0.0ß2015) (Conneau et al.ßdummyTurker
3137ß0.0ß2016) (Zhang & LeCun 2017).ßdummyTurker
3138ß0.0ßIt was faster than glyph-based models in Zhang & LeCun 2017, and not too slow compared to naively implemented logistic regression models (Zhang et al.ßdummyTurker
3139ß0.0ß2015) (Zhang & LeCun 2017), which depends on the number of n-grams considered as features.ßdummyTurker
3140ß0.0ßHowever, none of these are as fast as fastText (Joulin et al.ßdummyTurker
3141ß0.0ß2016), which can train a few million samples in a few minutes due to the optimization tricks they had (low-level stream reading, hierarchical softmax, and the hashing trick).ßdummyTurker
3142ß0.0ßWe will add a section on the comparisons of parameters size and training times between different models, taking advantage of the extra page limit.ßdummyTurker
3143ß0.0ßWe will make changes to the article for better clarity following your suggestions.ßdummyTurker
3144ß0.0ßThat said, the reason we detailed the training parameters was because we want to ensure absolute replicability, which is arguably becoming more important for any work involving a deep neural network.ßdummyTurker
3145ß0.0ßAlso, thanks for that grammar check!ßdummyTurker
3146ß0.0ßWe thank the reviewer for the valuable feedback.ßdummyTurker
3147ß0.0ßAs long as the writers know, our research is the first that empirically compares the softmax and conditional random field (CRF) layer.ßdummyTurker
3148ß0.0ßWe are also the first that use part-of-speech (PoS) as one of the input features for recognizing named entities (NEs).ßdummyTurker
3149ß0.0ßWe have added some sentences to the Abstract (page 1 line 27-30) to clarify those.ßdummyTurker
3150ß0.0ßWhile Lample (2016) only mentioned that CRF is better than softmax for NE tagging, we empirically prove that CRF outperform softmax albeit only a little, supporting their explanation.ßdummyTurker
3151ß0.0ßHowever, adding PoS as the input feature can even make the softmax slightly outperform the CRF.ßdummyTurker
3152ß0.0ßThe NER result is used to extract important keywords in user generated content articles.ßdummyTurker
3153ß0.0ßThe analysis shows that the important keywords are industry/brand name (IND), event name (EVT), person name (PER), food & beverage name (FNB), and location name (LOC).ßdummyTurker
3154ß0.0ßIn our case, the IND also includes organization names where in the target application, the organization are mostly industrial.ßdummyTurker
3155ß0.0ßBecause we use the IOB tagging format, our 5 NE tags are supplemented with the “O” tag that signifies non-NE words, amounting to 6 NE tags.ßdummyTurker
3156ß0.0ßWe appreciate this valuable feedback and we will add the quantitative experimental results of the relaxed matching should our manuscript be accepted.ßdummyTurker
3157ß0.0ßWe use lowercase characters for the character embedding because the input documents are user-generated contents where they tend to ignore the rules regarding uppercase letters.ßdummyTurker
3158ß0.0ßThus, we do not want the lack of uppercase letter(s) to affect our NE recognition result.ßdummyTurker
3159ß0.0ßThe PoS embedding is implemented via a layer that projects the index of the PoS to its embedding.ßdummyTurker
3160ß0.0ßThe layer is initialized randomly and trained alongside the whole network.ßdummyTurker
3161ß0.0ßIf the PoS embedding is added as the input, it is concatenated with the word2vec and C2W embedding of its respective word.ßdummyTurker
3162ß0.0ß- We appreciate this feedback and will revise our manuscript according to the suggestionßdummyTurker
3163ß0.0ß- We regret that the link is difficult to be seen and found by a reader.ßdummyTurker
3164ß0.0ßThe phrase “their website” (page 3 line 203) is a clickable link that points to the specified file that list the PoSes and their own explanation.ßdummyTurker
3165ß0.0ßBecause the full address is too long, we omit it and instead make a clickable link with the phrase as its displayed text.ßdummyTurker
3166ß0.0ß"We will include the full address and make it more ""visible"" in the revised manuscript."ßdummyTurker
3167ß0.0ß- At the time of the experiments, the automatic tokenization tool was still in development and we wanted the tokenization to be as accurate as possible.ßdummyTurker
3168ß0.0ßHence we opted for the manual tokenization.ßdummyTurker
3169ß0.0ß- The preprocessing of the currency only has an impact of its PoS.ßdummyTurker
3170ß0.0ßIf PoS embedding is employed, we believe it will have effect on the NE recognition.ßdummyTurker
3171ß0.0ß"""Length"" means the ratio between the number of unicode characters in simplified strings and the number of unicode characters in original strings."ßdummyTurker
3172ß0.0ß"For example, in English, suppose a user types ""cmptr"" to the system and the system prints ""computer"", then the ratio of length is 5/8 = 62.5%."ßdummyTurker
3173ß0.0ßAs pointed out, users need to learn the simplified system before typing it for input.ßdummyTurker
3174ß0.0ßHowever, it is easy for native speakers because many youngsters in Southeast Asia actually use such a simplified and lossy Latin-transcription in casual communication on social networks.ßdummyTurker
3175ß0.0ß"Taking English as an example, a user may type ""cmptr"" as an input, and a disambiguator will print ""computer"" to the smart phone display."ßdummyTurker
3176ß0.0ßIn this way, users save time of typing vowels (in the case of Abugidas, most diacritics).ßdummyTurker
3177ß0.0ßThat is, our motivation is developing a naturally simplified and lossy transcription system to help users input texts written in abugidas.ßdummyTurker
3178ß0.0ß"Sorry for our insufficient explanation on ""writing unit""."ßdummyTurker
3179ß0.0ß"The unit in process is actually not ""word"" but ""writing unit"" in abugidas, which is composed of standalone consonant characters modified by various diacritics of vowels and tones."ßdummyTurker
3180ß0.0ßThese writing units roughly correspond to syllables or morphemes in English.ßdummyTurker
3181ß0.0ßThey can be identified from texts by orthographic rules.ßdummyTurker
3182ß0.0ßPlease note that Burmese may use spaces to segment long phrases (not words) while in the other three, spaces are rarely used.ßdummyTurker
3183ß0.0ßBecause there is no explicit word separator in these writing systems, we have to use these orthographic writing units in process.ßdummyTurker
3184ß0.0ßThe numbers in Table 3 are statistics only from wrongly processed writing units in Burmese and Khmer.ßdummyTurker
3185ß0.0ßAs noted, one third of writing units were incomprehensible in Khmer here; however, it corresponds to less than 1.5% on the whole test data.ßdummyTurker
3186ß0.0ßThe reason why a single error in a writing unit may cause incomprehensible reading is that readers have to segment words or morphemes in mind along the reading process, and one error may cause difficulties in identifying those implicit boundaries around it.ßdummyTurker
3187ß0.0ßYes, the reducing in length is solely attributable to the removal of diacritics.ßdummyTurker
3188ß0.0ß"Taking the ""computer"" -> ""cmptr"" as an example, the ratio is len (cmptr) / len (computer) = 5/8 = 62.5%."ßdummyTurker
3189ß0.0ßThe length ratio is just between the number of unicode characters after/before the simplification.ßdummyTurker
3190ß0.0ßAs mentioned in previous reply, there is generally no available word boundary information input to the disambiguator (only a little in Burmese).ßdummyTurker
3191ß0.0ßAs shown in Fig.ßdummyTurker
3192ß0.0ß"2 (b), a Khmer user may only input ""JTNN"" and the disambiguator will transfer the four letters into the original Khmer strings."ßdummyTurker
3193ß0.0ßEach letter in input is transferred into one writing unit, with joint identification of correct characters and insertion of missing diacritics, no matter the entire string is a word, a phrase, or a sentence.ßdummyTurker
3194ß0.0ßSpaces in input are treated as common characters as the other letters.ßdummyTurker
3195ß0.0ß"So a Burmese user may type strings like ""DN KN"", then the two parts will be recovered and the space is retained."ßdummyTurker
3196ß0.0ßPart of the paper is an interpretation of why backtranslation should work.ßdummyTurker
3197ß0.0ßOur interpretation of the algorithm as a form of Bayesian inference helps us understand why the algorithm makes sense and what sort of extensions it might have.ßdummyTurker
3198ß0.0ßThe goal was not to complicate it, but rather to understand it more formally.ßdummyTurker
3199ß0.0ßThe hypothesis is that the wake-sleep extension of the back-translation algorithm leads to further improvements, since it provides a better approximation of the posterior distributions.ßdummyTurker
3200ß0.0ßWe will use the additional page for the final version to move the data description from the appendix to the Experiments section.ßdummyTurker
3201ß0.0ßWhile the idea of back-translation is definitively not new, the theoretical explanation of its practical success has been missing so far.ßdummyTurker
3202ß0.0ßIn addition, we propose an extension of the algorithm, which might seem simple, but proves to be very effective and has theoretical foundations.ßdummyTurker
3203ß0.0ßThe motivation behind this cross-domain experiment is to show that already with relatively little (in comparison to the size of the bitext) in-domain monolingual data, our algorithm leads to substantial improvements, providing an effective bootstrapping strategy.ßdummyTurker
3204ß0.0ßThis reflects the most common use case for semi-supervised MT, where the large bitext does not match the target domain.ßdummyTurker
3205ß0.0ßWe did not include results on larger/more datasets because of long training times.ßdummyTurker
3206ß0.0ßSince the submission deadline, results on WMT 2017 tasks for en-de/de-en and en-lv/lv-en have been obtained, so the final version will include these.ßdummyTurker
3207ß0.0ßIn our experiments we were limited to the available in-domain data.ßdummyTurker
3208ß0.0ßHowever, we would naturally expect that larger monotexts will lead to larger improvements.ßdummyTurker
3209ß0.0ßBack-translation was known to be useful before--however, a theoretic explanation was missing.ßdummyTurker
3210ß0.0ßThe outcome of this paper is this theoretical analysis, as well as an algorithmic extension based on these insights, that will allow researchers and practitioners to obtain even higher improvements from back-translation.ßdummyTurker
3211ß0.0ßWe will clarify the goal of the paper in the final version.ßdummyTurker
3212ß0.0ßTo summarize: While the idea of back-translation is definitively not new, the theoretic explanation of its practical success has been missing so far.ßdummyTurker
3213ß0.0ßThe goal of this paper is deliver this explanation, together with an extension of the algorithm that is theoretically sound and empirically practical and effective.ßdummyTurker
3214ß0.0ßWe will adapt the title so that it better reflects the goal of the paper.ßdummyTurker
3215ß0.0ßSorry for the unclear presentation.ßdummyTurker
3216ß0.0ßWe will conduct a careful proof-reading in revision.ßdummyTurker
3217ß0.0ß“Credible Detection Point” is a hypothetical concept, whose rationality is confirmed in the case analysis.ßdummyTurker
3218ß0.0ßIn the test process, this point indicates the time point when the prediction reaches the threshold \alpha for the first time.ßdummyTurker
3219ß0.0ßBefore this time point, the rumors are difficult to distinguish for both people and rumor detection models because there usually exist conflict reposts that support or oppose the original microblog.ßdummyTurker
3220ß0.0ßWhile after this time stamp, the debate about the original microblog reaches an agreement, and the following prediction should be stable and credible.ßdummyTurker
3221ß0.0ßThe major contribution with respect to previous work on rumor detection is our CED method can calculate a “Credible Detection Point” for every different microblog in the testing process, which means we can use the least information for each microblog to realize early detection.ßdummyTurker
3222ß0.0ß"The core idea of the CED method is to use ""Credible Detection Point"", in which step prediction result of RNN reaches the threshold for the first time."ßdummyTurker
3223ß0.0ß(1)	“Credible Detection Point” is a hypothetical concept, whose rationality is confirmed in the case analysis.ßdummyTurker
3224ß0.0ßIn the test process, this point indicates the time point when the prediction reaches the threshold \alpha for the first time.ßdummyTurker
3225ß0.0ßBefore this time point, the rumors are difficult to distinguish for both people and rumor detection models because there usually exist conflict reposts that support or oppose the original microblog.ßdummyTurker
3226ß0.0ßWhile after this time stamp, the debate about the original microblog reaches an agreement, and the following prediction should be stable and credible.ßdummyTurker
3227ß0.0ß(2)	\beta is the parameter that we introduce to represent the “Credible Detection Point” for each microblog.ßdummyTurker
3228ß0.0ßWhen testing, \beta indicates the ratio of the used repost messages when the threshold is reached for the first time to the total number of messages.ßdummyTurker
3229ß0.0ßIt also reflects the proportion of the time to make a credible detection.ßdummyTurker
3230ß0.0ß(3)	the RNN and the CNN：The RNN network processes the repost information and the CNN processes the corresponding original microblog text.ßdummyTurker
3231ß0.0ßIn CED-OM method，the RNN hidden state of each step is stitched with the CNN feature vector of the same original text before sent into the fully connected layer to make predictions.ßdummyTurker
3232ß0.0ßAnd the contribution with respect to previous work on rumor detection is our CED method can calculate a “Credible Detection Point” for every different microblog in the testing process, which means we use the least information necessary for each microblog to realize early detection.ßdummyTurker
3233ß0.0ßThe empirical evaluation entails playing with deep neural network models, which constitutes another big research topic.ßdummyTurker
3234ß0.0ßIf the present paper were a long paper, such evaluation would be mandatory, but as a short paper, we believe the theoretical part would be sufficient contribution.ßdummyTurker
3235ß0.0ßWe would like to include a pseudo code in appendix.ßdummyTurker
3236ß0.0ßWe highly appreciate your suggestion.ßdummyTurker
3237ß0.0ßWe will add appropriate references.ßdummyTurker
3238ß0.0ßThank you for reading this paper and giving meaningful commentsßdummyTurker
3239ß0.0ßlack of novelty.ßdummyTurker
3240ß0.0ßOur model proposes solutions for Chinese POI retrieving and effective methods to extract features of Chinese query such as granularity expression of Chinese characters and words as well as word embedding methods.ßdummyTurker
3241ß0.0ßMore importantly, our model is easy to be applied to on-line and lately serves millions of people every day.ßdummyTurker
3242ß0.0ßlack of citations.ßdummyTurker
3243ß0.0ßWe put citations in the place where one needs and compare our DPSM model with the classical deep models such as DSSM.ßdummyTurker
3244ß0.0ßThank you very much for the review.ßdummyTurker
3245ß0.0ßThank you very much for the review.ßdummyTurker
3246ß0.0ßWe believe the reason is as follows.ßdummyTurker
3247ß0.0ßGiven the analogy where first two words are in language A and second two words are in language B.ßdummyTurker
3248ß0.0ßIf the semantic space of A has much better monolingual performance than the space of language B, than the offset vector is likely to be better estimated compared with the case where both pairs are in language B.ßdummyTurker
3249ß0.0ßThis is especially evident on figures 4a, 4c, 4h.ßdummyTurker
3250ß0.0ßWe believe this makes sense only for semantic analogies.ßdummyTurker
3251ß0.0ßThe most of word pairs in syntactic analogies have the same lemma so that the knowledge about the syntax is hidden in inflection (usually suffix).ßdummyTurker
3252ß0.0ßThank you very much for the review.ßdummyTurker
3253ß0.0ßYes, you are right.ßdummyTurker
3254ß0.0ßWe select these six languages, because the designed analogies make sense for all of them.ßdummyTurker
3255ß0.0ßAs you mentioned, Chinese and Japanese do not have comparative, superlative, or plural forms, etc.ßdummyTurker
3256ß0.0ßWe believe that such experiments will be interesting for future work (together with designing additional analogies).ßdummyTurker
3257ß0.0ßWe will refine the paper for improving the writing of our approach description.ßdummyTurker
3258ß0.0ßThe innovation of this paper lies to identify the rumor as early as possible.ßdummyTurker
3259ß0.0ßTo do this, the RL (reinforcement learning) approach is incorporated into RNN to monitor the post stream in real time and learn how many posts the least required to confirm a rumor based on the reward of accuracy.ßdummyTurker
3260ß0.0ßRather than setting a static timeline, RL is able to dynamically decide the optimal occasion of rumor confirmation so that a rumor can be identified as early as possible.ßdummyTurker
3261ß0.0ßThis is how the RL benefit rumor detection.ßdummyTurker
3262ß0.0ßThe experimental results also showed that with the help of RL, the rumors can be identified 4.5 hours earlier in average than the state of the art with the comparable or even better accuracy.ßdummyTurker
3263ß0.0ß"The Sina Weibo corpus is an openly available dataset, which can be found in Ma et al., 2016 ""Detecting Rumors from Microblogs with Recurrent Neural Networks""."ßdummyTurker
3264ß0.0ßThis dataset has been used for evaluation in other papers, e.g.ßdummyTurker
3265ß0.0ßNatali et al.ßdummyTurker
3266ß0.0ß"2017, ""CSI: A Hybrid Deep Model for Fake News Detection"" which has been compared in our experiments."ßdummyTurker
3267ß0.0ßCertainly, we agree that it would be more convincing if similar results are achieved on other datasets.ßdummyTurker
3268ß0.0ßYes.ßdummyTurker
3269ß0.0ßIf the RLM makes decisions at time t, only the training data before time t is used in the RDM.ßdummyTurker
3270ß0.0ßIn Ma's work, all the data are used for training, but set 6th hour as the checkpoint during the test.ßdummyTurker
3271ß0.0ßIn both Eq.ßdummyTurker
3272ß0.0ß7 and Eq.ßdummyTurker
3273ß0.0ß11, the notion a_ij carries the same meaning, which is the Q-value for the current state and action.ßdummyTurker
3274ß0.0ßThe reason for the low accuracy at 24-30 checkpoint is that among 1166 events, there are only 5 events at that checkpoint.ßdummyTurker
3275ß0.0ßSo even if only 1 event is not identified correctly, the accuracy can just achieve 80%.ßdummyTurker
3276ß0.0ßIn our approach, the multiple threads means that we can deal with several events at the same time.ßdummyTurker
3277ß0.0ßFor one single event, it cannot be processed multiple threads.ßdummyTurker
3278ß0.0ßIn our approach, the inputs are a group of posts collected within a time interval(Section 3.3).ßdummyTurker
3279ß0.0ßWe feed the RLM with posts group by group, until RLM predict the checkpoint.ßdummyTurker
3280ß0.0ßThen all the groups of posts input into RDM to judge whether it is a rumor.ßdummyTurker
3281ß0.0ß"So here the ""multiple threads"" means ""multiple events""."ßdummyTurker
3282ß0.0ßWe agree that the question post is an important feature for rumor detection, and a rumor is really hard to be identified if there is no question post.ßdummyTurker
3283ß0.0ßBut under such case, it is even impossible for a human to identify the veracity of a rumor.ßdummyTurker
3284ß0.0ß"So this is really true for all the ""models""."ßdummyTurker
3285ß0.0ßIt is inevitable that some events are hard/impossible to be identified by our approach, even after waiting for a long time.ßdummyTurker
3286ß0.0ßBut our approach can help to identify most rumors much earlier than the existing approaches, while achieving comparable accuracy.ßdummyTurker
3287ß0.0ßFor the 5% cases, we could set a hard deadline alternatively.ßdummyTurker
3288ß0.0ß"We hope our method is helpful to the government or assists the social platform to detect the rumor earlier, but we cannot really ""stop"" it ourselves."ßdummyTurker
3289ß0.0ßWe will improve the writing of this paper and use an appropriate expression to avoid of misunderstanding.ßdummyTurker
3290ß0.0ßFirst, our model is a multi-task model since, as can be seen from Figure 1 and 2, we are learning a sequence-to-sequence and auto-encoder model together.ßdummyTurker
3291ß0.0ßWe will make sure the term multi-task appears in the introduction section in the revised version.ßdummyTurker
3292ß0.0ßSecond, we didn't exactly call it Viterbi since there is a slight difference between our version and the original Viterbi algorithm.ßdummyTurker
3293ß0.0ßWe didn't compare with Tra-la Lyrics generation or other existing models because of the following reasonßdummyTurker
3294ß0.0ßPrevious works focus on lyric generation instead of re-writing.ßdummyTurker
3295ß0.0ßThus their input requires the melody of the lyrics and beats but not the original lyrics, which is very different from ours which needs the original lyrics.ßdummyTurker
3296ß0.0ßLyric generation and re-writing then becomes two different tasks that are hard to compare.ßdummyTurker
3297ß0.0ßwe do have assessor anonymityßdummyTurker
3298ß0.0ßWell, although re-writing to fit a theme or style is a popular research topic, we don’t think it is as critical as re-writing to fit the tune for song-lyrics.ßdummyTurker
3299ß0.0ßWithout fitting the tune, the lyric cannot be performed no matter how well it is on the theme and style.ßdummyTurker
3300ß0.0ßFurthermore, re-writing to fit the tune itself imposes a very unique challenge as described in the paper.ßdummyTurker
3301ß0.0ßWe haven’t found any previous work tackling such task using a neural-network based model, which makes our technical contribution more significant.ßdummyTurker
3302ß0.0ßHow to construct a powerful lyric re-writing system to fit the not only the tune, theme, and style will be the major future work for us.ßdummyTurker
3303ß0.0ßThe context information in Figure 1 actually stands for the context vector a standard RNN model carries (we mentioned an RNN carries s_i and c_i as states and context vectors in Section 3.2, line 404).ßdummyTurker
3304ß0.0ßWe didn’t use any additional context features.ßdummyTurker
3305ß0.0ßWe'll re-emphasize such in the next version.ßdummyTurker
3306ß0.0ßThe rhymes are categorized as 39 kinds of vowel vocabulary, as mentioned in Section 5.2.ßdummyTurker
3307ß0.0ßIt is because we do not want to produce deterministic outputs every time.ßdummyTurker
3308ß0.0ßIf we choose argmax the RNN distribution, then the outcomes are fixed.ßdummyTurker
3309ß0.0ßFor re-writing, we hope different results can be generated every time.ßdummyTurker
3310ß0.0ßThus we choose to sample the distribution instead of argmax.ßdummyTurker
3311ß0.0ßWe found that our smoothing technique is effective for the sampling process since it avoids the case that a low-probability choice is sampled in the earlier step stage, which can lead to the ripple effect that hurts the fluency of the outcome.ßdummyTurker
3312ß0.0ßThe superior experiment results with smoothing justify such argument.ßdummyTurker
3313ß0.0ßAs Sec 5.2 mentions, there are 39 kinds of vowel ending for the last word in a sentence, we take the ending pronunciation(pinyin) of the ending word as input rhyme, as the example in section 3.1 the rhyme 'e' is picked from the last vowel of 'le'.ßdummyTurker
3314ß0.0ßWe'll add more description to make this clearer.ßdummyTurker
3315ß0.0ßThank you very much for your positive review.ßdummyTurker
3316ß0.0ßWe are so glad that you enjoyed our paper.ßdummyTurker
3317ß0.0ß>1) How is it related to Zipf law?ßdummyTurker
3318ß0.0ßTaylor's law has no relation to Zipf's law.ßdummyTurker
3319ß0.0ßAs you know, Zipf's law exponent  doesßdummyTurker
3320ß0.0ßnot distinguish the text kind as shown in Figure 4.ßdummyTurker
3321ß0.0ßZipf's law givesßdummyTurker
3322ß0.0ß(almost) -1.0 exponent for any natural language texts, music,ßdummyTurker
3323ß0.0ßprogramming language texts.ßdummyTurker
3324ß0.0ßWe will mention about this in the final version.ßdummyTurker
3325ß0.0ß>2) Does it also hold for social media text?ßdummyTurker
3326ß0.0ßWe predict that the Taylor's law also holds for social media texts andßdummyTurker
3327ß0.0ßthat the exponent is located between CHILDES and written text.ßdummyTurker
3328ß0.0ßWe will include the result in the final article if we manage to assemble the data.ßdummyTurker
3329ß0.0ß>** please replot the Figure 5: - not understandable due to the overlapping nature of it.ßdummyTurker
3330ß0.0ßThank you.ßdummyTurker
3331ß0.0ßWe will think about a better way of presenting/explaining Figure 5.ßdummyTurker
3332ß0.0ßThank you so much for the excellent review.ßdummyTurker
3333ß0.0ßWe will modify all your concerns in the final version.ßdummyTurker
3334ß0.0ßThe values of Taylor exponent should be understoodßdummyTurker
3335ß0.0ß"""relatively"", in comparison with theoretical 0.5 and 1.0, and"ßdummyTurker
3336ß0.0ßthose of other data kinds, (i.e.ßdummyTurker
3337ß0.0ßwiki, CHILDES data,ßdummyTurker
3338ß0.0ßmusic etc.)ßdummyTurker
3339ß0.0ßYour indication is excellent with respect to the comparison of TE among  language models.ßdummyTurker
3340ß0.0ß"We will mention more about this summarizing  our ""another work"", in the final version."ßdummyTurker
3341ß0.0ßAs you indicated, we could not introduce comparison with other textualßdummyTurker
3342ß0.0ßcomplexity measures, by lack of space.ßdummyTurker
3343ß0.0ßWe will provide a reference to them in the finalßdummyTurker
3344ß0.0ßversion.ßdummyTurker
3345ß0.0ßAbout CH, we will be careful about how to refer to this.ßdummyTurker
3346ß0.0ßThank you for presenting your concern.ßdummyTurker
3347ß0.0ßYou are right in all indications.ßdummyTurker
3348ß0.0ßAll of them will be clarified in theßdummyTurker
3349ß0.0ßfinal version.ßdummyTurker
3350ß0.0ß"(Bracket means the ""mean"" of the random variable.)"ßdummyTurker
3351ß0.0ßThank you so much for the excellent review.ßdummyTurker
3352ß0.0ßWe will modify all your concerns in the final version.ßdummyTurker
3353ß0.0ßThe main advantage of our method is that it distinguishes data kind.ßdummyTurker
3354ß0.0ßFirst, it distinguishes various natural data kind, as shown in FigureßdummyTurker
3355ß0.0ß4.ßdummyTurker
3356ß0.0ßThis is not the case with other previous fluctuation analysis or with other power law exponents.ßdummyTurker
3357ß0.0ßSecond, it shows whether aßdummyTurker
3358ß0.0ßlanguage model may produce a text with Taylor exponent larger thanßdummyTurker
3359ß0.0ß0.5, similar to natural language.ßdummyTurker
3360ß0.0ßFor basic models even for neural language models,ßdummyTurker
3361ß0.0ßTaylor exponent was 0.5.ßdummyTurker
3362ß0.0ßPerplexity is an overall evaluation measure of the probabilistic model, whereasßdummyTurker
3363ß0.0ßTE focuses on long range dependency quality.ßdummyTurker
3364ß0.0ßWe will summarize these points more clearly in the final version referring to our additional evidences.ßdummyTurker
3365ß0.0ßWe will improve the overall article, so that it becomes more readable.ßdummyTurker
3366ß0.0ßThank you so much for presenting your concern.ßdummyTurker
3367ß0.0ß> Additional CommentsßdummyTurker
3368ß0.0ßThank you so much for your kind indications about abstract, cite, CHILDESßdummyTurker
3369ß0.0ßand stylistic problems.ßdummyTurker
3370ß0.0ßWe will modify all of them if this paper getsßdummyTurker
3371ß0.0ßaccepted.ßdummyTurker
3372ß0.0ßWe thank the reviewer for feedback and plan to publish the code for better reproducibility.ßdummyTurker
3373ß0.0ßIn our proposed algorithm, the guided negative sampling is similar to the Tissue et al’s dict2vec paper.ßdummyTurker
3374ß0.0ßWe will explicitly mention the adoption of the algorithm in dict2vec in section 4.2.ßdummyTurker
3375ß0.0ßTo make the figure illustration better, we plan to divide the original figure into two parts, the one focusing on the main contribution from this paper about how we apply different mechanisms to the learning process, and the other detailing the inner structure of two modules(matrix multiplication...etc).ßdummyTurker
3376ß0.0ßBy doing so, the main contribution of our algorithm can be highlighted so that the whole algorithm can be understood better.ßdummyTurker
3377ß0.0ßThe latex ignored to display the footnote, which is “reward signal is only passed to the target word to stabilize model training due to directional architecture in the sense representation module”.ßdummyTurker
3378ß0.0ßWe decide to remove this footnote to give more space and explain it in section 4.3.ßdummyTurker
3379ß0.0ßWe will change the name to “learning rate control” to avoid misleading, increase the font size and reorganize the words to make them not overflowing to multiple lines and also more concise.ßdummyTurker
3380ß0.0ßWe will give each mathematical object a clear definition and note whether it is learnable or not.ßdummyTurker
3381ß0.0ßMore concretely, we will try to include every object in a graphical form in the newly drawn figure 1.ßdummyTurker
3382ß0.0ßOverall, we will redraw the figure, reorganize section 4, and use clearer and more standard notations to allow readers to better understand the work and focus more on the proposed algorithm of this work.ßdummyTurker
3383ß0.0ßThanks for feedback.ßdummyTurker
3384ß0.0ßWe will add one more extrinsic task using the additional page if accepted in order to demonstrate the coverage of the proposed system considering that our work does not rely on WordNet.ßdummyTurker
3385ß0.0ßThe shown examples are based on the manually chosen words with multiple senses in the training data.ßdummyTurker
3386ß0.0ßIndeed there are some failure cases, and we will include them to deeply analyze and discuss the strength and limitation of our proposed system.ßdummyTurker
3387ß0.0ßThe main contribution of our work here is providing interpretability of words by showing the associated descriptions for words.ßdummyTurker
3388ß0.0ßIn our experiments, we report the individual evaluation results in the second part of Table 1, where only a single mechanism is applied (there are three mechanisms: attentive group sense selection, learning rate control, and guided negative sampling).ßdummyTurker
3389ß0.0ßWithout the ablation test, readers can see the strength and weakness for each mechanism, and it can be shown that attentive group sense selection performs best.ßdummyTurker
3390ß0.0ßMoreover, we will add the experimental results of combining every two of the proposed mechanisms or all to see if they can provide further improvement.ßdummyTurker
3391ß0.0ßIn the work of Huang and Neelakantan, the vocabulary size they used is 99160 after removing some extremely infrequent words, so it means that we use same 99160 words for training and evaluation.ßdummyTurker
3392ß0.0ßWe decide to adopt the same vocabulary since we use the same wikipedia dump as training data.ßdummyTurker
3393ß0.0ßAs our footnote indicates, an important difference is they only used top 6000 words for training/testing and reported the score as final performance, while we used the whole 99160 vocabulary and obtained comparable or even better performance compared with theirs.ßdummyTurker
3394ß0.0ßIn other words, our coverage is 99160/6000=16 times larger than theirs, so our proposed system can deal much better with the noise when learning multiple senses of infrequent words.ßdummyTurker
3395ß0.0ßMoreover, note that the performance of Huang and Neelakantan are very likely to degrade if they adopt the full vocabulary like we do.ßdummyTurker
3396ß0.0ßThe reason that we do not use WordNet (or other already-annotated textual definitions) is about the generalizability of our model:ßdummyTurker
3397ß0.0ß1) It is hard to acquire for languages other than English; 2) It is known that the definition of WordNet is too fine-grained, some definition is hard for non-expert to differentiate.ßdummyTurker
3398ß0.0ßHowever, the performance using already-annotated textual definitions can be served as a reference for our system.ßdummyTurker
3399ß0.0ßMoreover, we will include the experiments about applying automatic definition extraction module to our work as the future work.ßdummyTurker
3400ß0.0ßIn Table 1, the baseline MUSE (+Dict) is the work from Lee and Chen, 2017, and other three mechanisms are the newly proposed approaches by this paper.ßdummyTurker
3401ß0.0ßAs we mentioned above, our evaluation is already done on every individual part of the proposed algorithms.ßdummyTurker
3402ß0.0ßIn order to improve the readability, we plan to reorganize section 4 so that readers can focus more on the newly proposed algorithms.ßdummyTurker
3403ß0.0ßWe will also give a clear conclusion of which part of the algorithm works best and also the combination of proposed algorithms by using the additional page if accepted.ßdummyTurker
3404ß0.0ßwe will correct the typo.ßdummyTurker
3405ß0.0ßThanks.ßdummyTurker
3406ß0.0ßWe proposed about the NLG merchandising system that should not make any mistakes.ßdummyTurker
3407ß0.0ßIn particular, we focused on solving grammatical problems that correspond to the characteristics of each language and methodology that minimizes time and effort.ßdummyTurker
3408ß0.0ßSome of technologies used in the paper have been applied to our best-selling phones in the world.ßdummyTurker
3409ß0.0ßIt is my bad.ßdummyTurker
3410ß0.0ßI will ask our language experts to review my English grammar.ßdummyTurker
3411ß0.0ßAnd I will improve the resolution.ßdummyTurker
3412ß0.0ßThe seq2seq model was evaluated using BLEU and ROUGE scores.ßdummyTurker
3413ß0.0ßI tried to explain correctness measure: mentioned in lines 662-733 and finally used in the result evaluation in Table 7.ßdummyTurker
3414ß0.0ßBut it seems it is not enough.ßdummyTurker
3415ß0.0ßI briefly explain examples of this paper.ßdummyTurker
3416ß0.0ßIt is easy to check for duplicated phrases caused by slot-filling.ßdummyTurker
3417ß0.0ßAnd we check the detailed data of the location and the hierarchical entity information.ßdummyTurker
3418ß0.0ßThrough these, it confirms whether there are any prepositional phrases to be written and corrects them if needed.ßdummyTurker
3419ß0.0ßTypos appeared.ßdummyTurker
3420ß0.0ßI am very sorry for that.ßdummyTurker
3421ß0.0ßI fixed them.ßdummyTurker
3422ß0.0ßThank you so much.ßdummyTurker
3423ß0.0ßThe commercialization approach must be careful.ßdummyTurker
3424ß0.0ßIt is also important to make training data at a level that can be commercialized very quickly, which is also competitive edge.ßdummyTurker
3425ß0.0ßWe have successfully commercialized the NLG service in a short period of time with more than millions of cases.ßdummyTurker
3426ß0.0ßWe got experimental results that the order was not important.ßdummyTurker
3427ß0.0ßHere we tried to show how to save and effort when generating lots of necessary sentences with minimum number of templates.ßdummyTurker
3428ß0.0ßThis NLG is a part of technology applied to the best-selling phones in the world.ßdummyTurker
3429ß0.0ßOf course, we are analyzing the assistant as a whole, and I thought it was not the scope of this paper.ßdummyTurker
3430ß0.0ßI hope that I have a chance to share detailed information at the conference.ßdummyTurker
3431ß0.0ßDue to space limit, some details are left out in this version.ßdummyTurker
3432ß0.0ßIf 5-th paper becomes available, we will include the mathematical formulation of the models.ßdummyTurker
3433ß0.0ßWe will include the optimization details and so on if 5-th paper becomes available.ßdummyTurker
3434ß0.0ßThe ability to model long term dependencies is one of the property of original FOFE (explained by Zhang (2015b)), which is carried over to dual-FOFE.ßdummyTurker
3435ß0.0ßThis is briefly explained in section 3.1.ßdummyTurker
3436ß0.0ßWe will provide more in-depth explanation if more space is available.ßdummyTurker
3437ß0.0ßThe FOFE ability to model long term dependencies is attributed to the fact that FOFE is a recursive encoding method; hence the values of current FOFE code depends on the value of previous FOFE code, so on so forth.ßdummyTurker
3438ß0.0ßFurthermore since alpha is less than 1, the alpha^1 > alpha^2 > … > alpha^n; this meant that the far-away context will be gradually forgotten and eventually vanish especially with finite-length representations in digital computers.ßdummyTurker
3439ß0.0ßThe dual-FOFE is good at solving this problem as explained in the paper.ßdummyTurker
3440ß0.0ßYes, the additional projected FOFE values (z) of higher orders are simply concatenated.ßdummyTurker
3441ß0.0ßWe deeply appreciate your insightful comments.ßdummyTurker
3442ß0.0ß"Regarding your comment ""Indirect use of language is not considered"", we did not consider indirect uses of language, such as sarcasm, since there was no such use in our logs as far as we are aware of."ßdummyTurker
3443ß0.0ßThis is probably because users know that it is difficult for the intelligent assistant to understand such uses.ßdummyTurker
3444ß0.0ßNevertheless, we will include the discussion on such language use in the paper since it is definitely an important aspect of natural language understanding.ßdummyTurker
3445ß0.0ßWe deeply appreciate your helpful comments.ßdummyTurker
3446ß0.0ß"Regarding your comment ""there are many duplicate but partial information"", could you please tell us some examples of such partial information in the paper?"ßdummyTurker
3447ß0.0ßWe will improve the presentation with sincerity based on your suggestion.ßdummyTurker
3448ß0.0ß"Regarding your question ""3.4 heuristic for candidate scorer... how is this motivated?"ßdummyTurker
3449ß0.0ß... did you try others for comparison?ßdummyTurker
3450ß0.0ß""", as we discussed in depth with a step-by-step example in Section 1, it is motivated by our observation from massive log data that frustrating conversations could largely be characterized by at least one of three characteristics"ßdummyTurker
3451ß0.0ßWe did not try other features since our focus was not to exhaustively examine possible features but to achieve supervised-level performance without manually labeled conversations, which we eventually did.ßdummyTurker
3452ß0.0ß"Regarding your comment ""Table 4"ßdummyTurker
3453ß0.0ß"Regarding your question about ""Definition of candidates"", as we described in Section 3.3, the candidates are conversations up to ten turns."ßdummyTurker
3454ß0.0ßWe agree that human evaluation can strengthen our results.ßdummyTurker
3455ß0.0ßBut it is generally used in the tasks that do not have gold answers or accept multiple answers like poetry and open-domain response generation.ßdummyTurker
3456ß0.0ßFor task-oriented dialog, it can be measured by metrics like per-response accuracy and per-dialog accuracy as in previous work [5,6] since it has gold answers and certain patterns.ßdummyTurker
3457ß0.0ßMoreover, objective evaluation can help to compare the performance of different models.ßdummyTurker
3458ß0.0ß1) As Figure 3 demonstrates, the performance reaches the highest when λ=0.5 (51.7% versus 50.7% in per-response accuracy and 60.1% versus 58.9% in entity accuracy when λ=0.0).ßdummyTurker
3459ß0.0ßSince it is not obvious enough in the graph, we will make it more clear.ßdummyTurker
3460ß0.0ßAs for the reason, the first loss helps the first seq2seq to learn the extraction and decoding of information while the second loss helps the model to learn the formation of sentences word by word.ßdummyTurker
3461ß0.0ßTherefore only using the second loss may fail to supervise the processing and decoding of information.ßdummyTurker
3462ß0.0ßCombining both losses can constrain the model in both aspects and achieves the best balance when λ=0.5.ßdummyTurker
3463ß0.0ßWe will add this explanation to the final version to improve the paper.ßdummyTurker
3464ß0.0ß2)At first we did not constrain the sum of λ1 and λ2 as you think, but we found the range of loss varied with the sum of λ1 and λ2.ßdummyTurker
3465ß0.0ßIf L=λ1Ls+λ2Lr, it can be rewritten as L=(λ1+λ2)[λLs+(1-λ)Lr], where λ=λ1/(λ1+λ2).ßdummyTurker
3466ß0.0ßIn this way, the range of L will be scaled to (λ1+λ2).ßdummyTurker
3467ß0.0ßTherefore, by constraining the sum to 1, we can fix L in a certain range and keep the same learning rate when using different λ.ßdummyTurker
3468ß0.0ßYou might first suspect that the selective and generative models are for open-domain dialog which may not be suitable for task-oriented dialog.ßdummyTurker
3469ß0.0ßBut actually generative method is a common framework even in traditional spoken dialog systems which contains the NLG module.ßdummyTurker
3470ß0.0ßSelective model is currently used in DSTC6 [7] given candidate answers.ßdummyTurker
3471ß0.0ßThese models do not conflict with another mainstream of state tracking and slot filling systems since they only change the back-end.ßdummyTurker
3472ß0.0ßThese end2end models also do latent dialog state tracking and slot-filling without supervision using neural networks.ßdummyTurker
3473ß0.0ßAs for reinforcement learning, it is a learning strategy instead of a structure or model, thus we did not discuss it in our paper.ßdummyTurker
3474ß0.0ßIn fact we first trained the slot prediction and response generation separately then concatenated them together.ßdummyTurker
3475ß0.0ßBut we found the result was not as good as the jointly trained one, with the same phenomenon discovered in [4].ßdummyTurker
3476ß0.0ßSo we only proposed the jointly learning model and did not report it because of limited paper length.ßdummyTurker
3477ß0.0ßIf we had developed a complete dialog system, we would report the task success rate as evaluation metric.ßdummyTurker
3478ß0.0ßHowever we are studying the generation module for task-oriented dialog and focusing on the informational expression problem.ßdummyTurker
3479ß0.0ßYes, we will shorten the related work in open-domain and add Wen et al's work to the final version.ßdummyTurker
3480ß0.0ßWe appreciate you helpful advice.ßdummyTurker
3481ß0.0ßFinally, we are confused that the average score you gave in each aspect was 3.5/5, but we got an overall score of 3/6.ßdummyTurker
3482ß0.0ßSince the overall score is 6, should we get 4.2/6 ?ßdummyTurker
3483ß0.0ß[4]Xiaodong Zhang and Houfeng Wang.ßdummyTurker
3484ß0.0ßA Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding, IJCAI 2016ßdummyTurker
3485ß0.0ß[5]Antoine Bordes, Y-Lan Boureau, Jason Weston.ßdummyTurker
3486ß0.0ßLearning End-to-End Goal-Oriented Dialog, ICLR 2017ßdummyTurker
3487ß0.0ß[6]Mihail Eric and Christopher D. Manning.ßdummyTurker
3488ß0.0ßA Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue, EACL 2017ßdummyTurker
3489ß0.0ß[7]Julien Perez, Y-Lan Boureau and Antoine Bordes.ßdummyTurker
3490ß0.0ßDialog System & Technology Challenge 6 Overview of Track 1 - End-to-End Goal-Oriented Dialog learning, COLIPS, 2017ßdummyTurker
3491ß0.0ßThanks for your detailed and insightful comments and kind advice.ßdummyTurker
3492ß0.0ß1) Yes, all of our results are measured on a held-out test set provided by DSTC2.ßdummyTurker
3493ß0.0ß2) As for the statistical significance results, since the improvement is quite obvious compared to the baselines, we did not repeat the experiments for many times as in previous work [6].ßdummyTurker
3494ß0.0ßWe will consider to add the significance results if accepted.ßdummyTurker
3495ß0.0ß3) We did not provide live evaluation since it is not a complete dialog system but a module.ßdummyTurker
3496ß0.0ßWe focus on the informational expression generation in this paper.ßdummyTurker
3497ß0.0ßYou are right.ßdummyTurker
3498ß0.0ßThe structure of the PDS will be described in detail in the final version if accepted.ßdummyTurker
3499ß0.0ßAnnotation will be added to the diagram.ßdummyTurker
3500ß0.0ßTypos and writings will be corrected.ßdummyTurker
3501ß0.0ßTerminology will be clarified.ßdummyTurker
3502ß0.0ß1) We will add their work in the introduction and related work in the final version.ßdummyTurker
3503ß0.0ßPlease go to the reply to Comment 4 from Reviewer#1 for detail.ßdummyTurker
3504ß0.0ß2) Yes, we did not explain it clearly and will rewrite it.ßdummyTurker
3505ß0.0ßA concrete example is given in section 3.1 from line 256 to 265.ßdummyTurker
3506ß0.0ßWe will put it to a more obvious position.ßdummyTurker
3507ß0.0ß3) Yes, Api calls are dialog acts actually and are regarded as a kind of special responses.ßdummyTurker
3508ß0.0ßThey are taken in to account when evaluate system's replies as in [5,6] and DSTC6 [7].ßdummyTurker
3509ß0.0ß4) We are using the dataset processed by [5] as mentioned in the Section 4.1 which does not contain dialog acts and slot-pairs.ßdummyTurker
3510ß0.0ßThe dialog acts in the original dataset are converted to Api calls.ßdummyTurker
3511ß0.0ß5) We use 'remarkable' in comparison with Seq2seq since the improvement is obvious.ßdummyTurker
3512ß0.0ßIt is not accurate and we will use 'obvious' instead.ßdummyTurker
3513ß0.0ß6) We will make a thorough rewrite and ask a native speaker to correct the errors.ßdummyTurker
3514ß0.0ßWe are sorry the paper was hard to follow.ßdummyTurker
3515ß0.0ßOur work builds on structured attention representations and is the first to model latent dependency trees within NMT.ßdummyTurker
3516ß0.0ßLanguages pairs were chosen to demonstrate variation in morphology.ßdummyTurker
3517ß0.0ßROOT is modeled implicitly but is not meaningful syntactically and so we did not include it in the visualizations to avoid clutter.ßdummyTurker
3518ß0.0ßWe will try and make a few general comments to help put the aims and discoveries of the paper in context (apologies if these are redundant and already clear).ßdummyTurker
3519ß0.0ß1.ßdummyTurker
3520ß0.0ßDependency Formalisms (a flawed metric) – As we all know, there is no agreement within linguistics or NLP for what constitutes a “correct” dependency (Collins vs Vardas vs Johansson vs Stanford vs UD vs CCG … ) but most formalisms share simple constructions (e.g.ßdummyTurker
3521ß0.0ßnsubj) with the famous exception of their analysis in the use of relative clauses or coordination which motivate the use of CCG and SRL for modeling predicate-argument structure.ßdummyTurker
3522ß0.0ßFor this reason, we have no expectation that our models will rediscover the arbitrary annotation decisions of UD nor is that a good measure of success, but we do expect the model to uncover properties shared across multiple formalisms (e.g.ßdummyTurker
3523ß0.0ßadj) and we are heartened to see the model ensuring verb agreement by uncovering long distance pred-arg style dependencies.ßdummyTurker
3524ß0.0ßThis discrepancy between formalism and unsupervised representation is well documented (e.g.ßdummyTurker
3525ß0.0ßmodel/prediction differences between Collins [Spitkovsky 17], Johansson [Gelling 12] and CCG [Bisk 13/15]).ßdummyTurker
3526ß0.0ßOur results shed light on the type of semantic/syntactic structure that’s necessary specifically for translation, complementing recent results by Williams 17.ßdummyTurker
3527ß0.0ßCombined we contribute an important datum in the discussion of how syntax/semantics should be represented and how much the downstream task affects that representation.ßdummyTurker
3528ß0.0ß2.ßdummyTurker
3529ß0.0ßMorphosyntactic inflection (Language pairs matter) – Every language has differing amounts of morphology and inflection.ßdummyTurker
3530ß0.0ßThese markers indicate agreement (e.g.ßdummyTurker
3531ß0.0ßnumber, gender, …).ßdummyTurker
3532ß0.0ßBecause many languages are morphologically poor (e.g.ßdummyTurker
3533ß0.0ßEnglish) less syntactic knowledge must be stored by the encoder.ßdummyTurker
3534ß0.0ßOur results show that long-distance syntactic knowledge is increasingly important for highly inflected languages (e.g.ßdummyTurker
3535ß0.0ßRussian <-> Arabic) and less so for pairs like English and German which are historically the most studied.ßdummyTurker
3536ß0.0ßThis motivates rethinking the limited role of syntax plays in current NMT models/attention mechanisms.ßdummyTurker
3537ß0.0ßThank you for your time and comments.ßdummyTurker
3538ß0.0ßYou’re absolutely correct that our work shares a common goal with Hashimoto & Tsuruoka in transferring structural information from the source to target.ßdummyTurker
3539ß0.0ßTheir work, however, is a subset of ours, specifically, their model is our FA-NMT which we will make more clear.ßdummyTurker
3540ß0.0ßWhile very important, that work lacks the benefits from our SA-NMT models, wasn’t evaluated on language pairs with interesting morphological or syntactic shift and therefore found weaker results on grammar induction.ßdummyTurker
3541ß0.0ßFurther, and perhaps most importantly, we worry that without our suite of experiments one might draw the wrong conclusion from their work which claims that syntax and translation quality are necessarily correlated.ßdummyTurker
3542ß0.0ßThis does not appear to be true in general, at least given our current community definitions of syntax.ßdummyTurker
3543ß0.0ßWhile this is a difficult and nuanced point, we wanted to draw attention to the idiosyncrasies of syntactic formalisms which should not be treated monolithically and raise the question: What type of structure or elements of syntax are important to the MT pipeline?ßdummyTurker
3544ß0.0ßFor example, perhaps future research should focus on integrating SRL instead of dependency syntax into NMT?ßdummyTurker
3545ß0.0ßFinally, while we can run a model translating from En→ X on WSJ, this, unfortunately, does not resolve the question of what dependency formalism is appropriate so it is unclear what insights it would provide.ßdummyTurker
3546ß0.0ßWe would very much appreciate your thoughts and a community discussion on how best to evaluate task-specific induced linguistic structure (see Sec 1 of response to R2 for some additional thoughts on this – There are many dependency annotation styles for WSJ – and the MatchLinguist paradigm of Noah Smith 2012.)ßdummyTurker
3547ß0.0ßThank you for your time and feedback, we have tried to address your questions and concerns below.ßdummyTurker
3548ß0.0ßW1: We agree.ßdummyTurker
3549ß0.0ßIt would not make sense for the trees to be perfect, in fact, the gate is present precisely to avoid forcing the model to make arbitrary attachment decisions when agreement is not required.ßdummyTurker
3550ß0.0ßWe will absolutely modify our analysis to show the frequency and length of long-distance dependencies.ßdummyTurker
3551ß0.0ßWe have previously done this analysis but cut it for space.ßdummyTurker
3552ß0.0ßW2: Our experiments indicate the gate is essential for BLEU and Grammar Induction, we have already run these experiments but will include comparisons in the camera ready.ßdummyTurker
3553ß0.0ßThe gate is also important linguistically for investigating when syntax/structure is necessary.ßdummyTurker
3554ß0.0ßW3: You’re correct, hard attention is used to simulate access to a complete parse tree.ßdummyTurker
3555ß0.0ßSo we used \hat{\beta} in Eq (15) to compute M = S_v \hat{\beta}ßdummyTurker
3556ß0.0ßW4: In Eq (8) and (9) in Section 4.1, \sum_{i=1}^{n}\beta_{ij} =1.0, you’re correct the performance gap is marginal but what we might not have made clear is that extracting interpretable syntax from flat attention does not ensure the production of a tree but rather a weighted graph and does not marginalize appropriately.ßdummyTurker
3557ß0.0ßThose concerns are addressed in our formulation.ßdummyTurker
3558ß0.0ßThe effect of this is perhaps most obvious when looking at the dependency scores.ßdummyTurker
3559ß0.0ßQ1: Alignment attention is the attention from the decoder to the encoder.ßdummyTurker
3560ß0.0ßThis type of alignment can be transferred directly to the target side without resolving syntactic dependency in the source side.ßdummyTurker
3561ß0.0ßWe’re sorry for the confusion.ßdummyTurker
3562ß0.0ßWe will clarify this point in the paper.ßdummyTurker
3563ß0.0ßDear reviewer, Thanks for your question.ßdummyTurker
3564ß0.0ßThere are 2 places demonstrate the common sense.ßdummyTurker
3565ß0.0ßFirstly, we design an experiment to test common sense in subsection 4.2.3.ßdummyTurker
3566ß0.0ßIt uses the mask trick to hide the entity mentions and reason the entity type, which is one way to test common sense, as shown in table 4.ßdummyTurker
3567ß0.0ßThe only way to solve this configuration is to use the reasoning ability.ßdummyTurker
3568ß0.0ßCompared to the LSTM-CRF model, it improves about 20% in F1 score.ßdummyTurker
3569ß0.0ß"Secondly, in the subsection 4.2.2 (case study), as shown in table 3, id 4 ""Bowyer, who"" hinds that Bowyer is a person when the best LSTM-CRF model didn't."ßdummyTurker
3570ß0.0ß"id 7 ""{1} are top and {2} third"" also demonstrates."ßdummyTurker
3571ß0.0ßPlease refer to the reply to weakness argument 1.ßdummyTurker
3572ß0.0ßOur NER common sense network is novel from the perspective that it makes use of the stable pattern entailment of sentences.ßdummyTurker
3573ß0.0ß"The scope of this paper is task-specific common sense, and the “common sense"" in our work can be understood as the entailments of a domain/range limitation existing in the stable pattern or similar context, not a specific example situation."ßdummyTurker
3574ß0.0ß"For example, ""{1} are top and {2} third"" implies that the {1} and {2} are the same kind of entity that can be compared or ranked as shown in table 3."ßdummyTurker
3575ß0.0ßPlease refer to the  “Reviewer2#Reply to weakness argument 1”.ßdummyTurker
3576ß0.0ßCode is released for further research.ßdummyTurker
3577ß0.0ßhttpsßdummyTurker
3578ß0.0ßThe novelty of this paper is described at the end of the introduction, which is summarized as 4 points.ßdummyTurker
3579ß0.0ßThis question is replied.ßdummyTurker
3580ß0.0ß"Please refer to ""Reviewer 2#Reply to weakness argument 1 and 3""."ßdummyTurker
3581ß0.0ßSome of the points is a universal idea for other tasks and for further research.ßdummyTurker
3582ß0.0ßPlease refer to the reply in “Reviewer2#Reply to question 1”.ßdummyTurker
3583ß0.0ßThis question has been replied in “Reviewer2#Reply to weakness argument 2”.ßdummyTurker
3584ß0.0ßPlease refer to it.ßdummyTurker
3585ß0.0ßThis research mainly focuses on the model architecture not the configuration of feature engineering.ßdummyTurker
3586ß0.0ßThe work of (Chiu and Nichols, 2015) adopts one more external knowledge resources, gazetteers(a geographical list), which add more handcraft knowledge on it.ßdummyTurker
3587ß0.0ßThe fair evaluation should only consider the work adopt the same feature configuration.ßdummyTurker
3588ß0.0ß"As reviewer 1 said, ""very good F score on CoNLL-2003""."ßdummyTurker
3589ß0.0ßSection 3.2 is mainly to introduce the CSNS network construction in which the common sense generator is to reveal the sentence pattern and the residual attention is to capture the context entailment and complement the unknown entity slot via the word-char embeddings.ßdummyTurker
3590ß0.0ßThese two components play different roles decided by their behavior nature.ßdummyTurker
3591ß0.0ßThe connection of these examples in the introduction can be found in the subsection 4.2.2 (case study), where we analyze how the results are improved via CSNS network.ßdummyTurker
3592ß0.0ß"As shown in table 3, id 8 demonstrates the ""1995 world cup"" example and id 1-7 are other improvements."ßdummyTurker
3593ß0.0ßPlease refer to the reply in the “Reviewer1#Reply to weakness argument 1”.ßdummyTurker
3594ß0.0ßCode is released for further research.httpsßdummyTurker
3595ß0.0ß"We would like to share a view from another reviewer (reviewer 1) which claim that ""very good F score on CoNLL-2003""."ßdummyTurker
3596ß0.0ßThe CoNLL-2003 dataset has reached its extreme/bottleneck in the past years.ßdummyTurker
3597ß0.0ßThere're some very difficult samples that can't be solved even with a top performance LSTM-CRF model as shown in table 3.ßdummyTurker
3598ß0.0ßThis paper solves some of the difficult samples, part of which are listed in table 3, which is a congratulated development for this problem.ßdummyTurker
3599ß0.0ßWe should notice the value to solve these difficult samples.ßdummyTurker
3600ß0.0ßWithout any handcraft features or external glossary resources, the F1 score is improved obviously through our method on two datasets, and through the subsection 4.2.2, we know how it improves and should draw the conclusion it is statistically significant.ßdummyTurker
3601ß0.0ßIn fact, we also submit a paper in Frontiers in neuroinformatics which applies the CSNS network to cross-domain NER on neurobiology and cognition, and the experiments demonstrate the improvement is significant.ßdummyTurker
3602ß0.0ßThere’re two main components, residual attention, and sense generator.ßdummyTurker
3603ß0.0ßEach component alone can't fulfill our idea to construct the NER common sense network, since it is decided by the behavior nature of these components.ßdummyTurker
3604ß0.0ß"As replied in ""Reply to weakness argument 1"", these two components play different roles in CSNS network, without one of which the CSNS network can't be constructed."ßdummyTurker
3605ß0.0ßThe residual attention in this paper is a general attention scheme for a sequential model, in which two proposed attention formulations also can be used in other attention components.ßdummyTurker
3606ß0.0ßThe technical meaning of our proposed attention is to compress the model parameter from O(n^2) to O(n) even O(1) i.e.ßdummyTurker
3607ß0.0ßGaussian kernel in attention layer.ßdummyTurker
3608ß0.0ßAs for the result, both kernel and moderate attention can achieve a statistically equal result as the single layer neural attention but with much less parameter and converge faster in experiments.ßdummyTurker
3609ß0.0ßThe meaning of kernel attention is an exploration to connect the relationship between kernel method and neural attention mechanism.ßdummyTurker
3610ß0.0ßThe kernel attention is extensible.ßdummyTurker
3611ß0.0ßThis paper wants to give more inspiration and space to the readers for further research.ßdummyTurker
3612ß0.0ßIt doesn't focus on learning the co-occurrence patterns of entities.ßdummyTurker
3613ß0.0ß"In fact, this question is also about the two key elements, which has been replied in the ""Reply to weakness argument 1 and 3""."ßdummyTurker
3614ß0.0ßBesides, the crossover operation crosses the occurrence of interesting entities from the same categories in different samples, which reduces the entity specificity and reveals the stable pattern entailment for model learning.ßdummyTurker
3615ß0.0ßThe recall and precision are calculated by the universal standard equation, recall=tp/(tp+fn), precision=tp/(tp+fp).ßdummyTurker
3616ß0.0ßAll the result considers the entity types and follows a strict rule.ßdummyTurker
3617ß0.0ßA correct entity prediction is established only when a predicted entity absolutely matches with the gold standard including the type, length and position of a chunking in sentences.ßdummyTurker
3618ß0.0ßlinear algebra and convex geometry are heavily used in the proof in section 3.ßdummyTurker
3619ß0.0ßBut that section only takes one page out of eight!ßdummyTurker
3620ß0.0ßI think that the reviewer is ignoring the results obtained in the rest of the paper (e.g., the proof in sections 4-5), which deal with OT and have nothing to do with ''learning linear weight matrix''.ßdummyTurker
3621ß0.0ßI would like to take issue with the reviewer's opinion about the appropriateness of the paper to ACL.ßdummyTurker
3622ß0.0ßThe reviewer submits that the paper would be suitable for a linguistic journal, showing that it has sufficient implications for linguistics.ßdummyTurker
3623ß0.0ßAnd that it would also be suitable for an applied math journal, showing that the paper is formally sophisticated.ßdummyTurker
3624ß0.0ßHow come that a formally sophisticated and linguistically relevant paper is not suitable for the ACL?ßdummyTurker
3625ß0.0ßIsn't that pretty much the definition of the ideal ACL paper?ßdummyTurker
3626ß0.0ßthe suggestion to add additional examples and to better motivate the notion of T-order is definitely valuable, and I will make sure to follow it, taking advantage of the additional page allowed in the final submission.ßdummyTurker
3627ß0.0ßI sincerely cannot understand this paragraph.ßdummyTurker
3628ß0.0ßWhat does it mean to ''graphically understand the constraints''?ßdummyTurker
3629ß0.0ßWhat is a ''holistic assessment''?ßdummyTurker
3630ß0.0ßAgain, I sincerely do not understand this question.ßdummyTurker
3631ß0.0ßI do not understand this question about the ''optimal hyperplane''.ßdummyTurker
3632ß0.0ßThe notion of ''optimal hyperplane'' indeed does not appear anywhere in the paper.ßdummyTurker
3633ß0.0ßAgain, I sincerely do not understand this question.ßdummyTurker
3634ß0.0ßNote that constraints are not ''winners'' or ''losers''.ßdummyTurker
3635ß0.0ßIt is instead candidates (not constraints) that are classified this way.ßdummyTurker
3636ß0.0ßNote that the paper does NOT propose a novel phonological model.ßdummyTurker
3637ß0.0ßRather, it formally investigates models which are widely used in the phonological literature.ßdummyTurker
3638ß0.0ßThank you for your feedback.ßdummyTurker
3639ß0.0ßWe purposely chose to limit details of the convolutional network used in order to keep anonymity and not risk revealing information about authorship.ßdummyTurker
3640ß0.0ßIf our paper is accepted we will make sure additional relevant technical details are added.ßdummyTurker
3641ß0.0ßThank you for your feedback.ßdummyTurker
3642ß0.0ßWe address each of the weakness arguments in order below:ßdummyTurker
3643ß0.0ß1) We agree that these technical details are lacking from the paper.ßdummyTurker
3644ß0.0ßWe purposely chose to do this to keep anonymity and not risk revealing information about authorship.ßdummyTurker
3645ß0.0ßIf our paper is accepted to ACL we intend to add a section describing the technical details of the convolutional network, loss and dataset used for training.ßdummyTurker
3646ß0.0ß2) We agree that the underlying idea is not new and emphasize in our work that one of our main contributions is applying these ideas at scale.ßdummyTurker
3647ß0.0ßAs part of table 1, we aimed to highlight all existing work that have used CNN features as word embeddings.ßdummyTurker
3648ß0.0ßWe thank the reviewer for providing missed citations and we will make sure that these are added to the paper.ßdummyTurker
3649ß0.0ß3) The main motivation for choosing our particular fusion mechanism is that allows for analysis and interpretability.ßdummyTurker
3650ß0.0ßA key contribution of our work was demonstrating our model’s capability to weight embeddings differently based on abstraction and concreteness of words - showing that this holds across a wide range of NLP tasks and datasets.ßdummyTurker
3651ß0.0ßWe agree that testing alternative fusion strategies is very interesting but we feel this lies outside the scope of our current contribution.ßdummyTurker
3652ß0.0ßWe will add an additional paragraph which highlights alternative fusion strategies in detail and how they could be applied to our embeddings.ßdummyTurker
3653ß0.0ß4) As with the second point above, we will add these citations to the paper and we thank for reviewer for highlighting work we have missed.ßdummyTurker
3654ß0.0ß5) We do use picturebook embeddings for the nearest neighbor experiments.ßdummyTurker
3655ß0.0ßWe apologize that this wasn’t clear and will update the writing accordingly.ßdummyTurker
3656ß0.0ßThank you for your feedback as well as highlighting several grammatical problems.ßdummyTurker
3657ß0.0ßWe will correct these.ßdummyTurker
3658ß0.0ßWe agree our work is difficult to reproduce (however note that most image search services have a public API).ßdummyTurker
3659ß0.0ßHowever, we believe our work is a valuable contribution to the ACL/NLP community.ßdummyTurker
3660ß0.0ßWe hope this work will spur further image-grounding research as a promising direction; and eventually more datasets will be publicly available.ßdummyTurker
3661ß0.0ßWe are looking into ways of releasing our Picturebook embeddings to the general public.ßdummyTurker
3662ß0.0ßAs we summarized in the last paragraph of the Introduction section, there are three major contributions in our paper.ßdummyTurker
3663ß0.0ßFirst, we proposed a new end-to-end neural architecture for the Chinese NER task.ßdummyTurker
3664ß0.0ßThis architecture can achieve significant better performance than the popular LSTM-CRF architecture on Chinese NER and has the potential to be the state-of-the-art neural architecture for Chinese NER.ßdummyTurker
3665ß0.0ßThe contribution of our paper to Chinese NER in terms of network architecture is similar with (Lample et al., 2016) and (Ma and Hovy, 2016) to English NER.ßdummyTurker
3666ß0.0ßSecond, we proposed a unified framework to jointlyßdummyTurker
3667ß0.0ßtrain Chinese NER and word segmentation models to improve the performance of NER model in predicting entity boundaries.ßdummyTurker
3668ß0.0ßThird, we proposed an automatic method to generate pseudo labeled samples to enrich training data which can effectively improve the performance of Chinese NER when training data is insufficient.ßdummyTurker
3669ß0.0ßWe clearly explained the motivation of our approach in the paper.ßdummyTurker
3670ß0.0ßFor example, in Section 3.1, in order to explain why we incorporate CNN into our model, we introduced that our motivation comes from the observation that local information of sentences is important for Chinese NER and CNN can effectively capture local context information.ßdummyTurker
3671ß0.0ßIn addition, we presented an example to help readers better understand this motivation.ßdummyTurker
3672ß0.0ßBesides, in Section 3.3 we also provide a detailed example to illustrate why and how we can generate pseudo labeled samples from labeled data.ßdummyTurker
3673ß0.0ßDifferent from English NER where there is a well-recognized leaderboard for different methods, to our best knowledge such a well-recognized leaderboard does not exist in Chinese NER task.ßdummyTurker
3674ß0.0ßIn our experiments, we compare our approach with different state-of-the-art methods based on neural networks, such as LSTM-CRF and the multi-task domain adaptation method (Peng and Dredze, 2017).ßdummyTurker
3675ß0.0ßThe experimental results clearly validate the effectiveness of our approach.ßdummyTurker
3676ß0.0ßThank the reviewer for this comment.ßdummyTurker
3677ß0.0ßTo the best of our knowledge, the CNN-LSTM-CRF architecture has not been proposed for Chinese NER.ßdummyTurker
3678ß0.0ßThe most popular neural architecture for Chinese NER is LSTM-CRF (Dong et al., 2016, Peng et al., 2017).ßdummyTurker
3679ß0.0ßFor English NER, the CNN-LSTM-CRF architecture has been proposed (Ma and Hovy, 2016).ßdummyTurker
3680ß0.0ßHowever, as we explained in the related work section, there are some significant differences between our approach and the method proposed in (Ma and Hovy, 2016).ßdummyTurker
3681ß0.0ßIn (Ma and Hovy, 2016), CNN is used to learn word representations from characters in each word, which are combined with word embeddings to build word features.ßdummyTurker
3682ß0.0ßTheir method is designed for English NER, which is a word-level sequence labeling problem.ßdummyTurker
3683ß0.0ßHowever, since there is no explicit word boundary in Chinese texts, Chinese NER is usually modeled as a character-level sequence labeling problem.ßdummyTurker
3684ß0.0ßThus, the method in (Ma and Hovy, 2016) cannot be directly applied to Chinese NER.ßdummyTurker
3685ß0.0ßIn our approach, CNN is used to learn character representations from local contexts in a sentence.ßdummyTurker
3686ß0.0ßSince Chinese words and entities usually consist of a few characters, explicitly capturing local contexts for learning character representations is beneficial for Chinese NER.ßdummyTurker
3687ß0.0ßThank the reviewer for this suggestion.ßdummyTurker
3688ß0.0ßIn our experiments we compare the performance of our approach with several state-of-the-art methods based on neural networks for Chinese NER, such as LSTM-CRF and the multi-task domain adaptation method (Peng and Dredze, 2017).ßdummyTurker
3689ß0.0ßWe will add more state-of-the-art results to our paper for further comparison according the reviewer’s suggestion.ßdummyTurker
3690ß0.0ßThank the reviewer very much for this great suggestion.ßdummyTurker
3691ß0.0ßWe are very interested in applying attention mechanism to Chinese NER task and have started to do some initial experiments.ßdummyTurker
3692ß0.0ßWe will add some discussions on it to our paper.ßdummyTurker
3693ß0.0ßWe see the contribution of our work as (a) a state-of-the-art classifier for both contemporary and historical German newspaper texts and (b) a detailed analysis of its performance.ßdummyTurker
3694ß0.0ßThis is admittedly not a conceptual breakthrough, but we believe that it is of practical relevance.ßdummyTurker
3695ß0.0ßThanks also for mentioning the spelling mistake.ßdummyTurker
3696ß0.0ßTransfer learning is not restricted to any specific task nor to the machine learning method.ßdummyTurker
3697ß0.0ßUsing sequence classification methods like e.g.ßdummyTurker
3698ß0.0ßCRF or BiLSTM, the tasks are restricted to sequence labeling tasks (e.g.ßdummyTurker
3699ß0.0ßNER, POS, complex word identification, etc.).ßdummyTurker
3700ß0.0ßHowever, using different machine learning algorithms (e.g.ßdummyTurker
3701ß0.0ßSVM) transfer learning could be also used for classification tasks e.g.ßdummyTurker
3702ß0.0ßsentiment analysis, lexical entailment classification, image classification, etc.ßdummyTurker
3703ß0.0ßWe would clarify the different tag sets used for both datasets in a revised version of paper.ßdummyTurker
3704ß0.0ßThe GermEval task has two levels of annotations: outer and inner span named entities (e.g.ßdummyTurker
3705ß0.0ß"""Chicago Bulls"" is as outer span annotation an organization, but ""Chicago"" (inner span) itself is a location)."ßdummyTurker
3706ß0.0ß"In addition to the standard entity types (location, person, other and organization) the datasets has fine grained versions of these entities which are marked with suffixes: ""-deriv"" marks derivations of the named entities (e.g."ßdummyTurker
3707ß0.0ß"""German actor"" -> German is a derived location) and ""-part"" marks compounds including a named entity (e.g."ßdummyTurker
3708ß0.0ß"in the word ""Rhineshore"" the compound Rhine is location)."ßdummyTurker
3709ß0.0ßTo compare to previous state-of-the-art methods, we show results on the official metric (outer span) in Table 1.ßdummyTurker
3710ß0.0ß"To be more conform with the tagsets of the CoNLL task, we focus on outer spans and remove the fine-grained tags (""-deriv"" and ""-part"") in the follow-up experiments (Table 3 and 4)."ßdummyTurker
3711ß0.0ßTo compare to the state-of-the-art methods of the GermEval task, we show GermEval results in Table 1 that are computed based on the official metric of the task.ßdummyTurker
3712ß0.0ß"The results are reported on fine-grained outer span annotations (also including tags with suffix ""-deriv"" and ""-part"")."ßdummyTurker
3713ß0.0ßIn the following experiments (see Table 3 and 4), we do not consider the fine-grained named entities, to be closer to the tag sets of the other datasets.ßdummyTurker
3714ß0.0ßWe would clarify this in a revised version.ßdummyTurker
3715ß0.0ßThanks for that note.ßdummyTurker
3716ß0.0ßWe would refer to previous work on historical data in the revised version of the paper.ßdummyTurker
3717ß0.0ßThanks a lot for your review, we really appreciate your valuable comments.ßdummyTurker
3718ß0.0ßSmall point: for your replicability score (3/5),  the dataset link is ready upon acceptance, as well as open source code for training the models, and the saved models themselves for evaluation, so this should be a very replicable paper.ßdummyTurker
3719ß0.0ßThanks a lot for your review, we really appreciate your valuable comments.ßdummyTurker
3720ß0.0ßConsistency of personality is measured in a few ways in the paper: i) by humans scoring 'consistency', ii) a perhaps more objective metric where they have to pick the persona after the event from a “lineup”, and finally iii) automated metrics that compare using the persona versus not using it.ßdummyTurker
3721ß0.0ßWe find that the models can use the persona to a degree using these metrics.ßdummyTurker
3722ß0.0ßWe agree that there is still a long way to go to make the model be truly consistent with some “personality”, but we humbly believe that our proposed dataset will at least become an interesting starting point for this research direction, and will hopefully inspire more exciting ideas from other researchers.ßdummyTurker
3723ß0.0ßThanks a lot for your review, we really appreciate your valuable comments.ßdummyTurker
3724ß0.0ßWe agree that in this paper we haven't proposed substantially new models.ßdummyTurker
3725ß0.0ßThe main goal of this paper is to introduce a new task and data set, so the various models we proposed are aimed for reasonable baselines which hopefully cover most of the popular approaches.ßdummyTurker
3726ß0.0ßWe would expect other researchers to further develop new models based on this new testbed.ßdummyTurker
3727ß0.0ßWhat we did show is that those architectures trained on this data provide very interesting trained models.ßdummyTurker
3728ß0.0ßWe think some of HIT@1 metrics are reasonably good (e.g.ßdummyTurker
3729ß0.0ß50% of the time the correct response is chosen out of 20 choices with a self persona for the KV Profile Memory), but we agree some of  our baseline models are weaker (especially generative ones).ßdummyTurker
3730ß0.0ßFor poor HIT@1 for generative models, one possible reason is that generative models were not trained to directly optimize the ranking loss HIT@1, so they are worse than all the ranking+retrieval models.ßdummyTurker
3731ß0.0ßDuring data collection we found that the MTurk workers were biased such that during the chat they were more likely to focus on their own personas rather than the other person's personas.ßdummyTurker
3732ß0.0ßSuch bias makes speaker's utterances more often depend on speaker's own “Self Persona”.ßdummyTurker
3733ß0.0ßAs a result, when training based on  'Their Persona' and 'Both Persona' , the other person's personas in model memory (for both generative and ranking models) may diverge the model's focus, make the model focus less on its own persona sentences and thus does not fit the true user bias very well.ßdummyTurker
3734ß0.0ßThis is mentioned in Sec 5.1 (“Their Persona”) but we can add more text to comment on this further.ßdummyTurker
3735ß0.0ßThank you for the detailed comments and the suggestion regarding the visualization of the results.ßdummyTurker
3736ß0.0ßIt is very helpful and we will work on it.ßdummyTurker
3737ß0.0ßOur model incorporates the position-invariance of CNN into RNN, which makes the model be able to capture both long-term dependencies and local information well.ßdummyTurker
3738ß0.0ßRegarding the model structure, DRNN replaces convolution kernels of CNN with recurrent units, which is similar to CNN and RNN.ßdummyTurker
3739ß0.0ßHowever, the modification is intuitive and effective, which achieves 10-50% relative error reduction compared with other similar models.ßdummyTurker
3740ß0.0ßMoreover, our proposed model is a general model framework.ßdummyTurker
3741ß0.0ßWe can both extend it to multilayer, multi-window and bidirectional DRNN to achieve better effectiveness, and apply DRNN in other tasks such as textual entailment, reading comprehension and information extraction.ßdummyTurker
3742ß0.0ßWe have also tried using model structure considering the context after the current input.ßdummyTurker
3743ß0.0ßHowever, this does not show significant improvement whilst making the DRNN model more time-consuming.ßdummyTurker
3744ß0.0ßThus, we do not apply the structure in this paper and consider it as a future work.ßdummyTurker
3745ß0.0ßWe will report the significance test results in the next revision.ßdummyTurker
3746ß0.0ßWe will visualize the model learning results in the appendix.ßdummyTurker
3747ß0.0ßWe will release our code on GitHub after the end of the anonymity period.ßdummyTurker
3748ß0.0ßThank you for the comments and excellent summary.ßdummyTurker
3749ß0.0ßWe have compared our model with char-CRNN which is addressing the same problem.ßdummyTurker
3750ß0.0ßWe consider to implement some more similar models and compare our model with these models for future work.ßdummyTurker
3751ß0.0ßWe thank the reviewer for valuable comments.ßdummyTurker
3752ß0.0ßThe most relevant work to our paper is Oshikiri (2017), which is written in ***English*** and was published in ***EMNLP***.ßdummyTurker
3753ß0.0ßIt proposes a pipeline of building n-gram lattices without word segmentation and subsequently feeding the lattices to skip-gram model.ßdummyTurker
3754ß0.0ßSection 3 is fully devoted to explanation of Oshikiri (2017) in addition to a brief introduction in Section 1.ßdummyTurker
3755ß0.0ßOn the other hand, Oshikiri & Shimodaira (2017), which is written in Japanese, used the same idea of building n-gram lattices, but subsequent word embedding model is different from Oshikiri (2017).ßdummyTurker
3756ß0.0ßIt employed Eigenwords (Dhillon et al., 2015) instead of skip-gram model.ßdummyTurker
3757ß0.0ßIn the paper, we conducted two different experiments: An “intrinsic” experiment of word coverage to evaluate how well the criterion (expected word frequency) works for selecting n-gram vocabularies, and an “extrinsic” experiment of noun category prediction as a downstream task.ßdummyTurker
3758ß0.0ßThe intrinsic experiment was exhaustively conducted on three different languages, showing the large improvement (about two fold in the word coverage, i.e., recall)  of the proposed method (WNE) over the previous method (FNE).ßdummyTurker
3759ß0.0ßWe think that the word coverage is important for several downstream tasks such as text classification and machine translation.ßdummyTurker
3760ß0.0ßFurther experiments of these downstream tasks will be included in our future works.ßdummyTurker
3761ß0.0ßOur proposed method (WNE) combines the probabilistic word frequency counting  (Mori & Takuma, 2004) with the n-gram lattice for word embedding (Oshikiri, 2017).ßdummyTurker
3762ß0.0ßAlthough the idea looks trivial, it is built on a firm mathematical foundation rather than heuristic.ßdummyTurker
3763ß0.0ßIn addition, our method significantly reduces the number of false positives (i.e., increasing the precision) for two or three fold in the n-gram vocabulary as shown in Figure 2, while increasing the word coverage (i.e., recall).ßdummyTurker
3764ß0.0ßThis improvement is highly effective in real world situation, such as dealing with large scale SNS data.ßdummyTurker
3765ß0.0ßNote that Figure 2 only shows the results for Japanese, and PR-curves for other two languages are similar to that of Japanese.ßdummyTurker
3766ß0.0ßFigures for these languages may be included in a camera-ready paper.ßdummyTurker
3767ß0.0ßWe would like to thank the reviewer for detailed and valuable comments.ßdummyTurker
3768ß0.0ßWe agree that the link to psychological research has to be improved upon in the final version, but also want to note, that the goal of the paper was not to draw conclusions about multilingual properties of dementia, but rather to find new ways to increase the size of the available data for machine learning in this domain.ßdummyTurker
3769ß0.0ßConcerning reproducibility of results: When working with patient data, sharing of data sets is always a problem.ßdummyTurker
3770ß0.0ßIn some cases, even the sharing of feature vectors computed from this data is prohibited.ßdummyTurker
3771ß0.0ßNevertheless, we will try to arrange a way for people to request access to the feature vectors used in our experiments.ßdummyTurker
3772ß0.0ßThanks for your review.ßdummyTurker
3773ß0.0ßWe will polish our writing accordingly later.ßdummyTurker
3774ß0.0ßThanks for your review.ßdummyTurker
3775ß0.0ßWe will improve the concerned part to make the formulas more accurate once we have more space available later.ßdummyTurker
3776ß0.0ßThank you for your thoughtful review.ßdummyTurker
3777ß0.0ßThe emphasis of our paper is on fine-grained typing.ßdummyTurker
3778ß0.0ßIn the UMLS biomedical data we discussed our results in terms of linking as well, since UMLS does not make a distinction between entity types and entities.ßdummyTurker
3779ß0.0ßThere is no restriction in our model that makes it apply only to medical data; if there were other data in which entity types are organized in a hierarchy and entity typing & entity linking were indistinguishable, we would expect it work well there also.ßdummyTurker
3780ß0.0ßAfter annotating individually, our annotators met to discuss resolve any conflicts they had for our final annotation set.ßdummyTurker
3781ß0.0ßHowever we agree this information is informative and we can add these numbers in the camera ready of the paper.ßdummyTurker
3782ß0.0ßThank you for your thoughtful review.ßdummyTurker
3783ß0.0ßThank you for this suggestion.ßdummyTurker
3784ß0.0ßWe agree that further analysis over hyperparameter setting would be informative to our readers.ßdummyTurker
3785ß0.0ßThus we plan to include hyperparameter analysis and ablation experiments in the camera ready.ßdummyTurker
3786ß0.0ßOur own experience in exploring these settings (before before the submission and afterwards) hasn’t caused any unexpected results or concerns.ßdummyTurker
3787ß0.0ßThank you for your thoughtful review.ßdummyTurker
3788ß0.0ßHowever, we believe that you have misunderstood the main contributions of the paper.ßdummyTurker
3789ß0.0ßThe primary contribution of our paper is in fine-grained typing---only touching on entity linking where they are fused.ßdummyTurker
3790ß0.0ßIn the UMLS biomedical data we discussed our results in terms of typing and linking, since UMLS does not make a distinction between entity types and entities (thus “typing” and “linking” are one and the same task in UMLS).ßdummyTurker
3791ß0.0ßBut our model is not intended for classic, stand-alone entity-linking.ßdummyTurker
3792ß0.0ßThus experimental comparisons to past work in entity linking is not applicable.ßdummyTurker
3793ß0.0ßThe main contributions of the paper are to (1) show that incorporating hierarchical structure improves entity typing accuracy, (2) introduce effective methods for incorporating that structure, and (3) provide the research community with two new comprehensive datasets exhibiting hierarchical structure---enabling further research in this area.ßdummyTurker
3794ß0.0ßUMLS entities-and-types (indistinguishable) are organized hierarchically, and therefore a good domain in which to demonstrate our contribution.ßdummyTurker
3795ß0.0ßIn the CoNLL and TAC entity-linking benchmarks, neither the types nor the entities are organized hierarchically, and thus are not suitable datasets for demonstrating our contributions.ßdummyTurker
3796ß0.0ßThank you, however, for your suggested references to relevant related work; we will certainly add them in the camera ready.ßdummyTurker
3797ß0.0ßYou suggest we structure the paper as first showing good performance on a known simple entity typing experiment, and then proceeding to more types.ßdummyTurker
3798ß0.0ßHowever, this is actually exactly what we did.ßdummyTurker
3799ß0.0ßOur first experiments were on FIGER, a known, widely used, limited typing dataset.ßdummyTurker
3800ß0.0ßWe show that even on this restricted hierarchical typeset, our methods improve entity typing accuracy, surpassing the previous state-of-the-art.ßdummyTurker
3801ß0.0ßWe then move on to show similar gains on our much larger TypeNet and MedMentions datasets.ßdummyTurker
3802ß0.0ßFinally, you express concern that our CNN text encoder is a “rather weak” model.ßdummyTurker
3803ß0.0ßHowever, we actually show that it outperforms the Attentive LSTM model of Shimaoka (Table 2).ßdummyTurker
3804ß0.0ßFurthermore, other authors have also shown that a CNN such as ours can match or outperform a more expensive LSTM model in multiple tasks, such as machine translation (https://arxiv.org/abs/1705.03122), relation extraction (http://www.aclweb.org/anthology/P16-1200), and named entity recognition (http://www.aclweb.org/anthology/D17-1283).ßdummyTurker
3805ß0.0ßSorry for the bad writing.ßdummyTurker
3806ß0.0ßWe will introduce our model and data collecting in separate sections.ßdummyTurker
3807ß0.0ßThank you for the useful remarks.ßdummyTurker
3808ß0.0ßOur claim should be more concentrating on completing code snippets with the aids of natural language descriptions, i.e., completing a whole code snippet from beginning by completing every next word/token based on the previous correct partial sequence.ßdummyTurker
3809ß0.0ßThe first token is predicted by encoded natural language information directly.ßdummyTurker
3810ß0.0ßOur current evaluation metric is the acc@1, i.e., how often does the ground truth word/token make the first place in the suggestion lists.ßdummyTurker
3811ß0.0ßActually, we will present MRR evaluations for expected position of the ground truths.ßdummyTurker
3812ß0.0ßFirst, to reflect the challenge of code length, we will provide a analysis of the precisions or mrr metric over different completing positions.ßdummyTurker
3813ß0.0ßSecond, to judge whether the NL descriptions help the task and how much contribution the additional text information makes, we have to analyze the attention weights to see whether certain NL tokens performing a leading role in decoding time steps.ßdummyTurker
3814ß0.0ßOtherwise, we will complete an error analysis, e.g., the error rates of keywords and identifiers, which can reflect where the incompetence of current models lies.ßdummyTurker
3815ß0.0ßMoreover, we shall present a case study such as analyzing the completions over one whole code snippet and report the statistics over the precision or mrr on different completing positions.ßdummyTurker
3816ß0.0ßwhich will help lot in constructing the completion pictures for readers.ßdummyTurker
3817ß0.0ßThank you for your suggestions.ßdummyTurker
3818ß0.0ßWe are sorry for not including these in our current version.ßdummyTurker
3819ß0.0ßThe naturalness of code such as repetitiveness and locality enables the usefulness of language model, this is why the baseline LSTM based language model gets a comparable high precision (78%).ßdummyTurker
3820ß0.0ßOwing to the gate mechanism of LSTM, any decoder using LSTM will take full advantage of the merits of language model.ßdummyTurker
3821ß0.0ßBut from the significant improvement (around 8% improvements on both datasets) in precision metric, we can observe that the information contained in NL descriptions do help a lot in code completion tasks.ßdummyTurker
3822ß0.0ßTo measure how the duplication affects the performance of our model, we intend to compare the precisions over the statistics of duplication rates of the code snippet set sharing a common NL description.ßdummyTurker
3823ß0.0ßWe obtain the duplication rate by calculating the average BLEUs over a project as well as the test dataset for the Java data.ßdummyTurker
3824ß0.0ßOur model achieves comparable  accuracy over the much more diverse Java data, which shows the learning capability of our model.ßdummyTurker
3825ß0.0ßDue to the limited length of paper, we didn’t provide the statistics of data.ßdummyTurker
3826ß0.0ßActually, we have built such a table with the vocabulary sizes, document lengths, etc.ßdummyTurker
3827ß0.0ßWe do believe it will help understanding the performance as well as the data a lot.ßdummyTurker
3828ß0.0ßWe will explain more preprocessing details as well as supplement of the data statistics.ßdummyTurker
3829ß0.0ßThe highlight of our work is the high encoding efficiency of NL descriptions which enables taking full advantage of NL descriptions to improve code completions.ßdummyTurker
3830ß0.0ßThe comparisons to the state-of-the-art models in [1] will be in supplement materials.ßdummyTurker
3831ß0.0ßFor comparisons, we do achieve comparable results with the ones showed in [2], while we only take advantage of part information used in [2].ßdummyTurker
3832ß0.0ßThe NL descriptions  are the docstring  we extracted from the corresponding method in the Java files.ßdummyTurker
3833ß0.0ßThey exploit the whole contexts in the same project, including docstring, classes, methods, project structures, etc., which are very important in guiding the completion.ßdummyTurker
3834ß0.0ßAs for code completion of making one word suggestions every time, it is not so useful to compute BLEU and to observe the actual execution traces.ßdummyTurker
3835ß0.0ßWe can supplement the respective error rates of keywords and identifiers, as there will probably be a syntactic error when encountering a wrong prediction of keywords or identifiers.ßdummyTurker
3836ß0.0ßAs for the evaluation metric, we will also report the MRR values which have much better exhibition of the model capability, following the practice in [1].ßdummyTurker
3837ß0.0ßThe comparisons are not presented in our current version.ßdummyTurker
3838ß0.0ßWe feel sorry about the mis-claims as well as confusions our paper makes.ßdummyTurker
3839ß0.0ß[1] Vincent J. Hellendoorn and Premkumar Devanbu.ßdummyTurker
3840ß0.0ßAre deep neural networks the best choice for modeling source code?ßdummyTurker
3841ß0.0ßIn Proceedings of ESEC/FSE 2017.ßdummyTurker
3842ß0.0ßThank you for your review.ßdummyTurker
3843ß0.0ßBefore I give the specific response, please allow me to show the apology for you.ßdummyTurker
3844ß0.0ßBecause of a fault from the doctoral student who was resposible for submitting paper, this paper is not the final version, this is just the middle version which we have reviewed for many times.ßdummyTurker
3845ß0.0ßAt the same time, when we saw the score you have given preliminarily, we somewhat felt unexpected and regretful.ßdummyTurker
3846ß0.0ßIt may be because it's an incorrect version.ßdummyTurker
3847ß0.0ßFor this paper, we have made a lot of efforts, and we have also spent a lot of time and great preparation.ßdummyTurker
3848ß0.0ßBased on the theme of machine reading comprehension, we have organized several academic exchanges in the latter half of 2017.In these meetings, we have invited other schools, teachers and researchers who are related to the research field.ßdummyTurker
3849ß0.0ßIn allusion to the model of our paper, we have made several group reports and exchanges, and we have won the first price in NLP Challenge on Multiple-choice Machine Reading Comprehension, with our model algorithm of this paper.ßdummyTurker
3850ß0.0ßWe are always confident, and at the same time, we are looking forward to having the greater opportunity to introduce and share our work in the ACL conference.ßdummyTurker
3851ß0.0ß"""the paper writing level is poor"", in allusion to the problem of several sentences and grammar which you have put forward, we will polish the sentences and the language at once."ßdummyTurker
3852ß0.0ßAt the same time, we will contact with two Doctoral overseas students from another group to help us polish and improve these sentences at the language level at once.ßdummyTurker
3853ß0.0ßIn the section of the result of the experiment, in addition to the intuitive comparison of the result, we have conducted the detailed ablation studies and analyze their results to disentangle the effect of our model and other aspects of the model, and to demonstrate the specific impact of the adversarial training.ßdummyTurker
3854ß0.0ßBecause if we didn't do so, the readers could not understand how or why the model works intuitively.ßdummyTurker
3855ß0.0ßIn particular, we would also release our latest source code when we upload the camera-ready version of the paper.ßdummyTurker
3856ß0.0ßAt the same time, we also hoped that we could have an oral presentation so that we could elaborate our model in detail and exchange our idea and share our idea with others.ßdummyTurker
3857ß0.0ßThe paper which we have uploaded was because of our fault.ßdummyTurker
3858ß0.0ßIt was just a middle version of our paper, and it was not the final version which we wanted to submit.ßdummyTurker
3859ß0.0ßWe have described these contents in the subsection of the experimental section in our final paper.ßdummyTurker
3860ß0.0ßWe would release our latest source code url when we upload the camera-ready version of the paper.ßdummyTurker
3861ß0.0ßThank you for your review.ßdummyTurker
3862ß0.0ßBefore I give the specific response, please allow me to show the apology for you.ßdummyTurker
3863ß0.0ßBecause of a fault from the doctoral student who was resposible for submitting paper, this paper is not the final version, this is just the middle version which we have reviewed for many times.ßdummyTurker
3864ß0.0ßAt the same time, when we saw the score you have given preliminarily, we somewhat felt unexpected and regretful.ßdummyTurker
3865ß0.0ßIt may be because it's an incorrect version.ßdummyTurker
3866ß0.0ßFor this paper, we have made a lot of efforts, and we have also spent a lot of time and great preparation.ßdummyTurker
3867ß0.0ßBased on the theme of machine reading comprehension, we have organized several academic exchanges in the latter half of 2017.ßdummyTurker
3868ß0.0ßIn these meetings, we have invited other schools, teachers and researchers who are related to the research field.ßdummyTurker
3869ß0.0ßIn allusion to the model of our paper, we have made several group reports and exchanges, and we have won the first price in NLP Challenge on Multiple-choice Machine Reading Comprehension, with our model algorithm of this paper.ßdummyTurker
3870ß0.0ßWe are always confident, and at the same time, we are looking forward to having the greater opportunity to introduce and share our work in the ACL conference.ßdummyTurker
3871ß0.0ß"""the deep learning architectures of the discriminator and generator are missing"", in allusion to the inquiry which you have come up with, thank you very much for your suggestion."ßdummyTurker
3872ß0.0ßIn fact, before we submitted this paper, we have held several group meetings to review this paper specifically.ßdummyTurker
3873ß0.0ßDuring these meetings, there are two teachers who also have pointed out that the description of this place was not sufficient and clear.ßdummyTurker
3874ß0.0ßTo this problem, we have already amended and improved.ßdummyTurker
3875ß0.0ßBut for the version of this paper was not the final correct version which we wanted to submit, this problem, as you have seen, was still existent.ßdummyTurker
3876ß0.0ßAt the same time, about the encoder and how it relates to the discriminator and generator also have been described clearly in our final paper.ßdummyTurker
3877ß0.0ßIn the section of the result of the experiment, in addition to the intuitive comparison of the result, we have conducted the detailed ablation studies and analyze their results to disentangle the effect of our model and other aspects of the model, and to demonstrate the specific impact of the adversarial training.ßdummyTurker
3878ß0.0ßBecause if we didn't do so, the readers could not understand how or why the model works intuitively.ßdummyTurker
3879ß0.0ßIn particular, we would also release our latest source code when we upload the camera-ready version of the paper.ßdummyTurker
3880ß0.0ßAt the same time, we also hoped that we could have an oral presentation so that we could elaborate our model in detail and exchange our idea and share our idea with others.ßdummyTurker
3881ß0.0ßThe paper which we have uploaded was because of our fault.ßdummyTurker
3882ß0.0ßIt was just a middle version of our paper, and it was not the final version which we wanted to submit.ßdummyTurker
3883ß0.0ßWe have described these contents in the subsection of the experimental section in our final paper.ßdummyTurker
3884ß0.0ß"""These derivations are considerate to the less-familiar reader on page 4"", we will further describe these derivations clearly on page 4."ßdummyTurker
3885ß0.0ß"""a number of minor typos, often in the form of verb tense agreement"", in allusion to the problem of several sentences and grammar which you have put forward, we will polish the sentences and the language at once."ßdummyTurker
3886ß0.0ßAt the same time, we will contact with two Doctoral overseas students from another group to help us polish and improve these sentences at the language level at once.ßdummyTurker
3887ß0.0ßTo the best of our knowledge, there is no literature about CNNs and RNNs for learning subword-level embeddings.ßdummyTurker
3888ß0.0ß"On top of it we introduce ""hybrid training"" scheme which indeed can be called “another augmentation to the Skip-gram objective”."ßdummyTurker
3889ß0.0ßFirst of all we show that pure subword-level models dramatically outperform conventional embeddings on morphological tasks.ßdummyTurker
3890ß0.0ßThis comes at a cost of low performance in semantic tasks which we are trying to solve with “hybrid training”.ßdummyTurker
3891ß0.0ß"Indeed this ""hybrid training"" is not always the best-performing model, but we believe that the initial findings might be valuable enough even by themselves."ßdummyTurker
3892ß0.0ßWe were  observing extremely similar  results in text classification task and did not include all the results in the paper.ßdummyTurker
3893ß0.0ßWe will use the additional page to add these details to the camera-ready version.ßdummyTurker
3894ß0.0ßThank you for pointing this out!ßdummyTurker
3895ß0.0ßWe will try to extend the analysis in the final version.ßdummyTurker
3896ß0.0ßReply to weakness argumentßdummyTurker
3897ß0.0ßThank you for pointing out the typos, we have corrected all of them and will do more proofreading for the camera-ready version.ßdummyTurker
3898ß0.0ßTo ensure some degree of fairness we compare subword-level models with their word-levels counterparts using same vocabulary (inferred from the corpus used for training of embeddings).ßdummyTurker
3899ß0.0ßAdditionally for subword-level models we generate embeddings for all the words in the training sets; these experiments are marked with OOV.ßdummyTurker
3900ß0.0ßCONCAT stands for concatenation of FASTTEXT, CNN, and RNN embeddings.ßdummyTurker
3901ß0.0ßMODEL_word+subword stands for concatenation of word and subword embeddings.ßdummyTurker
3902ß0.0ßWe will make this denominations more clearly defined in the paper.ßdummyTurker
3903ß0.0ßYes, we are indeed observing extremely similar  results in text classification task.ßdummyTurker
3904ß0.0ßWe will use the additional page to add these details to the paper.ßdummyTurker
3905ß0.0ßWe thank you for your constructive comments.ßdummyTurker
3906ß0.0ßYour comments are valuable for improving our paper and research.ßdummyTurker
3907ß0.0ßThe clarity of the paper should be improved.ßdummyTurker
3908ß0.0ßThe problem definition should be polished.ßdummyTurker
3909ß0.0ßThank you for the valuable advice.ßdummyTurker
3910ß0.0ßWe will improve the problem definition and our representation to make our paper more readable and understandable.ßdummyTurker
3911ß0.0ßExcept for the classification performance, the perplexity is another usually used evaluation metric.ßdummyTurker
3912ß0.0ßWe will append the experimental results on the perplexity.ßdummyTurker
3913ß0.0ßWe will present the results on two datasets with 50 topics.ßdummyTurker
3914ß0.0ßIn the 079-080 sentence, the authors mentioned that previous methods have the same trouble in making extensions as traditional topic models, what's the same trouble?ßdummyTurker
3915ß0.0ßHow did the proposed method overcome this trouble?ßdummyTurker
3916ß0.0ßAs mentioned in the sentence 045-046, these models have difficulty to rapidly explore variations for the approximate inference methods.ßdummyTurker
3917ß0.0ßTo overcome the issue, we propose NSTC via rebuilding STC to a neural network, and thus replace the complex inference process with the Backpropagation, which makes our model easy to explore extensions.ßdummyTurker
3918ß0.0ßTo illustrate the flexibility offered by the neural network based framework, we further present three extensions based on NSTC without re-deduced inference algorithms.ßdummyTurker
3919ß0.0ßWhat are the words generated by LDA for Table 2?ßdummyTurker
3920ß0.0ßDue to the space limit, we only present the topic words generated by our model.ßdummyTurker
3921ß0.0ßWe will add the results of LDA.ßdummyTurker
3922ß0.0ßWe thank you for your constructive comments.ßdummyTurker
3923ß0.0ßYour comments are valuable for improving our research.ßdummyTurker
3924ß0.0ßThe paper NVSTM is our other work.ßdummyTurker
3925ß0.0ßThe only relation between NVSTM and this paper is that, they both aim to deal with the inference issue of topic models.ßdummyTurker
3926ß0.0ßIn NVSTM, we focus on rebuilding STC to a deep generative model.ßdummyTurker
3927ß0.0ßHowever, in this paper, we devote to rebuilding STC to a neural network without using variational inference.ßdummyTurker
3928ß0.0ßMoreover, in NVSTM, we tackle the inference issue with the neural variational inference rather than the Backpropogation used in this paper.ßdummyTurker
3929ß0.0ßIn conclusion, they are two totally different papers to tackle the same issue.ßdummyTurker
3930ß0.0ßThe ACL 2016 paper focuses on combing the local information of word embeddings and the global information of LDA, and thus proposes a model TopicVec yielded by variational inference.ßdummyTurker
3931ß0.0ßHowever, our paper focuses more on rebuilding a generative model to a neural network and simplifying its inference process.ßdummyTurker
3932ß0.0ßAlthough they both combine the word embeddings with topic models, they have different concerns.ßdummyTurker
3933ß0.0ßWe will make more analysis, add the reference of the paper and compare our method with it.ßdummyTurker
3934ß0.0ßOur model and other baselines are implemented by different languages.ßdummyTurker
3935ß0.0ßWe think it is not objective to directly compare them in time efficiency.ßdummyTurker
3936ß0.0ßDifferent from baselines, our model can be easily accelerated owing to its parallel implementation via TensorFlow.ßdummyTurker
3937ß0.0ßWe will make the time complexity analysis.ßdummyTurker
3938ß0.0ßWe will make more analysis on the results of different models, such as the impact of topic numbers on the classification accuracy.ßdummyTurker
3939ß0.0ßThe super-Gaussian p(s_{d, n}|\theta_d) is a composite distribution p(s_{d, n}|\theta_d, \gamma)p(s_{d, n}|\rho), where the first one is an isotropic Gaussian distribution and the second one is a Laplace distribution [1].ßdummyTurker
3940ß0.0ßDue to the data sparsity caused by the increasing number of topics, DocNADE may generate the document topic distribution with many indistinct non-zeros , leading to a confused representation.ßdummyTurker
3941ß0.0ßNotice that LDA has the same performance on the web snippet dataset.ßdummyTurker
3942ß0.0ßWe will make more demonstration and analysis in our experiments section.ßdummyTurker
3943ß0.0ß[1] A. Hyvarinen.ßdummyTurker
3944ß0.0ßSparse code shrinkage: Denoising of nongaussian data by maximum likelihood estimation.ßdummyTurker
3945ß0.0ßNeural Computation, (11):1739–1768, 1999.ßdummyTurker
3946ß0.0ßWe thank you for your thoughtful comment.ßdummyTurker
3947ß0.0ßWe must use under 1000 words in addressing your questions; some answers were difficult to specify, we entreat your pardon.ßdummyTurker
3948ß0.0ßOur understanding is that your mention mainly discusses applying our model to the LSTM-CRF neural architecture mentioned in the future plan.ßdummyTurker
3949ß0.0ßThe reference to the LSTM-CRF architecture is absent in the current manuscript, and we think this argument is from our mistake.ßdummyTurker
3950ß0.0ßWe sincerely apologize for our unintended mistake.ßdummyTurker
3951ß0.0ßWe intended to cite the LSTM(Long Short-Term Memory)-CRF that is G.Lample et al.ßdummyTurker
3952ß0.0ßproposed (NAACL-HLT, 2016).ßdummyTurker
3953ß0.0ßThe model is considered as one of state-of-the-art NER models using deep-learning.ßdummyTurker
3954ß0.0ßWe will include the citation in the paper.ßdummyTurker
3955ß0.0ßSummarizing our study, we were exploring future study candidates to improve our model, and have found potential to apply our CRF model on the LSTM-CRF.ßdummyTurker
3956ß0.0ßThis is why we mentioned the LSTM-CRF as our future study.ßdummyTurker
3957ß0.0ßWe thank you for your thoughtful comments.ßdummyTurker
3958ß0.0ßWe think your comments are helpful for augmenting our paper.ßdummyTurker
3959ß0.0ßWe must use under 1000 words in addressing your questions; some answers were difficult to specify, thus we entreat your pardon.ßdummyTurker
3960ß0.0ßBefore responding to each argument, we wanted to correctly deliver important aspects of our study as below:ßdummyTurker
3961ß0.0ß- Focusing on clinical NER study for multiple years, we observed many named entities are separated by non-entities within a clinical sentence, and the first-order model is innately restricted to convey the dependencies between the separated named entities.ßdummyTurker
3962ß0.0ßThis also has been addressed for general NER in (Ratinov and Roth,ACL,2009).ßdummyTurker
3963ß0.0ß- Our approach was to solve the problem by simple modification without complex modification as well as complex notations.ßdummyTurker
3964ß0.0ßIt is to fragment non-entity tags inside the model and to utilize them as medium delivering long distant information.ßdummyTurker
3965ß0.0ß- It might be seen as proposing new tagging standard at first, but we wanted to emphasize our study is on the development of a new CRF model extension in the manuscript.ßdummyTurker
3966ß0.0ßBeyond manipulating tags, this process is required to modify feature functions, which can be done only inside the model.ßdummyTurker
3967ß0.0ß- Although we tested several versions of the precursor-induced CRF, we could not have achieved large performance improvement.ßdummyTurker
3968ß0.0ßAs a consequence, we decided to report our current best performance.ßdummyTurker
3969ß0.0ßThus, our purpose of submitting this study to the 'short paper' is to share our idea in NLP community utilizing non-entity tags as information transmission medium by induction even it shows slight performance improvement because this adheres to the purpose of the short paper.ßdummyTurker
3970ß0.0ßWe agree with your idea that examples or case studies should be presented in this paper.ßdummyTurker
3971ß0.0ßAlso, we had better add keywords related to 'clinical NER' in the title to prevent reader’s confusion.ßdummyTurker
3972ß0.0ßThe initiation of this study is to find ways improve clinical NER on real-world clinical texts.ßdummyTurker
3973ß0.0ßA sentence in a clinical text contains many named entities, unlike general domain texts.ßdummyTurker
3974ß0.0ßDescribing a patient's history of present illness or hospital courses, clinical named entities tend to appear as a semantic pair to describe the medical episode in a sentence.ßdummyTurker
3975ß0.0ßThus, we have expected that enhancing the long-range dependencies between the named entities in the model will improve the clinical NER performance.ßdummyTurker
3976ß0.0ßMoreover, we think we cannot easily assure clinical named entity has not strong dependencies in a text.ßdummyTurker
3977ß0.0ßThe pair of clinical named entities is considered to tend to make a semantic relationship, such as 'treatment improves medical problem' and 'test reveals medical problem' (Ozlem Uzuner, et al., JAMIA 2011 Vol.18(5):552-6)ßdummyTurker
3978ß0.0ßThe added tag information is intended to encode long-range dependencies between separated named entities as you noted.ßdummyTurker
3979ß0.0ßThe number of newly induced tags is equal to the number of named entity labels and the induced tags are only for non-entity tags.ßdummyTurker
3980ß0.0ßFor instance, if we have tag set as {A, B, O}, then our model fragments non-entity tag O to ‘A[O]+', ‘B[O]+', and ‘O[O]+'.ßdummyTurker
3981ß0.0ßFragmenting a single non-entity tag into multiple tags may cause data sparseness problems especially for observation features in CRF.ßdummyTurker
3982ß0.0ßTo prevent the data sparseness problem derived by our precursor-induced CRF, observation factor is customized to treat the fine-grained non-entity tags as same value (line number 236-244 in the third page in the manuscript).ßdummyTurker
3983ß0.0ßThis customization is expected to relax the problem what you concerned.ßdummyTurker
3984ß0.0ßWe agree with your suggestion that adding evaluation on CONLL-03 will improve the quality of the study.ßdummyTurker
3985ß0.0ßBecause this study is focused on clinical NER, we have reported results on only clinical and biomedical NER corpus.ßdummyTurker
3986ß0.0ßWe will evaluate our model on the CONLL-03 dataset and will discuss more the evaluation.ßdummyTurker
3987ß0.0ßWe thank you for your thoughtful comments.ßdummyTurker
3988ß0.0ßWe must use under 1000 words in addressing your questions; some answers were difficult to specify, thus we entreat your pardon.ßdummyTurker
3989ß0.0ßWe very appreciate your comments.ßdummyTurker
3990ß0.0ßFrankly speaking, we had not considered both of the in-depth data analysis and statistics what you mentioned.ßdummyTurker
3991ß0.0ßFollowing your comments, we will attempt more analysis on the results as much as possible and will include more contents for the analysis.ßdummyTurker
3992ß0.0ß**are long-distances implying several sentences?ßdummyTurker
3993ß0.0ß:The dependency is restricted within a single instance.ßdummyTurker
3994ß0.0ßA single sentence was set as an instance here, however, it can be varied according to how we set the span of an instance.ßdummyTurker
3995ß0.0ßWhat we know is that the modality and polarity of a named entity come from modifiers.ßdummyTurker
3996ß0.0ßThus, we assume the attributes may be strongly related to the distributional hypothesis and it can be used as any form of a feature.ßdummyTurker
3997ß0.0ßThank you for your suggestion.ßdummyTurker
3998ß0.0ßAs you mentioned, we agree that relationships between named entities are not the same type.ßdummyTurker
3999ß0.0ßTyping each kind of relations is possible, however, this typing method may cause exponential increase in the time/space complexity.ßdummyTurker
4000ß0.0ßThe information is actually used in the model, as the form of transition factor (through line 177-179 in the second page and line 219-220 in the third page).ßdummyTurker
4001ß0.0ßWe will invite a native editor to help us correct these errors.ßdummyTurker
4002ß0.0ß"Besides, we will improve the narrative about the use of ""Temporal Signal"" to make it easier to understand."ßdummyTurker
4003ß0.0ßDue to space limitations, we did not give a complete state in the paper.ßdummyTurker
4004ß0.0ßThe nature of dependency path may result in the loss of useful information, due to that many useful information are not in the dependency path, such as temporal signal words.ßdummyTurker
4005ß0.0ßThe statement “Due to the temporal relation strongly relies on shortest dependency paths (SDPs) between two entities in a dependency tree” derives from the previous work (Xu et al., 2015b; Xu et al., 2016; Cheng and Miyao, 2017; Meng et al., 2017).ßdummyTurker
4006ß0.0ßThe input of E-E is two event sentences, which carry more semantic information, while the time expressions and DCTs are often word-level or phrase-level, with less information.ßdummyTurker
4007ß0.0ßTherefore, we think the deep architecture is not so effective to predict the relation between the units with less information.ßdummyTurker
4008ß0.0ßThis question will be answered in conjunction with the following question 1.ßdummyTurker
4009ß0.0ßThe input of E-E is two event sentences, which carry more semantic information, while the time expressions and DCTs are often word-level or phrase-level, with less information.ßdummyTurker
4010ß0.0ßTherefore, we think the deep architecture is not so effective to predict the relation between the units with less information.ßdummyTurker
4011ß0.0ßTemporal signals are concatenated to the outputs of BiLSTM layers respectively.ßdummyTurker
4012ß0.0ßWhy we try to use additional temporal signal derives from the previous work (Derczynski and Gaizauskas, 2013; Mirza and Tonelli, 2016).ßdummyTurker
4013ß0.0ßTable 2 justifies the effectiveness of our temporal signals.ßdummyTurker
4014ß0.0ßIn regard of the depth of Bi-LSTM layers, we validated the stacking layer numbers from the set {1, 2, · · ·, 5, 6}.ßdummyTurker
4015ß0.0ßThe 4-layer LSTMs yield the highest performance.ßdummyTurker
4016ß0.0ßIndeed, it is still difficult for us to explain exactly what each level of Bi-LSTM is learning.ßdummyTurker
4017ß0.0ßThere is only one general summary at presentßdummyTurker
4018ß0.0ßTimeBank-Dense corpus was created to address the sparsity issue in existing TimeML corpora.ßdummyTurker
4019ß0.0ßA specialized annotation solution was introduced to solve the problem of sparse annotation of temporal relations.ßdummyTurker
4020ß0.0ßBesides, the VAGUE relation introduced at the first TempEval task (Verhagen et al., 2007) was also adopted in TimeBank-Dense to cope with ambiguous temporal relations, or to indicate pairs for which no clear temporal relation exists.ßdummyTurker
4021ß0.0ßThen, there are 6,088 E-E links actually.ßdummyTurker
4022ß0.0ßWe adopt grouped 5-fold cross-validation setting as previos work to calculate the micro average.ßdummyTurker
4023ß0.0ßThe reason may be that the percentage (6.6%) of INCLUDES relation is too small.ßdummyTurker
4024ß0.0ßCheng and Miyao (2017) make a “common root” assumption to extendßdummyTurker
4025ß0.0ßDP representations of cross-sentence links.ßdummyTurker
4026ß0.0ßTherefore, a cross-sentence dependency path can be represented as two shortest dependency path branches from the ends to the “common root” Because we regard his model as our baseline, this assumption is also introduced to our method.ßdummyTurker
4027ß0.0ßFor retrieval task, we follow the evaluation method used by Iyer et al.(2016).ßdummyTurker
4028ß0.0ßAlthough simple,  it is effective to demonstrate if a model outperforms another one.ßdummyTurker
4029ß0.0ßFor summarization, BVAE achieves performace comparable to CODE-NN.ßdummyTurker
4030ß0.0ß"The reviewer argues that CODE-NN is ""the simplest seq2seq model"", however, among the existing models, it achieves the-state-of-the-art performace on the same dataset."ßdummyTurker
4031ß0.0ßOur main contribution is to propose BVAE which provides a different framework from sequence-to-sequence model for bimodal learning of code and language and the instance models of BVAE presented in the paper demonstrate its potential on retrieval and summarization.ßdummyTurker
4032ß0.0ßThere is no significant improvement for summarization currently, because the instance models of BVAE (MLP) and the distributions (Gaussian) for latent variables are extremely simple.ßdummyTurker
4033ß0.0ßAs discussed in the last paragraph of the paper, more powerful structure for latent variables and better encoders and decoders can be designed in the future to improve the tasks.ßdummyTurker
4034ß0.0ßIn the practice of machine learning, evaluation metrics are rarely optimized directly.ßdummyTurker
4035ß0.0ßTaking classification as an example, cross entropy is usually used as the optimization target.ßdummyTurker
4036ß0.0ßIt is hard to optimize precison, recall or f-measure directly, because no gradient can be back-propagated to update parameters.ßdummyTurker
4037ß0.0ßSimilarly, the three metrics used to measure the quality of retrieval and summarization in this paper (MRR, METEOR and BLEU-4) cannot be optimized directly using gradient descent algorithms.ßdummyTurker
4038ß0.0ßBVAE and CODE-NN address the same tasks, but we do not agree BVAE is an incremental work from CODE-NN.ßdummyTurker
4039ß0.0ßFrom the perspective of architectural design, BVAE differs much from CODE-NN.ßdummyTurker
4040ß0.0ßCODE-NN is a sequence-to-sequence model, while BVAE is a framework that can build connections between the latent variables of code and language.ßdummyTurker
4041ß0.0ßThis feature of BVAE has many advantages, for example, it can generate vector representations for code and language, which is extremely suitable for retrieval on large scale datasets.ßdummyTurker
4042ß0.0ß"The reviewer argues that ""the authors should consider comparing BVAE with other state-of-the-art retrieval models (including neural-based)""."ßdummyTurker
4043ß0.0ßWe only compare BVAE with CODE-NN (Iyer et al., 2016) because CODE-NN is a neural-based model and prior to our study, it achieves the best performance for code retrieval on the same dataset.ßdummyTurker
4044ß0.0ßFirst, the main contribution of this work is to propose BVAE which provides a different framework from sequence-to-sequence model for bimodal learning of code and natural language and the instance models of BVAE presented in the paper demonstrate its potential on code retrieval and summarization.ßdummyTurker
4045ß0.0ßThere is no significant improvement for summarization currently, because the instance models of BVAE (MLP) and the distributions (Gaussian) for latent variables are extremely simple.ßdummyTurker
4046ß0.0ßAs discussed in the last paragraph of the paper, more powerful structure for latent variables and better encoders and decoders can be designed in the future to improve the tasks.ßdummyTurker
4047ß0.0ß"Second, it may not be a critical issue that ""the system tends to generate summary with 'how to' phrase"", since a summary with the phrase is not necessarily a bad one."ßdummyTurker
4048ß0.0ß"Further, it heavily depends on the training set that if the system tends to generate ""how to""."ßdummyTurker
4049ß0.0ß"We find that 23.24% examples in the C# training set have ""how to"", and for SQL, this number is 14.17%."ßdummyTurker
4050ß0.0ßAs a result, BVAE and CODE-NN learn this pattern from the training data.ßdummyTurker
4051ß0.0ßWe sincerely thank for all of your constructive and insightful comments.ßdummyTurker
4052ß0.0ßQ1: Comparison with DrQA algorithm (weakness argument 1)ßdummyTurker
4053ß0.0ßIn fact, Wang et al.ßdummyTurker
4054ß0.0ß[1] have shown that R^3 outperforms DrQA on various datasets including SQUAD.ßdummyTurker
4055ß0.0ßIn this paper, we only select R^3 as our baseline method and evaluate our proposed model on five datasets due to the limit of space.ßdummyTurker
4056ß0.0ßWe have carried out the experiments of DrQA and will add its comparison in our revised version.ßdummyTurker
4057ß0.0ß[1] Wang S, Yu M, Guo X, Wang Z, Klinger T, Zhang W, Chang S, Tesauro G, Zhou B, Jiang J. R3: Reinforced Ranker-Reader for Open-Domain Question Answering.ßdummyTurker
4058ß0.0ßIn Proceedings of AAAI 2018.ßdummyTurker
4059ß0.0ßQ2: The issue of paper writing.ßdummyTurker
4060ß0.0ßThanks for advice for the writing of our paper.ßdummyTurker
4061ß0.0ßWe will give explicit definitions to “distantly supervised” to make it more comprehensive.ßdummyTurker
4062ß0.0ßAnd we will also perform careful proofreading in our revised paper.ßdummyTurker
4063ß0.0ßQ3: Performance on those questions with small evidence (weakness argument 3)ßdummyTurker
4064ß0.0ßOur paragraph selector can measure the probability of each paragraph containing the answer and help us filter out those noisy paragraphs to exploit the information from all paragraphs.ßdummyTurker
4065ß0.0ßTherefore, our model would perform better compared to other baselines for those questions with small evidence.ßdummyTurker
4066ß0.0ßIn fact, the validation set of Quasar-T only has about 10% paragraphs that contain the answer, i.e., most questions in Quasar-T have small evidence.ßdummyTurker
4067ß0.0ßWe will discuss more about it in our revised paper.ßdummyTurker
4068ß0.0ßQ4: The setting of “distantly supervised” (weakness argument 2 & 4)ßdummyTurker
4069ß0.0ßIn our paper, the answers to questions on all datasets are labeled manually, which is the same as SQUAD.ßdummyTurker
4070ß0.0ßAnd “distantly supervised” indicates that we regard all retrieved paragraphs that contain the correct answer as positive instances for training, which follows the setting of distantly supervised relation extraction.ßdummyTurker
4071ß0.0ßWe will also add the evaluation on the SQUAD dataset in our revised paper.ßdummyTurker
4072ß0.0ßQ5: Comparison with Oracle system (additional comments)ßdummyTurker
4073ß0.0ßWe agree that the comparison with an Oracle system that can always extract the correct answer could help us further verify the effectiveness of our proposed model.ßdummyTurker
4074ß0.0ßWe will add the experimental results of an Oracle system in our revised version if space is enough.ßdummyTurker
4075ß0.0ßWe sincerely thank for all of your constructive and insightful comments.ßdummyTurker
4076ß0.0ßQ1: Comparison with the systems from TREC QA track (weakness argument 1 & 2)ßdummyTurker
4077ß0.0ßWe agree that adding a comparison with existing state-of-the-art systems from the TREC QA track such as LCC Chaucher-2 and LSV Alyssa will make our experimental results more convincing.ßdummyTurker
4078ß0.0ßWe will carry out the comparison in our revised version if space is enough.ßdummyTurker
4079ß0.0ßFor comparison, we will also use the AQUAINT2 corpus to maintain the result comparability with the TREC literature.ßdummyTurker
4080ß0.0ßQ2: Using Wikipedia instead of AQUAINT2 corpus (question 1)ßdummyTurker
4081ß0.0ßIn the experiments, we used Wikipedia instead of AQUAINT2 corpus on CuratedTREC dataset following our baseline model R^3 for the convenience and fair of the comparison.ßdummyTurker
4082ß0.0ßQ3: Issue of source code (question 2)ßdummyTurker
4083ß0.0ßWe will release our source codes on our group’s Github.ßdummyTurker
4084ß0.0ßIn fact, we believe that releasing source codes could help the research in this community and all codes of our previous works have been released.ßdummyTurker
4085ß0.0ßQ4: The issue of paper writing (additional comments)ßdummyTurker
4086ß0.0ßThanks for pointing out these typos in the paper.ßdummyTurker
4087ß0.0ßWe will perform careful proofreading and check typos in our revised paper.ßdummyTurker
4088ß0.0ßActually, besides the Bi-RNN model, we also present a hierarchical and supervised attention architecture to learn event detection oriented document embeddings, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events.ßdummyTurker
4089ß0.0ß(comments)ßdummyTurker
4090ß0.0ßWe think that, as compared to DEEB-RNN, DEEB-RNN2 changes the precision-recall balance is because of the following reasons: On one hand, as compared to DEEB-RNN, DEEB-RNN2 uses the gold sentence-level attention signal, indicating that it pays special attention to the sentences containing events with event triggers.ßdummyTurker
4091ß0.0ßIn this way, the Bi-RNN model for learning document embeddings will filter out the sentences containing events but without explicit event triggers.ßdummyTurker
4092ß0.0ßThat means the events detected by DEEB-RNN2 are basically the ones with explicit event triggers.ßdummyTurker
4093ß0.0ßTherefore, as compared to DEEB-RNN, the precision of DEEB-RNN2 is improved.ßdummyTurker
4094ß0.0ßOn the other hand, the above strategy may result in less learning of words, which are event triggers but do not appear in the training dataset.ßdummyTurker
4095ß0.0ßTherefore, those sentences with such event triggers cannot be detected.ßdummyTurker
4096ß0.0ßThe recall of DEEB-RNN2 is thus lowered, as compared to DEEB-RNN.ßdummyTurker
4097ß0.0ßThank you for your wonderful suggestion.ßdummyTurker
4098ß0.0ßSince this is a short paper, we focus only on the subtask of event extraction and conduct experiments on the commonly used benchmark dataset.ßdummyTurker
4099ß0.0ßBut, we agree with you that the proposed architecture may also be applicable to other tasks and other datasets.ßdummyTurker
4100ß0.0ßWe have been trying it on sentiment analysis.ßdummyTurker
4101ß0.0ß(Comments)ßdummyTurker
4102ß0.0ßThank you for your careful review.ßdummyTurker
4103ß0.0ßThis is due to the official Latex template.ßdummyTurker
4104ß0.0ßLater on, we will try to solve the hyphenation problem.ßdummyTurker
4105ß0.0ßThanks for your review.ßdummyTurker
4106ß0.0ßYes, we will surely consider to furthermore improve our architecture in the future.ßdummyTurker
4107ß0.0ßThough, please let us clarify our contribution about the novelty here.ßdummyTurker
4108ß0.0ßThis work presents a unified representation learning framework to sub-word level information for NLP tasks which is the first attempt in the text understanding tasks including text entailment and reading comprehension.ßdummyTurker
4109ß0.0ßThe proposed method takes variable-length subwords segmented by unsupervised segmentation measures, without relying on any predefined linguistic resource.ßdummyTurker
4110ß0.0ßOur work discloses that the deep internals of sub-word level embeddings are crucial, helping downstream models to absorb different signals.ßdummyTurker
4111ß0.0ßWe are very grateful to the reviewer for the insightful comments and suggestions.ßdummyTurker
4112ß0.0ßWe will revise the paper according to these suggestions.ßdummyTurker
4113ß0.0ßWe focus on binary extractions for now and will investigate how to extend our work for non-binary extractions in our future work.ßdummyTurker
4114ß0.0ßWe will further explore how to generate non-subspans for the input sequences in our future work.ßdummyTurker
4115ß0.0ßWe are very grateful to the reviewer for the insightful comments and suggestions.ßdummyTurker
4116ß0.0ßWe will revise the paper according to these suggestions.ßdummyTurker
4117ß0.0ßWe will add a related work section to introduce the related work in the revised version.ßdummyTurker
4118ß0.0ßDue to the lack of data, we use an existing OIE system to generate the training data for our neural open IE system.ßdummyTurker
4119ß0.0ßWe will try to figure out other approaches to obtain the training data for the open IE task in our future work.ßdummyTurker
4120ß0.0ßWe will add the citation for the statement in the revised version.ßdummyTurker
4121ß0.0ßWe did not force the arg1 start token to appear first and arg2 end last.ßdummyTurker
4122ß0.0ßInstead, the whole pipeline was automatically trained, and the results were generated using the seq2seq decoding with the copy mechanism.ßdummyTurker
4123ß0.0ßWe will release the training data as well as the scripts for the seq2seq training and decoding so that others can reproduce our results.ßdummyTurker
4124ß0.0ßWe will provide the comparison for long sentences in the revised version.ßdummyTurker
4125ß0.0ßWe are very grateful to the reviewer for the insightful comments and suggestions.ßdummyTurker
4126ß0.0ßWe will revise the paper according to these suggestions.ßdummyTurker
4127ß0.0ßWe agree with the reviewer that non-binary extraction is not a trivial task.ßdummyTurker
4128ß0.0ßWe will further explore how to leverage the current scheme to more complex relations in our future work.ßdummyTurker
4129ß0.0ßWe follow the confidence score threshold mentioned in  httpßdummyTurker
4130ß0.0ßFor the experiments, we re-use the scripts in httpsßdummyTurker
4131ß0.0ßWe mean we only use sentences with 40 words or less for the training.ßdummyTurker
4132ß0.0ßWe will make this point clear in the revised version.ßdummyTurker
4133ß0.0ßWe will revise the formulation and add related references about avoiding cascading errors in the revised version.ßdummyTurker
4134ß0.0ßDue to the limitation of manual annotation budget, we collect high-frequency queries from original query logs.ßdummyTurker
4135ß0.0ßBesides, we remove the data with second-level relations 'Cost' and 'Size' because of their small test sample size.ßdummyTurker
4136ß0.0ßAnd then, we get the final test dataset, which is adopted in all experiments in this paper.ßdummyTurker
4137ß0.0ßOur final results show great improvement compared to baselines.ßdummyTurker
4138ß0.0ßWe have calculated the significant results, such as the overall accuracy in Table 4 (Line 457-469), which will be supplemented after double-blind reviewing.ßdummyTurker
4139ß0.0ßThe content of line 493-497 explains the reason why our methods perform better than baselines.ßdummyTurker
4140ß0.0ßAnd the content of line 545-549 analyses the reason why baselines outperform our methods in first-relation 'Decrease'.ßdummyTurker
4141ß0.0ßWe will revise the paper to make the analysis more clear to our readers.ßdummyTurker
4142ß0.0ßAs stated in the introduction and abstract, the major contribution of this paper lies in the proposal of a new research framework for an important research question.ßdummyTurker
4143ß0.0ßAlthough knowledge graph embedding is not a novel technique, it has never been adopted in the mainstream research directions of IR.ßdummyTurker
4144ß0.0ßWe also revised the existing embedding methods to meet the challenges in the new research question.ßdummyTurker
4145ß0.0ßIt shows the potential of bridging the gap between advanced NLP techniques and existing IR research questions.ßdummyTurker
4146ß0.0ßTherefore, we think it is also highly correlated with the NLP community.ßdummyTurker
4147ß0.0ß'Item' represents the name of the products, such as T-shirt and shoes.ßdummyTurker
4148ß0.0ßIf original query 'T-shirt' is rewritten to 'shoes', we will annotate the rewriting intent as 'Replace-Item'.ßdummyTurker
4149ß0.0ßSo, we think that the 'Item - Second level' example is correct when T-Shirt becomes shoes.ßdummyTurker
4150ß0.0ßFor a test sample, there is an original query, its rewritten queries and corresponding rewriting intents are labeled with manual annotation.ßdummyTurker
4151ß0.0ßWe input the original query and the rewritten query into our model and predict the intent of the rewriting behavior (labeled by assessors).ßdummyTurker
4152ß0.0ßWe will release the dataset after double-blind reviewing.ßdummyTurker
4153ß0.0ßWe didn’t have the permission from the company which provides the original data and at the time of submission but now we have.ßdummyTurker
4154ß0.0ßThanks for the suggestion.ßdummyTurker
4155ß0.0ßWe should rephrase the sentence.ßdummyTurker
4156ß0.0ßOur exact meaning is that the processing of shorter queries is more challenging because it contains only a few terms and cannot describe users intent clearly.ßdummyTurker
4157ß0.0ßThanks for the suggestion.ßdummyTurker
4158ß0.0ßAfter double-blind reviewing, we will make corresponding changes in camera-ready.ßdummyTurker
4159ß0.0ßAs far as we know, TransE, TransH and TransR perform outstandingly in knowledge graph embedding, and they are regarded as quite strong baselines by many recent works, e.g.ßdummyTurker
4160ß0.0ßTransD (Ji et al., 2015) and TranSparse (Ji et al., 2016).ßdummyTurker
4161ß0.0ßFor query reformulation intentions predicting, we only focus on the top-ranked relations because the system relies on these relations to generate query suggestions or adjust rankings.ßdummyTurker
4162ß0.0ßIt is not necessary to involve the other lower-ranked intents because the search interface only provides a small number of query suggestions and ranking results.ßdummyTurker
4163ß0.0ßAs stated in the introduction and abstract, the major contribution of this paper lies in the proposal of a new research framework for an important research question.ßdummyTurker
4164ß0.0ßAlthough knowledge graph embedding is not a novel technique, it has never been adopted in the mainstream research directions of IR.ßdummyTurker
4165ß0.0ßWe also revised the existing embedding methods to meet the challenges in the new research question.ßdummyTurker
4166ß0.0ßIt shows the potential of bridging the gap between advanced NLP techniques and existing IR research questions.ßdummyTurker
4167ß0.0ßTherefore, we don’t agree with the reviewer that the novelty is slim.ßdummyTurker
4168ß0.0ßWe will release the data and the code after double-blind reviewing (We didn’t have the permission from the company which provides the original data and at the time of submission but now we have.).ßdummyTurker
4169ß0.0ßThe dataset contains high-frequency queries and their reformulation intentions.ßdummyTurker
4170ß0.0ßThere is no users’ privacy information in this dataset.ßdummyTurker
4171ß0.0ßSupposing VQ is the embedding vector of query Q, norm(VQ) change VQ[i] to VQ[i]/len(VQ) where i is the number from 0 to len(VQ)-1.ßdummyTurker
4172ß0.0ßWe follow the work of TransH (Wang et al., 2014) and corrupt correct triples (qc, r, qn) by replacing queries.ßdummyTurker
4173ß0.0ßThanks for the suggestion.ßdummyTurker
4174ß0.0ßAfter double-blind reviewing, we will add the technical definitions of TransH, TransR and the result of neural classifiers in camera-ready.ßdummyTurker
4175ß0.0ßWe will also have the paper proofread again.ßdummyTurker
4176ß0.0ßWe note in Sec 1 that the Koppel algorithm is widely used, both byßdummyTurker
4177ß0.0ßitself and as part of other high-performing algorithms, and is aßdummyTurker
4178ß0.0ßstandard inference attacker in PAN shared tasks.ßdummyTurker
4179ß0.0ß"This being only a short paper, it's intended to be a ""small, focussed"ßdummyTurker
4180ß0.0ß"contribution"" as per the CfP; the paper is already quite dense, and"ßdummyTurker
4181ß0.0ßspace didn't permit a deeper analysis.ßdummyTurker
4182ß0.0ßTo our knowledge, there are not in fact many deep learning methods forßdummyTurker
4183ß0.0ßauthorship attribution.ßdummyTurker
4184ß0.0ßThose we have noted (e.g.ßdummyTurker
4185ß0.0ßRuder et al) fallßdummyTurker
4186ß0.0ßoutside the similarity-based paradigm.ßdummyTurker
4187ß0.0ßWe note that there areßdummyTurker
4188ß0.0ßcontexts where similarity-based algorithms are better suited (which isßdummyTurker
4189ß0.0ßargued more extensively in Koppel et al, 2011), and to our knowledgeßdummyTurker
4190ß0.0ßthere are not yet deep learning methods for similarity-basedßdummyTurker
4191ß0.0ßapproaches.ßdummyTurker
4192ß0.0ßThe only parts that discuss relevant literature (due to spaceßdummyTurker
4193ß0.0ßconstraints in a short paper) are Sec 1 and Sec 5, totalling about 1ßdummyTurker
4194ß0.0ßpage out of 4; the other three are method and results.ßdummyTurker
4195ß0.0ßWe thereforeßdummyTurker
4196ß0.0ßdo not see how the whole article is similar to a literature review.ßdummyTurker
4197ß0.0ßWe don't believe that the results were obvious before the experimentsßdummyTurker
4198ß0.0ßwere carried out; we see them (as did Reviewer #3) as new informationßdummyTurker
4199ß0.0ßthat can inform other researchers about the effects of ourßdummyTurker
4200ß0.0ßmodifications to the algorithm.ßdummyTurker
4201ß0.0ßThat would be valuable, but we unfortunately couldn't include themßdummyTurker
4202ß0.0ßbecause the space constraints for a short paper didn't permit whatßdummyTurker
4203ß0.0ßwould be a moderate-sized digression from the experimental results; weßdummyTurker
4204ß0.0ß"decided on a paper focussed on experimental results (with ""focussed"ßdummyTurker
4205ß0.0ß"contribution"" being a purpose of a short paper), and moreover the"ßdummyTurker
4206ß0.0ßtheoretical analysis doesn't provide anything surprising (and isßdummyTurker
4207ß0.0ßessentially similar to that found in the approximate nearest neighbourßdummyTurker
4208ß0.0ßliterature).ßdummyTurker
4209ß0.0ßIn the one extra page allowed for published papers weßdummyTurker
4210ß0.0ßcould sketch these, with more detail in supplementary material.ßdummyTurker
4211ß0.0ßInßdummyTurker
4212ß0.0ßshort: the asymptotic complexity is the same as the original methodßdummyTurker
4213ß0.0ß(in terms of comparisons between text / snippet); it is just theßdummyTurker
4214ß0.0ßcoefficient that is different.ßdummyTurker
4215ß0.0ßThe methods are not tuned to the test sets.ßdummyTurker
4216ß0.0ßThe datasets FF100 andßdummyTurker
4217ß0.0ßFF1000 are perhaps in a sense development sets, in that we examineßdummyTurker
4218ß0.0ßchoice of similarity measure / representation / etc on that, andßdummyTurker
4219ß0.0ßrestrict presentation of our results on other datasets (given spaceßdummyTurker
4220ß0.0ßconstraints) to the best options.ßdummyTurker
4221ß0.0ßThe other datasets were only usedßdummyTurker
4222ß0.0ßin the final Table 4.ßdummyTurker
4223ß0.0ßGiven that, we should perhaps have separatedßdummyTurker
4224ß0.0ßout results on FF100/1000 in Table 4b as being in a sense developmentßdummyTurker
4225ß0.0ßset results.ßdummyTurker
4226ß0.0ßThe details are in Table 4, and the essence of the argument is at theßdummyTurker
4227ß0.0ß"end of Sec 4: ""CULL with a relatively small first stage subset can"ßdummyTurker
4228ß0.0ßproduce results generally as good as the original algorithm, but muchßdummyTurker
4229ß0.0ß"faster"", where speed was discussed in Sec 1 as a particular problem"ßdummyTurker
4230ß0.0ßfor the approach.ßdummyTurker
4231ß0.0ßThe additional page for a published version wouldßdummyTurker
4232ß0.0ßallow us to expand on that, and the plot suggested by Reviewer #3ßdummyTurker
4233ß0.0ßcould help deepen it.ßdummyTurker
4234ß0.0ßThe plot is a good idea, and we could include this in a publishedßdummyTurker
4235ß0.0ßversion.ßdummyTurker
4236ß0.0ßIt generally behaves as would be expected, starting at theßdummyTurker
4237ß0.0ßNAIVE (CULL with r=1) accuracy and trending upwards to equal orßdummyTurker
4238ß0.0ßsurpass (as for FF100 in Table 4b) the ORIG score.ßdummyTurker
4239ß0.0ßRunning timeßdummyTurker
4240ß0.0ßtrends similarly, although still much below ORIG.ßdummyTurker
4241ß0.0ßWe have not carriedßdummyTurker
4242ß0.0ßout any curve fitting (although we might expect it to be quadraticßdummyTurker
4243ß0.0ßgiven algorithm complexity).ßdummyTurker
4244ß0.0ßSee also next comment.ßdummyTurker
4245ß0.0ßThe aim with presenting results with particular values for the CULLßdummyTurker
4246ß0.0ßradius (5, 50) was just to show that there is some neighbourhood sizeßdummyTurker
4247ß0.0ßfor which CULL results are about as good as ORIG.ßdummyTurker
4248ß0.0ß"The other integer-valued ""parameter"" is the dimensionality of"ßdummyTurker
4249ß0.0ßcharacter n-gram counts or embeddings.ßdummyTurker
4250ß0.0ßIn our experience this isn't aßdummyTurker
4251ß0.0ßparameter that is systematically varied in work in the field, probablyßdummyTurker
4252ß0.0ßbecause of the cost of training the models.ßdummyTurker
4253ß0.0ßFor instance, theßdummyTurker
4254ß0.0ßoriginal GloVe paper [1] examined word embedding dim 100, 300 andßdummyTurker
4255ß0.0ß1000, apparently chosen arbitrarily; the cited Bojanowski et al thatßdummyTurker
4256ß0.0ßintroduced fastText examined only dim 300.ßdummyTurker
4257ß0.0ßWe chose 300 because thereßdummyTurker
4258ß0.0ßwas a pretrained embedding model available that is widely used, 20000ßdummyTurker
4259ß0.0ßbecause that is the default used for the Koppel11 method; we alsoßdummyTurker
4260ß0.0ßwanted to look at one dimensionality smaller than 300 and two betweenßdummyTurker
4261ß0.0ß300 and 20000.ßdummyTurker
4262ß0.0ßWe didn't succeed in training 20000-dimensional embeddings, and weßdummyTurker
4263ß0.0ßaren't aware of any fastText work that has used embeddings this large.ßdummyTurker
4264ß0.0ß(That dimensionality was of interest because Koppel11 results keepßdummyTurker
4265ß0.0ßimproving to at least that size for character n-gram countßdummyTurker
4266ß0.0ßrepresentations, at least for larger datasets.)ßdummyTurker
4267ß0.0ß[1] Jeffrey Pennington, Richard Socher, ChristopherßdummyTurker
4268ß0.0ßD. Manning.ßdummyTurker
4269ß0.0ß2014.ßdummyTurker
4270ß0.0ßGloVe: Global Vectors for WordßdummyTurker
4271ß0.0ßRepresentation.ßdummyTurker
4272ß0.0ßEMNLP.ßdummyTurker
4273ß0.0ßhttps://nlp.stanford.edu/pubs/glove.pdfßdummyTurker
4274ß0.0ßThe neighbourhood experiment (Table 2) is only for FF100 and FF1000.ßdummyTurker
4275ß0.0ßAs noted for Reviewer #2, perhaps we should have marked out in Table 4ßdummyTurker
4276ß0.0ßthat they are in a sense development sets, although the other sets areßdummyTurker
4277ß0.0ßnot.ßdummyTurker
4278ß0.0ßBut as noted above, we only aimed to show that there was someßdummyTurker
4279ß0.0ßvalue of the CULL r parameter that produced results around ORIG; TableßdummyTurker
4280ß0.0ß2 didn't direct set this parameter, only making the generalßdummyTurker
4281ß0.0ßobservation about how often the correct answer was in the top 10% ofßdummyTurker
4282ß0.0ßranked.ßdummyTurker
4283ß0.0ßNo, the results weren't definitive in terms of one ensemble methodßdummyTurker
4284ß0.0ßalways (or even most often) being better; for lack of space, we onlyßdummyTurker
4285ß0.0ßincluded an example of the way this manifested rather than the moreßdummyTurker
4286ß0.0ßextensive results we had, which we could include in a publishedßdummyTurker
4287ß0.0ßversion with an extra page.ßdummyTurker
4288ß0.0ßWe therefore continued with plurality,ßdummyTurker
4289ß0.0ßwhich is the original algorithm's ensemble voting method, for lack ofßdummyTurker
4290ß0.0ßa clear-cut better alternative.ßdummyTurker
4291ß0.0ßThanks for your kind review and detailed summary; we really appreciate your comments on our paper.ßdummyTurker
4292ß0.0ßWe agree that it is true for most of the models that doubling the RNN encoder provides gains, but our claim is that, in our model, doubling the RNN encoder is more effective and efficient than increasing layers or number of parameters in the CNN decoder in terms of getting better results.ßdummyTurker
4293ß0.0ßIt is one part of the study on the effect of hyperparameter settings.ßdummyTurker
4294ß0.0ßWhile the study was done to examine the relative costs of different modifications, it is also true that since our model runs efficiently during training, doubling the RNN encoder is more computationally feasible in our model than others.ßdummyTurker
4295ß0.0ßFastSent is an efficient algorithm for learning sentence representations.ßdummyTurker
4296ß0.0ßThe issue of the FastSent model is that it doesn’t utilise the word order information at all, and the word order information is beneficial in building sentence representations.ßdummyTurker
4297ß0.0ßThe right side of Table 2 shows sentiment analysis tasks where word order can be critical.ßdummyTurker
4298ß0.0ßFor example “This great movie is not bad.”  and “This bad movie is not great.”  would be encoded to the same vector which clearly has different sentiment.ßdummyTurker
4299ß0.0ßThanks for the positive review!ßdummyTurker
4300ß0.0ßSince no weaknesses were listed, we humbly suggest that the reviewer might want to consider raising their score to 5.ßdummyTurker
4301ß0.0ßOvernight paper?ßdummyTurker
4302ß0.0ßIf you mean the original paper by wang, actually it’s the dataset paper and we used Jia and Liang’s paper which proposed results in different metrics.ßdummyTurker
4303ß0.0ßthe main difference is in the settings.ßdummyTurker
4304ß0.0ßAs far as we know our paper is the first paper in zero-shot setting.ßdummyTurker
4305ß0.0ßATIS Geo?ßdummyTurker
4306ß0.0ßSpace limitation was the concern.ßdummyTurker
4307ß0.0ßWe also mentioned, that our focus is on semantic (not syntactic) transfer learning.we will add them in the additional space we receive in the camera-ready version.ßdummyTurker
4308ß0.0ßThe method section is hard?ßdummyTurker
4309ß0.0ßWe added two additional regularization terms.ßdummyTurker
4310ß0.0ßThe first one is the prediction after last state of the decoder and the second one is the prediction after the last state of the encoder.ßdummyTurker
4311ß0.0ßWe are designing a diagram for the next version to illustrate this well.ßdummyTurker
4312ß0.0ßLSTM cell modifications?ßdummyTurker
4313ß0.0ßIn supplementary material we provided the general LSTM cell in the transfer learning setting.ßdummyTurker
4314ß0.0ßBut as we explained we make a marginal modification of the loss function (Eq.ßdummyTurker
4315ß0.0ß8) and also utilize different rows of the shared encoder according to the prediction for current example.ßdummyTurker
4316ß0.0ßIn Eq.ßdummyTurker
4317ß0.0ß8, why is p(y_{1:j}) not conditioned on anything?ßdummyTurker
4318ß0.0ßYou are right, the condition is hidden here.ßdummyTurker
4319ß0.0ß"You seem to be renaming ""semantic parsing"" ?"ßdummyTurker
4320ß0.0ßThe reason was to make it clear for a wider range of readers in machine learning and natural language processing.ßdummyTurker
4321ß0.0ßWe can change it to semantic parsing if you think it’s necessary.ßdummyTurker
4322ß0.0ßrelation to Liu et al ?ßdummyTurker
4323ß0.0ßThe authors in this paper used a similar idea; Although the naming for adversarial loss in this paper is quite different from ours, we have similar intentions.ßdummyTurker
4324ß0.0ßWe used the award winning paper in ICML 2017 by Liang as a reference for adversarial attacks.ßdummyTurker
4325ß0.0ßAnd also our paper aims to study the sequence to logic problem not the classification or sentiment analysis problem which is the focus of this paper.ßdummyTurker
4326ß0.0ßso we used the Herzig & Berant in ACL 2017 and also Fan et al 2017 as baselines.ßdummyTurker
4327ß0.0ßWe will mention the similarity of this idea in the paper.ßdummyTurker
4328ß0.0ßdeifnition of S_k,?ßdummyTurker
4329ß0.0ßYes.ßdummyTurker
4330ß0.0ßto make our experiments on training domains fair enough.ßdummyTurker
4331ß0.0ßIn Eq.ßdummyTurker
4332ß0.0ß7,  LHS be \hat{t}k,?ßdummyTurker
4333ß0.0ßYes.ßdummyTurker
4334ß0.0ßit was a typo.ßdummyTurker
4335ß0.0ßIf we change Eq.ßdummyTurker
4336ß0.0ß8 to t_\hat{k} and t_\hat{k’} they will be compatible.ßdummyTurker
4337ß0.0ßIn Eq.ßdummyTurker
4338ß0.0ß8, what does it mean to take the L2 ?ßdummyTurker
4339ß0.0ßIt’s a good point but it was the only information we did have about the misclassification of the tasks.ßdummyTurker
4340ß0.0ßWe also tried to change it a binary indicator/membership function to be fair among different tasks, but the results are almost the same and we just kept the formulation simple.ßdummyTurker
4341ß0.0ßr_t ?ßdummyTurker
4342ß0.0ßYes, r_t is all the regularization terms in semantic parsing.ßdummyTurker
4343ß0.0ßFig.ßdummyTurker
4344ß0.0ß1?ßdummyTurker
4345ß0.0ßYes it’s confusing and we will make it clear.ßdummyTurker
4346ß0.0ßWe don’t make change the LSTM unit but h_{t-1} being sliced by the prediction number in Eq.ßdummyTurker
4347ß0.0ß7. we don’t use whole the decoder for predicting the next token and we use one row according to Eq.ßdummyTurker
4348ß0.0ß7.ßdummyTurker
4349ß0.0ßHVP?ßdummyTurker
4350ß0.0ßWe put it in the supplementary material.ßdummyTurker
4351ß0.0ßtaking the second derivative is very time consuming and as we showed in appendix and as is explained explained in Koh and Liang ICML 2017 there is an iterative and simple solution named HVP.ßdummyTurker
4352ß0.0ßThe authors fixed the problems according to the comments on the Figure 2 and Table 2.ßdummyTurker
4353ß0.0ßThe authors will provide more information in the appendix.ßdummyTurker
4354ß0.0ßSchematic figure is a nice recommendation and we will also shed light on other equations in more detail.ßdummyTurker
4355ß0.0ßFor the transfer learning part we used almost the same set of parameters in the baselines and explained a part of them in Section 4.1.ßdummyTurker
4356ß0.0ßFor the cross-domain attack we explained the settings in label flipping section on Page 6.ßdummyTurker
4357ß0.0ßWe have provided a GitHub repository and will point to that in final version.ßdummyTurker
4358ß0.0ßThe vertical axis shows the change of loss when we add noise to examples z, and the horizontal axis shows the number of times the influence functions were able to find these examples and make corrections.ßdummyTurker
4359ß0.0ßWe will provide more clarification on this figure.ßdummyTurker
4360ß0.0ßLogic sequence modeling refers to converting sequences of natural language to sequences of machine readable texts like logical forms, code or SQL queries.ßdummyTurker
4361ß0.0ßSince the training data in these domains are generally very sparse, in this paper we focus on zero-shot learning on sequence to logic rather than general text to text like machine translation that tends to have a different setting and a larger number of training data.ßdummyTurker
4362ß0.0ßOur focus is on semantic transfer learning and this is the reason we didn’t provide more experiments on different types of logical forms l. Our only concern was the page limit and we can provide them in the final version.ßdummyTurker
4363ß0.0ßFor each example we remove its label in a way similar to Gibbs sampling and then predicting the actual label by utilizing a regularization term which concerns the error on the actual label.ßdummyTurker
4364ß0.0ßThen we update and learn all the parameters by gradient descent using negative log likelihood.ßdummyTurker
4365ß0.0ßWe used random initialization and we set the learning rate 0.5 according to convex optimization literature;  the results on whole the data will be provided as well.ßdummyTurker
4366ß0.0ßSort of.ßdummyTurker
4367ß0.0ßWe don’t add further layers and further parameters and all we do is to limit the training process to search in-domain tokens for prediction while giving the chance to predict out-of-domain tokens if the current state is totally out of domain.ßdummyTurker
4368ß0.0ßYes shuffling and keeping the order for all the baselines.ßdummyTurker
4369ß0.0ßThe significance test will be done.ßdummyTurker
4370ß0.0ßThis is the way we were trying to simulate the zero shot setting with sparse training data.ßdummyTurker
4371ß0.0ßAs you mentioned before we will provide the results on the entire training data.ßdummyTurker
4372ß0.0ßYes there is a github repositoryßdummyTurker
4373ß0.0ßI Gadag and M Sagar, 2016 can be found in reference in line number 818; Actually all references has been cited in the paper, however due to the limitation of the latex format (library) the citation has a different format from the reference.ßdummyTurker
4374ß0.0ßWe have already check about it; We can change the sentence in the last paragraph of 5.2.2 to ‘’Since the proposed method only substituted the word based on synonym word list, then the naturalness of paraphrased sentence using the proposed method is better than Gadag’s method”; This method was used because we have to maintain the context and for maintaining the context we have to find the same tag for a word in similar sentences; The result can be more than 100% because we assume that the naturalness of original sentences was 100% while based on human judgment, there are possibility that the naturalness of resulting sentences using the proposed method is better than the original one.ßdummyTurker
4375ß0.0ßIn this case, the naturalness of resulting sentences using the proposed method is over 100%.ßdummyTurker
4376ß0.0ßBut actually we can arrange the scale such that the maximum value will be 100%, by defining the highest naturalness as 100%; The formulae used in section 5.1.1 is the one that is used in METEOR.ßdummyTurker
4377ß0.0ßWe can revise the abstract as follows “Generating paraphrases is one of the important components in Indonesian natural language processing and generation.ßdummyTurker
4378ß0.0ßThere are several applications which use paraphrasing method such as linguistic steganography, recommender system, machine translation, etc.ßdummyTurker
4379ß0.0ßThere are several methods for conducting synonym substitution.ßdummyTurker
4380ß0.0ßOne of them is NGM-Based proposed by Gadag et al.ßdummyTurker
4381ß0.0ßThe weakness of this method is that the paraphrasing is conducted by N-gram such that the probability of ambiguity of the meaning has frequently occurred.ßdummyTurker
4382ß0.0ßAs the impact, the naturalness is decreased.ßdummyTurker
4383ß0.0ßThis paper proposed the contextual synonym substitution proposed method to maintain the naturalness of the paraphrased sentences.ßdummyTurker
4384ß0.0ßFor maintaining the naturalness 3-gram analysis and filtering the word that will be substituted based on the 3-gram analysis result.ßdummyTurker
4385ß0.0ßBased on the experiment, the naturalness of resulting sentences using the proposed method is better than using Gadag’s method.”ßdummyTurker
4386ß0.0ßThe paraphrasing method was conducted not only by replacing the word with synonyms in the thesaurus, but we have to analyze the sentence based on 3-grams in the state of only replacing the word with synonyms.ßdummyTurker
4387ß0.0ßThis method is proposed to maintain the context.ßdummyTurker
4388ß0.0ßWe will revise the paper to make easier to read.ßdummyTurker
4389ß0.0ßFor considering the sense of substitution we can use 3-gram analysis.ßdummyTurker
4390ß0.0ß"For example for the word ""apel"" we have two meanings apple and ceremony."ßdummyTurker
4391ß0.0ß"Suppose the sentence is ""saya makan buah apel"" (I eat apple) and ""saya apel pagi ini"" (I gathered this morning)."ßdummyTurker
4392ß0.0ß"Based on the 3-gram analysis, we can obtain that the tag of 3-gram ""makan buah apel"" (eat apple) is VBT NN NN and the tag of 3-gram ""saya apel pagi""(I gathered this morning) is PRP VBT NN."ßdummyTurker
4393ß0.0ß"Based on this explaination we can see that based on 3-gram analysis the tag of ""apel"" in the first sentence is Noun, while in the second sentence is Verb."ßdummyTurker
4394ß0.0ßWe can add this example in our paper.ßdummyTurker
4395ß0.0ßFor comparing the method we can paraphrase the sentence by substituting more than one words such as “saya anak nakal” (I am naughty) can be paraphrased into “aku anak jahat” (I am a bad boy).ßdummyTurker
4396ß0.0ßWe can add additional references such paraphrasing method such as linguistic steganography (linguistic based steganography using lexical substitution and syntactical transformation), recommender system (Information Fusion for MultiDocument Summarization: Paraphrasing and Generation.ßdummyTurker
4397ß0.0ß), machine translation (Statistical machine translation for query expansion in answer retrieval), etc.ßdummyTurker
4398ß0.0ßThey are the parameters of the single-time attention mechanism, and are automatically learned.ßdummyTurker
4399ß0.0ßTo the best of our knowledge, none of the open-accessed datasets is available for the task.ßdummyTurker
4400ß0.0ßThe two categories (Clothes and Electronics) contain most of the reviews in Amazon.ßdummyTurker
4401ß0.0ßIn the 5th paragraph of Section 1 (introduction), we mentioned that “nearly 54% helpful reviews have extreme (1-star and 5-star) ratings”.ßdummyTurker
4402ß0.0ßThe experimental results (STNL with start ratings) on adding the star ratings as additional features are shown in Table 4, which demonstrate that MTNL still outperforms STNL with star ratings.ßdummyTurker
4403ß0.0ßThe two categories (Clothes and Electronics) contain most of the reviews in Amazon.ßdummyTurker
4404ß0.0ßIn the 5th paragraph of Section 1 (introduction), we mentioned that “nearly 54% helpful reviews have extreme (1-star and 5-star) ratings”.ßdummyTurker
4405ß0.0ßThe experimental results (STNL with start ratings) on adding the star ratings as additional features are shown in Table 4, which demonstrate that MTNL still outperforms STNL with star ratings.ßdummyTurker
4406ß0.0ßThank you for your suggestions.ßdummyTurker
4407ß0.0ßFirstly, we want to emphasize that our model is specifically designed for OpenQA, rather than RC.ßdummyTurker
4408ß0.0ßSecondly, we have run some experiments on standard SQuAD dataset (dev-set), which are not included in the paper due to space limitation.ßdummyTurker
4409ß0.0ßOur HAS-QA yields EM/F1 0.719/0.798, which is comparable with the best released single model Reinforced Mnemonic Reader Hu et al.ßdummyTurker
4410ß0.0ß(2017) in the leaderboard (dev-set) is EM/F1 0.721/0.816.ßdummyTurker
4411ß0.0ßOur performance is slightly worse because Reinforced Mnemonic Reader directly use the accurate answer span, while we use multiple distant supervised answer spans.ßdummyTurker
4412ß0.0ßThis may introduce many noises in the setting of SQUAD, since only one span is accurate.ßdummyTurker
4413ß0.0ßWe do not evaluate on the test-set of SQuAD because the competition need upload runnable model for evaluation while we do not have enough time for that.ßdummyTurker
4414ß0.0ßSorry that we do not do the time complexity analysis.ßdummyTurker
4415ß0.0ßCompared to RC models, the inference time is comparable.ßdummyTurker
4416ß0.0ßThat's because the most time-consuming part lie in the context encoder, which is similar to other RC models.ßdummyTurker
4417ß0.0ßWe are not quite sure which part is not identical.ßdummyTurker
4418ß0.0ßDo you mean the beam search strategy in the inference phase?ßdummyTurker
4419ß0.0ßIn the training phase, the ground-truth start and end positions are given, which can be directly used in the computation.ßdummyTurker
4420ß0.0ßWhile in the inference phase, to avoid the cost of computing every possible start position, we use a beam search strategy (line 494).ßdummyTurker
4421ß0.0ßTherefore, only the top 3 start positions and top 1 end position are considered in the computation, and the experiments show the efficiency (refer to Table3).ßdummyTurker
4422ß0.0ßThe choice of using only one negative sample is also because of the computation cost issue.ßdummyTurker
4423ß0.0ßIn fact, using more negative samples will not significantly influence the final performance.ßdummyTurker
4424ß0.0ßWe will state it more clearly in the modification of the paper.ßdummyTurker
4425ß0.0ßThank you for your suggestions, and we will try that in the future.ßdummyTurker
4426ß0.0ßWe follow the regularization strategies of Clark et al.ßdummyTurker
4427ß0.0ß(2017), i.e., a variational dropout Kingma et al.ßdummyTurker
4428ß0.0ß(2015) and a moving average of trained weight are applied.ßdummyTurker
4429ß0.0ßSpecifically, the dropout ratio is set to 0.8.ßdummyTurker
4430ß0.0ßFurthermore, we maintain the moving averages of the trained parameters to 0.99 in the training phase, while the averaged parameters are used in the inference phase.ßdummyTurker
4431ß0.0ßFor other parameters, the learning rate=1, batch size=3, and the dimension of BiGRU=200.ßdummyTurker
4432ß0.0ßIn general, the parameters are not difficult to tune, and we use the same setting across all the datasets.ßdummyTurker
4433ß0.0ßWe will add these details in the Experimental Settings part.ßdummyTurker
4434ß0.0ßWe notice that Choi et al.ßdummyTurker
4435ß0.0ß(2017) proposes a two-step QA model.ßdummyTurker
4436ß0.0ßIt first selects relevant sentences, and then generates an answer based on the summarization of the relevant sentences, which is similar to the architecture of our HAS-QA.ßdummyTurker
4437ß0.0ßHowever, their model does NOT have a unified probabilistic formulation, though they use a likelihood loss for optimization.ßdummyTurker
4438ß0.0ßThat's because the pretrain (distant supervision) and fine-tune (reinforcement learning) processes are separated.ßdummyTurker
4439ß0.0ßTherefore, our two-level probabilistic formulation is new.ßdummyTurker
4440ß0.0ßWe need to point out that the RC model is NOT applied to every paragraph in the documents.ßdummyTurker
4441ß0.0ßThat's because our setting is not based on long documents.ßdummyTurker
4442ß0.0ß"We have stated clearly in the first paragraph of the Introduction that ""we focus on the OpenQA task with the unstructured knowledge sources retrieved by search engine""."ßdummyTurker
4443ß0.0ßIn this setting, for a given question, the goal of OpenQA is to find an answer based top K relevant paragraphs (NOT documents), which is the same as previous work Clark et al.ßdummyTurker
4444ß0.0ß(2017) and Wang et al.ßdummyTurker
4445ß0.0ß(2017).ßdummyTurker
4446ß0.0ßIn real applications, these paragraphs are usually not long.ßdummyTurker
4447ß0.0ßFor example, QuasarT and SearchQA datasets provide 10-20 paragraphs as the evidences for each question, the paragraph average lengths of these datasets are about 800 and 400 words, respectively.ßdummyTurker
4448ß0.0ßTherefore, the scalability is not a big issue in our setting.ßdummyTurker
4449ß0.0ßYou are right that both Kadlec et al.ßdummyTurker
4450ß0.0ß(2016) and Seo et al.ßdummyTurker
4451ß0.0ß(2017) used the SUM aggregation, which is similar to ours.ßdummyTurker
4452ß0.0ßHowever, it is important to be noticed that the aggregation objects are different.ßdummyTurker
4453ß0.0ßFor example, the aggregation function of Kadlec et al.ßdummyTurker
4454ß0.0ß(2016) is word, while our aggregation is for answer spans.ßdummyTurker
4455ß0.0ßThis is mainly because our settings are different.ßdummyTurker
4456ß0.0ß"In the setting of OpenQA, our claim of ""previous works do not consider this SUM aggregation"" is reasonable."ßdummyTurker
4457ß0.0ßFor further modification, we will consider to add these related works to our description of the multiple spans aggregator.ßdummyTurker
4458ß0.0ßIn fact, the RC model is NOT applied to each paragraph in the documents.ßdummyTurker
4459ß0.0ß"Please refer to our response to ""Weakness argument 2""."ßdummyTurker
4460ß0.0ßIn the original TriviaQA, the entire long documents are provided, but these documents are not directly used in our experiments.ßdummyTurker
4461ß0.0ßTo better fit our setting, similar to Clark et al.ßdummyTurker
4462ß0.0ß2017 and Wang et al.ßdummyTurker
4463ß0.0ß2017, we first conduct a paragraph retrieval process.ßdummyTurker
4464ß0.0ßAs a result, 32 paragraphs with length 400 per question are obtained for TriviaQA.ßdummyTurker
4465ß0.0ßTherefore, the computation is efficient.ßdummyTurker
4466ß0.0ßIt is NOT true that a BiGRU layer is needed for every possible start position.ßdummyTurker
4467ß0.0ßIn the training phase, the ground-truth start position and end positions are given, therefore they can be directly used in the computation.ßdummyTurker
4468ß0.0ßWhile in the inference phase, to avoid the cost of computing every possible start position, we use a beam search strategy (line 494 in the paper).ßdummyTurker
4469ß0.0ßTherefore, only the top 3 start positions and top 1 end position are considered in the computation, and the experimental results show that the beam search strategies are effective (refer to Table 3 in our paper).ßdummyTurker
4470ß0.0ßWe are very happy to obtain your detailed and useful comments to improve our paper.ßdummyTurker
4471ß0.0ßThese comments also encouraged us, as well as our paper.ßdummyTurker
4472ß0.0ßThank you very much.ßdummyTurker
4473ß0.0ßWe will consider all of the additional comments in addition to the following replies.ßdummyTurker
4474ß0.0ßWe are sorry for this inconvenience.ßdummyTurker
4475ß0.0ßThe preprocessing is as follows.ßdummyTurker
4476ß0.0ßFirst of all, we filtered out spam and bot tweets using user-level signals such as follower count, friend count, favorite count, and whether a profile image is set or not (Line 136-140).ßdummyTurker
4477ß0.0ß"Also, we replaced all the URLs in the text with ""[u]"", considering them as noise."ßdummyTurker
4478ß0.0ßThis was the only preprocessing we applied, and we used the rest of the text including hashtags and stop words as it was.ßdummyTurker
4479ß0.0ßThis simple way surprisingly worked well with our character-based model as shown in Table 2.ßdummyTurker
4480ß0.0ßWe will add more explanations to clarify this point.ßdummyTurker
4481ß0.0ßThank you for the suggestion.ßdummyTurker
4482ß0.0ßIn fact, we assumed a proof-of-concept result, as you said.ßdummyTurker
4483ß0.0ßWe will re-frame the statement.ßdummyTurker
4484ß0.0ßWe will add more explanations for the annotation task as follows.ßdummyTurker
4485ß0.0ßThe dataset was created with about 300 person-days in the company (Line 158).ßdummyTurker
4486ß0.0ßThis means that the cost is at least 24K dollars (= 8 hours * 300 days * legal minimum wage in Japan 10 dollars/hour), although it would be much more since they put skilled proper employees to obtain high-quality results.ßdummyTurker
4487ß0.0ßIn addition, the annotators had gone through a few days of training to become able to appropriately judge the sentiment before they got down to actual annotation work, but the number, 300 person-days, does not include the time for this training.ßdummyTurker
4488ß0.0ßAs for the inter-annotator agreement, the ratio of unanimous ones (with three votes) was 56.7%, that of dominant labels having two votes was 42.6%, and the cases with no agreement (each label with one vote) was 0.7%.ßdummyTurker
4489ß0.0ßWe agree this weakness, so we will release the dataset (IDs of the tweets) to enable comparison with our results for testing.ßdummyTurker
4490ß0.0ßAt least, we believe that addressing the real world problem can be regarded as a strength.ßdummyTurker
4491ß0.0ßThere are two possible reasons.ßdummyTurker
4492ß0.0ßOne is that word-based models often fail in word segmentation of colloquial expressions with plentiful of emoticons and emoji.ßdummyTurker
4493ß0.0ßIn such cases, wrongly-segmented words can be regarded as unknown meta-words (i.e., UNK) because of a limited dictionary size.ßdummyTurker
4494ß0.0ßThis implies that many colloquial expressions can vanish in a training phase.ßdummyTurker
4495ß0.0ßCharacter-based models overcome this challenge since they can recognize almost all of the regular characters (the dictionary size of 4000 in Line 170-171 was enough for this purpose).ßdummyTurker
4496ß0.0ßThe other is that Japanese sentences consist of many Chinese characters (Kanji).ßdummyTurker
4497ß0.0ßBecause Kanji roughly corresponds to one or two syllables in English, each such character has much more information than a Roman alphabet.ßdummyTurker
4498ß0.0ßThis could be related to the recent results where sub-word-based models often performed better than word-based models in morphologically rich languages.ßdummyTurker
4499ß0.0ßYour understanding is correct.ßdummyTurker
4500ß0.0ßWe ignore the problem of sarcasm, since it cannot be easily detected and is beyond our scope.ßdummyTurker
4501ß0.0ßWe will clarify this point.ßdummyTurker
4502ß0.0ßThank you for the suggestion.ßdummyTurker
4503ß0.0ßWe will add more examples at least in the supplemental material.ßdummyTurker
4504ß0.0ßWe used WMT'14 En/De since many reference results are available for this task (which allows us to show that we start from a state-of-the-art baseline).ßdummyTurker
4505ß0.0ßThis task also provides a huge collection of monolingual data which we can use to mine for bitexts.ßdummyTurker
4506ß0.0ßFinally, another important aspect is that all the data is freely available which enables us to release the filtered and crawled data.ßdummyTurker
4507ß0.0ßIn the mean time, we have explored longer training and different sampling schemes.ßdummyTurker
4508ß0.0ßThis does not change the conclusions of this research, i.e.ßdummyTurker
4509ß0.0ßfiltering Common Crawl leads to better BLEU scores.ßdummyTurker
4510ß0.0ßWe used the official training data of the WMT1'4 EN/DE task so that our results are comparable with the published ones.ßdummyTurker
4511ß0.0ßWe have used a joint encoder for all languages since they use the same alphabet.ßdummyTurker
4512ß0.0ßFor ZH or JA, we could use additional encoders like in the work of Schwenk&Matthijs 2017.ßdummyTurker
4513ß0.0ßThe main goal of this paper is to show that the distance in the joint embedding space can be used to filter and mine data.ßdummyTurker
4514ß0.0ßThis is possible for both architectures.ßdummyTurker
4515ß0.0ßThe goal of this work is to filter bitexts.ßdummyTurker
4516ß0.0ßThere is indeed a body of research on data selection, but mainly applied to SMT not NMT.ßdummyTurker
4517ß0.0ßData selection algorithms do not necessarily remove wrong translations (eg source and target sentences are both in-domain, but not mutual translations).ßdummyTurker
4518ß0.0ßAlso the perplexity of each sentence may be low.ßdummyTurker
4519ß0.0ßThe use of word alignment is indeed a possibility, but this needs tools like giza to obtain them.ßdummyTurker
4520ß0.0ßThe goal of this work is to show that a completely different procedure can be used.ßdummyTurker
4521ß0.0ßIndeed, it is possible to show that noisy data hurts NMT by a couple of experiments.ßdummyTurker
4522ß0.0ßWe were referring to a research of a colleague which is under review and which we can't cite yet.ßdummyTurker
4523ß0.0ß"We will add the reference to another work, ""Synthetic and natural noise both break NMT"""ßdummyTurker
4524ß0.0ßwe don't explicitly mine short or long sentences.ßdummyTurker
4525ß0.0ßWe believe that it is highly unlikely to find perfect translations of long sentences in news data (the longer the sentence, the more likely it is that journalist have modified, removed or added some parts).ßdummyTurker
4526ß0.0ßWe assume that perfect translations of very long sentences can be only found in official translations of whole documents (eg.ßdummyTurker
4527ß0.0ßLinux, KDE documentation etc)``ßdummyTurker
4528ß0.0ßFiltering by perplexity can not remove wrong translations (see weakness 2).ßdummyTurker
4529ß0.0ßWord alignment quality could be used for this, but this would require to train an alignment modelßdummyTurker
4530ß0.0ßNo, we tried different vocabulary sizes, but there was not difference (since we use BPE vocabulary).ßdummyTurker
4531ß0.0ßWe thank the reviewers for the additional references.ßdummyTurker
4532ß0.0ßHowever, the first two address data selection which is not the same as filtering (eg source and target sentences are both in-domain, but not mutual translations.ßdummyTurker
4533ß0.0ßWe have observed this type of sentence pairs in Common Crawl).ßdummyTurker
4534ß0.0ßThe paper from Microsoft proposes, among many other things, a method which is very similar to ours, ie.ßdummyTurker
4535ß0.0ßfiltering data using a distance in a joint sentence representation space.ßdummyTurker
4536ß0.0ßThey use a different method and use a representation optimized to their NMT system.ßdummyTurker
4537ß0.0ßThis filtering is shown to improve performance when applied to a huge additional corpus.ßdummyTurker
4538ß0.0ßGiven those results, our 0.3 BLEU improvement when filtering a much smaller corpus seems quite respectful.ßdummyTurker
4539ß0.0ßit is not obvious to verify if (our) sentence vectors are less powerful for long sentences.ßdummyTurker
4540ß0.0ßRepresenting very long sentences by one vector and comparing them is indeed tricky.ßdummyTurker
4541ß0.0ßThis was actually the main reason to introduce attention in NMT.ßdummyTurker
4542ß0.0ßHowever, we don't need to handle very long sentencesßdummyTurker
4543ß0.0ßIn this work we focus on filtering noisy training data, in particular wrong translations when both sentences are perfectly well formed and potentially in-domain.ßdummyTurker
4544ß0.0ßThis type of error is mot captured by the usual data selection methods.ßdummyTurker
4545ß0.0ßPlease see our comment to weakness 2.ßdummyTurker
4546ß0.0ßWe don't believe that representing very long sentences by one vector of fixed size make sense.ßdummyTurker
4547ß0.0ßAt some point, we would probably need some type of dynamic representation which adapts to the sentence length.ßdummyTurker
4548ß0.0ßHowever, it is not obvious to know where is the limit, 20, 30, 50, 100 words, or even more ?ßdummyTurker
4549ß0.0ß"In the paper ""Deep Recurrent Models with Fast-Forward Connections"" is is shown that deep fixed size representations seem to be quite good for sentence with at least 30 words."ßdummyTurker
4550ß0.0ßThank you for giving detailed comments on our work.ßdummyTurker
4551ß0.0ßThe novelty of our work lies in the neural architecture motivated by the linguistic phenomenon that discriminating two endings are necessary in this task.ßdummyTurker
4552ß0.0ßThroughout the model, we are performing different level of discriminations which are accordance with our motivation.ßdummyTurker
4553ß0.0ßThe experimental results also support our claim that the discimination on two endings are necessary.ßdummyTurker
4554ß0.0ß1) The experimental setups in our model strictly follow the previous works in order to have a fair comparison.ßdummyTurker
4555ß0.0ß2) The results are obtained by running 5 independent run with the same random seed.ßdummyTurker
4556ß0.0ßIn neural network models, the change of random seed could significantly change the final results and the hyperparameters are binded to  the specific random seed.ßdummyTurker
4557ß0.0ß3) We report the best result across independent runs as previous works does.ßdummyTurker
4558ß0.0ßAlso different from previous works, we also report average and standard deviation value to show the system stability.ßdummyTurker
4559ß0.0ß4) Would definitely carry out detailed significant tests on these claims.ßdummyTurker
4560ß0.0ßDefinitely will correct writing issues.ßdummyTurker
4561ß0.0ßThe mention of the previous CBT/CNN/DailyMail dataset are supposed to introduce the Story Cloze Test.ßdummyTurker
4562ß0.0ßBecause the previous datasets are well-studied, but on the contrary, the SCT task need further studies.ßdummyTurker
4563ß0.0ßThe ensemble model includes 4 models.ßdummyTurker
4564ß0.0ßThe final scores are produced by average 4 models’ output and choose the one that has highest probability.ßdummyTurker
4565ß0.0ßAs this is not majority voting approach, the averaged probability seldom comes to a tie.ßdummyTurker
4566ß0.0ßThe two candidates are modeled in parallel and the operations on each ending are totally the same.ßdummyTurker
4567ß0.0ßSo our model is not sensitive to the order of each ending.ßdummyTurker
4568ß0.0ßThank you for your constructive comments on our work.ßdummyTurker
4569ß0.0ßOur model was motivated by the linguistic phenomenon and was not all about the NN approach.ßdummyTurker
4570ß0.0ßThe different part of each ending is the key to distinguish them and the two endings should be compared throughout the neural network modeling.ßdummyTurker
4571ß0.0ßSo our model was designed by this linguistic phenomenon and the experimental results show that by adding discriminative components into neural network model could give improvements on this task.ßdummyTurker
4572ß0.0ßThank you for giving constructive comments on the error analysis annotation procedure.ßdummyTurker
4573ß0.0ßThe kappa value of inter-annotator agreement was 0.72.ßdummyTurker
4574ß0.0ßThe novelty of our model can be listed as follows.ßdummyTurker
4575ß0.0ß1) We firstly propose to model the differences of each ending in Story Cloze Test and this was not discussed in previous works on SCT.ßdummyTurker
4576ß0.0ß2) We hierarchically discriminate two endings in three levels to capture the differences progressively, including contextual representations, matching representations and discriminative representations.ßdummyTurker
4577ß0.0ß3) By adding additional hinge loss in the training objective, we would further maximize the latent space of two endings, making it easier for model to discriminate them.ßdummyTurker
4578ß0.0ßYes, both HCM (Chaturvedi et al., EMNLP 2017) and our Diff-Net is to tackle the Story Cloze Test.ßdummyTurker
4579ß0.0ßHowever, the HCM focus on combining different aspects in solving this problem and effectively combine these models to form an ensembled result.ßdummyTurker
4580ß0.0ßThis is why the result in their paper are compared to various ENSEMBLE approches, such as `soft voting` or `majority voting` etc.ßdummyTurker
4581ß0.0ßIn our model, we are focusing on the building a unified model to solve this task, but not focusing on how to effectively combine these models.ßdummyTurker
4582ß0.0ßAs we can see from Table 1 in our paper,  both `majority voting` (69.1) and `soft voting` (75.1) result (quoted from the HCM paper, and was performed by the HCM’s authors) are lower than our ensemble result (76.8) and even our single model result (75.8), suggesting that our model is effective and further improvements may be obtained by adopting the HCM in the ensemble process.ßdummyTurker
4583ß0.0ß- Additional Comments: We are sorry for the misleading illustration on the SQuAD.ßdummyTurker
4584ß0.0ßWe will revise this part in the future version.ßdummyTurker
4585ß0.0ß- We are quite confused by the overall score of 2, while the other parts of the scores are 4 or 5.ßdummyTurker
4586ß0.0ßWe are not sure if there is anything wrong here.ßdummyTurker
4587ß0.0ßThank you for the valuable comments on our work.ßdummyTurker
4588ß0.0ß`About why the difference of two endings affect result`: We are sorry for not making it clear about our motivations.ßdummyTurker
4589ß0.0ßPrevious work by [Cai et al., 2017] propose that the ending itself is important in solving this task.ßdummyTurker
4590ß0.0ßInspired by this, we noticed several phenomenon.ßdummyTurker
4591ß0.0ß1) Existing works are not aware of comparing two endings, while the human would compare the endings for better discriminating and highlight the different part for seeking clues in the story (common part is less informative in solving the question).ßdummyTurker
4592ß0.0ß2) In this corpus, there are many cases that the two endings are very similar, for e.g.ßdummyTurker
4593ß0.0ßonly  one word is changed, such as from `happy` to `sad` (also discussed in Section 5.).ßdummyTurker
4594ß0.0ßSo we think it is better to leave out the common part and let the machine focus more on the different part in each ending.ßdummyTurker
4595ß0.0ßThis will lower the difficulty in choosing the correct ending.ßdummyTurker
4596ß0.0ßThis project mainly focuses on the comparison of deep learning models.ßdummyTurker
4597ß0.0ßThis is because there are many researches which applied traditional machine learning methods to the classification of Wikipedia articles.ßdummyTurker
4598ß0.0ßThe performance for current traditional machine learning models is not good enough as I mentioned in the paper.ßdummyTurker
4599ß0.0ßTherefore, the state of art deep learning models is applied.ßdummyTurker
4600ß0.0ßThe articles are from Wikipedia.ßdummyTurker
4601ß0.0ßIt includes long articles and short articles.ßdummyTurker
4602ß0.0ßIt also includes different topics.ßdummyTurker
4603ß0.0ßTherefore, this model is suitable for both long and short articles with different topics.ßdummyTurker
4604ß0.0ßDue to the paper space, it is hard to give clear explanation of the input representation.ßdummyTurker
4605ß0.0ßI have provided my code.ßdummyTurker
4606ß0.0ßIn the code, it gives more details.ßdummyTurker
4607ß0.0ßFor each article, single vector is used to represent.ßdummyTurker
4608ß0.0ßThen all of the single vectors are combined as a matrix.ßdummyTurker
4609ß0.0ßThen the shape of matrix is adjusted in order to meet the input requirement of deep learning models.ßdummyTurker
4610ß0.0ßAs to readability scores, every score is related to a long formula which takes a lot of space.ßdummyTurker
4611ß0.0ßThe references are as followed:ßdummyTurker
4612ß0.0ßReadability Scores	Description	ReferenceßdummyTurker
4613ß0.0ßAutomated Readability Index (Ari)ßdummyTurker
4614ß0.0ßThis indicator approximates the age needed to understand an article.ßdummyTurker
4615ß0.0ßThis score is estimated by average sentence length and average word length.ßdummyTurker
4616ß0.0ßBlumenstock J E. Automatically assessing the quality of Wikipedia articles.ßdummyTurker
4617ß0.0ßColeman liau index (coleman_liau)ßdummyTurker
4618ß0.0ßThis index estimates the reading level needed to understand an article.ßdummyTurker
4619ß0.0ßDalip D H, Gonçalves M A, Cristo M, et al.ßdummyTurker
4620ß0.0ßAutomatic assessment of document quality in web collaborative digital libraries.ßdummyTurker
4621ß0.0ßFlesch reading ease (flesh)ßdummyTurker
4622ß0.0ßThis index estimates the difficulty of reading and comprehending an article.ßdummyTurker
4623ß0.0ßThe lower the score, the more difficultly the article is comprehended.ßdummyTurker
4624ß0.0ßFlesch R. A new readability yardstick.ßdummyTurker
4625ß0.0ßFlesch-Kincaid (Kincaid)ßdummyTurker
4626ß0.0ßThis is the modification of Flesch reading score.ßdummyTurker
4627ß0.0ßKincaid J P, Fishburne Jr R P, Rogers R L, et al.ßdummyTurker
4628ß0.0ßDerivation of new readability formulas for navy enlisted personnel.ßdummyTurker
4629ß0.0ßGunning Fog Index (fog)ßdummyTurker
4630ß0.0ßThis score can measure how easy the text is, which can map to reading level.ßdummyTurker
4631ß0.0ßGunning R. The Fog Index After Twenty Years.ßdummyTurker
4632ß0.0ßLIX (Lix)ßdummyTurker
4633ß0.0ßThis score assesses text difficulty in different languages.ßdummyTurker
4634ß0.0ßB. Stvilia, M. Twidale, L. Smith, and L. Gasser.ßdummyTurker
4635ß0.0ßAssessing information quality of a community-based encyclopedia.ßdummyTurker
4636ß0.0ßIn Proc.ßdummyTurker
4637ß0.0ßof ICIQ’05, pages 442–454, 2005.ßdummyTurker
4638ß0.0ßSMOG score (Smog)ßdummyTurker
4639ß0.0ßThis score estimates reading grade readers must have in order to interpret the text.ßdummyTurker
4640ß0.0ßLaughlin G H M. SMOG Grading-a New Readability Formula.ßdummyTurker
4641ß0.0ßAs to “Assortativity_*”ßdummyTurker
4642ß0.0ßAssociativity of in degree and in degree (associativity_in_in)	Ratio between in degree of the node and in degree of its neighbor nodesßdummyTurker
4643ß0.0ßAssociativity of in degree and out degree (associativity_in_out)	Ratio between in degree of the node and out degree of its neighbor nodesßdummyTurker
4644ß0.0ßAssociativity of out degree and in degree (associativity_out_in)	Ratio between out degree of the node and in degree of its neighbor nodesßdummyTurker
4645ß0.0ßAssociativity of out degree and out degree (associativity_out_out)	Ratio between out degree of the node and out degree of its neighbor nodesßdummyTurker
4646ß0.0ßIn order to get (associativity_in_in), we need to figure out the indegree for the node (article).ßdummyTurker
4647ß0.0ßThen we need to find the its neighbors to get sum of in degree of its neighbors’ nodes.ßdummyTurker
4648ß0.0ßThe result is the ratio.ßdummyTurker
4649ß0.0ßIt is similar for other associativity.ßdummyTurker
4650ß0.0ßThese features are used to describe how popular the node is comparing with its neighbors.ßdummyTurker
4651ß0.0ßThose are difference instead of ratios.ßdummyTurker
4652ß0.0ßFirstly, we choose the classification result of stacked LSTM with full feature set as the baseline.ßdummyTurker
4653ß0.0ßThen every time, we will delete only one feature and compute the result with stacked LSTM.ßdummyTurker
4654ß0.0ßThe difference between this result and baseline is the feature importance.ßdummyTurker
4655ß0.0ßThanks for pointing the mistakes and typos.ßdummyTurker
4656ß0.0ßWe have revised it.ßdummyTurker
4657ß0.0ßWe will go through the paper again to fix more typos and mistakes.ßdummyTurker
4658ß0.0ßThere is table 1 in page 6.ßdummyTurker
4659ß0.0ßIn order to make article more readable, the content in line 550 follows line 527.ßdummyTurker
4660ß0.0ßThanks for your careful and valuable comment.ßdummyTurker
4661ß0.0ßActually, we implement GSM to generate the global representation.ßdummyTurker
4662ß0.0ßAnd, the topic vector θ is exactly the topic vector t in the Figure.ßdummyTurker
4663ß0.0ßWe will fix these errors in the final version.ßdummyTurker
4664ß0.0ßDifference between NVDM and GSM is that GSM utilizes softmax layer to normalize the topic vector.ßdummyTurker
4665ß0.0ßThe topic vector of GSM then can be explained as the topic distribution of a document.ßdummyTurker
4666ß0.0ßFor a topic vector of NVDM, it is a real-valued vector which is hard to interpret.ßdummyTurker
4667ß0.0ßFor instance, θ1= (-0.4399, -0.6795, 0.8275, -0.0920, 1.0320…) is an example of the topic vector.ßdummyTurker
4668ß0.0ßThe components of θ1 are possible negative or larger than 1, which in fact is hard to interpret.ßdummyTurker
4669ß0.0ßWhile θ2=(0.0111, 0.2235, 0.0041, 0.0822…) is a sample of GSM topic vector.ßdummyTurker
4670ß0.0ßThe sum of all dimensions of θ2 equals to 1, which is reasonable to represent the topic information of given document.ßdummyTurker
4671ß0.0ßWe have uploaded code in https://github.com/AEGISEDGE/GLEAM for understanding.ßdummyTurker
4672ß0.0ßThanks for your valuable advice on human subject evaluation.ßdummyTurker
4673ß0.0ßQuality evaluation is always a key issue in this area.ßdummyTurker
4674ß0.0ßIn this paper, we present several case studies on Table 4 to show the results to readers, as one kind of human evaluation.ßdummyTurker
4675ß0.0ßAs future work, we might sample 50 examples from the testing data, and conduct human subject evaluations to finalize our work.ßdummyTurker
4676ß0.0ßAnd this also can be a great contribution in the area of headline generation.ßdummyTurker
4677ß0.0ßSince not any work in headline generation by neural-based abstractive framework has conducted this valuable evaluation to our knowledge.ßdummyTurker
4678ß0.0ßAll the mentioned papers are very classic baselines in summarization area by adding topics to the models.ßdummyTurker
4679ß0.0ßThe main reason why we didn't refer to them mainly because they are extractive-based summarization, which are not in the scope of this paper.ßdummyTurker
4680ß0.0ßHowever, we might add them to the related work, to show that topic models are indeed relevant and effective to improve the summarization systems if our paper is accepted.ßdummyTurker
4681ß0.0ßAs you suggest, we did similar analysis method again for the GLEAM, and find that little effectiveness on ROUGE-1 metric (nearly 9% of the generated headlines have the problem described in Section 2).ßdummyTurker
4682ß0.0ßHowever, it is surprising more agreements are revealed in the metric of ROUGE-2 and ROUGE-L, compared to the baseline sequence-to-sequence (s2s) model, which validate the effectiveness of the GLEAM.ßdummyTurker
4683ß0.0ßCore sense here means the main idea of one article.ßdummyTurker
4684ß0.0ßThat is to say, when we write a summary, we do have the main idea to guide us, instead of writing every detailed aspect of a source document.ßdummyTurker
4685ß0.0ßIn Table 3, we give an overall comparison of baselines and the proposed model.ßdummyTurker
4686ß0.0ßSpecifically, the s2s+att-BiLSTM is actually the classic variant of sequence-to-sequence model(s2s), and it is exactly the counterpart model without topic modelling to the GLEAM.ßdummyTurker
4687ß0.0ßAccording to the comparison pair, we can find the improvement to validate our model.ßdummyTurker
4688ß0.0ßCopy mechanism is implemented for solving the problem of generating <UNK>(unknown words) in the s2s models.ßdummyTurker
4689ß0.0ßIt copies words from source documents or reference when it finds <UNK>.ßdummyTurker
4690ß0.0ßWe currently didn’t add this mechanism.ßdummyTurker
4691ß0.0ßNow, the coping effect is mainly because of the benefits of the s2s model.ßdummyTurker
4692ß0.0ßThanks for your advice.ßdummyTurker
4693ß0.0ßWe may pay attentions to prove the high-level of topic meaning, s.t., whether it is right or wrong (i.e., case 2), or whether the expression is succinct or not (i.e., case 1) in Section 6.3.ßdummyTurker
4694ß0.0ßBut we ignored the high-level of word.ßdummyTurker
4695ß0.0ßPlease find the following two cases to see the benefits of the GLEAM on this point, and also find its limitation.ßdummyTurker
4696ß0.0ßIn case 1, the baseline s2s model just reorganizes the order of words of the source document and generate a headline for it.ßdummyTurker
4697ß0.0ßWe can find that reference headline uses “deplores” as an abstractive word to express what amnesty have done, and headline generated by GLEAM uses “warns”.ßdummyTurker
4698ß0.0ßBoth “deplores” and “warns” are not exactly words in the source document, but they all express the meaning since reading the whole document.ßdummyTurker
4699ß0.0ßSpecifically, in our model, incorporating a topic representation of the document helps to do this.ßdummyTurker
4700ß0.0ßHowever, the proposed GLEAM also has certain limitations on generating abstractive words.ßdummyTurker
4701ß0.0ßFor example, “beckons” is used by humanly written headline in the following Case 2.ßdummyTurker
4702ß0.0ßIt’s kind of an elegant way of description.ßdummyTurker
4703ß0.0ßThe GLEAM cannot do it, but generates abstractive words like “vie for” that closes to the fact description.ßdummyTurker
4704ß0.0ßSource: arbitrary arrests, torture, prisoners dying in detention and the death penalty are current practices in guinea, human rights organization amnesty international said thursday in a report published here.ßdummyTurker
4705ß0.0ßReference: amnesty deplores human rights violations in guinea.ßdummyTurker
4706ß0.0ßs2s baseline: amnesty international says guinea of human rights abuses in guinea.ßdummyTurker
4707ß0.0ßGLEAM: amnesty warns of human rights abuses in guinea.ßdummyTurker
4708ß0.0ßCase 2.ßdummyTurker
4709ß0.0ßSourse: americans vote in an election of rare historic potential tuesday, with front - running democrat barack obama seeking to become the first black president and republican john mccain hoping for a UNK comeback.ßdummyTurker
4710ß0.0ßReference: history beckons obama mccain on election day.ßdummyTurker
4711ß0.0ßs2s baseline: obama seeks to become first black white house candidate mccain.ßdummyTurker
4712ß0.0ßGLEAM: obama mccain vie for president in historic election.ßdummyTurker
4713ß0.0ßThe best baseline SEASS is very state-of-the-art which was published in ACL2017.ßdummyTurker
4714ß0.0ßEven the improvement compared with the SEASS is not that significant, the proposed GLEAM model is significantly outperforming the counterpart baseline s2s+att-BiLSTM and other recent baselines.ßdummyTurker
4715ß0.0ßThanks for the reviewer again to notice that the cases in Table 4 are not sufficiently proving our declaration in the paper.ßdummyTurker
4716ß0.0ßWe will add the above cases if our paper is accepted.ßdummyTurker
4717ß0.0ßWe have covered those papers tightly related to our work including some papers published by Nianwen Xue.ßdummyTurker
4718ß0.0ßThe criterion of segmenting discourse in the paper “PDTB-style Discourse Annotation of Chinese Text” you mentioned is different from the Rhetorical Structure Theory (RST) and it doesn’t take no-punctuation EDU boundaries to consideration in Chinese.ßdummyTurker
4719ß0.0ßWe annotated our Chinese data based on RST and English training data are also based on RST.ßdummyTurker
4720ß0.0ßThis Discourse tagging Reference manual published by Radu Soricut and Daniel Marcu has been covered in our paper.ßdummyTurker
4721ß0.0ßShuyuan Cao and Nianwen Xue’s work based on RST is also cited in our paper.ßdummyTurker
4722ß0.0ßSince Penn Discourse Treebank is not based on RST, we don’t cover it.ßdummyTurker
4723ß0.0ßHowever, we will consider your advice and add these work you mentioned into related work.ßdummyTurker
4724ß0.0ßWe annotated our Chinese data based on Rhetorical Structure Theory (RST) and English training data are also based on RST.ßdummyTurker
4725ß0.0ßThe Discourse tagging Reference manual published by Radu Soricut and Daniel Marcu has been covered in our paper.ßdummyTurker
4726ß0.0ßWe will consider your advice and show some criteria more explicitly in our paper.ßdummyTurker
4727ß0.0ßAs shown in figure 5, when the size of labeled data is growing, the performance gap between our OWL model and the NoAdvers-L model is shrinking.ßdummyTurker
4728ß0.0ßI think with more data, both our OWL model and the NoAdvers-L model may perform well.ßdummyTurker
4729ß0.0ßIt’s better to use supervised methods because adversarial training consumes more time.ßdummyTurker
4730ß0.0ßOur model suits to the situation there are no or very little resourcesßdummyTurker
4731ß0.0ßWZL and WLU is the situation when there is no labeled Chinese data and there is only a little labeled Chinese data.ßdummyTurker
4732ß0.0ßDue to the small quantity of Chinese training data the sore of WZL(82.35%) and the sore of WLU(87.17%) are different enough.ßdummyTurker
4733ß0.0ßOur model only suits to the situation when there are no or very little resources as shown in figure 5.ßdummyTurker
4734ß0.0ßI apologize for all formatting issues and will revise them.ßdummyTurker
4735ß0.0ßWe annotated our Chinese data based on Rhetorical Structure Theory (RST).ßdummyTurker
4736ß0.0ßReported speech is only a small part of its criteria.ßdummyTurker
4737ß0.0ßQuotation marks appear not very often in the data sets.ßdummyTurker
4738ß0.0ßIn most cases, reported speech does not appear with quotation marks.ßdummyTurker
4739ß0.0ßAnd there are more cases where quotation marks represent proper noun rather than Reported speech, harming the segmentation results.ßdummyTurker
4740ß0.0ßSo I think it won’t improve the performance.ßdummyTurker
4741ß0.0ßWhen there is very little data, the improvements are statistically significant NoAdvers-L vs OWL as figure 5 shows.ßdummyTurker
4742ß0.0ßWith more data, both our OWL model and the NoAdvers-L model may perform well.ßdummyTurker
4743ß0.0ßWLU and OWL are both adversarial models.ßdummyTurker
4744ß0.0ßWe find WLU has more hyper-parameters and is harder to train.ßdummyTurker
4745ß0.0ßWhen there are 400 labeled Chinese sentences to train, we find OWL performs a little bit better and easier to train, so we use OWL to comprise with NoAdvers-L in Figure 5.ßdummyTurker
4746ß0.0ßWZL and WLU is the situation when there is no labeled Chinese data and there is only a little labeled Chinese data.ßdummyTurker
4747ß0.0ßDue to the small quantity of Chinese training data the sore of WZL(82.35%) and the sore of WLU(87.17%) are different enough.ßdummyTurker
4748ß0.0ßWe find the adversarial network we designed can effectively extract more common features of English and Chinese, which can be verified by the comparison of NoAdvers-Z and WZL.ßdummyTurker
4749ß0.0ßFor cross-lingual neural network models, bilingual word embeddings are the basis of the experiment.ßdummyTurker
4750ß0.0ßLike the work “Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources” and “Adversarial deep averaging networks for cross-lingual sentiment classification”, only bilingual word embeddings are leveraged.ßdummyTurker
4751ß0.0ßWe just add Universal POS tag to try if the performance can be improved.ßdummyTurker
4752ß0.0ßWithout the bilingual embedding, I think it’s even impossible to gain the performance of 79.40%.ßdummyTurker
4753ß0.0ßWhat we try solve is the situation there are no or very little resources, and as shown in figure 5, when the size of labeled data is growing, the performance gap between our OWL model and the NoAdvers-L model is shrinking.ßdummyTurker
4754ß0.0ßI think with more data, both our OWL model and the NoAdvers-L model may perform well.ßdummyTurker
4755ß0.0ßIt’s better to use supervised methods because adversarial training consumes more time.ßdummyTurker
4756ß0.0ßThere are actually 782 sentences we annotated, 200 for validation and 182 for test.ßdummyTurker
4757ß0.0ßI haven’t put all these sentences and EDUs in the attached dataset.ßdummyTurker
4758ß0.0ßIf needed, I can submit them.ßdummyTurker
4759ß0.0ßI apologize for wrongly showing the size of labeled data in Table 1, it should be 782.ßdummyTurker
4760ß0.0ßI apologize for all formatting issues and will revise them.ßdummyTurker
4761ß0.0ßThank you for your valuable comments and suggestions.ßdummyTurker
4762ß0.0ßIn this paper, we just give a preliminary experiment to verify the computability of our corpus and will focus on the macro discourse analysis in our future work.ßdummyTurker
4763ß0.0ßWe will further improve the organizational structure and written of our paper, making it easier understood.ßdummyTurker
4764ß0.0ßDue to the space limitation, we used a simplified version of chtb0155 in the paper.ßdummyTurker
4765ß0.0ßIf we have an additional page, we will give a full version in example 1.ßdummyTurker
4766ß0.0ßThe elementary discourse unit in macro Chinese discourse structure is a paragraph (not a sentence).ßdummyTurker
4767ß0.0ßTherefore, the syntactic information is not used in our annotation and experiment.ßdummyTurker
4768ß0.0ßMaybe the following explanation will be clearer：” We select the first conjunction and its POS in the first sentence of the discourse unit.ßdummyTurker
4769ß0.0ß” That is to say, it is the POS of the conjunction and not the POS of the first sentence.ßdummyTurker
4770ß0.0ßThe method of semantic similarity calculation we use is as follows：As showed in Equation (1), the semantic similarity of two words is defined as the cosine similarity that is Similarity (W_i, W_j), where V_i and V_j are the word vectors of the word W_i and W_j obtained through the W2VCTB model (trained in CTB by Word2Vec).ßdummyTurker
4771ß0.0ßAs showed in Equation (2), we define the maximum mapping similarity of the nth word in the ith discourse unit to the jth discourse unit as MaxSim_{inj}.ßdummyTurker
4772ß0.0ßAs showed in Equation (3), the semantic similarity between the ith discourse unit and the jth discourse unit is the Score.ßdummyTurker
4773ß0.0ßSimilarity(w_i,w_j)=(V_i*V_j)/(|v_i|*|v_j|)            Equation(1)ßdummyTurker
4774ß0.0ßMaxSim_{inj}=maxSimilarity(W_{in},W_k)  W_k ∈C_j  Equation(2)ßdummyTurker
4775ß0.0ßScore= (Sum_{n=1}^{|C_i|}MaxSim_{inj}+ Sum_{m=1}^{|C_j|}MaxSim_{imj})/(|C_i|+|C_j|) Equation(3)ßdummyTurker
4776ß0.0ßWe will add the above sentences to our revised version.ßdummyTurker
4777ß0.0ßThank you for your valuable comments and suggestions.ßdummyTurker
4778ß0.0ßWe will ask a native speaker to improve the English written.ßdummyTurker
4779ß0.0ßYes.ßdummyTurker
4780ß0.0ßWe have developed a macro Chinese discourse annotation platform with visual interface and will release our annotation platform in the future.ßdummyTurker
4781ß0.0ßThank you for your valuable comments and suggestions.ßdummyTurker
4782ß0.0ßThe macro discourse analysis is to understand the high–level structure of a text and is the foundation of natural language understanding.ßdummyTurker
4783ß0.0ßMacro-level discourse information is also helpful for any NLP applications and a detailed example is the example 1 in the paper (Lines 220-230, page 2):ßdummyTurker
4784ß0.0ß“Based on the discourse structure tree showed in Figure 1, the article of Example 1 is easy to be understood.ßdummyTurker
4785ß0.0ßIn the task of automatic summarization, for example, according to this discourse structure tree and the directed edges from the root node to the leaf nodes, the topic sentences of the leaf nodes can be used as the text summary.ßdummyTurker
4786ß0.0ßThe summary of Example 1 is a combination of the first paragraph and the second paragraph’s topic sentence, which is more appropriate than the simple use of the first paragraph as a summary of the full text.ßdummyTurker
4787ß0.0ß”ßdummyTurker
4788ß0.0ßThe element discourse unit in RST and PTDB is sentence-level or clause-level, they are the corpora on micro discourse structure, and do not annotate the macro discourse structure on paragraph-level and other macro discourse information (such as the topic sentence of paragraph, discourse summary, discourse theme and the pragmatic function of the discourse unit.ßdummyTurker
4789ß0.0ß).ßdummyTurker
4790ß0.0ßThe corpus mentioned in Zhou and Xue's work (2015) did not annotate the structure and relations among different paragraph and it is a sentence-level micro Chinese Discourse Treebank, not a macro Chinese discourse corpus.ßdummyTurker
4791ß0.0ßThank you for suggesting feature ablation for the RST features.ßdummyTurker
4792ß0.0ßIf the paper is accepted, we will add it.ßdummyTurker
4793ß0.0ßThank you for suggesting feature ablation for the RST features.ßdummyTurker
4794ß0.0ßIf the paper is accepted, we will add it (also suggested by reviewer#1).ßdummyTurker
4795ß0.0ßThe discourse structure of short sentences is usually simple (only one elemental discourse unit), so discourse structures is more beneficial with long sentences.ßdummyTurker
4796ß0.0ßWe decided to report results using all sentences in PB-FOC to compare with others and for completeness purposes.ßdummyTurker
4797ß0.0ßWe will clarify this, thank you.ßdummyTurker
4798ß0.0ßThe window size was tuned using cross-validation with train and development sets.ßdummyTurker
4799ß0.0ßThe corpus we work with, PB-FOC, annotates as focus the most likely semantic role (Section 3 in the paper).ßdummyTurker
4800ß0.0ßWe approach focus prediction in the same fashion.ßdummyTurker
4801ß0.0ßWe will consider the effect of implicit discourse relations, thank you.ßdummyTurker
4802ß0.0ßWe will sort Table 1 as suggested, thank you.ßdummyTurker
4803ß0.0ßThank you for pointing out the typos.ßdummyTurker
4804ß0.0ßWe greatly appreciate the helpful comments made by the reviewers regarding this paper.ßdummyTurker
4805ß0.0ßWe are sorry but we cannot perform additional experiments on other language because we could not find or purchase proper corpus of other languages.ßdummyTurker
4806ß0.0ßWe will revise the paper title according to your suggestion.ßdummyTurker
4807ß0.0ß* To give readers confidence on experimental results, we are performing additional experiments using 20,760 chat sentences (18,984 sentences for training and 1,776 sentences for test) which are pronounced by 9 students.ßdummyTurker
4808ß0.0ßIn the prelimnary experiments, the proposed model outperformed Baseline, Gold, and Hori’s Gold.ßdummyTurker
4809ß0.0ßWe will add the final results in the camera-ready paper.ßdummyTurker
4810ß0.0ßWe hope to show you these results.ßdummyTurker
4811ß0.0ßThank you for your good suggestion.ßdummyTurker
4812ß0.0ßWe will upload our system and experimental data to Github.ßdummyTurker
4813ß0.0ßWe are sorry for the inconvenience.ßdummyTurker
4814ß0.0ßWe will revise it.ßdummyTurker
4815ß0.0ßWe are sorry for the mistake.ßdummyTurker
4816ß0.0ßWe will revise it.ßdummyTurker
4817ß0.0ßWe greatly appreciate the helpful comments made by the reviewers regarding this paper.ßdummyTurker
4818ß0.0ßWe took some mistakes.ßdummyTurker
4819ß0.0ßWe will revise them.ßdummyTurker
4820ß0.0ßWe will survey some previous works, as follows.ßdummyTurker
4821ß0.0ßAtsunori Ogawa, Takaaki Hori: ASR error detection and recognition rate estimation using deep bidirectional recurrent neural networks.ßdummyTurker
4822ß0.0ßICASSP 2015: 4370-4374.ßdummyTurker
4823ß0.0ßB. Logan, P. Moreno, and O. Deshmukh, “Word and sub-word indexing approaches for reducing the effects of oov queries on spoken audio,” in Proc.ßdummyTurker
4824ß0.0ßHLT, San Diego, CA, USA, 2002, pp.ßdummyTurker
4825ß0.0ß31–35.ßdummyTurker
4826ß0.0ß* To give readers confidence on experimental results, we are performing additional experiments using 20,760 chat sentences (18,984 sentences for training and 1,776 sentences for test) which are pronounced by 9 students.ßdummyTurker
4827ß0.0ßIn the prelimnary experiments, the proposed model outperformed Baseline, Gold, and Hori’s Gold.ßdummyTurker
4828ß0.0ßWe will add the final results in the camera-ready paper.ßdummyTurker
4829ß0.0ßWe hope to show you these results.ßdummyTurker
4830ß0.0ßThank you for your good comment.ßdummyTurker
4831ß0.0ßBased on a domain-specific language model, our model can generate words unrecognized by ASR engines.ßdummyTurker
4832ß0.0ßThat can be dependent or independent because our model is a generation model.ßdummyTurker
4833ß0.0ßWe greatly appreciate the helpful comments made by the reviewers regarding this paper.ßdummyTurker
4834ß0.0ßWithout your comments, we may miss the important previous work.ßdummyTurker
4835ß0.0ßWe will add the work as a reference paper.ßdummyTurker
4836ß0.0ßFiscus’s model is different from our model in the point that his model needs multiple ASR engines.ßdummyTurker
4837ß0.0ßIn commercial service, it is not easy to use multiple ASR engines because of response speed and purchase cost.ßdummyTurker
4838ß0.0ßSo, we think that our model has the merits such as higher speed and lower cost.ßdummyTurker
4839ß0.0ßMoreover, based on a domain-specific language model, our model can generate words unrecognized by ASR engines.ßdummyTurker
4840ß0.0ßWe hope to show you the performance differences between our model and Fiscus’s model in the camera-ready paper.ßdummyTurker
4841ß0.0ß* To give readers confidence on experimental results, we are performing additional experiments using 20,760 chat sentences (18,984 sentences for training and 1,776 sentences for test) which are pronounced by 9 students.ßdummyTurker
4842ß0.0ßIn the prelimnary experiments, the proposed model outperformed Baseline, Gold, and Hori’s Gold.ßdummyTurker
4843ß0.0ßWe will add the final results in the camera-ready paper.ßdummyTurker
4844ß0.0ßWe hope to show you these results.ßdummyTurker
4845ß0.0ßWe are sorry for the mistake.ßdummyTurker
4846ß0.0ßAs shown in the paper, Hori’s model is a selection model, but our model is a generation model.ßdummyTurker
4847ß0.0ßSo, based on a domain-specific language model, our model can generate words unrecognized by ASR engines.ßdummyTurker
4848ß0.0ßThanks a lot for your review, we really appreciate your valuable comments.ßdummyTurker
4849ß0.0ßWe agree.ßdummyTurker
4850ß0.0ßHowever, to our knowledge this is the first work to learn to combine them like this, and to analyze the combination in this fashion.ßdummyTurker
4851ß0.0ßSee also our response to R1.ßdummyTurker
4852ß0.0ßThis would indeed be very interesting, and we point out that we do indeed cite ELMO.ßdummyTurker
4853ß0.0ßWe refer to the ACL author guidelinesßdummyTurker
4854ß0.0ßYes, this is a good point, although the differences in speed are actually not as big as one might think.ßdummyTurker
4855ß0.0ßThe training time on SNLI per-epoch goes from 15 minutes to 19 minutes.ßdummyTurker
4856ß0.0ßThe model also takes a bit longer to converge, with differences in total training time around one hour.ßdummyTurker
4857ß0.0ßWe are currently exploring ways to speed it up by reducing the dimensionality of the LSTM input and making better use of appropriate CUDA kernels, but we don't think these differences are prohibitive.ßdummyTurker
4858ß0.0ßThanks a lot for your review, we really appreciate your valuable comments.ßdummyTurker
4859ß0.0ßThanks for the feedback.ßdummyTurker
4860ß0.0ßWe think, as also noted by R1, that there is a lot of value in analyzing the method in this kind of detail.ßdummyTurker
4861ß0.0ßWe believe that the method opens up a lot of different areas for future exploration.ßdummyTurker
4862ß0.0ßAs evidenced by their citations and github stars, FastText and GloVe are the most popular embeddings.ßdummyTurker
4863ß0.0ßWe fully agree that there should be a more systematic study of different embeddings, possibly relating their performance to word-level benchmarks and downstream tasks.ßdummyTurker
4864ß0.0ßSuch a study, however, lies outside of the scope of the current work.ßdummyTurker
4865ß0.0ßWe will add this in, and also put confidence bounds on the accuracy numbers (repeating with different seeds and reporting mean and std performance).ßdummyTurker
4866ß0.0ßWe chose SNLI because it is one of the biggest text classification datasets and MultiNLI because it allowed us to test our hypothesis that embeddings from different domains are preferred for different downstream domains.ßdummyTurker
4867ß0.0ßWe added sentiment analysis because we wanted to show, as R1 also noted, that the results also apply to non-NLI tasks.ßdummyTurker
4868ß0.0ßBoth NLI and SA are important and canonical NLP tasks.ßdummyTurker
4869ß0.0ßThanks a lot for your review, we really appreciate your valuable comments.ßdummyTurker
4870ß0.0ßWe agree.ßdummyTurker
4871ß0.0ßHowever, to our knowledge this is the first work to learn to combine them like this, and to analyze the combination in this fashion.ßdummyTurker
4872ß0.0ßThe method does outperform naive concatenation.ßdummyTurker
4873ß0.0ß"Within this model class, where there is no shared access between the premise and hypothesis, the method does perform at the state of the art (see the distinction made on the SNLI leaderboard between ""sentence encoding-based methods"" and ""other neural network models"")."ßdummyTurker
4874ß0.0ßThe increase in the number of word embeddings means we can use qualitatively different information from different sources and models.ßdummyTurker
4875ß0.0ßAs such, the method can be thought of as an ensemble, but with added interpretability.ßdummyTurker
4876ß0.0ßWe experimented with increasing the word embedding dimensionality, comparing with 600-dimensional fasttext embeddings as well as having two fasttext embeddings, but this did not improve performance.ßdummyTurker
4877ß0.0ßAs our analysis shows, different embeddings capture different things and are useful in different contexts, which is exactly what our model exploits.ßdummyTurker
4878ß0.0ßThe rationale behind use of question-option tuple based architecture as already mentioned in paper is to obtain complete context of question with option.ßdummyTurker
4879ß0.0ßAccording to our understanding, as explained in the paper, in order to capture structural information rather than sequential, so we employed CNN based architecture.ßdummyTurker
4880ß0.0ßIt is further mentioned in the paper, sentence level attention was used since it helps the model to answer look-up questions.ßdummyTurker
4881ß0.0ßSeveral approaches are possible for handling special questions, currently we have tried one and some more can be explored as an extension to this work.ßdummyTurker
4882ß0.0ßThe best baselines we have used present the state of the art results for SciQ and TQA dataset.ßdummyTurker
4883ß0.0ßHence, these are the natural choice as baselines when we have reported resulsts on SciQ and TQA.ßdummyTurker
4884ß0.0ßWe have compared the CNN model in two different settings and shown that using smaller filter size leads to better performance.ßdummyTurker
4885ß0.0ßWe have also shown that our proposed model outperforms GRU based baseline on the two datasets.ßdummyTurker
4886ß0.0ßGiven the space constraint, we couldn’t do any extensive qualitative error analysis.ßdummyTurker
4887ß0.0ßHowever, in the conclusions and future work section, we have mentioned that our model gives near-random accuracy for questions where complex reasoning is needed.ßdummyTurker
4888ß0.0ßWe were unable to provide examples due to space constraint.ßdummyTurker
4889ß0.0ßWe agree with your point that for some comparisons, the differences in accuracy is not significant.ßdummyTurker
4890ß0.0ßHowever, our proposed model shows significant improvement on the SciQ dataset and outperforms the state of the art results reported in Kembhavi et.ßdummyTurker
4891ß0.0ßal.ßdummyTurker
4892ß0.0ßfor the TQA dataset.ßdummyTurker
4893ß0.0ßFor multiple choice question answering datasets like SciQ and TQA, only one of the options is correct.ßdummyTurker
4894ß0.0ßSo generating multiple answers may not have any usefulness.ßdummyTurker
4895ß0.0ßHence, our approach is based on accuracy instead of multi-answer list.ßdummyTurker
4896ß0.0ßAlso our model assigns a score to each question-option tuple which is equivalent to ranking each option.ßdummyTurker
4897ß0.0ßThe AS Reader is not performing better than ours.ßdummyTurker
4898ß0.0ßThe AS Reader model from Kadlec et al attains 74.1 % accuracy on the SciQ test set whereas our proposed model attains 84.7 % accuracy on the same dataset.ßdummyTurker
4899ß0.0ßWe wanted to point out that since the two models are using different passages, direct comparison of the two models is unfair.ßdummyTurker
4900ß0.0ßWe agree that running our proposed model on the same text passage as AS Reader model could be a possible extension of the present work.ßdummyTurker
4901ß0.0ßThe SciQ dataset didn’t have any questions with forbidden options.ßdummyTurker
4902ß0.0ßWhereas, in the validation set of TQA, 433 out of 1530 questions had forbidden options.ßdummyTurker
4903ß0.0ßUsing the threshold strategy, our proposed model gets 188 out of 433 questions correct.ßdummyTurker
4904ß0.0ßWithout using this strategy and giving every question-option tuple as input to the model, our proposed model gets 109 out of 433 questions correct.ßdummyTurker
4905ß0.0ßWe agree that we should have given this detail.ßdummyTurker
4906ß0.0ßIf accepted, we will put this details by squeezing some other part in the camera-ready version.ßdummyTurker
4907ß0.0ß"Yes, in equation 4,5,6,7,8,9,10,  the letter ""j"" represents the number of different slots."ßdummyTurker
4908ß0.0ßWe used the different learnable parameters in state output and state update layer to perform the transformation.ßdummyTurker
4909ß0.0ßOn the other hand, in the loss function，we summed it up.ßdummyTurker
4910ß0.0ßFrom another point of view, it can be regarded as multi-task learning that the underlying representation of raw text transformed by the shared parameter.ßdummyTurker
4911ß0.0ßBy the way, thank you for pointing out the grammatical errors and we will make a positive correction.ßdummyTurker
4912ß0.0ßThank you for your careful review, To our knowledge, The state updater is the first automatical adjusted gate in dialogue state tracking.ßdummyTurker
4913ß0.0ßCompared with the state of the art model(NBT) that adopts rule-based update.ßdummyTurker
4914ß0.0ßFrom the case analysis in our experiments, the gate of GSTN has interpretability, which conforms to our intuitive understanding during the dialogue.ßdummyTurker
4915ß0.0ßMoreover, the proposed end to end model makes computation reduced by disregarding pre-processing.ßdummyTurker
4916ß0.0ßWe can make the model from original input to final output, giving the model more space to adjust automatically according to the data.ßdummyTurker
4917ß0.0ßThus, We think the strongest advantage is that it can simplify the process and reduce human customization, such as preprocessing or rule-based updater which makes models have less integrity and flexibility.ßdummyTurker
4918ß0.0ßThank you for your advisement.ßdummyTurker
4919ß0.0ßDue to the reason of space, we haven't added the comparison of computing performance.ßdummyTurker
4920ß0.0ßWe want to express that this model discarded preprocessing steps, and reduced the computation.ßdummyTurker
4921ß0.0ßCompared with the network architecture of the state of the art model(NBT), our model does not add other modules and increase computations.ßdummyTurker
4922ß0.0ßThe different points that need to be highlighted in this paper are that we replace rule-based updater to adjust-updater, and give up pre-processing.ßdummyTurker
4923ß0.0ßIn the conclusion section, we have some misuse of words, and we're going to make a positive change.ßdummyTurker
4924ß0.0ßAlso, we will add the computational performance comparison.ßdummyTurker
4925ß0.0ßThanks for your comments.ßdummyTurker
4926ß0.0ßIn fact, the only similarity is that SCLM and CNN_CLM both use highway network.ßdummyTurker
4927ß0.0ßCNN_CLM uses CNN and Max pooling to extract character n-grams feature about the word and regards it as word representation.ßdummyTurker
4928ß0.0ßSCLM concatenates the character embedding and uses highway network to construct word representation.ßdummyTurker
4929ß0.0ßWhat’s more, the experiments on TCLM can also prove the effectiveness of the word representation extracted by SCLM.ßdummyTurker
4930ß0.0ßHowever, in my opinion, CNN-CLM may have limitation in extending to a tied character language model.ßdummyTurker
4931ß0.0ßThanks for your comments.ßdummyTurker
4932ß0.0ßActually, we have re-implemented the CNN-CLM on other two datasets but the result is unsatisfactory.ßdummyTurker
4933ß0.0ßSince the CNN-CLM has too much hyper-parameters (filter numbers, filter sizes, strides, character embedding size and etc.)ßdummyTurker
4934ß0.0ßwhich are too difficult to be determined.ßdummyTurker
4935ß0.0ßAdditionally, the results of CNN-CLM (implemented by ourselves) on other two datasets may not be its best performance.ßdummyTurker
4936ß0.0ßIn this situation, we think the comparison between CNN-CLM and SCLM (TCLM) on other two datasets would be improper and the result would not be convincing.ßdummyTurker
4937ß0.0ßThis paper mainly aims to show that the word representations extracted by proposed models can work better than traditional word representations and have fewer parameters in the embedding layer.ßdummyTurker
4938ß0.0ßWhat’s more, the TCLM is the first attempt to explore the effectiveness of tied character language model and the results are good enough.ßdummyTurker
4939ß0.0ßSo we think the results of CNN-CLM on other two datasets are referable but not necessary.ßdummyTurker
4940ß0.0ßThanks for your question.ßdummyTurker
4941ß0.0ßIt has been shown in our paper that this encoding method is useful for small corpus.ßdummyTurker
4942ß0.0ßWe also make experiments on some larger corpus (wiki2 corpus, which is nearly three times larger than PTB) and the result is still good enough.ßdummyTurker
4943ß0.0ßActually, exploring the ability of this encoding method in larger corpus (like 1B dataset) is one of our future work.ßdummyTurker
4944ß0.0ßThe ability of our encoding method depends on the character embedding and highway layers.ßdummyTurker
4945ß0.0ßWe believe that our encoding method is capable of representing all words with larger character embedding size and larger highway network.ßdummyTurker
4946ß0.0ßThanks for your question.ßdummyTurker
4947ß0.0ßFor PTB dataset, in the training stage, one epoch of SCLM will take 110.94s and one epoch of TCLM will take 260.09s.ßdummyTurker
4948ß0.0ßThe experiment is conducted on a single GPU (P40).ßdummyTurker
4949ß0.0ßAlthough building word representation before every iteration will be time-consuming, the model can get a better performance and has fewer parameters.ßdummyTurker
4950ß0.0ßWe truly understand that an ACL conference paper needs comparison experiments against state-of-the-art models published in main research paper or journal.ßdummyTurker
4951ß0.0ßHowever, two external factors hinder the experiments from more suitable comparison.ßdummyTurker
4952ß0.0ßFirstly, it was from February 2017 that news stance detection of article bodies towards their headlines was studied as the key step of fake news detection, which means that there isn’t enough time for researchers to develop more well-performed models.ßdummyTurker
4953ß0.0ßSecondly, as far as we know, the Fake News Challenge round-1 participants haven’t transformed or enriched their models to academic publications in conferences or journals.ßdummyTurker
4954ß0.0ßThe adversarial model consists of two objectives -- one is to maximize the distribution discrepancy between the two classification layers and another is to minimize the loss of the classification at these two layers.ßdummyTurker
4955ß0.0ßBoth of these two objectives involve the parameter optimization of the feature layer, but in opposite optimization directions.ßdummyTurker
4956ß0.0ßThe two objectives play an adversarial role to each other for the feature layer optimization, and the optimization process won’t stop until a saddle point (the feature layer parameters can be applied in both objectives) is reached.ßdummyTurker
4957ß0.0ßTherefore, we call it an adversarial model.ßdummyTurker
4958ß0.0ßThe proposed model mainly consists of two designs that lead to improving the performance in empirical experiments.ßdummyTurker
4959ß0.0ßFirstly, the hierarchical structure guarantees that the minority class (e.g., the “disagree” samples, the amount of which is less than 2% in the total samples) won’t be directly compared/separated with the majority class (e.g., the “unrelated” samples, the amount of which is more than 70% of the total samples), which helps solve the severe class imbalance problem.ßdummyTurker
4960ß0.0ßThe second design is the regularization, which aims to maximize the separation between the majority class (i.e., “unrelated”) and the remaining three stances.ßdummyTurker
4961ß0.0ßMaximization of the regularization term makes the model be trained in an adversarial way.ßdummyTurker
4962ß0.0ßSince “agree”, “disagree” and “discuss” can be assumed as “related”, the regularization actually promotes the emergence of features that are useful in separating “agree”/ “unrelated”, “disagree”/ “unrelated”, “discuss”/ “unrelated” and “agree”/“disagree”/ “discuss”.ßdummyTurker
4963ß0.0ßThe proposed model mainly consists of two designs that lead to improving the performance in empirical experiments.ßdummyTurker
4964ß0.0ßFirstly, the hierarchical structure guarantees that the minority class (e.g., the “disagree” samples, the amount of which is less than 2% in the total samples) won’t be directly compared/separated with the majority class (e.g., the “unrelated” samples, the amount of which is more than 70% of the total samples), which helps solve the severe class imbalance problem.ßdummyTurker
4965ß0.0ßThe second design is the regularization, which aims to maximize the separation between the majority class (i.e., “unrelated”) and the remaining three stances.ßdummyTurker
4966ß0.0ßMaximization of the regularization term makes the model be trained in an adversarial way.ßdummyTurker
4967ß0.0ßSince “agree”, “disagree” and “discuss” can be assumed as “related”, the regularization actually promotes the emergence of features that are useful in separating “agree”/ “unrelated”, “disagree”/ “unrelated”, “discuss”/ “unrelated” and “agree”/“disagree”/ “discuss”.ßdummyTurker
4968ß0.0ßFake news is a more general application for stance detection.ßdummyTurker
4969ß0.0ßFirstly, news articles whose headlines are “unrelated” with or “disagree” the article body, tend to originate from untrustworthy websites.ßdummyTurker
4970ß0.0ßThese websites are more likely to manipulate and spread fake news.ßdummyTurker
4971ß0.0ßSecondly, the stance detection task can be generalized to two pieces of text, with one as a news article and the other as the evidence drawn from the Internet to help verify the news truthiness.ßdummyTurker
4972ß0.0ßTherefore, we introduce fake news detection at the beginning of this paper.ßdummyTurker
4973ß0.0ßThe adversarial model has two objectives -- one is to maximize the distribution discrepancy between the two classification layers and another is to minimize the loss of the classification at these two layers.ßdummyTurker
4974ß0.0ßBoth of these two objectives involve the parameter optimization of the feature layer, but in opposite optimization directions.ßdummyTurker
4975ß0.0ßThe two objectives play an adversarial role to each other for the feature layer optimization, and the optimization process won’t stop until a saddle point (the feature layer parameters can be applied in both objectives) is reached.ßdummyTurker
4976ß0.0ßTherefore, we call it an adversarial model.ßdummyTurker
4977ß0.0ßMaximization of distribution discrepancy means simpler classifiers can achieve more decent “related”/ “unrelated” classification results.ßdummyTurker
4978ß0.0ßWithout regularization, the classification accuracy for “agree”, “disagree”, “discuss” and “unrelated” are 61.34%, 42.93%, 59.38% and 99.05%, respectively.ßdummyTurker
4979ß0.0ßAll stances except “unrelated” have dramatic decreases in terms of evaluation performance.ßdummyTurker
4980ß0.0ßThe main idea is to divide a task into subtasks that are organized in a hierarchical structure, and design constraints (as the regularization in the objective function) between subtasks to make the parameter training more sensible.ßdummyTurker
4981ß0.0ßNLP tasks like information extraction and sentence generation may be inspired from the idea.ßdummyTurker
4982ß0.0ßFor example, when a machine learning model intends to generate a sentence, the first subtask is to generate a sentence skeleton, and the subsequent subtasks are to generate some details to enrich the skeleton.ßdummyTurker
4983ß0.0ßCertainly, there should be some connections or constraints between the subtasks.ßdummyTurker
4984ß0.0ßThe most influential hyper parameters are $\alpha$ and $\beta$, with the former controlling the relative importance of the more fine-grained classification and the latter leveraging the regularization.ßdummyTurker
4985ß0.0ß$\alpha$ is searched in the range between 0.1 and 3 (i.e., the ratio of the “unrelated” sample amount over the “related” sample amount) with step of 0.1.ßdummyTurker
4986ß0.0ßTypically, the overall performance improves quickly as $\alpha$ increases and peaks at 1.5.ßdummyTurker
4987ß0.0ßThen the metric experiences a slight decrease when $\alpha$ is from 1.5 to 3.ßdummyTurker
4988ß0.0ß$\beta$ is searched in {0.1, 0.01, 0.001, 0.0001, 0.00001 and 0}.ßdummyTurker
4989ß0.0ßIt is observed that the performance is much higher when $\beta$ is set to 0.01 and 0.001 than other values, although 0.001 has a moderate advantage over 0.01.ßdummyTurker
4990ß0.0ßThe learning is another important hyper parameter, and we set it to 0.0001.ßdummyTurker
4991ß0.0ßIncreasing the learning rate could lead to a huge performance decrease.ßdummyTurker
4992ß0.0ßThis is probably because the model is complicated and there exists many local minimums.ßdummyTurker
4993ß0.0ßOther hyper parameters such as mini-batch size, the number of training epochs exert less influence on the final model performance.ßdummyTurker
4994ß0.0ßWe conducted experiments without regularization, i.e., $\beta=0$.ßdummyTurker
4995ß0.0ßThe classification accuracy for “agree”, “disagree”, “discuss” and “unrelated” are 61.34%, 42.93%, 59.38% and 99.05%, respectively.ßdummyTurker
4996ß0.0ßAll stances except “unrelated” have apparent decreases in classification accuracy.ßdummyTurker
4997ß0.0ßOur hierarchical structure guarantees that the minority class (e.g., the “disagree” samples, the amount of which is less than 2% in the total samples) won’t be directly compared/separated with the majority class (i.e., the “unrelated” samples, the amount of which is more than 70% in the total samples), which helps solve the severe class imbalance problem.ßdummyTurker
4998ß0.0ßOne reason that “agree” has better classification results than “disagree” is that more data samples are used for training the “agree” class than those for “disagree”.ßdummyTurker
4999ß0.0ßThis means that “agree” could suffer less by the class imbalance problem that all the baseline systems did not consider.ßdummyTurker
5000ß0.0ßWithout regularization, the classification accuracy for “agree”, “disagree”, “discuss” and “unrelated” are 61.34%, 42.93%, 59.38% and 99.05%, respectively.ßdummyTurker
5001ß0.0ßAll stances except “unrelated” have dramatic decreases in terms of evaluation performance.ßdummyTurker
5002ß0.0ßMaximization of the regularization term makes the model train in an adversarial way.ßdummyTurker
5003ß0.0ßSince “agree”, “disagree” and “discuss” can be assumed as “related”, the regularization actually promotes the emergence of features that are useful in separating “agree”/ “unrelated”, “disagree”/ “unrelated”, “discuss”/ “unrelated” and “agree”/“disagree”/”discuss”.ßdummyTurker
5004ß0.0ßThe broad intuition of dealing with polysemy can be divided into three parts: First, extract the most common semantic information for each word.ßdummyTurker
5005ß0.0ßSecond, generate context-specific vectors for each word based on the common semantic information, which can also learn more context-specific information during training process.ßdummyTurker
5006ß0.0ßThird, choose the proper CWE according to the different context information.ßdummyTurker
5007ß0.0ßFollowing the intuition, we design the CDNN structure as follows:ßdummyTurker
5008ß0.0ßFirst, we apply the convolution layer and max-pooling layer for extracting the common semantic information for each word.ßdummyTurker
5009ß0.0ßThe width of the 1d-convolution layer is set as 1 and the pooling operation is applied over different kernels.ßdummyTurker
5010ß0.0ßThe max-pooling operation is to compare among kernels and to extract the most significant features with highly activate value for each single word.ßdummyTurker
5011ß0.0ßIt is the only reasonable operation for extracting the most common semantic information for every single word.ßdummyTurker
5012ß0.0ßSecond, generate context-specific vectors for each word using the deconvolution layer and select the proper CWE according to the context vector.ßdummyTurker
5013ß0.0ßThe deconvolution layer can extend the input information and learn different information using different deconvolution kernels.ßdummyTurker
5014ß0.0ßAs we use the context representation vector for selecting the proper CWE vectors, the spatial resolution gradually increases with the deconvolution operation proceeding, and the parameters of deconvolution kernels will be trained to distinguish and update the context-specific word vectors for every single word during the selecting process.ßdummyTurker
5015ß0.0ßThank you for your valuable comment.ßdummyTurker
5016ß0.0ßWe will describe the basics of convolution more concisely in the camera-ready version.ßdummyTurker
5017ß0.0ßThank you for your valuable comment.ßdummyTurker
5018ß0.0ßWe will introduce the broad intuition before diving into the describing the neural network in the camera-ready version.ßdummyTurker
5019ß0.0ßThank you for your valuable comment.ßdummyTurker
5020ß0.0ßWe are sorry for these mistakes and will correct it in the camera-ready version.ßdummyTurker
5021ß0.0ßThe AWE-CNN corresponds to the bar of the highest accuracy.ßdummyTurker
5022ß0.0ßThe broad intuition of dealing with polysemy can be divided into three parts: First, extract the most common semantic information for each word.ßdummyTurker
5023ß0.0ßSecond, generate context-specific vectors for each word based on the common semantic information, which can also learn more context-specific information during training process.ßdummyTurker
5024ß0.0ßThird, choose the proper CWE according to the different context information.ßdummyTurker
5025ß0.0ßFollowing the intuition, we design the CDNN structure as follows:ßdummyTurker
5026ß0.0ßFirst, we apply the convolution layer and max-pooling layer for extracting the common semantic information for each word.ßdummyTurker
5027ß0.0ßThe width of the 1d-convolution layer is set as 1 and the pooling operation is applied over different kernels.ßdummyTurker
5028ß0.0ßThe max-pooling operation is to compare among kernels and to extract the most significant features with highly activate value for each single word.ßdummyTurker
5029ß0.0ßIt is the only reasonable operation for extracting the most common semantic information for every single word.ßdummyTurker
5030ß0.0ßSecond, generate context-specific vectors for each word using the deconvolution layer and select the proper CWE according to the context vector.ßdummyTurker
5031ß0.0ßThe intuition of using the deconvolution layer for generation context-specific vectors is inspired by the usage of the deconvolution in up-sampling process.ßdummyTurker
5032ß0.0ßThe deconvolution layer can extend the input information and learn different information using different deconvolution kernels.ßdummyTurker
5033ß0.0ßAs we use the context representation vector for selecting the proper CWE vectors, the spatial resolution gradually increases with the deconvolution operation proceeding, and the parameters of deconvolution kernels will be trained to distinguish and update the context-specific word vectors for every single word during the selecting process.ßdummyTurker
5034ß0.0ßThank you for your time and the great suggestions.ßdummyTurker
5035ß0.0ßRe: The choice of dimensions for M\lambda and U:ßdummyTurker
5036ß0.0ßThe first dimension of M\lambda is set to be the same as the dimensionality of [V_w1; V_w2].ßdummyTurker
5037ß0.0ßSimilarly, the first and second dimensions of U are set to be the same as the dimensionality of [V_y; V_r].ßdummyTurker
5038ß0.0ßBoth of the second dimension of M\lambda and the third dimension of U can be viewed as hyperparameters and are usually set within [d : 2d].ßdummyTurker
5039ß0.0ßIn our experiments, we set them as 2d.ßdummyTurker
5040ß0.0ßWe will elaborate and discuss these in the paper.ßdummyTurker
5041ß0.0ßThank you so much for your insightful comments and great suggestions.ßdummyTurker
5042ß0.0ßRe: The reason for testing transfer learning on ACE dataset:ßdummyTurker
5043ß0.0ßCurrently ACE is one of the most widely adopted event schemas for event extraction task and ACE05 corpus includes the richest annotations for event extraction to date, so we tested our transfer learning framework on ACE dataset (see Section 6.2 and Section 6.3).ßdummyTurker
5044ß0.0ßIn addition, to assess the impact of preprocessing with an AMR parser, as a source of errors on event extraction, we also tested the transfer learning performances on a subset of ERE corpus which contains perfect AMR annotation (in Section 6.4).ßdummyTurker
5045ß0.0ßThe event type ontology required in our approach is a light-weight structure that, for each event type, calls for the names of the event type and argument roles.ßdummyTurker
5046ß0.0ßFor example, if “Transport Person” is a new event type, our framework needs to know its definition “Event type = Transport Person; Argument roles include Person, Instrument, Agent, Origin, Destination, Time”.ßdummyTurker
5047ß0.0ßThe definitions of most event types in general domains such as news are available in linguistic resources such as FrameNet.ßdummyTurker
5048ß0.0ßOur approach assumes that, for the users’ domain of interest, they will be the ones to determine whether the range of event types in the ontology is adequate for their data.ßdummyTurker
5049ß0.0ßHowever we agree that such definitions may not be available for some applications such as scientific domains.ßdummyTurker
5050ß0.0ßWe will outline the basic requirement more carefully and add more discussions about the limitations.ßdummyTurker
5051ß0.0ßIn addition we will also discuss the next possible relaxation of avoiding an ontology altogether in the future work section.ßdummyTurker
5052ß0.0ßThanks for this great suggestion.ßdummyTurker
5053ß0.0ßWe will do a more thorough survey on existing methods that yield high performance on ACE, and add more comparison between our approach and state-of-the-art methods including DMCNN.ßdummyTurker
5054ß0.0ßRe: The necessity of discussing the impact of AMR parsing:ßdummyTurker
5055ß0.0ßWe will make the discussions on the impact of AMR parsing more concise.ßdummyTurker
5056ß0.0ßWe appreciate the feedback from the reviewers, and will address these in our revision.ßdummyTurker
5057ß0.0ßWe thank Reviewer 3 in particular for suggesting stylistic modifications, in particular shifting the focus of background exposition.ßdummyTurker
5058ß0.0ßAs the reviewer themselves have noted, this can be easily fixed, and we will be glad to do so.ßdummyTurker
5059ß0.0ßAny further suggestions will also be appreciated.ßdummyTurker
5060ß0.0ßQ: contribution to learning, rather than NLPßdummyTurker
5061ß0.0ßWe believe this perception might be due to merely stylistic issues, as pointed out by Reviewer 3.ßdummyTurker
5062ß0.0ßOtherwise, the focus of the paper is squarely on the following key NLP problem: how to obtain alignment-aware vector representations (embeddings) for variable length texts, such as sentences, paragraphs, and documents?ßdummyTurker
5063ß0.0ßWe provide the first such variable-text embedding approach that takes into account word alignments and pre-trained high quality word embeddings.ßdummyTurker
5064ß0.0ßAnd we demonstrate its superiority through extensive sets of experiments on various tasks (i.e.ßdummyTurker
5065ß0.0ßclassification and semantic textual similarity).ßdummyTurker
5066ß0.0ßQ: use of document embedding focused only on classificationßdummyTurker
5067ß0.0ßWe provide empirical demonstrations of the semantic richness of our learned sentence/document embeddings not only on 9 classification tasks (Sec 4.2 and 4.3), but also on 22 semantic textual similarity tasks consisting of STS tasks (2012-2015), SemEval 2014 Semantic Relatedness task, and SemEval 2015 Twitter task (Sec 4.4).ßdummyTurker
5068ß0.0ßQ: evaluation section not detailed enoughßdummyTurker
5069ß0.0ßWe detail the experimental setup, baselines, and their settings in Section 4.1 - 4.4.ßdummyTurker
5070ß0.0ßBut we also provide considerable additional details (running over five pages) for each experiment in the Appendix; ranging over our code implementation, proprocessing steps, detailed ranges for hyperparameter searches, links for downloading each dataset, among others.ßdummyTurker
5071ß0.0ßWe would be glad to move additional detail drawing from these to the main section of the paper.ßdummyTurker
5072ß0.0ß"Q: meaning of ""supervised, unsupervised, and semi-supervised"" in Table 4"ßdummyTurker
5073ß0.0ßWe partially defined these in Baselines in Sec 4.4.ßdummyTurker
5074ß0.0ßWe will be sure to add clarifications in the revision.ßdummyTurker
5075ß0.0ßQ: results on STS task for Skip-thought model do not agree with paperßdummyTurker
5076ß0.0ßThe different results stem from the different settings.ßdummyTurker
5077ß0.0ßSpecifically, the results reported in the Skip-thought paper are obtained in a supervised setting, through a linear projection using unsupervisedly computed sentence embeddings as features: see also Sec 4.4 in [Wieting et al., 2016] and Sec 4.2 in [Arora et al., 2017].ßdummyTurker
5078ß0.0ßThe results in our paper are actually collected from these two recent papers [Wieting et al., 2016; Arora et al., 2017].ßdummyTurker
5079ß0.0ßDepending on the different settings (supervised/semi-supervised/unsupervised), the sentence embeddings are obtained either from unsupervisedly pretrained word vectors like GloVe, or from supervisedly trained word vectors like PSL, or from the initialization of PSL and trained on the PPDB dataset.ßdummyTurker
5080ß0.0ßFor additional details on the above, we refer to Sec.ßdummyTurker
5081ß0.0ß4.1 in [Arora et al., 2017].ßdummyTurker
5082ß0.0ßNote that our results are solely based on our unsupervised sentence representation using either GloVe or PSL, so that we perform a fair comparison in our experiments.ßdummyTurker
5083ß0.0ßQ: paper is difficult to followßdummyTurker
5084ß0.0ßOur paper had a lot of building blocks that we needed to cover, in part for keeping the paper self-contained for the reader who wished to reproduce the technical results of the paper.ßdummyTurker
5085ß0.0ßBut following Reviewer 3's suggestion, we will make sure we convey at least the broader intuitions behind all of the technical building blocks first.ßdummyTurker
5086ß0.0ßWe greatly appreciate the kind comments of the reviewer, in particular on the key contributions of the paper.ßdummyTurker
5087ß0.0ßThank you for pointing this out, we will make the descriptions clearer and provide more examples.ßdummyTurker
5088ß0.0ßWe leave out more comparisons because of space limitation.ßdummyTurker
5089ß0.0ßCurrently our methods are competitive but have not reached the state-of-the-art accuracies yet, which is denoted in Section 5.3.ßdummyTurker
5090ß0.0ßHowever, recent advances in parsing relies much on more powerful neural models, while we only adopted a simple LSTM+MLP one.ßdummyTurker
5091ß0.0ßThe main purpose and contribution of this paper is to explore non-model issues for incremental dependency parsing and we hope that our work can inspire more researches on these issues.ßdummyTurker
5092ß0.0ßThank you for your comments and we will address your concerns correspondingly.ßdummyTurker
5093ß0.0ßFirstly, in this paper, we investigate easy-first parsing, which belongs to a class of transition parser, while the state-of-the-art pure single-model dependency parser is the deep bi-affine model based graph-parser (Dozat and Manning, 2017).ßdummyTurker
5094ß0.0ßHere are the reasons of our choice:ßdummyTurker
5095ß0.0ß1.ßdummyTurker
5096ß0.0ßGraph-based parsers have the time complexity of O(n^2) or higher, while easy-first parsing can be performed in O(nlog(n)), which has the advantage of efficiency.ßdummyTurker
5097ß0.0ßTraditional stack-based transition parser can analyze in linear time, but has to maintain relatively complex partial data structures of stack and buffer.ßdummyTurker
5098ß0.0ßIn the aspect of simplicity, easy-first parser eliminates the need of too complex structures and resembles graph-based parsing.ßdummyTurker
5099ß0.0ßTherefore, easy-first parsing contains the advantages of both graph-based and transition-based methods.ßdummyTurker
5100ß0.0ßIn addition, we think easy-first parsing is more likely to be the way that humans do parsing: we don’t strictly analyze from left to right or directly figure out the globally optimal structure, but build the target incrementally and usually deal with the easy parts first.ßdummyTurker
5101ß0.0ß2.ßdummyTurker
5102ß0.0ßTypical graph-based parsers already do exact search through DP chart-parsing algorithms, thus will not suffer from search errors.ßdummyTurker
5103ß0.0ßTo explore different strategies for parsers with inexact search, a suitable method is an incremental parser, which builds the tree with a sequence of actions.ßdummyTurker
5104ß0.0ßTransition-based parsers fall into this type and we especially choose easy-first parsing for its flexibility and simplicity.ßdummyTurker
5105ß0.0ßMoreover, we think easy-first parsing will suffer more from larger action space.ßdummyTurker
5106ß0.0ßThis makes it more suitable for the purpose of this research: addressing problems brought by inexact search.ßdummyTurker
5107ß0.0ßThe improvements brought by merging show this in some way.ßdummyTurker
5108ß0.0ß3.ßdummyTurker
5109ß0.0ßWe want to stress that our aim is not to beat the state-of-the-art results, but is to explore several non-model issues for parsers with inexact search.ßdummyTurker
5110ß0.0ßWe think the success of the state-of-the-art graph-based method relies on powerful deep neural models and carefully fine-tuned hyper-parameters, which is orthogonal to our concerns.ßdummyTurker
5111ß0.0ßWe choose to explore a different type of parser that is more related to our aim.ßdummyTurker
5112ß0.0ßWe admit that our results are not on par with the state-of-the-art ones, but we only adopted a simple LSTM+MLP model and further improvements can be expected if improving the modeling part.ßdummyTurker
5113ß0.0ßSecondly, we think that this paper should be regarded mainly as an empirical work for exploring the aspects of searching and learning for incremental parsing, which differs from much recent works that research on modeling.ßdummyTurker
5114ß0.0ßNotably, our main contribution is showing that although recent advances in neural methods quip us with powerful models, the decoding and search-aware learning problems still worth researching.ßdummyTurker
5115ß0.0ßAbout the generalization of our method, our method can be adapted to general transition-based parsing (Bohnet et al., 2016).ßdummyTurker
5116ß0.0ßThis class of incremental parsers builds the final structure by conducting a sequence of actions and easy-first parser is only a typical example.ßdummyTurker
5117ß0.0ßOur methods of merging similar partial states and manipulating gradients for partial states are not restricted to the specific transition system of easy-first parsing.ßdummyTurker
5118ß0.0ßFor example, in Arc-Standard transition system, we can specify signatures and merge partial states based on features of current stack and buffer.ßdummyTurker
5119ß0.0ßFor instance, the ``Top'' signature in our method can be similarly specified, by only considering the features of current stack position and all the nodes that do not have parents.ßdummyTurker
5120ß0.0ßWe realize that the extensions to more transition-based systems should be explored and we are working on this line of research.ßdummyTurker
5121ß0.0ßBut we think that the aspect of search-aware learning is also quite important and thus, we also put much of our focus on it.ßdummyTurker
5122ß0.0ßWe think that even a long paper might not have that much space for exploring all those searching and learning strategies for different transition systems, and thus, we only choose easy-first parsing as an example.ßdummyTurker
5123ß0.0ßWe will provide more discussions if space permits.ßdummyTurker
5124ß0.0ßTo sum up, we choose easy-first parsing as a representative of transition-based parsing, which is more related to our researching topic.ßdummyTurker
5125ß0.0ßIt has the merits of efficiency, simplicity and flexibility.ßdummyTurker
5126ß0.0ßAs for contributions, our empirical studies show the importance of searching and search-aware learning for parsing and the explored methods are not restricted to easy-first parsing, but can be naturally adapted to other incremental parsers.ßdummyTurker
5127ß0.0ßWe hope that our work can inspire future researches on these searching and learning aspects.ßdummyTurker
5128ß0.0ßThank you so much for your time and great suggestions.ßdummyTurker
5129ß0.0ßReply to weakness argument 1ßdummyTurker
5130ß0.0ßThanks for the suggestions.ßdummyTurker
5131ß0.0ßWe will fix the typos by very careful proof-reading.ßdummyTurker
5132ß0.0ßReply to weakness argument 2ßdummyTurker
5133ß0.0ßThanks for the suggestions.ßdummyTurker
5134ß0.0ßWe will correct and improve the bibliography format.ßdummyTurker
5135ß0.0ßThank you so much for your insightful comments and great suggestions.ßdummyTurker
5136ß0.0ßRe: The main difference between our approach and the ones briefly mentioned in section 2:ßdummyTurker
5137ß0.0ßCompared with CCA and linear mapping based multilingual embedding approaches, the novel contributions of our approach include:ßdummyTurker
5138ß0.0ß1.ßdummyTurker
5139ß0.0ßBesides word alignment as in previous approaches, we introduced three cluster-level alignments: (1) neighbor based clustering and alignment, (2) character based clustering and alignment, and (3) linguistic property based clustering and alignment.ßdummyTurker
5140ß0.0ß2.ßdummyTurker
5141ß0.0ßTo achieve cluster-level consistency, we introduced a new cluster-consistent correlational neural network to align both words and clusters.ßdummyTurker
5142ß0.0ßWe will elaborate these differences further in section 2.ßdummyTurker
5143ß0.0ßWe extracted 10 closed word classes (colors, weekdays, months, numbers, single/plural pronouns, cardinal/ordinal numbers, prepositions, conjunctions, clothes) from linguistic knowledge bases (CLDR and Wiktionary).ßdummyTurker
5144ß0.0ßIn addition, Wiktionary and Panlex include the suffix/prefix alignments between English and any other languages.ßdummyTurker
5145ß0.0ßWe filtered out the many-to-many suffix/prefix alignments and obtained hundreds of alignments between each language and English.ßdummyTurker
5146ß0.0ßFor example, we extracted 516 suffix/prefix alignment pairs between Italian and English.ßdummyTurker
5147ß0.0ßTable 1 shows some examples.ßdummyTurker
5148ß0.0ßWe will add these details in the final version.ßdummyTurker
5149ß0.0ßThank you so much for your time and insightful comments.ßdummyTurker
5150ß0.0ßReply to weakness argument 1ßdummyTurker
5151ß0.0ßRe: Comparison of the performances between our proposed approach and baselines:ßdummyTurker
5152ß0.0ßComparing CorrNet (W) with CorrNet (W+N+Ch+L): Multiple cluster-level alignments can enhance the multilingual embedding learning from various aspects.ßdummyTurker
5153ß0.0ß(1) The neighbor based clustering and alignment incorporates neighboring information into each word while projecting it into the shared semantic space.ßdummyTurker
5154ß0.0ßThe neighboring information can be viewed as the semantic meanings of each word within its own monolingual semantic space.ßdummyTurker
5155ß0.0ßBy aligning the neighborhoods across multiple languages, both monolingual and multilingual embedding quality can be improved, which can be seen by comparing CorrNet (W) with CorrNet (W+N) in Table 3.ßdummyTurker
5156ß0.0ß(2) Many related languages shared very similar character set and many words that refer to the same concept share very similar compositional characters.ßdummyTurker
5157ß0.0ßThus, by aligning character-level information across multiple languages, the multilingual embedding quality can be improved, which is shown by comparing CorrNet (W+N) with CorrNet (W+N+Ch).ßdummyTurker
5158ß0.0ßSince the character-level word representations are not optimized using distributional contexts within monolingual data, concatenating character based compositional word representations to word embeddings may introduce noize and hurt the monolingual embedding quality.ßdummyTurker
5159ß0.0ß(3) The linguistic property alignment can be viewed as an extension of word alignment, but it collects more monolingual information than single words.ßdummyTurker
5160ß0.0ßBy aligning these linguistic properties, both monolingual and multilingual embedding qualities can be improved, which can be seen by comparing CorrNet (W+N+Ch) with CorrNet (W+N+Ch+L).ßdummyTurker
5161ß0.0ßGenerally the sizes of linguistic property alignment are small compared to word alignments.ßdummyTurker
5162ß0.0ßThus the improvement of CorrNet (W+N+Ch+L) in Table 3 is slight since the size of bilingual word dictionary is larger than 40,000.ßdummyTurker
5163ß0.0ßHowever, when the bilingual word alignments are not enough, CorrNet (W+N+Ch+L) can still maintain high performance, which can be seen in Table 4.ßdummyTurker
5164ß0.0ßWe will elaborate these analysis in the paper.ßdummyTurker
5165ß0.0ßFollowing Reviewer #2’s suggestions, we will conduct significance test and add detailed discussions into the paper.ßdummyTurker
5166ß0.0ßReply to weakness argument 2ßdummyTurker
5167ß0.0ßRe: Comparison with MultiCluster, MultiSkip, MultiCross in the extrinsic evaluations:ßdummyTurker
5168ß0.0ßWe agree that the character-level alignment incorporates additional character information compared to the current baselines.ßdummyTurker
5169ß0.0ßWe will add MultiCluster, MultiSkip, and MultiCross as baselines into the extrinsic evaluations.ßdummyTurker
5170ß0.0ßReply to weakness argument 3ßdummyTurker
5171ß0.0ßRe: Incorrect or insufficient descriptions about the models and the evaluation metric:ßdummyTurker
5172ß0.0ßWe thank reviewer #2 for the great suggestions.ßdummyTurker
5173ß0.0ßWe will fix these typos and add detailed descriptions for D and P in the evaluation metric.ßdummyTurker
5174ß0.0ßReply to Question 1ßdummyTurker
5175ß0.0ßRe: The tuning of the number of nearest neighboring words:ßdummyTurker
5176ß0.0ßWe use top N nearest neighboring words to denote the semantic meaning of each word, thus N can be set to any positive integers.ßdummyTurker
5177ß0.0ßIn our experiments, we set N as 5.ßdummyTurker
5178ß0.0ßWe will carefully tune it and add detailed discussions.ßdummyTurker
5179ß0.0ßReply to Question 2ßdummyTurker
5180ß0.0ßRe: The analysis of the worse name tagging performance of our approach compared to baselines:ßdummyTurker
5181ß0.0ßThe name tagging models are optimized on the dev set, thus the scores on test set depend on the best name tagging model selected on the dev set.ßdummyTurker
5182ß0.0ßWe found that, in the Tig-Tig case in Table 6, though the test score of CorrNet(W+N+Ch+L) is not as good as MultiCCA, the score of CorrNet(W+N+Ch+L) significantly outperformed MultiCCA on the same development set: the best dev score for CorrNet(W+N+Ch+L) is 78.2 while the best score for MultiCCA is 74.1.ßdummyTurker
5183ß0.0ßWe did consider one “simple” approach, namely, to combine all corpora and train word2vec on them.ßdummyTurker
5184ß0.0ßOur models show improvements over this approach.ßdummyTurker
5185ß0.0ßWe point out that the numbers in the paper are all absolute accuracy or error rates.ßdummyTurker
5186ß0.0ßFor example, absolute improvements in several cases are 1%.ßdummyTurker
5187ß0.0ßHowever, in relative terms this translates to 4% in many of the cases reported in the paper.ßdummyTurker
5188ß0.0ßWe will add a notation table.ßdummyTurker
5189ß0.0ßThe underlying motivation for this work comes from a project with a large corporation where several different corpora are involved.ßdummyTurker
5190ß0.0ßThe company would need to train a w2v model on each corpus which is tedious and not robust.ßdummyTurker
5191ß0.0ßFor this reason, it is a better proposition to train a single model.ßdummyTurker
5192ß0.0ßIt is easier to maintain.ßdummyTurker
5193ß0.0ßAnother motivation is that embeddings produced by our model are more robust which is reflected by improved results in our paper vs w2v for each corpus.ßdummyTurker
5194ß0.0ßThe embeddings also summarize information from each corpora.ßdummyTurker
5195ß0.0ßWe will add this into the paper.ßdummyTurker
5196ß0.0ßWe have indeed fine-tuned hyperparameters of word2vec.ßdummyTurker
5197ß0.0ßWe detailed this in our first responseßdummyTurker
5198ß0.0ßThank you very much for your detailed and helpful comments.ßdummyTurker
5199ß0.0ßWe will improve our paper accordingly.ßdummyTurker
5200ß0.0ß* Reply to weakness #1：We fully agree that it is more appropriate to also discuss the cost of manual annotation of the bi-tree aligned data when comparing direct treebank conversion with multi-task learning.ßdummyTurker
5201ß0.0ßWe will add such discussions.ßdummyTurker
5202ß0.0ßThank you.ßdummyTurker
5203ß0.0ß* Reply to weakness #2： We will give discussions on the adaptation of our proposed approaches to other non-dependency treebanks in the next version.ßdummyTurker
5204ß0.0ßThanks.ßdummyTurker
5205ß0.0ßThe straightforward choice is to convert such treebanks to dependency treebanks based on heuristic head-finding rules.ßdummyTurker
5206ß0.0ßThe second choice is to directly extend our proposed approaches by adapting the patterns and treeLSTMs for non-dependency structures, which should be straightforward as well.ßdummyTurker
5207ß0.0ßIn this work, we take HIT-CDT as a case study.ßdummyTurker
5208ß0.0ßOur next-step plan is to annotate bi-tree aligned data for PKU-CDT and then convert PKU-CDT to our guideline.ßdummyTurker
5209ß0.0ßWe will add discussions on the mentioned related works, give a high-level justification of our guideline, and give discussions on the impact of constrained decoding.ßdummyTurker
5210ß0.0ßWe consider the straight-forward conclusion- using more than one source of input increases prediction accuracy - a strength rather than a weakness.ßdummyTurker
5211ß0.0ßThe simple hypothesis allows us to analyze in detail which particular areas are improved by combining multiple sources of the input data.ßdummyTurker
5212ß0.0ßWe do not claim that the model we use is novel.ßdummyTurker
5213ß0.0ßIt is rather the application of neural networks to this task which allows one to easily combine multi-modal input data that is novel.ßdummyTurker
5214ß0.0ßSome of the technical implementation details are in the supplementary material.ßdummyTurker
5215ß0.0ßWe can shift the material and elaborate more on the model description in the camera ready version.ßdummyTurker
5216ß0.0ßSee reply to Reviewer 3's weakness 1.ßdummyTurker
5217ß0.0ßHangul (a phonetic Korean alphabet) did not come into widespread official use in early 20th century (Fischer 2004).ßdummyTurker
5218ß0.0ßThus, until that time it was necessary to be fluent in reading and writing Hanja (Han logograms) in order to be literate in Korean, as the vast majority of Korean literature and most other Korean documents were written in Literary Chinese, using Hanja as its primary script.ßdummyTurker
5219ß0.0ßTherefore if we consider Hanja, Korean is a logographic language.ßdummyTurker
5220ß0.0ß[Fischer 2004] Fischer, Stephen Roger (2004).ßdummyTurker
5221ß0.0ßA History of Writing.ßdummyTurker
5222ß0.0ßGlobalities.ßdummyTurker
5223ß0.0ßLondon: Reaktion BooksßdummyTurker
5224ß0.0ßSee reply to Reviewer 3's weakness 1.ßdummyTurker
5225ß0.0ßSee reply to Reviewer 3's weakness 1.ßdummyTurker
5226ß0.0ßResponses are in the Questions to Authors section.ßdummyTurker
5227ß0.0ßWe acknowledge this approach is particularly suited for Chinese Han logographic languages because of the extensive borrowing of words between the Han logographic languages and this phenomenon does not apply for the other non-Chinese Han logographic languages.ßdummyTurker
5228ß0.0ßHowever, the framework of predicting pronunciation from the decomposition of logographs is general and therefore can be applied to any language where phonetic information is encoded in logographs.ßdummyTurker
5229ß0.0ßAs a matter of fact, all logographic systems include a phonetic dimension, as it is impractical to have a separate basic character for every word or morpheme in a language (Coe, Michael and Van Stone, Mark, 2001).ßdummyTurker
5230ß0.0ß[Coe and Van Stone 2001] Coe, Michael; Van Stone, Mark (2001).ßdummyTurker
5231ß0.0ßReading the Maya GlyphsßdummyTurker
5232ß0.0ßYes, we mean Hanja instead of Hangul.ßdummyTurker
5233ß0.0ßYes, we do consider this scenario.ßdummyTurker
5234ß0.0ßThe prediction output can be a #NULL token corresponding to the case where there is no phoneme at that position.ßdummyTurker
5235ß0.0ßThis assumption allows us to simplify the model decoder by using three fully-connected layers to predict the three output phonemes.ßdummyTurker
5236ß0.0ßIf an RNN is used as a decoder, then we do not need this assumption.ßdummyTurker
5237ß0.0ßHowever, from our past experience, the length of the sequence (= 3) is too short to benefit from an RNN decoder.ßdummyTurker
5238ß0.0ßIdeally, it would be nice if we can use Lin and Tsai (2012)’s work as a baseline system to compare with our system.ßdummyTurker
5239ß0.0ßHowever, this is inappropriate due to the following reasons:ßdummyTurker
5240ß0.0ß1)	Lin and Tsai (2012)’s model includes input features from a Chinese rhyme books and similar resources are not available for Vietnamese and Korean as far as we know.ßdummyTurker
5241ß0.0ßThis is one advantage of our approach over Lin and Tsai (2012) since our approach requires less language resources and therefore can be used in more under-resourced languages.ßdummyTurker
5242ß0.0ßIn hindsight, we realized that this advantage was not made clear in the paper and it should be highlighted.ßdummyTurker
5243ß0.0ßWe thank the reviewer for bringing up this question.ßdummyTurker
5244ß0.0ß2)	Our approach and Lin and Tsai’s work use different input data (rhymes book versus logograph decomposition).ßdummyTurker
5245ß0.0ßThe performance of the models depends on the quality of the input data so a direct comparison would not be fair to either model.ßdummyTurker
5246ß0.0ßWe can compare to Zhou (1980), but chose to omit it because the pronunciation of most Chinese characters in Mandarin are known and readily available, so Mandarin would not benefit much from our approach unlike an under-resourced language like Cantonese.ßdummyTurker
5247ß0.0ßWe could have provided the result on Mandarin in the supplementary section as a benchmark for our approach: While Zhou (1980) reports 61% string error rate (SER), our approach using only logograph inputs yields 61.6% SER and our approach using logograph and phonemes from Cantonese, Vietnamese and Korean yields 35.9% SER.ßdummyTurker
5248ß0.0ßWe did not include other baselines such as decision tree and joint-source channel model for the cases where there is only one input language, and decision forest where there are multiple input languages because their performance are on par or slightly worse than the fully-connected neural networks baseline.ßdummyTurker
5249ß0.0ßWe will include these baselines in the paper.ßdummyTurker
5250ß0.0ßFor the multi-modal case, to the best of our knowledge, no prior work has been reported in the past to serve as a suitable baseline.ßdummyTurker
5251ß0.0ßWe plan to incorporate the technical implementation details in the main text in the camera ready version.ßdummyTurker
5252ß0.0ßWe agree with the reviewer.ßdummyTurker
5253ß0.0ßHowever, we believe that our work is an important scientific endeavor in its own right.ßdummyTurker
5254ß0.0ßGrapheme decomposition in general has been applied to many other tasks as mentioned in the related work section so it is not novel by itself.ßdummyTurker
5255ß0.0ßHowever, for predicting pronunciation, it has not been used before and we believe and show grapheme decomposition is beneficial for this task because of the way phonetic clues are encoded in the logographs.ßdummyTurker
5256ß0.0ßIf possible, we would like to add some of experimental results before the acl2018ßdummyTurker
5257ß0.0ßThe innovation may be not-sufficient enough but we believe that this article could help to understand a link between word-document embedding and traditional document-word matrix.ßdummyTurker
5258ß0.0ßWe will add some more appropriate referencesßdummyTurker
5259ß0.0ßIt could be obvious for some of experts in the field but in general we think that this illustrates mathematical details step by step and I believe as a mathematician that looks obvious does not necessary mean that it is mathematically proven.ßdummyTurker
5260ß0.0ßIt could be relevant if we have limited number of samples or are under computationally limited circumstances.ßdummyTurker
5261ß0.0ßSome of researchers actually appreciate for the easy calculations.ßdummyTurker
5262ß0.0ßThis is  true.ßdummyTurker
5263ß0.0ßwe would like to add some of experiments by the conference presentation, if possible.ßdummyTurker
5264ß0.0ßThank you for the feedback.ßdummyTurker
5265ß0.0ß== Evaluation standard ==ßdummyTurker
5266ß0.0ßFirstly, to ensure a fair comparison of different algorithms, one cannot compare with the output of the Google/Bing translator, because of the difference in both the training data and the model size.ßdummyTurker
5267ß0.0ßIn addition, as far as we know, IWSTL14 is a main MT benchmark dataset for algorithm developments, due to its proper size and complexity.ßdummyTurker
5268ß0.0ßIn comparison, EuroParl and WMT datasets are too large to allow rapid algorithm developments.ßdummyTurker
5269ß0.0ßFinally, beyond the theoretical result, our paper develops new algorithms with clearly improved performance.ßdummyTurker
5270ß0.0ßTo get a concrete idea, please refer to our response to Reviewer 1 for a comparison with other algorithm oriented papers on the same dataset.ßdummyTurker
5271ß0.0ß== ACL as the venue ==ßdummyTurker
5272ß0.0ßPlease refer to our response to Reviewer 1 for our opinions on this.ßdummyTurker
5273ß0.0ßThank you for the valuable feedback.ßdummyTurker
5274ß0.0ß== ACL as the venue ==ßdummyTurker
5275ß0.0ßInitially, this was indeed a concern for us before we submitted the work.ßdummyTurker
5276ß0.0ßHowever, we finally decided to submit the work for the following reasons.ßdummyTurker
5277ß0.0ß(1) This work aims to tackle a canonical NLP problem, i.e.ßdummyTurker
5278ß0.0ßdiscrete sequence prediction, from the very beginning.ßdummyTurker
5279ß0.0ßAs machine learning has been an important driving force of NLP advancements, we sincerely believe that there should be machine learning theories and algorithms dedicated to canonical NLP problems.ßdummyTurker
5280ß0.0ßDue to the dedication to NLP, ACL should give room for such a work.ßdummyTurker
5281ß0.0ß(2) Secondly, we think theoretical results are still of interests and of importance in the NLP community because theoretical results not only help to articulate existing ideas, but also make future explorations more systematic.ßdummyTurker
5282ß0.0ß(3) Finally, the non-trivial source code we plan to release will provide a concrete example of how these complicated algorithms can be incorporated into NLP tasks.ßdummyTurker
5283ß0.0ß== ERAC vs. the entropy regularization in Mnih et al.ßdummyTurker
5284ß0.0ß==ßdummyTurker
5285ß0.0ßWe experimented with the entropy regularization in Mnih et al on the MT task, where the entropy regularization is only applied to the actor but not the critic.ßdummyTurker
5286ß0.0ßThe results are summarized below.ßdummyTurker
5287ß0.0ß<table>ßdummyTurker
5288ß0.0ß<tbody>ßdummyTurker
5289ß0.0ß<tr><td>Methods</td><td>| BLEU </td></tr>ßdummyTurker
5290ß0.0ß<tr><td>AC</td><td>| 28.04 +/- 0.05 </td></tr>ßdummyTurker
5291ß0.0ß<tr><td>AC (ER in Mnih et al.ßdummyTurker
5292ß0.0ß)</td><td>| 28.06 +/- 0.05 </td></tr>ßdummyTurker
5293ß0.0ß<tr><td>ERAC</td><td>| 28.30 +/- 0.06 </td></tr>ßdummyTurker
5294ß0.0ß</tbody>ßdummyTurker
5295ß0.0ß</table>ßdummyTurker
5296ß0.0ßThe improvement from the actor-only entropy regularization as in Mnih et al.ßdummyTurker
5297ß0.0ßis quite limited.ßdummyTurker
5298ß0.0ßWe'll add the same comparison for image captioning for the camera ready if accepted.ßdummyTurker
5299ß0.0ß== Comparison with contemporary techniques ==ßdummyTurker
5300ß0.0ßThe reason we didn’t provide references to other results is that existing papers used different architectures (e.g.ßdummyTurker
5301ß0.0ßencoder, input feeding), which makes a fair comparison difficult.ßdummyTurker
5302ß0.0ßNonetheless, we provide a comparison below, which includes some new results on MT to enable a fair comparison.ßdummyTurker
5303ß0.0ß(1) Machine TranslationßdummyTurker
5304ß0.0ß<table>ßdummyTurker
5305ß0.0ß<tbody>ßdummyTurker
5306ß0.0ß<tr><td>Methods</td><td>| Encoder</td><td>| Decoder</td><td>| Input Feed</td><td>| BLEU (max)</td></tr>ßdummyTurker
5307ß0.0ß<tr><td></td></tr>ßdummyTurker
5308ß0.0ß<tr><td>MIXER [1]</td><td>|  CNN      (256)</td><td>| LSTM (256)</td><td>| N</td><td>| 20.73ßdummyTurker
5309ß0.0ß</td></tr>ßdummyTurker
5310ß0.0ß<tr><td>BSO [2]</td><td>| BiLSTM (128x2)</td><td>| LSTM (256)</td><td>| Y</td><td>| 27.9</td></tr>ßdummyTurker
5311ß0.0ß<tr><td>Q-BLEU [3]</td><td>| BiLSTM (128x2)</td><td>| LSTM (256)</td><td>| Y</td><td>| 28.3</td></tr>ßdummyTurker
5312ß0.0ß<tr><td>MLE [4]</td><td>| BiGRU   (256x2)</td><td>| GRU  (256)</td><td>| Y</td><td>| 27.56</td></tr>ßdummyTurker
5313ß0.0ß<tr><td>AC [4]</td><td>| BiGRU   (256x2)</td><td>| GRU  (256)</td><td>| Y</td><td>| 28.53</td></tr>ßdummyTurker
5314ß0.0ß<tr><td></td></tr>ßdummyTurker
5315ß0.0ß<tr><td>Our MLE</td><td>| BiLSTM (128x2)</td><td>| LSTM (256)</td><td>| N</td><td>| 27.27</td></tr>ßdummyTurker
5316ß0.0ß<tr><td>Our AC</td><td>| BiLSTM (128x2)</td><td>| LSTM (256)</td><td>| N</td><td>| 28.10</td></tr>ßdummyTurker
5317ß0.0ß<tr><td>Our ERAC</td><td>| BiLSTM (128x2)</td><td>| LSTM (256)</td><td>| N</td><td>| 28.42</td></tr>ßdummyTurker
5318ß0.0ß<tr><td></td></tr>ßdummyTurker
5319ß0.0ß<tr><td>Our MLE+</td><td>| BiLSTM (128x2)</td><td>| LSTM (256)</td><td>| Y</td><td>| 28.20</td></tr>ßdummyTurker
5320ß0.0ß<tr><td>Our AC+</td><td>| BiLSTM (128x2)</td><td>| LSTM (256)</td><td>| Y</td><td>| 28.97</td></tr>ßdummyTurker
5321ß0.0ß<tr><td>Our ERAC+</td><td>| BiLSTM (128x2)</td><td>| LSTM (256)</td><td>| Y</td><td>| 29.26</td></tr>ßdummyTurker
5322ß0.0ß</tbody>ßdummyTurker
5323ß0.0ß</table>ßdummyTurker
5324ß0.0ßHere, MLE+, AC+, and ERAC+ are our new results with input feeding.ßdummyTurker
5325ß0.0ßWe obtained these new results during the response period using a single run with the same hyper-parameters.ßdummyTurker
5326ß0.0ß(2) Image Captioning (w/o attention)ßdummyTurker
5327ß0.0ß<table>ßdummyTurker
5328ß0.0ß<tbody>ßdummyTurker
5329ß0.0ß<tr><td>Methods</td><td>| Encoder</td><td>| Decoder</td><td>| Curriculum</td><td>| BLEU (max)</td></tr>ßdummyTurker
5330ß0.0ß<tr><td></td></tr>ßdummyTurker
5331ß0.0ß<tr><td>MLE [5]</td><td>| Resnet-101</td><td>| LSTM (512)</td><td>| N/A</td><td>| 29.6</td></tr>ßdummyTurker
5332ß0.0ß<tr><td>MIXER [5]</td><td>| Resnet-101</td><td>| LSTM (512)</td><td>| Y</td><td>| 31.7</td></tr>ßdummyTurker
5333ß0.0ß<tr><td>SCST [5]</td><td>| Resnet-101</td><td>| LSTM (512)</td><td>| Y</td><td>| 31.9</td></tr>ßdummyTurker
5334ß0.0ß<tr><td></td></tr>ßdummyTurker
5335ß0.0ß<tr><td>Our MLE</td><td>| Resnet-101</td><td>| LSTM (256)</td><td>| N/A</td><td>| 29.89</td></tr>ßdummyTurker
5336ß0.0ß<tr><td>Our AC	</td><td>| Resnet-101</td><td>| LSTM (256)</td><td>| N</td><td>| 31.16</td></tr>ßdummyTurker
5337ß0.0ß<tr><td>Our ERAC</td><td>| Resnet-101</td><td>| LSTM (256)	</td><td>| N</td><td>| 31.82</td></tr>ßdummyTurker
5338ß0.0ß</tbody>ßdummyTurker
5339ß0.0ß</table>ßdummyTurker
5340ß0.0ßWe'll add the comparison with the same decoder size the camera ready if accepted.ßdummyTurker
5341ß0.0ß[1] Ranzato et al.ßdummyTurker
5342ß0.0ßSequence level training with recurrent neural networks.ßdummyTurker
5343ß0.0ß[2] Wiseman et al.ßdummyTurker
5344ß0.0ßSequence-to-sequence learning as beam-search optimization.ßdummyTurker
5345ß0.0ß[3] Li et al.ßdummyTurker
5346ß0.0ßLearning to decode for future success.ßdummyTurker
5347ß0.0ß[4] Bahdanau et al.ßdummyTurker
5348ß0.0ßAn actor-critic algorithm for sequence prediction.ßdummyTurker
5349ß0.0ß[5] Rennie et al.ßdummyTurker
5350ß0.0ßSelf-critical sequence training for image captioning.ßdummyTurker
5351ß0.0ß== Training time comparison: VAML vs. RAML ==ßdummyTurker
5352ß0.0ßVAML takes an additional forward pass to compute the target distribution based on the critic.ßdummyTurker
5353ß0.0ßEmpirically, VAML takes about 1.4X of the running time of RAML.ßdummyTurker
5354ß0.0ßThank you very much for your comments.ßdummyTurker
5355ß0.0ßSure we are very willing to simplify visualization section as you suggested to make it more obvious.ßdummyTurker
5356ß0.0ßAlso, we will discuss the limitation and future improvements via error analysis in the camera-ready version if accepted.ßdummyTurker
5357ß0.0ßThank you very much for your comments.ßdummyTurker
5358ß0.0ßWe will certainly fix all the typos and unclear parts in the camera-ready version if accepted.ßdummyTurker
5359ß0.0ßSure we will include those early studies on retrieval-based chatbots in the camera-ready version if accepted.ßdummyTurker
5360ß0.0ßAs for self-attention, yes, the query, key and value vectors are the repetition of the output of the previous one, as same as the attention mechanism in Transformer.ßdummyTurker
5361ß0.0ßThe rationale behind is two-foldßdummyTurker
5362ß0.0ßSure, we would like to clarify the unclear parts and add more visualization examples to better explain how attention works in the camera-ready version if accepted.ßdummyTurker
5363ß0.0ßWe think it might not be harmful to justify the effectiveness of our work with empirical evidence, like the performance comparison, ablation, quantitative analysis and visualization done in our work.ßdummyTurker
5364ß0.0ßAnd we would like to explore the theoretical justification of attention-based models in our future work, which we think is also a quite challenging task to the whole deep learning research community as well.ßdummyTurker
5365ß0.0ß“The authors should also compare the performance of this approach for transfer learning with other approaches for transfer learning.”ßdummyTurker
5366ß0.0ßWe compare our approach to McCann et al.ßdummyTurker
5367ß0.0ß(2017), which we outperform on IMDb and TREC-6.ßdummyTurker
5368ß0.0ßPeters et al.ßdummyTurker
5369ß0.0ß(2018) require their method to be integrated with a state-of-the-art model.ßdummyTurker
5370ß0.0ßWe found it non-trivial to incorporate their Pytorch-based model into the C++ implementation provided by Johnson and Zhang (2017) (https://github.com/riejohnson/ConText).ßdummyTurker
5371ß0.0ß“The presentation of the learning rate used in this paper takes attention away from the real values of their paper.”ßdummyTurker
5372ß0.0ßWe agree that the main focus of the paper is the universal appeal of the fine-tuning mechanism that can be applied to many other areas of research in NLP.ßdummyTurker
5373ß0.0ßWe will dedicate more space to its discussion and outline possible future directions in the camera-ready version.ßdummyTurker
5374ß0.0ß“The idea of the inductive transfer learning is not very novel and it is the cornerstone of this paper.ßdummyTurker
5375ß0.0ßThere seems to be no adaptation specific to NLP.ßdummyTurker
5376ß0.0ßThis may be a strength but at the same time it is also a weakness from novelty perspective.”ßdummyTurker
5377ß0.0ßThe techniques we proposed are all measures to adapt inductive transfer learning via fine-tuning to NLP.ßdummyTurker
5378ß0.0ßFor instance, most NLP tasks employ shallow models, where features rapidly transition from general to task-specific.ßdummyTurker
5379ß0.0ßFor this reason, we have to be careful about how we update different layers.ßdummyTurker
5380ß0.0ßAnother aspect are long sequence lengths, which we handle using concat pooling and BPT3C.ßdummyTurker
5381ß0.0ßAt the same time, we tried to keep our proposed approach as general as possible so that it can be applied to many different NLP tasks.ßdummyTurker
5382ß0.0ß“Authors do not provide results on how much the system is affected by the quality of the background lm.ßdummyTurker
5383ß0.0ßOther approaches for adapting the learning rate along time are not explored.”ßdummyTurker
5384ß0.0ßThank you for the useful suggestions.ßdummyTurker
5385ß0.0ßWe were not able to include the evaluation of the quality of the LM due to space constraints, but will add this to the camera-ready version.ßdummyTurker
5386ß0.0ßWe compare to the two---to our knowledge---most successful recent approaches to adapting the learning rate, cosine annealing and chain-thaw.ßdummyTurker
5387ß0.0ß“Authors should explain why the strl in fig 2 shows a peak in 200 while the validation error rates in fig 4 are values less than 30 epochs.ßdummyTurker
5388ß0.0ßDoes this mean that the strl should be adjusted or that even the results will not be so different for the full approach if you do an early stopping?”ßdummyTurker
5389ß0.0ßThe values on the x-axis in Figure 2 are the number of iterations (i.e.ßdummyTurker
5390ß0.0ßbatch updates), while the values on the x-axis in Figure 4 are epochs.ßdummyTurker
5391ß0.0ßFigure 2, for instance, corresponds to the learning rate schedule for training for about 20 epochs on the TREC-6 dataset.ßdummyTurker
5392ß0.0ßWe will clarify this in the camera-ready version.ßdummyTurker
5393ß0.0ßIn most cases, we have found early stopping not to help and training for the full schedule (see Figure 4, IMDb and TREC-6) yielded the best performance.ßdummyTurker
5394ß0.0ßWe will elaborate on this in the camera-ready version.ßdummyTurker
5395ß0.0ß“what is the max length of your sentences?ßdummyTurker
5396ß0.0ßor how many steps did you use?ßdummyTurker
5397ß0.0ßthere is also not information about the GPU used, time for training, speed, etc.”ßdummyTurker
5398ß0.0ßIn order to make use of the entire context, we did not restrict the max length of the examples or set a maximum step size.ßdummyTurker
5399ß0.0ßInstead, as discussed in Section 3.3, we split each example into batches of size b through which we back-propagate using BPT3C.ßdummyTurker
5400ß0.0ßWe trained each model on a Tesla V100.ßdummyTurker
5401ß0.0ßFine-tuning the classifier for 50 epochs took from one minute on TREC-6 and about 5 minutes on IMDb to about one hour on the largest datasets (Yelp-full).ßdummyTurker
5402ß0.0ßWe agree that evaluation on other tasks would provide a more complete picture, and will include it in the final version.ßdummyTurker
5403ß0.0ßwe agree that evaluation on the other tasks would provide a more complete picture, and will include it in the final version.ßdummyTurker
5404ß0.0ßthis is a good point, and we will expand the discussion about diversity of domains in the final version, including ideas for future work - training on multiple tasks annotated over the same text.ßdummyTurker
5405ß0.0ßI sincerely do not understand the complaint that ''the key proof in Section 3 is stated as statements in English as opposed to mathematical claims.''ßdummyTurker
5406ß0.0ßMathematical claims can of course be made in plain English!ßdummyTurker
5407ß0.0ßNo other reviewers have issues with the mathematical rigor of the paper.ßdummyTurker
5408ß0.0ßI agree that the notion ''typology'' should have been more carefully introduced and will take care of that in the final submission.ßdummyTurker
5409ß0.0ßAn example of the candidacy relation Gen is explicitly provided in lines 448-453.ßdummyTurker
5410ß0.0ß''Gen'' stands for the ''generating function'', aka the candidacy relation; ''G'' stands for a grammar.ßdummyTurker
5411ß0.0ßthe idea of previewing informally the general structure of the reasoning as suggested here by the reviewer seems to me viable and indeed useful.ßdummyTurker
5412ß0.0ßI will make sure to implement the suggestion taking advantage of the extra page allowed by the final submission.ßdummyTurker
5413ß0.0ßthe reviewer is correct that corollary 1 technically requires an assumption of the distribution D which is stated and discussed in the text but not repeated in the statement of the corollary.ßdummyTurker
5414ß0.0ßThis is indeed a mistake on my side that will be corrected in the final version.ßdummyTurker
5415ß0.0ßNote nonetheless that this qualification does not reduces the scope of the result, as the technical assumption on D is extremely mild and indeed satisfied by all implementations proposed in the literature.ßdummyTurker
5416ß0.0ßthe ''difference plot'' proposed by the reviewer makes a lot of sense to me.ßdummyTurker
5417ß0.0ßI had indeed plotted the data that way at the beginning, but then decided to go for ''individual plots'' instead, as the difference plot can be inferred by the individual plots but not vice versa.ßdummyTurker
5418ß0.0ßIn the final submission, I can take advantage of the additional 9th page and perhaps have both plots.ßdummyTurker
5419ß0.0ßthis paper is part of a larger project on T-orders across categorical and probabilistic implementations of constraint-based phonology.ßdummyTurker
5420ß0.0ßStochastic HG (considered here) and MaxEnt (considered in another paper of mine) can be considered as two stochastic extensions of categorical HG.ßdummyTurker
5421ß0.0ßYet, these two extensions make very different typological predictions.ßdummyTurker
5422ß0.0ßIn fact, stochastic HG maintains all the T-orders of categorical HG (as shown here).ßdummyTurker
5423ß0.0ßWhile MaxEnt instead heavily prunes them (as shown in another paper).ßdummyTurker
5424ß0.0ßI completely agree that ''It is unclear who the intended audience of this work is''.ßdummyTurker
5425ß0.0ßIndeed, this might not be the typical ACL paper.ßdummyTurker
5426ß0.0ßBut the goal here is to contribute to the development of a NEW community of researchers across the ACL/linguistics border.ßdummyTurker
5427ß0.0ßInterest for this project on the theoretical linguistic side is shown for instance by the new LSA SCiL society (httpsßdummyTurker
5428ß0.0ßThe point is well taken and the problem will be addressed taking advantage of the extra page in the final submission.ßdummyTurker
5429ß0.0ßsame as above.ßdummyTurker
5430ß0.0ßThis paper focuses on relation classification of non-saturated compound sentences in Chinese.ßdummyTurker
5431ß0.0ßOur model use pre-trained word embeddings, combining the feature of relation words.ßdummyTurker
5432ß0.0ßWe know the previous work on discouse relation classification, Due to the page limit, we eliminate those papers from references.ßdummyTurker
5433ß0.0ßWe defined a non-saturated compound sentence in paper line 72 -78, and we gave examples S3, S4, S5.ßdummyTurker
5434ß0.0ßThe conditions refers to the compound sentence in which a relation mark does not appear completely.ßdummyTurker
5435ß0.0ßRelation marks omitting in the clause is uncommo in English article, no relation work on relation classification of non-saturated compound sentences in English.ßdummyTurker
5436ß0.0ßWe can use PDTB standard on relation categories.ßdummyTurker
5437ß0.0ßWe used cross-validation in experiment, but not explained in our paper.ßdummyTurker
5438ß0.0ßExtract features for C5.0 use the same dataset as SCNN.ßdummyTurker
5439ß0.0ßSource code is available on following linkßdummyTurker
5440ß0.0ßTraining dataset need manual annotation, which is time-consuming work.ßdummyTurker
5441ß0.0ßRelational word table is available on followingßdummyTurker
5442ß0.0ßhttps://github.com/Dreamliking/Relation-Classification-of-Non-saturated-Compound-Sentences-in-Chinese-via-CNNßdummyTurker
5443ß0.0ßWe believe that the monolingual case is interesting in itself, and as we show, combining treebanks for a single language can already lead to substantial improvements.ßdummyTurker
5444ß0.0ßWe do think that our parser with treebank embeddings actually learns something about linguistically based differences between treebank annotations, which our error analysis points at.ßdummyTurker
5445ß0.0ßWe will expand the error analysis in the final version.ßdummyTurker
5446ß0.0ßAs stated, we will expand the section about our error analysis, and try to include some example(s) of parsed sentences.ßdummyTurker
5447ß0.0ßIt is a good point with one hot encoding as a baseline.ßdummyTurker
5448ß0.0ßHowever, we think the treebank embeddings are actually also quite simple.ßdummyTurker
5449ß0.0ßWe did not yet try one hot encoding, but we will consider it.ßdummyTurker
5450ß0.0ßWe have only tried this for the UD treebanks, where annotations are standardized to some extent, so we cannot really speculate about the case with wildly varying annotations.ßdummyTurker
5451ß0.0ßHowever, just being able to utilize all available UD treebanks does, as we show, lead to large improvements, also for the PUD set, which is not included in training data, and we believe that it will be useful also for parsing completely new data.ßdummyTurker
5452ß0.0ßThe dimension of the treebank embedding is 12.ßdummyTurker
5453ß0.0ßIt is only available from the cited work now, but we will spell it out and discuss it in the final version.ßdummyTurker
5454ß0.0ßUsing larger embeddings does not seem to make any consistent impact, just some small variation.ßdummyTurker
5455ß0.0ßUsing smaller embeddings, in initial experiments, seems to help smaller treebanks, while hurting larger treebanks, but the differences are mostly small.ßdummyTurker
5456ß0.0ßWe will perform significance testing, and establish a threshold.ßdummyTurker
5457ß0.0ßWe have aimed at focusing only on large differences in our discussion, and for our major conclusions, though.ßdummyTurker
5458ß0.0ßWe agree that more work can be done on the choice of treebank embeddings for new sentences, as we write in future work.ßdummyTurker
5459ß0.0ßHowever, we think that our framework also gives users of the parser a choice between the treebank style they prefer, in the case of inconsistencies between annotations.ßdummyTurker
5460ß0.0ßThank you for pointing this reference out.ßdummyTurker
5461ß0.0ßWe will add Sato et al, and also Che et al., to the related work, and discuss adversarial training as another option.ßdummyTurker
5462ß0.0ßWe used gold segmentation, which we will specify in the final version.ßdummyTurker
5463ß0.0ßWe did not use any POS-tags at all, which is mentioned in the paper, but we will make this more clear.ßdummyTurker
5464ß0.0ßThere are several reasons that we didn’t take Li et al., 2016 and other VAE models as the baseline.ßdummyTurker
5465ß0.0ßFirst, our CTVAE model does not use any external features or knowledge but the models in Li et al.,2016a and Zhao et al.,2017 do.ßdummyTurker
5466ß0.0ßSecond, our CTVAE model aims to generate multiple responses for input post, which is the inherent advantage of VAEs which own a latent distribution for sampling.ßdummyTurker
5467ß0.0ßHowever, sequence-to-sequence models are essentially not able to do this.ßdummyTurker
5468ß0.0ßThe models in Li et al., 2016b and Li et al.,2015 are basically sequence-to-sequence models with complex reinforcement learning strategy or result reranking strategy.ßdummyTurker
5469ß0.0ßThus, it's improper to use them as the baseline system.ßdummyTurker
5470ß0.0ßInstead, we used the simple but effective sequence-to-sequence model as our baseline.ßdummyTurker
5471ß0.0ßThird, our CTVAE model aims at generating 1-turn Chinese Twitter response, but the models in Li et al.,2016b and the VAE models in Serban et al., 2017 are all designed for multi-turn dialogue.ßdummyTurker
5472ß0.0ßFirst, we adopted the p-value of t-test to measure the significance of difference between models, and we can see significant difference (p<0.001) between models according to our results although there were only 5 users for evaluation.ßdummyTurker
5473ß0.0ßSecond, many other related work just used similar amount of users for evaluation.ßdummyTurker
5474ß0.0ßFor example, Li et al., 2016b used only 3 judges and Ghazvininejad et al.,2017 used 7 judges.ßdummyTurker
5475ß0.0ßWe showed some analysis in the manuscript, such as line 593-596, 630-632, 640-642, and 668-670.ßdummyTurker
5476ß0.0ßWe admit that they lack detailed explanation and discussion due to the limited space.ßdummyTurker
5477ß0.0ßWe will add more discussions when we have the chance to revise the manuscript.ßdummyTurker
5478ß0.0ßAs stated in line 593-596, all the variational models had observed the responses (by sampling from the recognition network) before reconstructing them at training stage.ßdummyTurker
5479ß0.0ßThus, variational models (CVAEs/CTVAEs in paper) had much lower training perplexity than seq2seq model.ßdummyTurker
5480ß0.0ßHowever, when CVAEs/CTVAEs generated a response at testing stage, they sampled from prior distribution N(0,I), which was very different from the training stage.ßdummyTurker
5481ß0.0ßSo it's improper to compare the training perplexities of these models to evaluate their performance.ßdummyTurker
5482ß0.0ßThe goal of presenting these training perplexity results was to help readers better understand the intrinsic different characteristics between variational models and seq2seq models.ßdummyTurker
5483ß0.0ßInstead, we used LM perplexities of generated responses (the third column in Table 2) to evaluate our models.ßdummyTurker
5484ß0.0ßWe agree that there are various ways to improve the seq2seq outputs.ßdummyTurker
5485ß0.0ßHowever, the aim of this paper is the model structure, rather than other strategies.ßdummyTurker
5486ß0.0ßRegarding with the number of model parameters, first, all encoders/decoders in seq2seq, CVAE, and CTVAE models adopted 1-layer LSTM.ßdummyTurker
5487ß0.0ßThus, it's fair for comparison.ßdummyTurker
5488ß0.0ßSecond, the CVAE/CTVAE models did not have much more model parameters or more complex model structures than seq2seq model, because the recognition network won't be used to generate responses at the testing stage as the Figure 2 shows, and the seq2seq model had an extra attention part comparing with CVAEs/CTVAEs.ßdummyTurker
5489ß0.0ßOne reason is that all the LSTM layer settings in related papers we investigated are no more than 2 layers, such as 1-layer LSTM setting in Zhao et al.,2017, Li et al.2015 and Zhou et al.,2017 , and 2-layer LSTM setting in Ghazvininejad et al.,2017.ßdummyTurker
5490ß0.0ßAlso, we agree that it would be better to increase the number of LSTM layers and compare seq2seq with CVAEs/CTVAEs under a fair configuration.ßdummyTurker
5491ß0.0ßThis could be a part of our future work.ßdummyTurker
5492ß0.0ßWe will give detailed introduction to the re-ranking method in appendix upon this paper gets accepted.ßdummyTurker
5493ß0.0ßWe are sorry to make the hyphenation at linebreak-s. We will fix them during revision.ßdummyTurker
5494ß0.0ßAnd due to the limited space, there're some explanation and illustration which may not be detailed and easy-following enough.ßdummyTurker
5495ß0.0ßUpon this paper gets accepted, we will add necessary explanation to help readers understand our paper more easily.ßdummyTurker
5496ß0.0ßWe agree that the explanations could be further improved because the current explanations may require readers to have some backgrounds in text generation and VAE models.ßdummyTurker
5497ß0.0ßActually, the KL constrain term just tried to ensure that the latent distribution at training stage (output of recognition network) and testing stage (just N(0,I)) wouldn’t be far away from each other.ßdummyTurker
5498ß0.0ßThus, when sampling a point from N(0,I), CVAEs/CTVAEs can generate reasonable responses.ßdummyTurker
5499ß0.0ßAs Table 1 shows, EMD metrics was just adopted to evaluate the consistency of the sampling at training stage and testing stage, in other words, the distance between distributions of latent variable at training stage and testing stage.ßdummyTurker
5500ß0.0ßThe reason is that we found most of the existing methods with the NTCIR-12 dataset adopted the retrieval approach, i.e., to select a response from a large repository, not the generation approach.ßdummyTurker
5501ß0.0ßThe only work that adopted the same generation approach as this paper (Shang et al.,2015) had a different data split with us.ßdummyTurker
5502ß0.0ßTo the best of our knowledge, the NTCIR-12 dataset is not publicly available and there is no standard data split.ßdummyTurker
5503ß0.0ß"We admit that ""Memory Block"" was not explained sufficiently in current manuscript although the experimental results in Section 6.1 and Section 6.2 indicated its effectiveness."ßdummyTurker
5504ß0.0ß"The basic idea of introducing ""Memory Block"" into our model is to achieve topic/style clustering of the post/response pairs."ßdummyTurker
5505ß0.0ßWe will add more explanations to the concept of “Memory Block” once the paper gets accepted.ßdummyTurker
5506ß0.0ßDear reviewer, we appreciate your valuable feedback.ßdummyTurker
5507ß0.0ßIn our paper we proposed a deep-tree generation algorithm for semi-supervised node prediction.ßdummyTurker
5508ß0.0ßThe method utilizes both observed direct connections and shared neighborhood structures of vertices.ßdummyTurker
5509ß0.0ßOur proposed method aims to achieve better prediction accuracy with a better representation of the original data instead of using a more sophisticated model with more parameters.ßdummyTurker
5510ß0.0ßWe provided equation 4 and 5 to explain the vanishing impact of the alpha parameter.ßdummyTurker
5511ß0.0ßThe computational complexity is not address because we are limited to 4 pages only.ßdummyTurker
5512ß0.0ßWe will provide the source code in the newer version and readers can run our code and notice the run time is still reasonable with the longer tree, because the substructures share parameters.ßdummyTurker
5513ß0.0ßWe will also discuss the runtime complexity in the new version of the draft or in the appendix.ßdummyTurker
5514ß0.0ßDear reviewer, we appreciate your valuable feedback.ßdummyTurker
5515ß0.0ßThe computational complexity is not address because we are limited to 4 pages only.ßdummyTurker
5516ß0.0ßWe will provide the source code in the newer version and readers can run our code and notice the run time is still reasonable with the longer tree, because the substructures share parameters.ßdummyTurker
5517ß0.0ßWe will also discuss the runtime complexity in the new version of the draft or in the appendix.ßdummyTurker
5518ß0.0ßWe will include the link to the source code in the updated version of this draft.ßdummyTurker
5519ß0.0ßDear reviewer, we appreciate your valuable feedback.ßdummyTurker
5520ß0.0ßWe agree that the work will be more complete with more explanation about LSTM, but due to the page limit we assumed that most readers have an understanding of LSTM.ßdummyTurker
5521ß0.0ßWe plan to add more explanation in the newer version if the page limit allows, otherwise, we will add it to in the appendix.ßdummyTurker
5522ß0.0ßWe assumed BFS is a very fundamental graph traversal method most readers are familiar with.ßdummyTurker
5523ß0.0ßWe will introduce BFS in the second version.ßdummyTurker
5524ß0.0ßAlpha is generated from the softmax, which has real values in the range of (0,1) that add up to 1.ßdummyTurker
5525ß0.0ßWe believe the bound is correct.ßdummyTurker
5526ß0.0ßWe wanted to add the attention model in the experiments section but it will lead to us having more than 4 pages.ßdummyTurker
5527ß0.0ßWe will include more details on the attention mechanism and the corresponding experiments in the appendix in the newer version.ßdummyTurker
5528ß0.0ßIn our paper, we used micor-F1 score to show that our method outperforms the other methods.ßdummyTurker
5529ß0.0ßAbout comparison with overnight---The reason we do not have the comparison for now is because the Overnight approach focuses on level-1 compositionality only.ßdummyTurker
5530ß0.0ßHowever, we will add more discussion and analysis, in the final draft.ßdummyTurker
5531ß0.0ßFor more details please see the response to Reviewer #1, first point.ßdummyTurker
5532ß0.0ßCompare to Seq2seq, our model is significantly different since we  essentially use a sequence-to-derivation tree model, where the decoder is a stack-LSTM (Dyer et al., 2015) which is able to encode tree structures and generate tree nodes.ßdummyTurker
5533ß0.0ßAbout comparison with overnight---The reason we do not have the comparison for now is because Overnight paper focuses on level-1 compositionality only.ßdummyTurker
5534ß0.0ßHowever, we will add more discussion and analysis, in the final draft.ßdummyTurker
5535ß0.0ßFor more details please see the response to Reviewer #1, first point.ßdummyTurker
5536ß0.0ßYou are right that the template-based approach, analogously to other crowdsourcing tasks that include paraphrasing, may yield less natural queries in some cases.ßdummyTurker
5537ß0.0ßBut this is not something we systematically observed in our data, and the format of queries also depends on the domain (e.g., a company manager may input more formal queries when searching for employees; compared to a random web search user).ßdummyTurker
5538ß0.0ßFor questions: We will provide more discussion of the limitation of our approach in principle and practice when one extra page is given.ßdummyTurker
5539ß0.0ßIn principle, with a well-designed front end, our approach should handle grammars of arbitrary complexity.ßdummyTurker
5540ß0.0ßIn practice, our current experiment does not consider a few linguistic phenomena such as universal quantifiers.ßdummyTurker
5541ß0.0ßMore details of the dynamic evaluation will be provided too.ßdummyTurker
5542ß0.0ßIn our experiments, each HIT consists of 2 tasks.ßdummyTurker
5543ß0.0ßThe resulting queries obtained from the same worker indeed display some degree of word overlap (5.24), compared to queries from different workers (4.76).ßdummyTurker
5544ß0.0ßThis suggests thatßdummyTurker
5545ß0.0ßworkers have the potential of using repetitive language if they work on many examples.ßdummyTurker
5546ß0.0ßHowever, this can be alleviated with the requirement of mandatory paraphrasing in the instructions and also by limiting the maximum examples one worker can do to encourage variety.ßdummyTurker
5547ß0.0ßWe will add a separate section discussing the differences between Overnight and our framework.ßdummyTurker
5548ß0.0ßWe mention here briefly the following cases which our framework can handle better than Overnight (which involves ambiguity caused by PP attachment).ßdummyTurker
5549ß0.0ßThey were assessed by human evaluatorsßdummyTurker
5550ß0.0ß*Case 1: conjunction (or disjunction)ßdummyTurker
5551ß0.0ßR1=find the diseases with incubation period longer than 5days;ßdummyTurker
5552ß0.0ßR2=find the diseases with number of complications larger than 6;ßdummyTurker
5553ß0.0ßR3=find R1 and R2 with largest number of symptomsßdummyTurker
5554ß0.0ßdiseases with incubation period longer than 5days and diseases with number of complications larger than 6 with largest number of symptomsßdummyTurker
5555ß0.0ß"Here, the attachment point of the ""with largest number of symptoms"" is ambiguous."ßdummyTurker
5556ß0.0ß*Case 2: lexicon readabilityßdummyTurker
5557ß0.0ßR1=find the complications of fever;ßdummyTurker
5558ß0.0ßR2=find the symptoms of R1;ßdummyTurker
5559ß0.0ßR3=find the treatments of R2ßdummyTurker
5560ß0.0ßtreatments of symptoms of complications of feverßdummyTurker
5561ß0.0ßHere, the output of overnight grammar is difficult to interpret.ßdummyTurker
5562ß0.0ß*Case 3: when the previous querying result is used for comparison in the next queryßdummyTurker
5563ß0.0ßR1=find the incubation period of fever;ßdummyTurker
5564ß0.0ßR2=find disease with incubation period longer than R1;ßdummyTurker
5565ß0.0ßR3=find R2 with largest number of symptomsßdummyTurker
5566ß0.0ßOutput of overnight grammar: disease with incubation period longer than incubation period of fever with largest number of symptomsßdummyTurker
5567ß0.0ß"Here, the attachment point of the ""with largest number of symptoms"" is ambiguous."ßdummyTurker
5568ß0.0ßMore ambiguity can arise with higher levels of compositionality or a mixture of the above cases.ßdummyTurker
5569ß0.0ßWe agree that for limited compositionally, the template-based approach has no merit over the grammar-based one, in terms of the easiness of interpretation.ßdummyTurker
5570ß0.0ßHowever, the advantage of the template-based approach lies in cases  when compositionally increases.ßdummyTurker
5571ß0.0ßConducting experiments which assess the speed of data acquisition on AMT is difficult and cannot be done in a very controlled fashion.ßdummyTurker
5572ß0.0ßWe have conducted empirical evaluation and analysis on when the template-based approach is advantageous over the overnight-based one, and found out that in cases involving difficult PP attachment discussed above, crowd-workers favour our approach.ßdummyTurker
5573ß0.0ßWe will add this analysis.ßdummyTurker
5574ß0.0ßFinally, another advantage of the template-based approach is that it enables the user to describe the complicated querying task with a sequence of sentences or instructions.ßdummyTurker
5575ß0.0ßAbout string-to-derivation---Please note that our goal here is to  mark the derivation at the stage of data collection---we are not aware of any work providing such level of annotation that supports supervised training.ßdummyTurker
5576ß0.0ßYou are right that a derivation can be recovered sometimes; however, this requires the additional modelling step of grammar induction and only applies to unambiguous logical-forms.ßdummyTurker
5577ß0.0ßIt is common that a logical-form can be split in multiple ways[1]ßdummyTurker
5578ß0.0ß[1]Kwiatkowksi et al., 2010, Inducing Probabilistic CCG Grammars from Logical Form with Higher-order UnificationßdummyTurker
5579ß0.0ßThe usefulness of the dynamic mode is dependent on the user  (whether they want to use it to elicit data from the crowd or from a developer; or whether they just want to use the software as an annotation tool, see the code attached).ßdummyTurker
5580ß0.0ßThis is hard to justify quantitatively.ßdummyTurker
5581ß0.0ßHowever, we will provide more experimental details (budget, diversity of logical forms) and findings of the dynamic mode with AMT when one extra page and appendix are given.ßdummyTurker
5582ß0.0ßFinal remarks: Rules in Table 1 follow Overnight as we stated in the paper.ßdummyTurker
5583ß0.0ßOur intention is to show that a user does not need to understand these backend rules.ßdummyTurker
5584ß0.0ßHowever, we will add more specifications and direct readers to related work.ßdummyTurker
5585ß0.0ßWe will also improve the readability of the modelling part based on your advice (what you understand about attention is correct).ßdummyTurker
5586ß0.0ßFinally, as we say in the paper (Section 2.1),  our template-based approach is based on Overnight; and we do hope to improve it to handle more complicated querying tasks.ßdummyTurker
5587ß0.0ßThank you for the valuable comments and suggestions!ßdummyTurker
5588ß0.0ßRegarding [1] and [2], using pointer networks is most beneficial when working on the word level, because of the difficulty of selecting rare or unknown words from the vocabulary.ßdummyTurker
5589ß0.0ßIn addition to a standard LSTM word-level model, we evaluate two recent subword and character-level models in the paper.ßdummyTurker
5590ß0.0ßWord/character models perform better at generating rare or novel words not in the vocabulary, as shown for machine translation [6] for example -- thus we didn’t look into the pointer models.ßdummyTurker
5591ß0.0ßWe will include this comment in the paper.ßdummyTurker
5592ß0.0ßOur focus here is specifically on data-driven summarization methods, we will update the paper to make this more explicit.ßdummyTurker
5593ß0.0ßAs far as we know, this is the first attempt at generating the title of a paper directly from the abstract, and at generating the abstract from the body.ßdummyTurker
5594ß0.0ßThe datasets that we propose here are also substantially larger than what has been used in previous workßdummyTurker
5595ß0.0ß[1] Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, C¸ a glar Gulc¸ehre, and Bing Xiang.ßdummyTurker
5596ß0.0ß2016.ßdummyTurker
5597ß0.0ßAbstractive text summarization using sequence-to-sequence rnns and beyond.ßdummyTurker
5598ß0.0ßCoNLL 2016 [2] Abigail See, Peter J Liu, and Christopher D Manning.ßdummyTurker
5599ß0.0ß2017.ßdummyTurker
5600ß0.0ßGet to the point: Summarization with pointer generator networks.ßdummyTurker
5601ß0.0ßACL.ßdummyTurker
5602ß0.0ß[3] Mei, Qiaozhu, & Zhai, ChengXiang.ßdummyTurker
5603ß0.0ß2008.ßdummyTurker
5604ß0.0ßGenerating Impact-Based Summaries for Scientiﬁc Literature.ßdummyTurker
5605ß0.0ßPages 816–824 of: Proceedings of the 46th Annual Conference of the Association for Computational Linguistics (ACL-08).ßdummyTurker
5606ß0.0ß[4] Qazvinian, Vahed, & Radev, Dragomir R. 2008a.ßdummyTurker
5607ß0.0ßScientiﬁc Paper Summarization Using Citation Summary Networks.ßdummyTurker
5608ß0.0ßIn: Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08).ßdummyTurker
5609ß0.0ß[5] Radev, Dragomir R., Muthukrishnan, Pradeep, Qazvinian, Vahed, & Abu-Jbara, Amjad.ßdummyTurker
5610ß0.0ß2013.ßdummyTurker
5611ß0.0ßThe ACL anthology network corpus.ßdummyTurker
5612ß0.0ßLanguage Resources and Evaluation, 1–26.ßdummyTurker
5613ß0.0ß[6] Sennrich, Rico, Barry Haddow, and Alexandra Birch.ßdummyTurker
5614ß0.0ß"""Neural Machine Translation of Rare Words with Subword Units."""ßdummyTurker
5615ß0.0ßProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).ßdummyTurker
5616ß0.0ßVol.ßdummyTurker
5617ß0.0ß1.ßdummyTurker
5618ß0.0ß2016.ßdummyTurker
5619ß0.0ßThis is an evaluation paper that investigates the performance of existing data-driven models on two novel datasets composed from scientific journal articles.ßdummyTurker
5620ß0.0ßWhile there is no novelty on the modelling side, we believe the datasets and the extensive analysis that we provide, of the performance of various extractive and abstractive summarization systems, are valuable to the summarization community.ßdummyTurker
5621ß0.0ßSome clarification would be helpful to understand your comment.ßdummyTurker
5622ß0.0ßWe are very grateful for making us find the weakness points of our paper.ßdummyTurker
5623ß0.0ßWe are sorry for the confusing description of our motivation.ßdummyTurker
5624ß0.0ß"The way of ""no fusion of the sentences"" is the important point."ßdummyTurker
5625ß0.0ßIn classification tasks, there are roughly three approaches for model ensemble: model selection (preprocess), model average (runtime-process), and majority vote (postprocess).ßdummyTurker
5626ß0.0ß"In text generation studies, the model selection (""Single"" in Line 446-448) and the model average (""EnsSum"" in Line 450-452) have been frequently used, but the majority vote has less attention because of the fact that a majority output may not exist, as described in Line 076-083."ßdummyTurker
5627ß0.0ßOur work can be regarded as an extension of the simple majority vote for text generation.ßdummyTurker
5628ß0.0ßWe believe that our method is also simple, but meaningful as an alternative direction of ensemble techniques.ßdummyTurker
5629ß0.0ßWe will add more explanations to clarify this point.ßdummyTurker
5630ß0.0ßWe are sorry for the confusing description of our perspective.ßdummyTurker
5631ß0.0ßFirst of all, we introduced several recent studies of ensemble methods (involving distillation) for MT in Line 737-779.ßdummyTurker
5632ß0.0ßAlthough there have been several studies just using the runtime-ensemble (described in Line 182-186), we omitted them since the above-mentioned recent papers include the definition.ßdummyTurker
5633ß0.0ßAs for reranking, we agree that reranking is one of the post-ensemble methods from a standpoint of ensemble research.ßdummyTurker
5634ß0.0ßHowever, there is no ensemble method reranking outputs of multiple models, and how to rerank outputs for ensemble is not trivial.ßdummyTurker
5635ß0.0ßTherefore, we decided to leave this direction as future work, as described in Line 795.ßdummyTurker
5636ß0.0ßWe will add more explanations to clarify this point.ßdummyTurker
5637ß0.0ßThank you for the suggestion.ßdummyTurker
5638ß0.0ßLet the complexity of the beam-search be $O(B)$.ßdummyTurker
5639ß0.0ßThe complexity of the runtime-ensemble is $O(nB)$, while that of our method is $O(nB)+O(mn^2)$, where $n$ is the number of models, and $m$ is the dimension of output space, as shown in Line 596-598.ßdummyTurker
5640ß0.0ßHowever, in our method, we can reduce it to $O(B)+O(mn)$ by simply parallelize the lines 2-4 and 6-8 in Algorithm.1 without any change of the model codes on the deep learning framework.ßdummyTurker
5641ß0.0ßSince the operations of $B$ includes all matrix calculations in the model, we can basically assume $B>>mn$.ßdummyTurker
5642ß0.0ßIf we want to parallelize the runtime-ensemble, we need to modify more than a hundred codes after understanding a whole system.ßdummyTurker
5643ß0.0ß"In fact, there is no implementation of a parallel ensemble decoder so far, because of the lack of APIs for ""model parallel"" (not ""data parallel"") in most frameworks."ßdummyTurker
5644ß0.0ßWe are sorry for the lack of explanation.ßdummyTurker
5645ß0.0ßROUGE-1 is commonly used for the evaluations of many summarization tasks and has been shown to have the highest correlation with human summaries (Lin and Hovy 2003).ßdummyTurker
5646ß0.0ßThe 1\% improvement can be regarded as reasonable according to the improvement-history of the state-of-the-art methods in Table.1.ßdummyTurker
5647ß0.0ßWe agree that the improvement looks small at a first glance, but we believe such a small improvement can yield a big impact for a real service.ßdummyTurker
5648ß0.0ßIn fact, even 0.01\% improvement of the click-through rate in a popular service can yield ad revenue of million dollars.ßdummyTurker
5649ß0.0ßTherefore, the 1\% improvement is promising from a practical standpoint, since headline quality is related to the CTR of ads.ßdummyTurker
5650ß0.0ß"At least, we conducted statistical tests, so please see ""Reply to weakness-argument 2"" for Reviewer 3."ßdummyTurker
5651ß0.0ßWe really feel delightful for your detailed and insightful comments expressing many strength points of our paper.ßdummyTurker
5652ß0.0ßThank you very much for encouraging us and our paper believing possibilities of this direction, in spite of knowing some weaknesses.ßdummyTurker
5653ß0.0ßYour understanding is correct.ßdummyTurker
5654ß0.0ßWe will add more explanations to clarify it.ßdummyTurker
5655ß0.0ßThank you for the suggestion.ßdummyTurker
5656ß0.0ßWe will expand the discussion in Line 794-799.ßdummyTurker
5657ß0.0ßYes.ßdummyTurker
5658ß0.0ßWe can apply our method to any text generation model.ßdummyTurker
5659ß0.0ßThe left figure in Figure 2 shows a 2D visualization by multi-dimensional scaling (MDS) of the sentence embeddings of ten outputs, where MDS can plot the outputs such that the between-outputs distances are preserved as well as possible.ßdummyTurker
5660ß0.0ßSince the central part in the figure looks dense because of the output-IDs (0,1,2,3,4), they have higher density scores (higher color-intensities) than the other outputs.ßdummyTurker
5661ß0.0ßAgree.ßdummyTurker
5662ß0.0ßMy native language is not EnglishßdummyTurker
5663ß0.0ßWe tried to define the problem in the abstract.ßdummyTurker
5664ß0.0ßThe problem is simply how to improve predictability by using geographic locationsßdummyTurker
5665ß0.0ßPredictionßdummyTurker
5666ß0.0ßIf it is a ML problem.ßdummyTurker
5667ß0.0ßThe entries are the localities of origin of the tweets and the composition of the electorate of those localities.ßdummyTurker
5668ß0.0ßThe outputs are the geographic weighting factorsßdummyTurker
5669ß0.0ßThe method was the combination of different models.ßdummyTurker
5670ß0.0ßClassification models were NB, Random Forest, SVM and use of dictionaries of positive and negative terms.ßdummyTurker
5671ß0.0ßAgree.ßdummyTurker
5672ß0.0ßBut the cited authors do not give the mechanisms for the definition and calculation of the geographic weighting factorsßdummyTurker
5673ß0.0ßWhat we wanted to say is that there are two types of actors, who we follow in their accounts, who are the politicians and the others who are the votersßdummyTurker
5674ß0.0ßThe dictionary is not fixed in advanceßdummyTurker
5675ß0.0ßThank you for your comment!ßdummyTurker
5676ß0.0ßIn our case, the user would revise the input by deleting the final food entry from the app and recording it again.ßdummyTurker
5677ß0.0ßEven if they were able to revise the input value, each revision requires an additional turn in the dialogue, which still adds frustration.ßdummyTurker
5678ß0.0ßThis problem highlights the importance of the agent's ability to ask questions that are intuitive for the user (e.g., asking milkfat for milk items, but not for vegetables), since otherwise the user may get confused and answer the followup question incorrectly.ßdummyTurker
5679ß0.0ßThank you for checking--we will be sure to mention this in the camera-ready version of the paper, if it gets accepted, since we will have an extra page!ßdummyTurker
5680ß0.0ßWe briefly described the corpus in section 4.1, but due to space constraints, were not able to mention every detail.ßdummyTurker
5681ß0.0ßThe 78,980 samples in our corpus were randomly shuffled, then divided into 10% test, 10% validation, and 80% training sets.ßdummyTurker
5682ß0.0ßThe originality of this paper is primarily, as you noted when commenting on the strengths of our work, in the application of RL to an interesting new task in the nutrition domain, for asking followup clarification questions to narrow down the correct food item.ßdummyTurker
5683ß0.0ßThe originality of the methods is three-foldßdummyTurker
5684ß0.0ßThank you for your insightful comments--comparing to conversational recommender systems is an interesting idea, and we would like to add this discussion in the camera-ready version of the paper (if accepted)!ßdummyTurker
5685ß0.0ßWe would also love to hear your thoughts on which deep RL techniques and user simulator designs to experiment with, which we think would make a great addition to the paper, space permitting.ßdummyTurker
5686ß0.0ßActually we have performed experiments on the En-De dataset from WMT16 as well, however we are unable to include the results due to the limited space, which show a similar tendency as our Table 1.ßdummyTurker
5687ß0.0ßYes, they are combined by concatenation.ßdummyTurker
5688ß0.0ßWe will describe this part in detail, including the formulas of Tree-GRU.ßdummyTurker
5689ß0.0ßIt is indeed too brief for this part, and we will describe it in detail.ßdummyTurker
5690ß0.0ßTree-GRU is built over the sequential RNN.ßdummyTurker
5691ß0.0ßBy removing the sequential RNN (-Sequential), Tree-GRU is built over the sequence of input word embeddings.ßdummyTurker
5692ß0.0ßThus the {x_i} are word embeddings after removing sequential RNNs.ßdummyTurker
5693ß0.0ßThe same/similar dataset has been exploited by a number of studies in the last year, including Li et al.ßdummyTurker
5694ß0.0ß(2017), Chen et al.ßdummyTurker
5695ß0.0ß(2017a) , Chen et al.ßdummyTurker
5696ß0.0ß(2017b), Wu et al.ßdummyTurker
5697ß0.0ß(2017a), Wu et al.ßdummyTurker
5698ß0.0ß(2017b), Yang et al.ßdummyTurker
5699ß0.0ß(2017) and Zhou et al.ßdummyTurker
5700ß0.0ß(2017) as mentioned in our work.ßdummyTurker
5701ß0.0ßThus we believe this dataset is reasonable.ßdummyTurker
5702ß0.0ßIn addition, we have also conducted experiments on the En-De dataset from WMT16, which show a similar tendency as Ch-En.ßdummyTurker
5703ß0.0ßDue to the limited space, we are unable to include them in this version.ßdummyTurker
5704ß0.0ßWe agree that a stronger baseline is better, and actually we do make the baseline much stronger by ensemble.ßdummyTurker
5705ß0.0ßWe agree you can list a number of other techniques to make the baseline stronger (e.g.ßdummyTurker
5706ß0.0ßmore layers, UNK replacement, subword units and coverage).ßdummyTurker
5707ß0.0ßIs it necessary to include all these techniques?ßdummyTurker
5708ß0.0ßSeveral of these techniques can bring extra issues which are not closely-related to our claims.ßdummyTurker
5709ß0.0ßFor example, if we use more layers, several questions will comeßdummyTurker
5710ß0.0ßOur proposed approaches are new to the NMT community.ßdummyTurker
5711ß0.0ßWe focus on NMT and are the first to apply the three methods on NMT, comparing and analyzing them in detail.ßdummyTurker
5712ß0.0ßIt aims to explain the benefit of Tree-Linearization from the linguistic perspective, which lies in that it handles function words better.ßdummyTurker
5713ß0.0ßWe explain your opposite issue of Tree-Linearization theoretically and empirically.ßdummyTurker
5714ß0.0ßAs known, the attention mechanism is used to solve word alignments together with translation jointly.ßdummyTurker
5715ß0.0ßTheoretically, most words are still aligned to the same original words (e.g., modern – 现代) in Tree-Linearization, and only several high-frequency function words are aligned to high-frequency reduce actions with less ambiguities, while their suitable source words are difficult to be determined as discussed in the paper.ßdummyTurker
5716ß0.0ßThus the increasing difficulties are relatively small (maybe even easier as we reduce the alignment ambiguities).ßdummyTurker
5717ß0.0ßEmpirically, the resulting word alignments are good enough with the same scale of parallel corpus (the word alignments of other words in the example is similar among our proposed models), bringing better performances.ßdummyTurker
5718ß0.0ßWe will describe this point clearly in this Section.ßdummyTurker
5719ß0.0ßThank you for your comments!ßdummyTurker
5720ß0.0ßWe aim to provide easy-to-use contextualized representation, which does not require extensive GPU memories, but can be incorporated in a plug-in-and-play manner and pruned for the target task.ßdummyTurker
5721ß0.0ßThis goal is very challenging while our proposed method demonstrates promising empirical results.ßdummyTurker
5722ß0.0ßThese results validate the capability of LD-Net to prune the PTLMs effectively without retraining.ßdummyTurker
5723ß0.0ßWe believe that more efforts can be made to further improve the performance of the pruned models.ßdummyTurker
5724ß0.0ßSo far, most existing model compression approaches also have some degree of performance loss, e.g., (Han et al., 2015) and (Wen et al., 2016).ßdummyTurker
5725ß0.0ßWe plan to develop more sophisticated methods to achieve this goal in the future work.ßdummyTurker
5726ß0.0ßFor the noise reduction task, we compared our methods based on state-of-the-art CNN models.ßdummyTurker
5727ß0.0ßTo the best of our knowledge, ours is the first effort to examine experimentally the effects of noise reduction methods on those neural network architectures.ßdummyTurker
5728ß0.0ßYes.ßdummyTurker
5729ß0.0ßIt is correct.ßdummyTurker
5730ß0.0ßHowever, the ratio of overlapping instances in the outputs is small.ßdummyTurker
5731ß0.0ßDue to the limited space, we have not included details of experimental results, e.g., the P@50 with a different number of seeds (5, 10, ..., 50).ßdummyTurker
5732ß0.0ßWe will add them if possible.ßdummyTurker
5733ß0.0ßOur dataset will be published along with the paper, as mentioned in the manuscript.ßdummyTurker
5734ß0.0ßTo the best of our knowledge, our method is the first approach that merges two crucial tasks (automatic seed selection for bootstrapping relation extraction and noise reduction for distant supervised relation extraction) into one problem with the same goal.ßdummyTurker
5735ß0.0ßThe experiment results also show the effectiveness of our proposed methods.ßdummyTurker
5736ß0.0ßDue to the limited space, we have not included details of experimental results, e.g., the P@50 with a different number of seeds (5, 10, ..., 50), which show the explanation for results in the bootstrapping process.ßdummyTurker
5737ß0.0ßDepends on the task and the characteristic of two datasets, each proposed method shows its effectiveness, e.g., the K-means-based approach using the embedding offset between two entities is appropriate for the automatic seed selection task since the taxonomy of relations in the part-whole dataset is systematically organized.ßdummyTurker
5738ß0.0ßAs a result, the K-means-based approach showed the best results.ßdummyTurker
5739ß0.0ßHowever, when applying to the noise reduction task, many relations are ambiguous.ßdummyTurker
5740ß0.0ßIn that case, an ensemble method got better results.ßdummyTurker
5741ß0.0ßThanks for your suggestion.ßdummyTurker
5742ß0.0ßWe will add more large-scale experiments on other languages, such as WMT14 En-Fr, WMT17 Zh-En in the new version of the paper.ßdummyTurker
5743ß0.0ßWe have the statistics about translation length on the IWSLT2014 German-English test set.ßdummyTurker
5744ß0.0ßThe average translation length of our methods is 18.85, the average length of the baseline is 18.43, while the average reference length is 19.42.ßdummyTurker
5745ß0.0ßWe have done the significance test on IWSLT2014 German-English test set (6750 sentences).ßdummyTurker
5746ß0.0ßWe make 20 samplings from dataset and each sampling contains 1000 sentences.ßdummyTurker
5747ß0.0ßThe p-value of the significance test is 8.5e-12<0.01, which verifies our improvements.ßdummyTurker
5748ß0.0ßThanks for the comments.ßdummyTurker
5749ß0.0ßM needs to be added to qk^T, but not alpha in Equation (8) and (10).ßdummyTurker
5750ß0.0ßWe will correct them in the new version of the paper.ßdummyTurker
5751ß0.0ßThe squeezed gate mechanism (SG) takes the branch x as input and outputs the gating values SG(x), which is of the same dimension with x.ßdummyTurker
5752ß0.0ßThe final gated output of this branch is x*SG(x), where * means element-wise multiplication, and is denoted as the scale block in Figure 3.ßdummyTurker
5753ß0.0ßAs for the squeezed gate mechanism, it consists of a two-layer fully connected network with ReLU inside, which is used to increase the non-linearity, and a sigmoid function, which is used to output a gated value between 0~1.ßdummyTurker
5754ß0.0ßIn order to reduce the model parameters of the SG, the size of the hidden layer in the two-layer fully connected network is reduced to 1/16 of the input dimension.ßdummyTurker
5755ß0.0ßThere is also similar gate mechanism in the field of computer vision [1].ßdummyTurker
5756ß0.0ßWe will add the above clear explanations in the new version of the paper.ßdummyTurker
5757ß0.0ß[1].Squeeze-and-Excitation Networks.ßdummyTurker
5758ß0.0ßJie Hu, Li Shen, Gang Sun.ßdummyTurker
5759ß0.0ßArxiv, 2017.ßdummyTurker
5760ß0.0ßYes, we follow the same experiment setting as described in (Vaswani et al.ßdummyTurker
5761ß0.0ß2017), including the layers, hidden size, optimizer, batch size, learning rate and some other training details.ßdummyTurker
5762ß0.0ßOur paper has introduced these details in Sec 4.2.ßdummyTurker
5763ß0.0ßThanks for your comments.ßdummyTurker
5764ß0.0ßWe will re-fine and improve the writings of paper in the new version.ßdummyTurker
5765ß0.0ßThanks for your suggestion, we will add more experimental analysis based on the WMT14 task, and find the best configuration on this task through ablation study.ßdummyTurker
5766ß0.0ßThanks for your suggestion.ßdummyTurker
5767ß0.0ß"We have conducted an experiment that removes global self-attention both in the encoder and decoder, based on setting ""e1"" in Table 1."ßdummyTurker
5768ß0.0ß"The BLEU score is 31.97, 0.31 point lower than setting ""e1""."ßdummyTurker
5769ß0.0ßWe are pleasant to provide more ablation experiments of global self-attention in the new version.ßdummyTurker
5770ß0.0ßWe have the decoding time statistics on IWSLT2014 German-English test set.ßdummyTurker
5771ß0.0ßThe baseline costs 112s while our method costs 120s, with an additional 7.1%time increase, which is acceptable.ßdummyTurker
5772ß0.0ßThanks for your comments on improving our papers.ßdummyTurker
5773ß0.0ßIn the new version, we will fix the typo errors and improve the writing of the paper.ßdummyTurker
5774ß0.0ßBesides, thank you for your suggestion about disabling positional embedding.ßdummyTurker
5775ß0.0ßWe conduct two experiments that removing the positional embedding for baseline and our method (setting e1  in Table 1) respectively.ßdummyTurker
5776ß0.0ßBy removing positional embedding, we find there is just a 2.27 BLEU score drop (from 32.28 to 30.01) for our method, while 15.71 BLEU score drop (from 31.27 to 15.56) for the baseline.ßdummyTurker
5777ß0.0ßThis result shows that our proposed directional and local self-attention may act as some functionalities of positional embedding.ßdummyTurker
5778ß0.0ßWe will conduct more analysis on this in the future work.ßdummyTurker
5779ß0.0ßOur model is easy to reimplement and train, we will provide training details as part of our code release.ßdummyTurker
5780ß0.0ßD-match is particularly designed for scoped meaning representation evaluation.ßdummyTurker
5781ß0.0ßWe will add examples of system output.ßdummyTurker
5782ß0.0ßThe model takes nearly 50 hours to train on one TITAN X GPU without batches.ßdummyTurker
5783ß0.0ßWe did not systematically explore the various model hyper-parameters.ßdummyTurker
5784ß0.0ßBetter tunning  which could yield better performance.ßdummyTurker
5785ß0.0ßOf course, we will add examples in the appendix in the final version.ßdummyTurker
5786ß0.0ßThere is not much recent work on DRT parsing.ßdummyTurker
5787ß0.0ßA few structured prediction models have been recently proposed for syntactic parsing.ßdummyTurker
5788ß0.0ßWe have applied these models (e.g., Dyer et al., 2016) to our task with some modifications, but we found out that they are difficult to train and perform worse than our proposed model and comparison baselines due to the large number of referents in the meaning representation.ßdummyTurker
5789ß0.0ßWe will include this discussion in the paper· Other semantic parsing models (e.g., Dong and Lapata, 2016; Alvarez-Meils and Jaakkola, 2017) can  in theory to predict well-formed logical forms, however, they are not directly applicable to DRT parsing since they cannot handle scoped meaning   representations,  i.e., variables can refer to a block meaning representation (e.g., variable p1 refers to an entire  proposition and variable k1 refers to a segment of meaning).ßdummyTurker
5790ß0.0ßBoxer depends on a CCG parser and rules that map CCG supertags to semantic representation.ßdummyTurker
5791ß0.0ßOur aim was to design a data-driven DRS parsing system which is does not heavily rely on pre-processing tools.ßdummyTurker
5792ß0.0ßWe will include an analysis to show where Boxer and the proposed model diverge.ßdummyTurker
5793ß0.0ßPlease see our response to  argument 1.ßdummyTurker
5794ß0.0ßThanks for the useful comments.ßdummyTurker
5795ß0.0ßThe major problem with using APSyn with dense vectors is that its score would be too much affected by the first few ranks, while almost ignoring all the other features.ßdummyTurker
5796ß0.0ßThis works well on long-tail distributions (such as those in sparse count-based vectors, which may contain tens of thousands of features, each of which still contributes to the final score), but it does not on 300 dense dimensions.ßdummyTurker
5797ß0.0ßGiven lower dimensional vectors, we showed that the rank still plays an important role, but its weight decay must be smoother.ßdummyTurker
5798ß0.0ßIn the literature, previous papers have mentioned failing using APSyn on dense representations, and we identified in this sharp weight decay the reason for such failures.ßdummyTurker
5799ß0.0ßAs for the comparison with Spearman's Rho, it performs very alike vector cosine, and APSynP almost always outperforms it (an exception being in the outlier detection task, with Glove trained on Wikipedia and 1-vs-1 comparison).ßdummyTurker
5800ß0.0ßThis baseline will be certainly added in the camera ready, if accepted.ßdummyTurker
5801ß0.0ßWe do agree with the reviewer's observation about the low reliability of similarity judgements.ßdummyTurker
5802ß0.0ßIn our view, distributional models should be either evaluated on multiple tasks (as we have tried to do in this short paper; downstream applications should also be approached) or against cognitively-derived gold standards.ßdummyTurker
5803ß0.0ßIn this respect, it might be worthy noticing that Lapesa and Evert (2013) showed good performance of rank-based metrics on semantic priming data.ßdummyTurker
5804ß0.0ßThanks for the comments and the observations.ßdummyTurker
5805ß0.0ßIt is true that the inspiration for the metric comes from the cognitive literature and that the idea of prototypes built out of salient features of word clusters can be applied to many other tasks (e.g.ßdummyTurker
5806ß0.0ßselectional preferences, for predicate-argument composition).ßdummyTurker
5807ß0.0ßConcerning your criticism about the assumption, it is an unhappy formulation on our side: we just meant that word embedding dimensions can be used to coherently group semantic clusters, as in the outlier detection task, and not that they achieve a dimension-wise encoding of semantic features in themselves (and sure, there's no doubt that Levy and  colleagues would never say such a thing, thanks for pointing this out!ßdummyTurker
5808ß0.0ß).ßdummyTurker
5809ß0.0ßWe thank the reviewer for the insightful comments and feedback!ßdummyTurker
5810ß0.0ßWe agree that RNNs and attention are not new.ßdummyTurker
5811ß0.0ßHowever, for dialogue state tracking, we show that simply combining them does not perform as well (see ‘-local’ and ‘-global’ in Table 2 and Figure 5 of ablation study).ßdummyTurker
5812ß0.0ßOur main contribution, the global-local attentive module, was designed specifically for dialogue state tracking and learns parameter sharing between different dialogue slots.ßdummyTurker
5813ß0.0ßOur analysis shows that this significantly outperforms existing models as well as ablation models, including simple combinations of RNN and attention with fixed weight sharing (‘-local’) and no weight sharing (‘-global’).ßdummyTurker
5814ß0.0ßA slot-value embedding is a list of word embeddings of words in the slot-value pair (e.g.ßdummyTurker
5815ß0.0ß[food, =, french]).ßdummyTurker
5816ß0.0ßWe thank the reviewer for the insightful comments and feedback!ßdummyTurker
5817ß0.0ßWe ran the experiment with 10 random seeds for DSTC2 and obtained a mean joint goal accuracy of 74.4% with 0.2% standard deviation and a mean request accuracy of 97.5% with 0.1% standard deviation.ßdummyTurker
5818ß0.0ßThese are both significantly higher than prior work.ßdummyTurker
5819ß0.0ßWe will add this to our results in the paper.ßdummyTurker
5820ß0.0ßScalability to large ontologies is a good question.ßdummyTurker
5821ß0.0ßLike previous DSTs, our parameter size and speed scales linearly in the number of slots (if we do not run each slot tracker in parallel).ßdummyTurker
5822ß0.0ßFor much larger ontologies, we can employ a hierarchical system (e.g.ßdummyTurker
5823ß0.0ßa ranker) which first narrows down the set of plausible slots for a question, and then run a fine-grain DST such as ours.ßdummyTurker
5824ß0.0ßA slot-value embedding is a list of word embeddings of words in the slot-value pair (e.g.ßdummyTurker
5825ß0.0ß[food, =, french]).ßdummyTurker
5826ß0.0ßThey do collide with real words and we can try allocating a separate embedding matrix to avoid collision.ßdummyTurker
5827ß0.0ßFirst of all, thank you very much for the positive feedback and appreciation for our work.ßdummyTurker
5828ß0.0ßOur response to the questions.ßdummyTurker
5829ß0.0ßHow are mixed categories handled in terms of latent actions, e.g.ßdummyTurker
5830ß0.0ßrequesting an action?ßdummyTurker
5831ß0.0ßSince our latent action can have multiple latent variables so that ideally it would be the best if we can learn disentangled representation in z.ßdummyTurker
5832ß0.0ßIn this way, mixed categories might be decomposed into several independent factors and dimension of z is in charge of one factor.ßdummyTurker
5833ß0.0ßWe would like to explore more in the direction and see if such decomposition can be discovered from the data.ßdummyTurker
5834ß0.0ßWhat are the limitations of the strict separation between context-independent semantics (z) and explicitly modeled context (c), where the former is determined based on the latter by the policy network?ßdummyTurker
5835ß0.0ßOne limitation is the exposure bias problem because the decoder is trained with the latent action from the recognition network during training, while the decoder uses z output by the policy network.ßdummyTurker
5836ß0.0ßTherefore, there is a difference between the training and testing procedure which might lead to lower performance.ßdummyTurker
5837ß0.0ßThis problem has already been studied in seq2seq generation and imitation learning and it would be an interesting topic to explore in the future.ßdummyTurker
5838ß0.0ßFirst of all, thank you very much for the positive feedback and appreciation for our work.ßdummyTurker
5839ß0.0ßOur response to the questions.ßdummyTurker
5840ß0.0ßI wonder how much of the final performance depends on the LAttr regularizer, which seems quite powerful.ßdummyTurker
5841ß0.0ßCould you shed some light on how lambda was determined?ßdummyTurker
5842ß0.0ßLattr is more important for more complex domains.ßdummyTurker
5843ß0.0ßIn a more controlled domain, e.g.ßdummyTurker
5844ß0.0ßtask-oriented, Lattr sometimes is unnecessary.ßdummyTurker
5845ß0.0ßWe do not observe that the model is very sensitive to the value of lambda, as long as it's in a reasonable range, e.g.ßdummyTurker
5846ß0.0ß[0.1 - 2].ßdummyTurker
5847ß0.0ß"Giving a too big lambda leads to undesired behavior, because the decoder may tend to generate many ""key"" words that make the discriminator outputs very high confidence, but the generated sentence has more grammatical errors."ßdummyTurker
5848ß0.0ßThrough validation, we choose lambda to be 0.1 to balance between adversarial accuracy vs perplexity.ßdummyTurker
5849ß0.0ßIn table 2ßdummyTurker
5850ß0.0ßThanks for the good suggestions.ßdummyTurker
5851ß0.0ßWe implemented a 2-way Softmax and its performance trend is the same as the one in Table 2.ßdummyTurker
5852ß0.0ßThis is not included due to space limits.ßdummyTurker
5853ß0.0ßBinary nodes using Sigmoid should be similar to 2-way Softmax, and we will try to include some results about it for the final version.ßdummyTurker
5854ß0.0ßWe thank the 3rd reviewer’s acknowledgement on our work.ßdummyTurker
5855ß0.0ßOur responses are below:ßdummyTurker
5856ß0.0ß1) model A vs B comparisons.ßdummyTurker
5857ß0.0ßHere we show some examples generated by AugmentGAN and ReComb:ßdummyTurker
5858ß0.0ß1. select article having count (venue) <= 2ßdummyTurker
5859ß0.0ß(GAN) article with at most 2 venuesßdummyTurker
5860ß0.0ß(RCB) article whose venues are at most twoßdummyTurker
5861ß0.0ß(Truth) article that has maximum two venuesßdummyTurker
5862ß0.0ß2. select meal where restaurant = (select restaurant where star = 3)ßdummyTurker
5863ß0.0ß(GAN) meal served at 3 star restaurantßdummyTurker
5864ß0.0ß(RCB) meal that restaurant whose star rating is 3 servesßdummyTurker
5865ß0.0ß(Truth) what is a meal served at a three star rated restaurantßdummyTurker
5866ß0.0ßThe examples generated by ReComb have more strict rules, whereas thoses by AugmentGAN are more flexible in word choices and sentence structures, and so more resemble to human usage.ßdummyTurker
5867ß0.0ß2) training detailsßdummyTurker
5868ß0.0ßEach iteration takes a batch size of 128 data for training.ßdummyTurker
5869ß0.0ßEvaluation on the development set happens every 50 iterations.ßdummyTurker
5870ß0.0ßFor OVERNIGHT dataset, it usually requires 4000~7000 iterations for the models to achieve the performance shown in the manuscript.ßdummyTurker
5871ß0.0ß3) The caption on Figure 2 should be clearer.ßdummyTurker
5872ß0.0ßThanks for the reviewer for pointing it out, we will make it clearer for camera-ready version.ßdummyTurker
5873ß0.0ß4) Comment more on some tricks for AugmentGANßdummyTurker
5874ß0.0ßDuring the training of AugmentGAN, we discover that for better generation we need more learning iterations for generator than discriminator.ßdummyTurker
5875ß0.0ßWe end up updating 3 (or 4) times for generator while once for discriminator), to ensure G and D are decreasing in a commensurate rate.ßdummyTurker
5876ß0.0ßWe thank the 2nd reviewer’s acknowledgement on our paper and appreciate the comments regarding the domain transfer (GAN) portion:ßdummyTurker
5877ß0.0ß1) Comparison between GAN and ReComb.ßdummyTurker
5878ß0.0ßWe are glad that the 2nd reviewer pays attention to the detail of the filtering step.ßdummyTurker
5879ß0.0ßIn fact, we were not intended to claim a supreme method based on unfair comparison (Sec 6.3).ßdummyTurker
5880ß0.0ßAlthough the current GAN model is not perfect, we believe there is a huge potential to improve; we’d like to evaluate the perfect performance of GAN-based data augmentation, and show that the GAN-based method is fundamentally different from ReComb.ßdummyTurker
5881ß0.0ßA comparison between the generated examples is shown below according to Reviewer 3’s advice.ßdummyTurker
5882ß0.0ßIn addition, the filtering step is different from directly composing query through crowd-sourcing.ßdummyTurker
5883ß0.0ßThe pre-generation through AugmentGAN significantly reduces human involvement, as the time and efforts taken by ‘selecting’ queries is much less than directly ‘writing’.ßdummyTurker
5884ß0.0ß2) How DRN helps with domain transferßdummyTurker
5885ß0.0ßWe are grateful that the 2nd reviewer provides this insightful comment.ßdummyTurker
5886ß0.0ßWe agree that in Table 3 the sole comparison between DRN+Seq2Clauses and seq2seq is difficult to demonstrate the effect of DRN on helping domain transfer.ßdummyTurker
5887ß0.0ßTherefore, we display the result of DRN+seq2seq (both in-domain and domain-transfer) as below:ßdummyTurker
5888ß0.0ßDRN+seq2seq (improved label embed.)ßdummyTurker
5889ß0.0ßon OVERNIGHTßdummyTurker
5890ß0.0ßIn-domain: 0.815ßdummyTurker
5891ß0.0ßLimited-target-domain-data: 0.744ßdummyTurker
5892ß0.0ßLimited-target-domain-data + GAN: 0.824ßdummyTurker
5893ß0.0ßMassive-target-domain-data: 0.839ßdummyTurker
5894ß0.0ßThe results of DRN+seq2seq are better than seq2seq, which demonstrates the improvement of using “schema information separation” in domain-transfer task.ßdummyTurker
5895ß0.0ßFor the camera-ready version, we also plan to include the result of Seq2Clauses-only model to further verify DRN’s positive effects on domain-transfer.ßdummyTurker
5896ß0.0ßWe thank the 1st reviewer’s acknowledgement on our results and our efforts on using GAN in NLP.ßdummyTurker
5897ß0.0ßWe also appreciate his/her critical comments regarding our weaknesses:ßdummyTurker
5898ß0.0ß1) Scattershot approaches, lack of unified messageßdummyTurker
5899ß0.0ßInstead of trying to propose a new semantic parsing model, we are trying to solve a practical NLP problem --- domain adaptation of semantic parsing with limited target-domain data.ßdummyTurker
5900ß0.0ßWe believe that:ßdummyTurker
5901ß0.0ß(a) by separating the schema information from the NL query (DRN), we can enhance domain-transferability and simultaneously improve the in-domain learning, and (b) with efficient augmentation technique (AugmentGAN), we can mitigate the cold-start issue.ßdummyTurker
5902ß0.0ßIn addition to the main message, we also propose the Seq2Clauses model, which is a better structure-aware decoding solution to SQL.ßdummyTurker
5903ß0.0ßTo demonstrate the effect of “schema information separation” in domain-transfer task, we take the 2nd reviewer’s advice and show the results on DRN+seq2seq (both in-domain and domain-transfer), which are better than seq2seq model.ßdummyTurker
5904ß0.0ßWe hope this clarification could answer the 1st reviewer’s concern on our main message, and help illustrate that all components of our contributions are solid and well-explained.ßdummyTurker
5905ß0.0ß2) Novelty on the three modelsßdummyTurker
5906ß0.0ßi) DRNßdummyTurker
5907ß0.0ßDespite sharing some similarities with tagging problems, our purpose of proposing DRN is different.ßdummyTurker
5908ß0.0ßThe tags for queries in different domains are generally the same, but the schema information for different relational tables is different.ßdummyTurker
5909ß0.0ßTo our best knowledge, we are the first work trying to explicitly separate schema information from the NL query in NLIDB problem, and a direct method like convolutional-based model is preferred.ßdummyTurker
5910ß0.0ßeach token in original query will be predicted with an output label (like ‘c1’, ‘v1’); if there are consecutive tokens in input query corresponding to a multi-word column, it is possible to predict the same label for all these tokens, so it’s okay for DRN to deal with multi-word columns.ßdummyTurker
5911ß0.0ßii) Seq2ClausesßdummyTurker
5912ß0.0ßWe’d like to clarify that although Seq2Clauses and ASN (Rabinovich ACL 2017) share some similarities (tree-like structure, recursive decoding), they are inherently different.ßdummyTurker
5913ß0.0ßASN incorporates several modules working in different stages, while Seq2Clauses is using a single shared model for all different statement heads, which makes it more efficient and straightforward.ßdummyTurker
5914ß0.0ßWe will discuss and add the work ASN into our manuscript.ßdummyTurker
5915ß0.0ßThe main drawback of SQLNet is that the introduction of sketch requires a lot of human efforts.ßdummyTurker
5916ß0.0ßFor new SQL types, SQLNet requires new sketch and retraining.ßdummyTurker
5917ß0.0ßTherefore, SQLNet is hard for scaling and limited only to simple SQL types.ßdummyTurker
5918ß0.0ßiii) AugmentGANßdummyTurker
5919ß0.0ßWe are trying to adopt GAN in a completely different problem --- data augmentation.ßdummyTurker
5920ß0.0ßThe generator we used is basically the same as in Li et al., 2017.ßdummyTurker
5921ß0.0ßHowever, to suit better for the problem, we design a different discriminator to generate bi-class rewards.ßdummyTurker
5922ß0.0ßWe appreciate the 1st reviewer for pointing out the word choosing issue, and we will take into more consideration for rephrasing.ßdummyTurker
5923ß0.0ßWe appreciate your helpful comments.ßdummyTurker
5924ß0.0ßWe respond to only the major points to keep the response short, but we will be sure to address all suggestions in the final draft.ßdummyTurker
5925ß0.0ß> Reply to weakness argument 1 about lack of description on qualitative resultsßdummyTurker
5926ß0.0ßThanks for pointing out this lacuna!ßdummyTurker
5927ß0.0ßWe apologize for the insufficient description of our qualitative results.ßdummyTurker
5928ß0.0ßWe could not include details in the main paper due to lack of space.ßdummyTurker
5929ß0.0ßFollowing your suggestion, we will be sure to include more observations on Table 5 and Figure 3, and describe detailed setup for human study in the final version.ßdummyTurker
5930ß0.0ß> Reply to the comments on motivation for the choice of baseline models and the thresholds set in hyper-parameter tuningßdummyTurker
5931ß0.0ßOur goal was to provide a proof-of-concept system for our hypothesis: “external stylistic features are useful in VAE language modeling.ßdummyTurker
5932ß0.0ßIf so, what is the best way of incorporating them to the VAE?”.ßdummyTurker
5933ß0.0ßThus, it may not be fair to directly compare with other variants of VAE without external styles.ßdummyTurker
5934ß0.0ßThe simplicity even makes our model more  extensible and applicable to any other variants.ßdummyTurker
5935ß0.0ßAlso, we replicate two baseline models based on prior works; (Sennrich et al., 2016) for ENC-disc and (Li et al., 2016, Hoang et al., 2016) for DEC-disc.ßdummyTurker
5936ß0.0ßTo the best of our knowledge, this is the first attempt to constrain (discrete or continuous) styles into latent spaces (e.g., {H,Z}-{disc,conti}).ßdummyTurker
5937ß0.0ßThe thresholds set in our hyper-parameter tuning follow ones in (Bowman et al., 2015) and (Yang, Zichao et al, 2017).ßdummyTurker
5938ß0.0ßWe will be sure to include them in the final version.ßdummyTurker
5939ß0.0ßThanks for your insightful suggestion.ßdummyTurker
5940ß0.0ßIt’s not easy to measure diversity since there is no well-adopted standard diversity metrics in literature and it is quite difficult to ask human to evaluate diversity.ßdummyTurker
5941ß0.0ßHence, DISTINCT is a popular diversity metric used in recent works [1][3] and proves to be an acceptable metric for diversity measurement.ßdummyTurker
5942ß0.0ßWe compare several strong baselines: classical Seq2Seq model, MMI method (named MMI-bidi in the experiment part) and adversarial method (named Adver-REIN in the experiment part).ßdummyTurker
5943ß0.0ßBoth MMI-bidi [1] and Adver-REIN[2] method are proposed by Li et al.ßdummyTurker
5944ß0.0ßThese baselines demonstrate that our DAL model can effectively improve both diversity and overall quality of the generated responses.ßdummyTurker
5945ß0.0ßFor each experiment, we repeated several times.ßdummyTurker
5946ß0.0ßThe averaged results are reported in the paper.ßdummyTurker
5947ß0.0ßThe diversity experiments compare all the methods and prove that our proposed dual learning can solve the safe response problem.ßdummyTurker
5948ß0.0ßThe overall quality comparison (both pairwise and pointwise) is annotated by human expert, whose statistical results prove the dual signal and the adversarial signal can boost the overall quality.ßdummyTurker
5949ß0.0ßOur DAL-DuAd method achieves slightly better performance on overall quality than Adver-REIN.ßdummyTurker
5950ß0.0ßOur DAL-DuAd gain much better performance on diversity than Adver-REIN, 0.046 over 0.035.ßdummyTurker
5951ß0.0ßBoth our DAL-Dual and DAL-DuAd is much better than baselines.ßdummyTurker
5952ß0.0ßWe will further clarify it in the revised versionßdummyTurker
5953ß0.0ßThanks for the comments.ßdummyTurker
5954ß0.0ßWe would cite this paper in the revised version.ßdummyTurker
5955ß0.0ßWe use Seq2Seq , MMI-bidi[1], Adver-REIN[2] as the baselines to demonstrate the improvement of dual signal and adversarial signal on both diversity and overall quality.ßdummyTurker
5956ß0.0ßWe conduct diversity comparison among all the methods.ßdummyTurker
5957ß0.0ßFor human’s overall quality evaluation, we conduct pairwise and pointwise comparison.ßdummyTurker
5958ß0.0ßWe use Fleiss’ Kappa to calculate the agreement among human annotators.ßdummyTurker
5959ß0.0ß[1] Li.et al.ßdummyTurker
5960ß0.0ßA Diversity-Promoting Objective Function for Neural Conversation Models.ßdummyTurker
5961ß0.0ß[2] Li.et al.ßdummyTurker
5962ß0.0ßAdversarial learning for neural dialogue generationßdummyTurker
5963ß0.0ß[3] Xing.et al.ßdummyTurker
5964ß0.0ßTopic Aware Neural Response Generation.ßdummyTurker
5965ß0.0ßThanks for your insightful suggestion.ßdummyTurker
5966ß0.0ßWe will add GAN-AEL as one baseline in revised paper.ßdummyTurker
5967ß0.0ßWe will test our model on Xu’s dataset and cite related papers.ßdummyTurker
5968ß0.0ßWe conduct t-test and get the p value as 0.51, which shows good agreement.ßdummyTurker
5969ß0.0ßFurther, we also calculate the Fleiss’ Kappa in Table 2, which also shows good agreement among the annotators.ßdummyTurker
5970ß0.0ßWe observed same phenomenon.ßdummyTurker
5971ß0.0ßSometimes the relevance and coherence increase, while the diversity decrease.ßdummyTurker
5972ß0.0ßDAL-DuAd balances overall quality and diversity by different \lambda value.ßdummyTurker
5973ß0.0ßDISTINCT-1 score is 0.052 for DAL-Dual and 0.046 for DAL-DuAd, which indicates that DAL-DuAd sacrifices the performance in term of DISTINCT to some extent and optimizes the relevance of the responses.ßdummyTurker
5974ß0.0ßb denotes the mean of all the generated sample’s rewards.ßdummyTurker
5975ß0.0ßb is used to reduce the variance of the estimation while keeping the estimation unbiased.ßdummyTurker
5976ß0.0ßIt is empirically nontrivial to adapt MMI[1] to Seq2Seq training.ßdummyTurker
5977ß0.0ßLi et al.ßdummyTurker
5978ß0.0ßchose to only use MMI in the testing phase, even so.ßdummyTurker
5979ß0.0ßMMI also face ungrammatical response(MMI-antiLM) and unlimited decoding space (MMI-bidi).ßdummyTurker
5980ß0.0ßMore seriously, the decoding phase adopting MMI is very time-consuming due to the reranking-phase, takes around 10 times time for the testing phase even for a small rerank set.ßdummyTurker
5981ß0.0ßOur dual approach considers the reverse dependency directly in the training phase, which show its time superiority over MMI.ßdummyTurker
5982ß0.0ßRefer to the Section 5.2 of Comparison of Efficiency in our paper.ßdummyTurker
5983ß0.0ßYou propose an acute way to interpret our proposed DAL.ßdummyTurker
5984ß0.0ßWe can also interpret DAL in another way.ßdummyTurker
5985ß0.0ßThe motivation of dual learning and adversarial learning in our paper is different.ßdummyTurker
5986ß0.0ßWe focus on the conditional distribution p(r|q) defined by response generator G_{\theta_{qr}}.ßdummyTurker
5987ß0.0ßDual learning: makes p(r | q) have same momentum as the reverse conditional distribution p(q | r).ßdummyTurker
5988ß0.0ßIn this case, for diverse response r_d, Bigger value of p(q | r_d) promotes bigger value in p(r_d | q) through duality constraint.ßdummyTurker
5989ß0.0ßThis is the reason why dual learning could improve the diversity.ßdummyTurker
5990ß0.0ßOur paper is the first work to use dual learning to solve the safe response problems.ßdummyTurker
5991ß0.0ßAdversarial learning: the discriminator guides the conditional distribution p(r | q; \theta) defined by response generator as close to the real-world p(r | q) as possible.ßdummyTurker
5992ß0.0ßThe adversarial approach in response generation [1] is different from the conventional GAN for image generation task or text generation [2].ßdummyTurker
5993ß0.0ßFor conventional GAN, the generator takes noise as input and generate a picture or sentence that seems to be the real one.ßdummyTurker
5994ß0.0ßBut for the adversarial approach in dialogue scenario, the generator takes queries as the input and outputs responses.ßdummyTurker
5995ß0.0ßThe generator adopts Seq2Seq formulation while the discriminator judges the quality of the generated responses.ßdummyTurker
5996ß0.0ßThe adversarial approach uses the adversarial thought in GAN but is different in the generator formulation.ßdummyTurker
5997ß0.0ßThe backward model: r-->q can also be trained by MLE.ßdummyTurker
5998ß0.0ßOriginally we trained the backward model by MLE but we found that its effect is inferior to that trained by adversarial method.ßdummyTurker
5999ß0.0ßWe will add the GAN-AEL as the baseline and add experiments based upon aforementioned datasets.ßdummyTurker
6000ß0.0ßMMI-bidi is a closer counterpart than MMI-antiLM since M also considers the reverse dependency.ßdummyTurker
6001ß0.0ßFor completeness, we will add MMI-antiLM method in the revised version.ßdummyTurker
6002ß0.0ß[1] Li.et al.ßdummyTurker
6003ß0.0ßAdversarial learning for neural dialogue generationßdummyTurker
6004ß0.0ß[2] Yu.et al.ßdummyTurker
6005ß0.0ßSeqGAN: Sequence Generative Adversarial Nets with Policy GradientßdummyTurker
6006ß0.0ßThank you for your comments.ßdummyTurker
6007ß0.0ßWe will modify the position of the examples and add more explainations in the revision.ßdummyTurker
6008ß0.0ßThe CoNLL 2003 NER task has been studied a lot and these two baselines are already very strong.ßdummyTurker
6009ß0.0ßWe will apply our proposed model to other tasks in our future work to see if greater improvement could be achieved.ßdummyTurker
6010ß0.0ßThis paper focuses on the model architecture for neural sequence labeling.ßdummyTurker
6011ß0.0ßTo integrate external resources to our proposed model will be a task of our future work.ßdummyTurker
6012ß0.0ßThank you for your comments.ßdummyTurker
6013ß0.0ßWe will correct the typos, modify the formulas, and add examples to the introduction section in the revision.ßdummyTurker
6014ß0.0ßActually, the decoding strategy can be considered as utilizing the sum of NLLs given by both CRF and HSCRF layers to rescore the two decoding outputs of using the CRF layer and the HSCRF layer separately.ßdummyTurker
6015ß0.0ßWe will improve the descriptions to our method in the revision.ßdummyTurker
6016ß0.0ßThis paper focuses on the model architecture for neural sequence labeling.ßdummyTurker
6017ß0.0ßTo integrate external resources to our proposed model will be a task of our future work.ßdummyTurker
6018ß0.0ßOur method is similar with LSTM-minus, but not the same.ßdummyTurker
6019ß0.0ß"For example, in a sentence of word sequence ""ABCDEFG"", in order to calculate the features of segment (BCD), LSTM-minus uses w(D) - w(A) for forward LSTM and w(B) - w(E) for backward LSTM."ßdummyTurker
6020ß0.0ßHowever, our method calculates w(D) - w(B) for both directions in BLSTM.ßdummyTurker
6021ß0.0ßIn our method, we think that the output from BLSTM represents the information of one word and its context.ßdummyTurker
6022ß0.0ßTherefore, we hypothesize that the difference between representations of two words which are in the same named entity should be much smaller than the difference of those not in a named entity.ßdummyTurker
6023ß0.0ß"For example, in a sentence ""Barack Obama was the president"", when we calculate the score of segment (Barack Obama), we employ the features of w(Obama) – w(Barack), where w(*) is the output of BLSTM."ßdummyTurker
6024ß0.0ßAnd when we calculate the score of segment (Barack Obama was), we employ the features of w(was) – w(Barack).ßdummyTurker
6025ß0.0ß"Obviously, the difference of the previous one should be much smaller than the latter one (""Barack"" is closer to ""Obama"" than a verb ""was""), which is helpful to recognize a segment."ßdummyTurker
6026ß0.0ßWe will add this explanation in the revision.ßdummyTurker
6027ß0.0ßWe will also try LSTM-minus in our future work.ßdummyTurker
6028ß0.0ßThank you for your suggestions.ßdummyTurker
6029ß0.0ßAs introduced in Section 3.1, we adopted the same word-level tagging scheme as the one used in Liu et al.ßdummyTurker
6030ß0.0ß(2018).ßdummyTurker
6031ß0.0ßWe will add brief descriptions in the revision.ßdummyTurker
6032ß0.0ßWe thank the reviewer for the detailed comments and helpful suggestions.ßdummyTurker
6033ß0.0ßThe potential gain of incorporating partial order information to active learning largely depends on the underlying structure of the relational graph.ßdummyTurker
6034ß0.0ßWe can see that the gains are different for the four different domains we show.ßdummyTurker
6035ß0.0ßNote that the dataset used in the paper only covers part of the relational graph since labeling all edges in the graph will require too many annotations (~C(n, 2)).ßdummyTurker
6036ß0.0ßThe advantage of using the transitive closure would become clearer if the data is a complete relational graph.ßdummyTurker
6037ß0.0ßIn addition, we can see from Figure 2 that the baseline AUC scores are already relatively high, which makes the absolute gain not as large.ßdummyTurker
6038ß0.0ßWe will add an example on small toy/synthetic data to illustrate the differences between queries selected by our proposed strategies and the baselines.ßdummyTurker
6039ß0.0ßWe thank the reviewer for the detailed comments and helpful suggestions.ßdummyTurker
6040ß0.0ßWe are not sure we understand the ranking viewpoint to which the reviewer is referring.ßdummyTurker
6041ß0.0ßAs we understand the formulation of ranking problems is completely different from the one discussed in this paper.ßdummyTurker
6042ß0.0ßTypically in ranking problems, a ground truth TOTAL order is assumed to exist, and the goal is to recover this ground truth total order from pairwise comparisons.ßdummyTurker
6043ß0.0ßHowever, in our case, there is no ground truth total order among the instances, i.e., there is no prerequisite relation that forms a linear chain on our instances.ßdummyTurker
6044ß0.0ßRather, the prerequisite relation only forms a directed acyclic graph, which we feel is more applicable in practice.ßdummyTurker
6045ß0.0ßIf the reviewer is referring to “ranking” with the exception that not every pair is comparable, our proposed framework can still be useful since it is essentially learning a pointwise scoring function.ßdummyTurker
6046ß0.0ßAs a note, to our knowledge the existing literature on concept prerequisite learning in the ACL community applies a classification framework (Talukdar et al., 2012; Liang et al, 2015; Pan et al., 2017).ßdummyTurker
6047ß0.0ßSolving the problem from a ranking viewpoint could be an interesting direction.ßdummyTurker
6048ß0.0ßIn the current version, Figure 1 and the paragraph (Line 268-279) are used to provide an informal explanation of Theorem 1.ßdummyTurker
6049ß0.0ßAs suggested, we will improve the readability of Section 2.2 in the revised paper.ßdummyTurker
6050ß0.0ßIt is true that our proposed active learning framework is built upon an oracle that makes no mistakes.ßdummyTurker
6051ß0.0ßLearning strict partial orders from a noisy oracle is an important and interesting problem which is still open.ßdummyTurker
6052ß0.0ßWe define a “concept” in our experiments as an English Wikipedia title (Line 549 in the paper), such as Natural-language_processing and Machine_learning.ßdummyTurker
6053ß0.0ßWe agree with the reviewer that the construction of confidence intervals requires independent observations, but we would like to point out that the 300 results obtained via trials with random initial conditions do form a sample of independent observations.ßdummyTurker
6054ß0.0ßAlthough in the 300 trials we use the same dataset, the algorithmic execution, the selected query, etc.ßdummyTurker
6055ß0.0ßare different and independent due to the randomness in the classifiers.ßdummyTurker
6056ß0.0ßWe use these samples to show the uncertainty of our algorithm.ßdummyTurker
6057ß0.0ßInstead of reporting a C.I., we will use the error bar to show the standard deviation and change the term C.I.ßdummyTurker
6058ß0.0ßin the paper to std.ßdummyTurker
6059ß0.0ßaccordingly.ßdummyTurker
6060ß0.0ßAs also suggested by Reviewer #1, we will add an example from a small toy/synthetic data set to illustrate the differences between queries selected by our proposed strategies and the baselines.ßdummyTurker
6061ß0.0ßWe thank the reviewer for the detailed comments and helpful suggestions.ßdummyTurker
6062ß0.0ßFor clarity, we in our revised paper will elaborate on the distinction between our work and Liang et al.ßdummyTurker
6063ß0.0ß(2018).ßdummyTurker
6064ß0.0ßIn short, here we propose query strategies tailored for strict partial orders while Liang et al.ßdummyTurker
6065ß0.0ßonly experimented with standard query strategies without using the relational information of concept prerequisites.ßdummyTurker
6066ß0.0ßWe apply the proposed methods to concept prerequisite learning (CPL) for two reasons: (i) Concept prerequisite relations are useful for representing concept graphs for various knowledge disciplines.ßdummyTurker
6067ß0.0ßTo our knowledge, CPL has not been well studied; (ii) Active learning is especially useful for handling the lack of large-scale prerequisite data.ßdummyTurker
6068ß0.0ßWe will further investigate other relations that are also strict partial orders from the information extraction literature and discuss the potential application of the proposed algorithm in the revision.ßdummyTurker
6069ß0.0ßHowever, we believe that for some well-studied strict partial orders such as “is-a”, there is not a strong motivation to apply active learning to extracting these relations since there is abundant labeled data such as FB15K (even enough to support neural-net based learning).ßdummyTurker
6070ß0.0ßIn NPY, Mochihashi used the Bakeoff-2 test data together with a random subset of Bakeoff-2 training data, which means that the statistical information of the training and test data was all used.ßdummyTurker
6071ß0.0ßIt is only test data that are used in ESA, MCA and our test, and so, ESA, MCA are preferable as benchmarks instead of NPY.ßdummyTurker
6072ß0.0ßThe source code will be released in github.ßdummyTurker
6073ß0.0ßYes, I will examine this paper carefully and improve the readabilityßdummyTurker
6074ß0.0ßYes, I will examine this paper carefully and correct grammatical mistakesßdummyTurker
6075ß0.0ßYes, I will examine this paper carefully and improve the readability.ßdummyTurker
6076ß0.0ßThe task of a supervised word segmentation method is only to segment text on basis of a predefined lexicon with linguistic knowledge.ßdummyTurker
6077ß0.0ßHowever, for an unsupervised method, there are no prior knowledge affordable.ßdummyTurker
6078ß0.0ßWhat an unsupervised method does indeed, includes three major procedures, such as word discovery, language modelling and segmentation.ßdummyTurker
6079ß0.0ßBut most non- Bayesian unsupervised methods only concern segmentation, and are vulnerable to segmentation errors due to the absence of word discovery and language modelling.ßdummyTurker
6080ß0.0ßThough the proposed method is different from Bayesian ones, but the fact that local segmentation and word discovery are done separately, suggests that language modelling such as HDP and HMM can be integrated into this framework, which is our future work.ßdummyTurker
6081ß0.0ßThis work proposes an iterative three-step approach, and integrates some useful information, in particular the locally normalized mutual independence between a string and its context (NMI).ßdummyTurker
6082ß0.0ßThe experimental results show the proposed method outperforms other methods including Bayesian ones.The gain of up to 1.5 in F1-score is achieved on CITYU, and the average gain on all the 4 Chinese corpora is 0.675.ßdummyTurker
6083ß0.0ßThe source code will be released in github.ßdummyTurker
6084ß0.0ßYes, I will give more analysis on the experimental results.ßdummyTurker
6085ß0.0ßYes, I will examine this paper carefully and improve the readability.ßdummyTurker
6086ß0.0ßThe task of a supervised word segmentation method is only to segment text on basis of a predefined lexicon with linguistic knowledge.ßdummyTurker
6087ß0.0ßHowever, for any unsupervised method, there are no prior knowledge affordable.ßdummyTurker
6088ß0.0ßWhat an unsupervised method does indeed, includes three major procedures, such as word discovery, language modelling and segmentation.ßdummyTurker
6089ß0.0ßBut most non- Bayesian unsupervised methods only concern segmentation, and are vulnerable to segmentation errors due to the absence of word discovery and language modelling.ßdummyTurker
6090ß0.0ßYes, the Bayesian framework is elegant, but now is being complicated due to incorporation with various of tools.ßdummyTurker
6091ß0.0ßFor instances, in Miaohong Chen, Baobao Chang, and Wenzhe Pei (2014), a character-based HMN model is integrated into a word-based HDP model, initialized by nVBE; and in MCA, about 5 models are integrated into the Bayesian framework.ßdummyTurker
6092ß0.0ßThis work proposes an iterative three-step approach, and integrates some useful information, in particular the locally normalized mutual independence between a string and its context (NMI).ßdummyTurker
6093ß0.0ßThe experimental results show the proposed method outperforms others including Bayesian methods.ßdummyTurker
6094ß0.0ßThe gain of up to 1.5 in F1-score is achieved on CITYU, and the average gain on all the 4 Chinese corpora is 0.675.ßdummyTurker
6095ß0.0ßThough the proposed method is different from Bayesian ones, but the fact that local segmentation and word discovery are done separately, suggests that Bayesian can be integrated into this framework, which is our future work.ßdummyTurker
6096ß0.0ßThe source code will be released in github.ßdummyTurker
6097ß0.0ßI will add a citation (Goldwater S J, Johnson M. Nonparametric Bayesian Models of Lexican Acquisition[M].ßdummyTurker
6098ß0.0ßBrown University, 2007).ßdummyTurker
6099ß0.0ßMany works assumed that the prior probability of a hypothesis decrease exponentially with its length, e.g.ßdummyTurker
6100ß0.0ßa Poisson distribution is used In NPY and MCA, and a Beta distribution in Sharon Goldwater, Thomas L Griffiths, and Mark Johnson(2009).ßdummyTurker
6101ß0.0ßHowever, as stated in Pierre Magistry and Benoît Sagot(2012), hypotheses of various lengths tend to have different distributions, therefore a simple assumption over length is insufficient to approximate the probability for a hypothesis to be a word.ßdummyTurker
6102ß0.0ßTo Review#3ßdummyTurker
6103ß0.0ßThanks for your insightful consideration and comments.ßdummyTurker
6104ß0.0ßTo our best knowledge, as this is the FIRST work about cross-lingual NE embedding mapping, we are afraid that there comes no previous work for the required comparison.ßdummyTurker
6105ß0.0ßWithout our core contribution that concludes monolingual NE can be generally put into a hypersphere, no mapping methods can be taken into consideration.ßdummyTurker
6106ß0.0ßFurthermore, with the least requirement, our adopted method requires nothing but only two independently-trained embedding spaces, which is right the baseline you mean.ßdummyTurker
6107ß0.0ßAny other mapping methods as we know have to depend on more or less linguistic resource, typically, parallel corpus, bilingual lexicons.ßdummyTurker
6108ß0.0ßThe pre-trained embedding we used in this paper is described in line 123.ßdummyTurker
6109ß0.0ßWe use pre-trained Word2Vec word embedding (Mikolov et al., 2013).ßdummyTurker
6110ß0.0ßActually, we used both 2D and 3D in this paper when necessary.ßdummyTurker
6111ß0.0ß3-D visualizations may present the gathering feature and sphere-like shape more clearly.ßdummyTurker
6112ß0.0ßPlease refer to line 125.ßdummyTurker
6113ß0.0ßWe do dimensional reduction work to both 2-D and 3-D in t-SNE.ßdummyTurker
6114ß0.0ßAs we claimed in line 111, our visualization task is completed based on the self-collected dictionaries shown in Table 1.ßdummyTurker
6115ß0.0ßAs person names have a more stable syntactic and semantic role in text than location and organization names, they can gather more closely and more stably with embedding representation.ßdummyTurker
6116ß0.0ßThank you for your comments concerning our manuscript.ßdummyTurker
6117ß0.0ßThose comments are all valuable and very helpful for revising and improving our paper.ßdummyTurker
6118ß0.0ßWe have studied comments carefully and have made correction which we hope meet with approval.ßdummyTurker
6119ß0.0ßThe main corrections in the paper and the responds to your comments are as following:ßdummyTurker
6120ß0.0ß"For some notations, like ""Features Sharing"", and ""Transfer Learning"", we are very sorry for our unclear explanation of them, and we will give further explanation and more related references to help understanding these terms."ßdummyTurker
6121ß0.0ßOur method is to some extent similar with scheduled sampling (Bengio et al., 2015).ßdummyTurker
6122ß0.0ßThe differences are that there are two tasks considered in training stage,and predicted parser is not only used to next decision but also applied to semantic composition, what's more we adjust the objective weight depending on learning stage to optimize the performance of the two tasks(in lines 457-471).In our experiment, the training algorithm is effective to improve the performance of joint learning.ßdummyTurker
6123ß0.0ßThe application of phrase-to-phrase matching to MultiNLI task (Williams et al., 2017) gains a significant improvement,which verify the effectiveness of our model.ßdummyTurker
6124ß0.0ßWe also explore the attention results and provided comprehensive explanation for this model.ßdummyTurker
6125ß0.0ßSubsequently, we will add the above two experimental results to this paper.ßdummyTurker
6126ß0.0ßAt the same time, we will use our model to question answering (Wang et al., 2017) and paraphrase detection (Yin et al., 2015) to get more validation.ßdummyTurker
6127ß0.0ßIn line 319, we intercept a proportion of the vector as features for parsing, and d=50 means that we only use the first 50 dimensions in the hidden states of LSTM or tree-LSTM.ßdummyTurker
6128ß0.0ßThe intercepted vector is used as shared features for semantic composition and parsing prediction.ßdummyTurker
6129ß0.0ßIn table 1, the bolding indicates the best results with different methods in our experiment, including sequential structures and tree structures.ßdummyTurker
6130ß0.0ßOur original intention is to provide an elaborate analysis from different perspectives,but too many comparisions fail to present a clear logic.ßdummyTurker
6131ß0.0ßWe will modify the related contents.ßdummyTurker
6132ß0.0ßThank you for your comments concerning our manuscript.ßdummyTurker
6133ß0.0ßThose comments are all valuable and very helpful for revising and improving our paper.ßdummyTurker
6134ß0.0ßWe have studied comments carefully and have made correction which we hope meet with approval.ßdummyTurker
6135ß0.0ßThe main corrections in the paper and the responds to your comments are as following:ßdummyTurker
6136ß0.0ßCompared with models with external parser input, our model using jointly predicted tree decrease the performance, which similarly occured in Bowman et al.ßdummyTurker
6137ß0.0ß2016.ßdummyTurker
6138ß0.0ßIn this paper, our main purpose is to explore a joint model for semantic composition and parsing, and the integrated parser is used to produce the needed syntax for tree-structured semantic composition at test time.ßdummyTurker
6139ß0.0ßOur model can operate on unparsed input with little loss in accuracy.ßdummyTurker
6140ß0.0ßIt is also easy to incorporate our method into other tasks, such as tree-to-sequence neural machine translation (Eriguchi et al., 2016), visual-semantic embedding (Niu et al., 2017) that use syntax information.ßdummyTurker
6141ß0.0ßAs for the joint objective which incorporates semantic composition with parsing, our initiative is to avoid the dependency on externally-provided parser trees through training the two tasks simultaneously, not to improve the performance of semantic composition.ßdummyTurker
6142ß0.0ßAlthough it seems feasible to improve each other, we leave it as a future work.ßdummyTurker
6143ß0.0ßWe are very sorry for our unclear report on it.ßdummyTurker
6144ß0.0ßIn section (5.2), our original intention is to provide an elaborate analysis from different perspectives,but too many comparisions fail to present a clear logic.ßdummyTurker
6145ß0.0ßWe will move some contents to related work section.ßdummyTurker
6146ß0.0ßBased on the proposed overall framework, our sequential model alone obtains the best performance over the state-of-the-art in sequential tasks, which deserves as a stronger baseline.ßdummyTurker
6147ß0.0ßAlthough the gain from phrase-to-phrase mapping compared to word-to-word mapping is not too obvious in NLI task, the application of phrase-to-phrase mapping to MultiNLI task (Williams et al., 2017) gains a significant improvement, which verify the effectiveness of our model.ßdummyTurker
6148ß0.0ßWe also explore the attention results and provide comprehensive explanation for this model.ßdummyTurker
6149ß0.0ßSubsequently, we will add the above two experimental results to this paper.ßdummyTurker
6150ß0.0ßAt the same time, we will use our model to question answering (Wang et al., 2017) and paraphrase detection (Yin et al., 2015) to get more validation.ßdummyTurker
6151ß0.0ßIn line 313-319, we intercept a proportion of the vector as features for parsing, and d=50 means that we only use the first 50 dimensions in the hidden states of LSTM or tree-LSTM.ßdummyTurker
6152ß0.0ßThe intercepted vector is used as shared features for semantic composition and parsing prediction.ßdummyTurker
6153ß0.0ßIn line 382, \bar{a_i} and \tilda{a_i} which are from line 373-374 use the same notations.ßdummyTurker
6154ß0.0ßThe \bar{a_i} is a node representation, and the \tilda{a_i} is a weighted summation of all nodes in another tree with attention.ßdummyTurker
6155ß0.0ßN-gram LM baseline was previously the best method for the task of sentence segmentation.ßdummyTurker
6156ß0.0ßBecause there is no reported comparison between N-gram LM and RNN LM on this task, we felt it necessary to include it as a baseline.ßdummyTurker
6157ß0.0ßWe are not certain that we total understand your comment.ßdummyTurker
6158ß0.0ßThe network's input, which is word, is encoded with one-hot, and then an embedding layer converts it into a dense vector.ßdummyTurker
6159ß0.0ßIt is the same technique as the one commonly used in neural language model and neural machine translation.ßdummyTurker
6160ß0.0ßWe did not explore complicated network, because the main purpose of online sentence segmentation is for the application of simultaneous interpretation.ßdummyTurker
6161ß0.0ßTherefore, there is a strict constraint on response time.ßdummyTurker
6162ß0.0ßNevertheless, attention mechanism is a good idea.ßdummyTurker
6163ß0.0ßIf a boundary is hypothesized, when the successive positions are processed, the window of words will start from w_{t+1}.ßdummyTurker
6164ß0.0ßIt is because a new sentence has started.ßdummyTurker
6165ß0.0ßIn the case that y_t predicts a boundary after t-2, while y_{t+1} gives a negative prediction, the method described in the paper will follow the first positive prediction and output a boundary after t-2.ßdummyTurker
6166ß0.0ßSuch a setting is to serve the application of simultaneous interpretation.ßdummyTurker
6167ß0.0ßFirstly, we will give an example to make the background knowledge clearer.ßdummyTurker
6168ß0.0ßSecondly, we will use another name for the Chinese WebQA dataset.ßdummyTurker
6169ß0.0ßThirdly,ßdummyTurker
6170ß0.0ßwe used open sources for AS Reader (https://github.com/rkadlec/asreader), BiDAF (https://allenai.github.io/bi-att-flow/) and GA Reader (https://github.com/bdhingra/ga-reader), and then applied our KRN to these models.ßdummyTurker
6171ß0.0ßSince CAS Reader and GSMN are not publicly available, so we implemented them by ourselves.ßdummyTurker
6172ß0.0ßWe will make the KRN source code as well as the implements of CAS Reader and GSMN publicly available.ßdummyTurker
6173ß0.0ßThe paper Learning to Understand Phrases by Embedding the Dictionary trained a recurrent neural network to map dictionary definitions to the representations of the words those definitions define.ßdummyTurker
6174ß0.0ßOur approach is quite different with that Dictionary Embedding method.ßdummyTurker
6175ß0.0ßFirstly, we aim to better understand the document by incorporating the external knowledge for question answering, while the Dictionary Embedding method aims to understand phrase for two applicationsßdummyTurker
6176ß0.0ßWe used the collective entity linking technology [1] to address the polysemy problem.ßdummyTurker
6177ß0.0ßFor multi-word entities, the embedding vector is just the average vector of its words' embedding vectors.ßdummyTurker
6178ß0.0ß[1] Han, X., Sun, L., & Zhao, J.ßdummyTurker
6179ß0.0ß(2011).ßdummyTurker
6180ß0.0ßCollective entity linking in web text: a graph-based method.In SIGIR.ßdummyTurker
6181ß0.0ßThe related methods in the introduction are also the baseline methods in our experiments, so we presented them a little more.ßdummyTurker
6182ß0.0ßWe also described the drawbacks of these methods in the introduction.ßdummyTurker
6183ß0.0ßIn the revised paper, we will focus more on the motivation of our approach.ßdummyTurker
6184ß0.0ßThe experiments show that when only incorporating tags or description may introduce noise for knowledge representation, which would worsen the performance of reading comprehension models.ßdummyTurker
6185ß0.0ßBut the experiments also show that if the tags and description are both incorporated through tags-aware attention and description-aware attention, the noise would be reduced and the reading comprehension performance would be improved.ßdummyTurker
6186ß0.0ßThus, tags-aware attention and description-aware attention are both important.ßdummyTurker
6187ß0.0ßFirstly, we proposed a knowledge representation model, which can be plugged into many reading comprehension models.ßdummyTurker
6188ß0.0ßThus, we plugged our model into many state-of-the-art RC models to validate the effectiveness of our model.ßdummyTurker
6189ß0.0ßSecondly, although we used the smaller subsets for the factoid question answering purpose, all the models are executed on the same datasets, so it is a fair comparison.ßdummyTurker
6190ß0.0ßIn addition, the smaller subsets still occupied 93.86% and 44.42% data of the original WebQA and Quasar-T datasets respectively.ßdummyTurker
6191ß0.0ßThirdly, it is not our problem of the memory issue.ßdummyTurker
6192ß0.0ßOur model can represent each entity effectively, but the existing reading comprehension models could cause the memory issue when we just plugged the knowledge representation into these models.ßdummyTurker
6193ß0.0ßThe question is always clear but short, which would bring the difficulty for entity linking.ßdummyTurker
6194ß0.0ßThus, we didn't apply KRN to question representations.ßdummyTurker
6195ß0.0ßIn future, we would like to investigates whether there is benefit and how beneficial applying KRN to question representations is.ßdummyTurker
6196ß0.0ßWe introduced recent MC models and explaining attention-based techniques, because these methods are also the baseline methods in our experiments, and these attention mechanisms are not suitable for incorporating the background knowledge well, so we presented them a little more.ßdummyTurker
6197ß0.0ßIn the revised paper, we will focus more on the motivation of our approach.ßdummyTurker
6198ß0.0ßIn addition, we will also give an example in Quasar-T dataset as follows.ßdummyTurker
6199ß0.0ßWhat country's current government was established in 1958 and is known as the Fifth Republic?ßdummyTurker
6200ß0.0ßDocument: The coming of the Fifth Republic In May 1958 a revolt of French settlers and army officers in Algeria against what they regarded as the effeteness of the government in Paris and its handling of the AlFifth Republic, modern France France HistoryßdummyTurker
6201ß0.0ßAnswer：FranceßdummyTurker
6202ß0.0ß"The tags of the entity ""France"" are ""Nationality"", ""Capital"" and ""French"", and the description is ""The Fifth Republic, led by Charles de Gaulle, was formed in 1958 and remains today."ßdummyTurker
6203ß0.0ß"Algeria and nearly all the other colonies became independent in the 1960s and typically retained close economic and military connections with France."""ßdummyTurker
6204ß0.0ß"Obviously, the tags and description of ""entity"" are quite related to the question, which makes the answer to be ""France"" more likely."ßdummyTurker
6205ß0.0ßIn the above example, our approach obtains the right answer, since the tags and description provide much hits for the right answer.ßdummyTurker
6206ß0.0ß"However, the other methods without KRN obtained the wrong answer ""Algeria""."ßdummyTurker
6207ß0.0ß"The reason may be that the context of ""Algeria"" in the document is more similar to the question."ßdummyTurker
6208ß0.0ßThis is because tags would bring noise for reading comprehension in that case.ßdummyTurker
6209ß0.0ßHowever, we always achieve better performance when applying KRN to any existing reading comprehension model in the experiments, although some models only with tags-aware attention or description-aware attention will obtain an even better performance, just as the question mentions.ßdummyTurker
6210ß0.0ßFacing different models and datasets, we don't need to judge whether we should apply tags-aware attention, description-aware attention, or both to get better performance.ßdummyTurker
6211ß0.0ßFrom Table 2, we can see that we always achieve better performance when applying KRN to the most commonly reading comprehension models.ßdummyTurker
6212ß0.0ßThe precision would even increase by 6.81% and 4.3% for GA Reader and BiDAF on the WebQA dataset.ßdummyTurker
6213ß0.0ßAfter entity linking, the multi-token entities are just treated like words, and its embedding vector is just the average vector of its words' embedding vectors.ßdummyTurker
6214ß0.0ßThe novelty in the model is sharing sub templates.ßdummyTurker
6215ß0.0ßWithout this novelty - Chamber's model cannot scale to complicated data with multiple levelsßdummyTurker
6216ß0.0ßYou can use multi modal templates to model paragraphs with text, graphs and picture.ßdummyTurker
6217ß0.0ßYou need a separate set of template for content of every modal, and the template of the paragraph contains separate distributions over  templates of every modal.ßdummyTurker
6218ß0.0ßTemplates of paragraph model the narrative structure of the paragraph based, among other, on the templates of graphs and pictures used in this paragraph.ßdummyTurker
6219ß0.0ßFor modeling a full biomedical article - you need levels for (1) article, (2)zone (abstract m introduction , materiel and methods ....) (3)paragraph, (4)sentence, (5) clauses and (6) slots.ßdummyTurker
6220ß0.0ßWhen you use 2 levels, you can use parametric model and tune number of templates.ßdummyTurker
6221ß0.0ßWhen you use multiple levels (even 3) it is practically impossible to tune number of templates on all levels together.ßdummyTurker
6222ß0.0ßWe tested the model on biomedical abstracts on Nano Particles, and used 3 levels: document, sentence and clause.ßdummyTurker
6223ß0.0ßIn these abstract usually every sentence presents a different topic , and is generated from a different template.ßdummyTurker
6224ß0.0ßBackground information on CRP and HDP is indeed not necessary and will be removed.ßdummyTurker
6225ß0.0ßTemplate induction is totally unsupervised.ßdummyTurker
6226ß0.0ßpredefined templates are used only in evaluationßdummyTurker
6227ß0.0ßThe model support multiple level, and we used 3 level templates on biomedical abstracts, so templates in level 1 include distributions over learned templates in level 2.ßdummyTurker
6228ß0.0ßin Table 3 the template share slots which are learned distributions, and not predefined SRLs.ßdummyTurker
6229ß0.0ßThe slot labels are manual notation of the content of the slot.ßdummyTurker
6230ß0.0ßI agree that to demonstrate the power of the model we need to present experiments on more complicated dataßdummyTurker
6231ß0.0ßText preprocessing is done with same tools and steps as in Chamber's.ßdummyTurker
6232ß0.0ßWe use active/passive verb normalization.ßdummyTurker
6233ß0.0ßI agree it does not answer the examples you gave, but it works well on most of the data.ßdummyTurker
6234ß0.0ßWe agree, our paper is interesting.ßdummyTurker
6235ß0.0ßWe also agree that it is theoretical and does not contain experimental component.ßdummyTurker
6236ß0.0ßComputing the effects of communicative acts is well within the scope of ACL .ßdummyTurker
6237ß0.0ßWe do NOT admit that our approach is not applicable to actual communication and actual messages.ßdummyTurker
6238ß0.0ßOn the contrary, our approach, while theoretical, IS applicable to actual communication.ßdummyTurker
6239ß0.0ßIt's not clear which work the reviewer claims we are restating.ßdummyTurker
6240ß0.0ßIf the reviewer claims that it's work on IPOMDP than we disagreeßdummyTurker
6241ß0.0ßDEC-POMDPs, not IPOMDPs, and is not even sequential (i.e., there is no notion of planning, not to speak of decision-theoretic version thereof.)ßdummyTurker
6242ß0.0ßAll parts of the paper are describing original contributions, most notably including communicative actions in IPOMDPs, derivation of Bayesian pragmatics as Bayesian belief update due to communication, and formulating sequential planning for communicative behavior using Bellman optimality principle.ßdummyTurker
6243ß0.0ß"Reviewer's claim that ""...the paper just deals with computing scores of trustworthiness"" is a serious misunderstanding: We are not computing any trustworthiness scores, rather building general belief update due to communication."ßdummyTurker
6244ß0.0ßReviewer's suspicion that \theta_j is assumed to be the true model is another misunderstanding: As clearly defined agent i maintains probability distribution over possible models of agent j.ßdummyTurker
6245ß0.0ßAs such it is very much in the spirit of probabilistic pragmatics.ßdummyTurker
6246ß0.0ßPrevious tagging methods assign a single tag to a word.ßdummyTurker
6247ß0.0ßAs a result, one word can at most participate in one triplet.ßdummyTurker
6248ß0.0ßBy contrast, MultiDecoder is based on Seq2Seq model with copy mechanism and could apply multiple separated decoders to generate multiple triplets.ßdummyTurker
6249ß0.0ßIn this process, an entity could be copied several times for constructing multiple different triplets.ßdummyTurker
6250ß0.0ßExperiment results also show that MultiDecoder model performs better.ßdummyTurker
6251ß0.0ßThe NYT and WebNLG dataset are public and widely used datasets.ßdummyTurker
6252ß0.0ßDatasets like ACE are too small and barely contains overlapping sentences.ßdummyTurker
6253ß0.0ßExamples in figure 1 are the most typical cases.ßdummyTurker
6254ß0.0ßA sentence could contain numbers of triplets.ßdummyTurker
6255ß0.0ßIf there is no situation that an entity that appear in more than one triplets, this sentence belongs to Normal class.ßdummyTurker
6256ß0.0ßFor example sentence contains (e1, r1, e2), (e3, r2, e4).ßdummyTurker
6257ß0.0ßIf there is an entity pair that appear more than one triplets, this sentence belongs to EntityPairOverlap class.ßdummyTurker
6258ß0.0ßFor example, sentence contains (e1, r1, e2), (e1, r2, e2), (e3, r3, e4).ßdummyTurker
6259ß0.0ßIf there is an entity that appear more than one triplet (the first constraint), and these relevant triplets don't share an entity pair (the second constraint), this sentence belongs to SingleEntityOverlap.ßdummyTurker
6260ß0.0ßIt's not enough to only have to first constraint.ßdummyTurker
6261ß0.0ßLook at the example in EntityPairOverlap, e1 meets this constraint.ßdummyTurker
6262ß0.0ßTherefor, we need to add the second constraint.ßdummyTurker
6263ß0.0ßSentence contains (e1, r1, e2), (e1, r2, e3) belongs SingleEntityOverlap.ßdummyTurker
6264ß0.0ßFor a more complax case, a sentence contains (e1, r1, e2), (e1, r2, e2), (e1, r2, e3), (e4, r4, e5), this sentence belongs to both EntityPairOverlap (because of the first two triplets) and SingleEntityOverlap (because of the first (or second) and third triplets).ßdummyTurker
6265ß0.0ßWe are sorry that our definition confused you, we will update our paper to make it clearer.ßdummyTurker
6266ß0.0ßThe hyper-parameters are tuned on the validation set.ßdummyTurker
6267ß0.0ßThanks for your identification and we will update this to our paper.ßdummyTurker
6268ß0.0ßWe will add the statistical significance in the next version.ßdummyTurker
6269ß0.0ßIt's true that NovelTagging model ignore the overlapping problems.ßdummyTurker
6270ß0.0ßThus it performs bad on sentences containing several triplets.ßdummyTurker
6271ß0.0ßOur models, by contrast, are able to handle overlapping problems.ßdummyTurker
6272ß0.0ßWe think this is our advantage but not weakness.ßdummyTurker
6273ß0.0ßWe agree and appreciate that NovelTagging model performs well in no overlapping scenario and our experiment verified this (line 683-686 in our paper).ßdummyTurker
6274ß0.0ßThe comparison with NorvelTagging model tries to verify that on a dataset under overlapping scenario, only consider the basic case is not enough.ßdummyTurker
6275ß0.0ßOur models need to judge how many triplets in the given sentence and extract the valid triplets.ßdummyTurker
6276ß0.0ßAlthough it many hurt the performance of Normal class, it turns out to be helpful in the whole dataset.ßdummyTurker
6277ß0.0ßThanks for your suggestion, we will fix it in the next version.ßdummyTurker
6278ß0.0ßExamples in figure 1 are all from WebNLG dataset.ßdummyTurker
6279ß0.0ßThanks for your careful review, we will correct them in the next version.ßdummyTurker
6280ß0.0ßThe choice of all the hyperparameters in our model is based more on experience or on the values used in other papers.ßdummyTurker
6281ß0.0ßBecause we use the same model (Text Encoder Module) to encode DUs and paragraphs, the dimension of representation for paragraphs is the same as the dimension of DUs.ßdummyTurker
6282ß0.0ßFor the division of discourse units, we directly use the gold results in the corpus.ßdummyTurker
6283ß0.0ßThe discourse unit in CDTB is usually a clause, and each unit contains at least one predicate and expresses a proposition.ßdummyTurker
6284ß0.0ßYou are right, how to learn the semantic representation of long text is challenging.ßdummyTurker
6285ß0.0ßThis is one of the reasons that we use Bi-LSTM and CNN at the same time to capture the semantics information at local and long-distance context.ßdummyTurker
6286ß0.0ßTo address this issue, in our future work, we will try to combine topic models (such as LDA) with our work and perform text matching methods at topic level.ßdummyTurker
6287ß0.0ßThe model structures for aspect-level classification and document-level classification are different.ßdummyTurker
6288ß0.0ßFor aspect-level, one challenge is to separate opinion contexts for different targets, thus the model additionally takes target as one of the inputs and employs an attention layer.ßdummyTurker
6289ß0.0ßTherefore, PRET is not the same as training on a larger training set.ßdummyTurker
6290ß0.0ßThere are two main reasons why the improvements of macro-f1 scores are more obvious on D3 and D4 than on D1: 1) D1 has more neutral examples in the training set  (in terms of both numbers 637 and percentages 18%) for the classifier to learn from, while D3 and D4 only contains very few neutral training examples (less than 4% ).ßdummyTurker
6291ß0.0ßThis makes a lot difference, as a classifier without any external knowledge might still be able to learn some neutral-related features on D1 but is very hard to learn that on D3 and D4.ßdummyTurker
6292ß0.0ß2) The  number of neutral examples in the test set of D3 and D4 are very small (35 and 38 respectively).ßdummyTurker
6293ß0.0ßThis means, the precision and recall on neutral class will be largely affected by a small difference (e.g.ßdummyTurker
6294ß0.0ß5/35 = 0.143: with 5 more neutral examples been correctly identified, the recall can be increased by 14.3%).ßdummyTurker
6295ß0.0ßThe macro-f1 scores will thus be more obviously affected on D3 and D4.ßdummyTurker
6296ß0.0ßThe significant tests can be easily conducted for PRET and MULT individually compared to all baselines, as we have the 5 runs results of each model.ßdummyTurker
6297ß0.0ßWith the same test setting used for PRET+MULT, the improvements of PRET are significant on all values except the acc.ßdummyTurker
6298ß0.0ßof D1.ßdummyTurker
6299ß0.0ßThe improvements of MULT are significant on both acc.ßdummyTurker
6300ß0.0ßand macro-f1 of D3 and on macro-f1 of D4.ßdummyTurker
6301ß0.0ßThis can be easily addressed by adding more extensive discussions on related works in the camera-ready version with more space.ßdummyTurker
6302ß0.0ßOverall, transfers of LSTM layer and embedding layer are more helpful than the output layer.ßdummyTurker
6303ß0.0ßThis is as what we expected, as the output layer is more task-specific.ßdummyTurker
6304ß0.0ßIn the aspect-level model, an additional attention layer is added before the output layer, which may make the representations received by the output layer different from those of the document-level model.ßdummyTurker
6305ß0.0ßThe reasons why transfer of embedding layer is more helpful on D3 and D4 remain to be explored.ßdummyTurker
6306ß0.0ßOne plausible explanation is that the label distributions are extremely unbalanced for D3 and D4.ßdummyTurker
6307ß0.0ßAs mentioned in section 4.4, sentiment information are not naturally captured by GloVe word embeddings.ßdummyTurker
6308ß0.0ßE.g.ßdummyTurker
6309ß0.0ßthe embeddings of “good” and “bad” are very similar as they appears in similar contexts.ßdummyTurker
6310ß0.0ßTherefore, with small number of training examples (e.g.ßdummyTurker
6311ß0.0ßneg or neu class), the learned embeddings for aspect-level classification still cannot effectively capture the true semantics of the relevant opinion words.ßdummyTurker
6312ß0.0ßTransfer of embedding layer can be greatly helpful in this case.ßdummyTurker
6313ß0.0ßDue to space limitation, we were unable to further discussions for Table 3.ßdummyTurker
6314ß0.0ßWe will add them in camera-ready version.ßdummyTurker
6315ß0.0ßSince our main focus is aspect-level classification, we do not want to be affected too much by document-level classification.ßdummyTurker
6316ß0.0ßThus, we tuned lambda from 0.05 to 0.5 with step size 0.05.ßdummyTurker
6317ß0.0ßThe tuning process was only conducted on the development set of D1 and then the selected lambda value (0.1) was fixed for all other experiments on D2, D3, and D4.ßdummyTurker
6318ß0.0ßAs mentioned in section 4.1 paragraph 3, we sampled 20% of the original training data from the aspect-level dataset as the development set and only use the remaining 80% for training.ßdummyTurker
6319ß0.0ßWe only tuned the hyper-parameter lambda on the dev set of D1 and fixed its value for all other experiments.ßdummyTurker
6320ß0.0ßFor experiment on each dataset, we trained our models for a fix number of epochs, and saved the best model for evaluation based on the performance on the development set.ßdummyTurker
6321ß0.0ßThanks for your comments.ßdummyTurker
6322ß0.0ßWe have considered informative and trivial labels in our human evaluation.ßdummyTurker
6323ß0.0ßYes, it would be better if we have more finer grades in labeling and we will try to explore this in our future work.ßdummyTurker
6324ß0.0ßThank you very much for your suggestion.ßdummyTurker
6325ß0.0ßWe have checked the text and corrected several errors as suggested.ßdummyTurker
6326ß0.0ßThank you very much for your suggestion.ßdummyTurker
6327ß0.0ßWe will correct the grammar error and add clear explanation of PubMed.ßdummyTurker
6328ß0.0ßThanks for your suggestion.ßdummyTurker
6329ß0.0ßWe will add clear indication that our codes will be made publicly available if the paper is accepted.ßdummyTurker
6330ß0.0ßThank you very much for your suggestion.ßdummyTurker
6331ß0.0ßIn fact, this idea is inspired by the gate using for combining character-level embeddings and word embeddings in named entity recognition.ßdummyTurker
6332ß0.0ßIf we didn’t introduce a gate for trade-off, the two parts will be mixed in a fixed weight by transfer matrix, which is not reasonable as described in the paper.ßdummyTurker
6333ß0.0ßThanks for your suggestion.ßdummyTurker
6334ß0.0ßYour understanding is right.ßdummyTurker
6335ß0.0ßThe triggers in the sentence is sparse, so we want to use the ability of GRU to remember the trigger label in the sentence and forget the ‘O’ in the sentence.The experimental results seem to show that the decoder takes effect and the collapsing of multiple pieces of information seems not obvious.ßdummyTurker
6336ß0.0ßWe may give more detailed analysis of the output and the weight of the decoder in the future.ßdummyTurker
6337ß0.0ßThanks again for your useful suggestion.ßdummyTurker
6338ß0.0ßWe apologize for our negligence and we will replenish the corresponding citations.ßdummyTurker
6339ß0.0ßIt is really true as you suggested that clear explanation should be provided.ßdummyTurker
6340ß0.0ßAnd we will add supplement to these subsections.ßdummyTurker
6341ß0.0ßWe will cite that BILOU has been used previously in [2].ßdummyTurker
6342ß0.0ßThank you very much for your suggestion.ßdummyTurker
6343ß0.0ßIn fact, the attention in [2] and the gate in the proposed model are different things.ßdummyTurker
6344ß0.0ßThe attention in [2] aims at combining the attention vectors achieved from different views.ßdummyTurker
6345ß0.0ßThe gate in the proposed model is introduced to trade off the word embeddings and the contextual clues as described in section 2.3.ßdummyTurker
6346ß0.0ßWe will add more clear statement on the motivation of the gate.ßdummyTurker
6347ß0.0ßThe Bi-GRU unit used in strategy S2 is similar to the Bi-GRU in Encoder.ßdummyTurker
6348ß0.0ßHowever, their activation function is different.ßdummyTurker
6349ß0.0ßThe Bi-GRU in S2 uses sigmod as activation function for calculating gate value.ßdummyTurker
6350ß0.0ßh_a is the output of the attention in encoder, as you can see in the equation (6) in section 2.2.ßdummyTurker
6351ß0.0ßThank you very much for reviewing our paper.ßdummyTurker
6352ß0.0ßWe appreciate your precise understanding on our work.ßdummyTurker
6353ß0.0ßHTEM is relevant to the proposed model in the sense that both models consider the topic dependency along time and the one at each time.ßdummyTurker
6354ß0.0ßHowever, there is an important difference between them, i.e., the proposed model considers the temporal evolution of the topic-word distributions, whereas HTEM does not capture it.ßdummyTurker
6355ß0.0ßConsequently, the semantics we may find from the outputs of these models would be useful in different applications in general.ßdummyTurker
6356ß0.0ßThe main purpose of the experiment in Section 4.2 was to evaluate the effectiveness of the combination of the building blocks of the proposed method, i.e., PAM and DRTM.ßdummyTurker
6357ß0.0ßHence, comparison to other models like HTEM was not conducted.ßdummyTurker
6358ß0.0ßAs future work, we would like to work on further comparison of modeling performance.ßdummyTurker
6359ß0.0ßThank you for the advice.ßdummyTurker
6360ß0.0ßWe would like to add explanations possibly utilizing the additional page in the final version.ßdummyTurker
6361ß0.0ß"In Figure 2, ""Planning 1"" topic bifurcates into the two topics, ""Planning 1"" and ""Planning 2"", between 2014 and 2015."ßdummyTurker
6362ß0.0ß"Our interpretation on these topics is as follows; ""Planning 1"" is on both motion planning and path planning until 2014, but in 2015, the motion planning part diverges to ""Planning 2."""ßdummyTurker
6363ß0.0ßA part of the reason of this phenomena would be that researches on planning were gaining increasing attention because easily-controllable drones were becoming accessible more and more in recent years.ßdummyTurker
6364ß0.0ßThank you very much for reviewing our paper.ßdummyTurker
6365ß0.0ßWe appreciate your precise understanding and favorable comments on our work.ßdummyTurker
6366ß0.0ßWe omitted some arguments on the design of the model and the inference procedures due to the page limit.ßdummyTurker
6367ß0.0ßFor example, as for the approximative inference by running EM at each epoch, we found the strategy utilized by Iwata et al.ßdummyTurker
6368ß0.0ß(2010) useful and followed it (we accidentally dropped a citation to this work in Section 3.2; we will modify it in the final version).ßdummyTurker
6369ß0.0ßWe would like to add as much explanation on how we designed the proposed model and the inference procedures as possible.ßdummyTurker
6370ß0.0ßIn HTEM, a topic tends to appear commonly in two documents whose timestamps are close.ßdummyTurker
6371ß0.0ßThis is a kind of the dynamic relation modeled by HTEM.ßdummyTurker
6372ß0.0ßOn the other hand, in the proposed model, the dynamic relation is modeled as the temporal evolution of the topic-word distributions that depends on multiple topics in the previous timestep.ßdummyTurker
6373ß0.0ßThe aim of the proposed model was to extract understandable topics by allowing their contents to evolve along time and by making them related to multiple topics in the past.ßdummyTurker
6374ß0.0ßHTEM does not necessarily fit this purpose.ßdummyTurker
6375ß0.0ßOne of the key aspects of the proposed method is that there are not so many hyperparameters to be set manually, and that those hyperparameters can be set according to the existing know-how of topic modeling.ßdummyTurker
6376ß0.0ßAnd, we described the specific parameter settings of the experiments in the supplementary material.ßdummyTurker
6377ß0.0ßThank you for your supportive review.ßdummyTurker
6378ß0.0ßWe use the same optimizer with baseline models.ßdummyTurker
6379ß0.0ßSpecifically, we use Adam for small-scale dataset and simple gradient descent for large-scale dataset.ßdummyTurker
6380ß0.0ßThere are a few unsupervised methods for bilingual lexicon induction and MonoGiza system (Dou et al., 2015) and adversarial training approach (Zhang et al., 2017) are representative ones, which is why we used them as our baselines.ßdummyTurker
6381ß0.0ßMost approaches reply on seed lexica, such as TM (Mikolov et al., 2013) and IA (Zhang et al., 2016b), and thus we also compare our model with them.ßdummyTurker
6382ß0.0ßIn addition, Zhang et al.ßdummyTurker
6383ß0.0ß(2017) also compare with these baselines.ßdummyTurker
6384ß0.0ßThere is not much space and we would add “Conclusion”, “Future Work” and “Acknowledgement” sections once given five pages after acceptance.ßdummyTurker
6385ß0.0ßWe use Adam for small-scale dataset and simple gradient descent for large-scale dataset.ßdummyTurker
6386ß0.0ßThere are a few unsupervised methods for bilingual lexicon induction and MonoGiza system (Dou et al., 2015) and adversarial training approach (Zhang et al., 2017) are representative ones.ßdummyTurker
6387ß0.0ßMost approaches reply on seed lexica, such as TM (Mikolov et al., 2013) and IA (Zhang et al., 2016b).ßdummyTurker
6388ß0.0ßTherefore, we compare our model with them.ßdummyTurker
6389ß0.0ßtechnical novelty of the iterative back translationßdummyTurker
6390ß0.0ß>> Intuitively, it’s not surprising that repeating the back translation process leads to improved translation performance.ßdummyTurker
6391ß0.0ßHowever not all intuitive ideas pan out in practice, and it’s important for progress in the field to show that this does indeed hold true.ßdummyTurker
6392ß0.0ßWe show empirically that this works well for both high and low resource conditions (in fact, we got SOTA results on English <-> German translation task on WMT 17).ßdummyTurker
6393ß0.0ßThe proposed method is simple but highly applicable in practice.ßdummyTurker
6394ß0.0ßeffectiveness of Moses’s back translationßdummyTurker
6395ß0.0ß>> We already know that the NMT is a data-hungry approach, requiring a lot of parallel data to perform well.ßdummyTurker
6396ß0.0ßIn a low-resource setting, SMT still performs consistently better than NMT.ßdummyTurker
6397ß0.0ßThe SMT-based back-translated synthetic data has better quality, hence leading to comparable results with the entirely NMT-based back-translated data.ßdummyTurker
6398ß0.0ßthe convergence condition of the algorithmßdummyTurker
6399ß0.0ß>> In our paper, repeating the back translation process in 2-3 times can lead to improved translation.ßdummyTurker
6400ß0.0ßHowever, this can be different in other language pairs and domains.ßdummyTurker
6401ß0.0ßAlso, in our paper, we sampled the monolingual data uniformly at random, so sentences may be used more than once in subsequent rounds.ßdummyTurker
6402ß0.0ßIt’s quite likely that other techniques for data sampling, e.g., non-uniform sampling like transductive selection or active learning - which potentially diversifies the quality and quantity of monolingual data - would lead further improvements in translation performance.ßdummyTurker
6403ß0.0ßcomputational complexity and feasibilityßdummyTurker
6404ß0.0ß>> The efficiency of the NMT toolkits we used (sockeye, marian-nmt) is excellent.ßdummyTurker
6405ß0.0ßBoth support batch decoding for fast translation, e.g., with a batch-size of 200 (beam-size 5) marian-nmt can achieve over 5000 words per second on one GPU (<1 day for translating 4M sentences); and also this scales linearly to the number of GPUs we have.ßdummyTurker
6406ß0.0ßAlternatively, we can split the monolingual data into smaller parts and distribute these parts over different GPUs.ßdummyTurker
6407ß0.0ßThis can greatly speed up the back-translation process.ßdummyTurker
6408ß0.0ßThis leaves the problem of training the model in each iteration, which we do 2-3 times.ßdummyTurker
6409ß0.0ßOverall the computational complexity is not a big deal (even with larger dataset), and the iterative back translation is quite feasible with existing modern GPU server(s).ßdummyTurker
6410ß0.0ßoriginality of the proposed methodßdummyTurker
6411ß0.0ß>> Intuitively, it’s not surprising that repeating the back translation process can lead to improved translation performance.ßdummyTurker
6412ß0.0ßHowever not all intuitive ideas pan out in practice, and it’s important for progress in the field to show that this does indeed hold true.ßdummyTurker
6413ß0.0ßWe show empirically that this works well for both high and low resource conditions.ßdummyTurker
6414ß0.0ßThe method is simple but highly applicable in practice.ßdummyTurker
6415ß0.0ßerror analysisßdummyTurker
6416ß0.0ß>> Thanks for your suggestion.ßdummyTurker
6417ß0.0ßIf this paper gets accepted and given an extra page, we will add a section for the error analysis.ßdummyTurker
6418ß0.0ßthe results from phrase-based Moses SMT toolkit compared to NMTßdummyTurker
6419ß0.0ß>> The results on phrase-based SMT with Moses are not surprising.ßdummyTurker
6420ß0.0ßFor English-to-French translation task, NMT actually needs  a fair amount of parallel data to outperform SMT (this happens similarly in other European languages as shown in [1]).ßdummyTurker
6421ß0.0ßThis validates our simulated situation, with 100K and 1M corpus size.ßdummyTurker
6422ß0.0ß[1] Koehn et al., Six Challenges for Neural Machine Translation.ßdummyTurker
6423ß0.0ßIn WMT 2017.ßdummyTurker
6424ß0.0ßnote on English - Farsi results; and analysing the impact of iteratively back-translated data in relation to the varying domain (dis)similarity between parallel and monolingual dataßdummyTurker
6425ß0.0ß>> The English - Farsi parallel data is much lower quality than the other datasets.ßdummyTurker
6426ß0.0ßDue to the limited data available, we collected and mixed all available training data, including news (from LDC) and TED talks.ßdummyTurker
6427ß0.0ßThe testing data is solely on news.ßdummyTurker
6428ß0.0ßThis probably causes the inconsistency in terms of translation performance.ßdummyTurker
6429ß0.0ßMeasuring the impact of iteratively back-translated data in relation to varying domain mismatch between parallel and monolingual data,  is a very interesting problem which we will explore in future work; but is out of the scope for this paper.ßdummyTurker
6430ß0.0ßconvergence condition of the iterative back translationßdummyTurker
6431ß0.0ß>> In our paper, repeating the back translation process in 2-3 times can lead to improved translation.ßdummyTurker
6432ß0.0ßHowever, this can be different in other language pairs and domains.ßdummyTurker
6433ß0.0ßAlso, in our paper, we sampled the monolingual data in a random way, so overlapping may happen.ßdummyTurker
6434ß0.0ßA more effective way for data sampling, e.g., non-uniform sampling like transductive selection or active learning - which potentially diversifies the quality and quantity of monolingual data - hence, allowing to repeat the back translation process many times, and potentially achieving further improvements in translation performance.ßdummyTurker
6435ß0.0ßthe issue of term consistency in the tablesßdummyTurker
6436ß0.0ß>> Thanks for your correction.ßdummyTurker
6437ß0.0ßWe will fix this in the revised version.ßdummyTurker
6438ß0.0ßTo explain it, “reback” means repeat the back-translation one time; “iter +1” means “reback” and “iter +2” means repeated “re-back” (to repeat the back translation 2 times).ßdummyTurker
6439ß0.0ßThis corresponds to the convergence condition or the number of iterations in algorithm 1.ßdummyTurker
6440ß0.0ßWe will refine the notation should the paper be accepted.ßdummyTurker
6441ß0.0ßThanks for the comment.ßdummyTurker
6442ß0.0ßThe teacher-student framework has been applied to many fields.ßdummyTurker
6443ß0.0ßHowever, we believe this is the first work applying *adversarial enhanced* feature learning schema for the task of event detection.ßdummyTurker
6444ß0.0ßOur research provides a new perspective towards the task of event detection and provides new ideas for the field of Information Extraction.ßdummyTurker
6445ß0.0ßThis suggestion is constructive.ßdummyTurker
6446ß0.0ßWe will consider using dev set for ablation study and learning visualization.ßdummyTurker
6447ß0.0ßWe will add the setting of hyper-parameters in our paper for completeness.ßdummyTurker
6448ß0.0ßIn training phase, we monitor the performance on dev set.ßdummyTurker
6449ß0.0ßWhen the evaluation criterion meets some condition (e.g.ßdummyTurker
6450ß0.0ßF1 score over 72%), we stop the training phase.ßdummyTurker
6451ß0.0ßNote this is the common setting of adversarial-based methods.ßdummyTurker
6452ß0.0ßWe perform paired t-test under a significant level of 0.05.ßdummyTurker
6453ß0.0ßWe showed how using domain information like genre to supervise learning weights for individual feature types boosts prediction performance.ßdummyTurker
6454ß0.0ßThe model harness the information present in domain specific metadata.ßdummyTurker
6455ß0.0ßThis model can be easily extended to other domains by simply replacing genre with other important domain specific metadata.ßdummyTurker
6456ß0.0ßThank you for pointing  to refer Recommendation models.ßdummyTurker
6457ß0.0ßGenerally these systems' base  model use collaborative filtering with user-product matrix, but here we are learning to weight different types of features according to their relevance for the task of likability prediction.ßdummyTurker
6458ß0.0ßWe will compare and add references on existing recommendation models.ßdummyTurker
6459ß0.0ßAs stated in Section 3, each book belongs to one of the 8 possible genres.ßdummyTurker
6460ß0.0ßWe first feed different feature types (x_i’s) through dense layers  to obtain higher feature representations (h_i’s).ßdummyTurker
6461ß0.0ßNow we introduce genre.We use genre in two different ways (Section 2.1).ßdummyTurker
6462ß0.0ßFirst way is to feed h_i’s with genre vector (g) to the score function, which is defined by W_a W_g, b_a, and v parameters.ßdummyTurker
6463ß0.0ßNote that as stated in Section 4.2, the g vectors are initialized orthogonally.ßdummyTurker
6464ß0.0ßThese parameters are learned during training and are used to compute the alphas (weights) for each feature types.ßdummyTurker
6465ß0.0ßThus alphas depend on the input, genre, and Genre Aware Attention model parameters.ßdummyTurker
6466ß0.0ßNow, we use the alphas to do a weighted average of different feature types (Eq 2) to get a book vector (r).ßdummyTurker
6467ß0.0ßThis is the first setting where we only use genre for computing attention weights.ßdummyTurker
6468ß0.0ßThe next setting, i.e.ßdummyTurker
6469ß0.0ßwith genre concatenation, is similar to our first setting but we also concatenate the genre vector (g) to the book vector (r) before classification.ßdummyTurker
6470ß0.0ßThis is like using skip connections similar to the Resnet architecture.ßdummyTurker
6471ß0.0ßA model that uses genre supervision is genre-aware.ßdummyTurker
6472ß0.0ßOne of our baselines, Concatenation,  does exactly that.ßdummyTurker
6473ß0.0ßBut our main hypothesis is that learning  how to weight each of these x_1, x_2, .. ,x_n.ßdummyTurker
6474ß0.0ßindividually would perform better than concatenation.ßdummyTurker
6475ß0.0ßWe even experimentally verified these results.ßdummyTurker
6476ß0.0ßWe used the standard Resnet and VGG setups for extracting visual features.ßdummyTurker
6477ß0.0ßWe used a validation set to tune the dropouts.ßdummyTurker
6478ß0.0ßApart from that, since there are eight genres we need 8 neurons at the output layer and for success there are two classes so one neuron will suffice for classification.ßdummyTurker
6479ß0.0ßOur novelty lies in the use of genre information to determine weights for the different types of features.ßdummyTurker
6480ß0.0ßThis is the first time that is shown that an adaptive learning of feature weights for each book boosts prediction performance.ßdummyTurker
6481ß0.0ßWe also have very interesting results that validate the superiority of our model.. We extend existing vanilla attention model by adding genre supervision component (Eq 4).ßdummyTurker
6482ß0.0ßIn literature, there are basically three ways to do attention (bilinear, dot product, and MLP).ßdummyTurker
6483ß0.0ßHere, we use MLP version of attention model.ßdummyTurker
6484ß0.0ßAlong with these novelties, we define several baselines to compare with and tried to understand the model through different visualization and error analysis.ßdummyTurker
6485ß0.0ßWe will revise the paper to clarify the role of genre.ßdummyTurker
6486ß0.0ßThe two ideas of low latency translation cannot be compared directly.ßdummyTurker
6487ß0.0ßThis approaches needs an ability to update previous outputs, therefore we do not decrease the translation performance compared to an offline system.ßdummyTurker
6488ß0.0ßAs written in question 1 this would significantly increase the computation and therefore be diffcult in pratical situations.ßdummyTurker
6489ß0.0ßFurthermore, the GIZA test did not should promising results.ßdummyTurker
6490ß0.0ßI agree you can agruee both.ßdummyTurker
6491ß0.0ßSince our main use case if to use this training data for low-lantency translation, we selected this titleßdummyTurker
6492ß0.0ßAs shown by the GIZA alignment, the very simple idea already performs very well.ßdummyTurker
6493ß0.0ßTherefore, we did not continue this directions, since it siginifcantly increases the necessary resources to build a system.ßdummyTurker
6494ß0.0ßAs should by many authors of the IWSLT submission, a WMT system would perform very bad, since it is not adapted to the TED domain.ßdummyTurker
6495ß0.0ßThe system uses the standart openNMT-py setup and should therefore be a reasonable baseline of this task.ßdummyTurker
6496ß0.0ßSince the two tasks are very similar, we only need to change the training data and do not have to change the implementation.ßdummyTurker
6497ß0.0ßWe randomly shuffle the two task by shuffeling the training data.ßdummyTurker
6498ß0.0ßAs written the systems are trained on EPPS and TED.ßdummyTurker
6499ß0.0ßWe used the default parameters for OpenNMT and did no change them between the different systems.ßdummyTurker
6500ß0.0ßOptimization was done with AdamßdummyTurker
6501ß0.0ßThe BPE size is 40KßdummyTurker
6502ß0.0ßSorry, when shorting the text, it must have gone away.ßdummyTurker
6503ß0.0ßIn this approach, the less words are replaced, the earlier the final presentation can be seen and therefore, latency is smaller.ßdummyTurker
6504ß0.0ßFor the latency in this framwork, there are two measures.ßdummyTurker
6505ß0.0ßThe first time a word is written or the time the final word is written.ßdummyTurker
6506ß0.0ßTherefore, we measured only the number of replaced words and not the latency directly.ßdummyTurker
6507ß0.0ßWe will also report parital numbers in the final paper.ßdummyTurker
6508ß0.0ßThey show the same picture as the English-Spanish.ßdummyTurker
6509ß0.0ßWe were using these test sets for other experiments and therefore did not change them.ßdummyTurker
6510ß0.0ßSince all the test sets are talks from TED, we did not think this matters.ßdummyTurker
6511ß0.0ßWe can add more test sets in the final paper.ßdummyTurker
6512ß0.0ßThank you for the comments.ßdummyTurker
6513ß0.0ßYes the final version of the paper will have a better written work,ßdummyTurker
6514ß0.0ßThe results of the proposed method are competitive with the supervised part of Table (1) (left) and is comparable to the GAN based models.ßdummyTurker
6515ß0.0ßWhile optimization of GANs are quite hard, our method is more easier to understand due to the convex nature of the relaxed problem.ßdummyTurker
6516ß0.0ßThis is something which we did but did not put up in the final version due to space issues.ßdummyTurker
6517ß0.0ßFAIR recently released MUSE for mapping of monlingual embeddings and we could see significant improvement in those cases.ßdummyTurker
6518ß0.0ßWe also played with GLOVE word embeddings vs word2vec and observed better results for word2vec.ßdummyTurker
6519ß0.0ßThe final version would cover up a detailed analysis of (i)Change in dimensions (ii) Vocabulary size.ßdummyTurker
6520ß0.0ßWe did not carry on such analysis of rare words but we played around with larger daataset called MUSE where we did see significant improvement.ßdummyTurker
6521ß0.0ßPlease also note that the proposed method is free of dimensions as the method relies on transformation between objects in a source space and matches it to target space unlike prior methods which could possible rely on by doing inner product which assumes the dimensions in both space are same.ßdummyTurker
6522ß0.0ßThough we did not conduct experiments on this to show the quantitative benefits of this.ßdummyTurker
6523ß0.0ßThank you for the comments.ßdummyTurker
6524ß0.0ßWe could discuss much on theoretical guarantee due to space but will add an extra layer in the final version.ßdummyTurker
6525ß0.0ßThe training corpus have not been discussed as the language pairs which we use have been collected as pointed in Table 1.ßdummyTurker
6526ß0.0ßHowever the final version will have a more exhaustive reference on the training corpus used.ßdummyTurker
6527ß0.0ßSo i do not fully grasp this question.ßdummyTurker
6528ß0.0ßIf I understand correctly if the complaint is that the permutation matrix is a binary mapping please observe that we made a convex relaxation to the problem relaxing the constraints to be more 'soft' than a hard threshold.ßdummyTurker
6529ß0.0ßWe will also like to draw your attention to another point.ßdummyTurker
6530ß0.0ßThe method discussed in the paper can be flexibly used with arbitrary dimensions i.e we do not need the kernels in a monolingual space source language to be same to the other target language.ßdummyTurker
6531ß0.0ßWhile this has not been experimented in the current scope of the paper, we would like to still highlight this as earlier method relied on fixed word embedding space.ßdummyTurker
6532ß0.0ßThis is a very valid point.ßdummyTurker
6533ß0.0ßYes the convergence can be guaranteed.ßdummyTurker
6534ß0.0ßWe did not put this across as the space would have been an issue.ßdummyTurker
6535ß0.0ßWe would like to highlight the convex method of KTA can (i)maintain two different dimensions from source and target unlike prior methods (while this has not been experimented with) (ii)Is fast and can converge due to the convex nature of the problem.ßdummyTurker
6536ß0.0ßAdditionally it can be mapped back to the binary constraints by Simkhorn-Knopp though this has not been discussed.ßdummyTurker
6537ß0.0ßThank you for the comments.ßdummyTurker
6538ß0.0ßThe paper needs to cleaned up or written well as you pointed.ßdummyTurker
6539ß0.0ßYes you are right.ßdummyTurker
6540ß0.0ßWe observed that EMDOT though claims itself as a GAN never has a noise vector unlike to GAN methods but learns a transformation function.ßdummyTurker
6541ß0.0ßThis transformation function if constrained to be orthogonal provides you map back to the original space.ßdummyTurker
6542ß0.0ßThe difference between the W-GAN and EMDOT is necessarily that which buys EMDOT extra score.ßdummyTurker
6543ß0.0ßThank you for raising this question.ßdummyTurker
6544ß0.0ßWe were relatively new into this field so we could not provide the reasoning behind having a fixed ontology for tasks like DSTC2 except the fact the for smaller tasks, enumerating possible slot values is totally feasible.ßdummyTurker
6545ß0.0ßBut based on our experiences in the industry, we are seeing more and more real-world applications where having such a fixed ontology is impossible.ßdummyTurker
6546ß0.0ßThe list of song names, restaurant names are virtually unbounded.ßdummyTurker
6547ß0.0ßIn traditional systems, the SLU component can usually hide this problem, by providing the candidate list needed for some of the previous DST approaches.ßdummyTurker
6548ß0.0ßHowever, in E2E system where the SLU does not exist, there won't be a fixed candidate list for the tracker to operate on.ßdummyTurker
6549ß0.0ßAn important goal of our work is therefore to raise more awareness of the inadequacy of the 'closed-world' assumption, especially in this era where more things are moving toward E2E.ßdummyTurker
6550ß0.0ßYes, thank you for raising this issue as well.ßdummyTurker
6551ß0.0ßSame as our response to Reviewer 1's concern: The biggest challenge we faced for this work was the lack of previous benchmarks specifically targeting unknown slot values.ßdummyTurker
6552ß0.0ßAny direct comparisons will require replicating previous systems on our modified DSTC2 dataset.ßdummyTurker
6553ß0.0ßWe hope the fact that our approach targets a brand new problem setup (unknown values, E2E) is a merit of its own, although in future work, we definitely would love to compare with previous systems in which one or both of the constraints are relaxed (unknown and/or E2E).ßdummyTurker
6554ß0.0ßThank you for raising this issue.ßdummyTurker
6555ß0.0ßWe were also aware of this issue.ßdummyTurker
6556ß0.0ßSome basic error analysis we did, which was not included in the paper did not indicate anything systematically wrong in the model behavior.ßdummyTurker
6557ß0.0ßBased on our previous experiences working on slot filling, on such a dataset where slot values repeat so frequently, it is not entirely shocking to observe such degradations on unknown values due to overtraining.ßdummyTurker
6558ß0.0ßAlso as you have insightfully suggested, word embeddings could very likely ameliorate this issue.ßdummyTurker
6559ß0.0ßWe plan to include this in future work.ßdummyTurker
6560ß0.0ßYes.ßdummyTurker
6561ß0.0ßThe biggest challenge we faced for this work was the lack of previous benchmarks specifically targeting unknown slot values.ßdummyTurker
6562ß0.0ßAny direct comparisons will require replicating previous systems on our modified DSTC2 dataset.ßdummyTurker
6563ß0.0ßWe hope the fact that our approach targets a brand new problem setup (unknown values, E2E) is a merit of its own, although in future work, we definitely would love to compare with previous systems in which one or both of the constraints are relaxed (unknown and/or E2E).ßdummyTurker
6564ß0.0ßThe problem of stock movement prediction from social media was addressed in the NLP community several times, for example,ßdummyTurker
6565ß0.0ß[1] Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie Deng.ßdummyTurker
6566ß0.0ß2013.ßdummyTurker
6567ß0.0ßExploiting topic based twitter sentiment for stock prediction.ßdummyTurker
6568ß0.0ßIn Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.ßdummyTurker
6569ß0.0ßSoﬁa, Bulgaria, volume 2, pages 24–29.ßdummyTurker
6570ß0.0ß[2] Thien Hai Nguyen and Kiyoaki Shirai.ßdummyTurker
6571ß0.0ß2015.ßdummyTurker
6572ß0.0ßTopic modeling based sentiment analysis on social media for stock market prediction.ßdummyTurker
6573ß0.0ßIn Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing.ßdummyTurker
6574ß0.0ßBeijing, China, volume 1, pages 1354–1364.ßdummyTurker
6575ß0.0ß[3] Ziniu Hu, Weiqing Liu, Jiang Bian, Xuanzhe Liu, and Tie-Yan Liu.ßdummyTurker
6576ß0.0ß2018.ßdummyTurker
6577ß0.0ßListening to chaotic whispers: A deep learning framework for news-oriented stock trend prediction.ßdummyTurker
6578ß0.0ßIn Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining.ßdummyTurker
6579ß0.0ßACM, Los Angeles, California, USA, pages 261–269.ßdummyTurker
6580ß0.0ßWe tested two strong baselines, TSLDA and HAN that jointly mine prices and texts as our model does.ßdummyTurker
6581ß0.0ßAs introduced in Section 5.4, the other three baselines from different genres demonstrated the effectiveness of our three model components.ßdummyTurker
6582ß0.0ßIn fact, Fig.ßdummyTurker
6583ß0.0ß1 illustrates the generative process (variables and relations) modeled in this paper, Fig.2 provides a model architecture which instantiates the generative process and Fig.3 details our temporal attention mechanism capturing predictive dependency.ßdummyTurker
6584ß0.0ßStock movement prediction from social media is an interdisciplinary research problem and thus requires some essential finance background.ßdummyTurker
6585ß0.0ßWe will make such explanations clearer and more accessible if the paper is accepted.ßdummyTurker
6586ß0.0ßAs shown in the figure title and legends, X axis is alpha, left Y axis is accuracy and right Y axis is MCC.ßdummyTurker
6587ß0.0ßThanks for pointing this out, we will add these labels to this figure.ßdummyTurker
6588ß0.0ßAs stated in our abstract, introduction and data sections, we created a new dataset and one of its components is Yahoo historical price data.ßdummyTurker
6589ß0.0ßThe model interpretability is indeed important for making investment decisions.ßdummyTurker
6590ß0.0ßHowever, it is difficult to provide linguistic analysis since tweets contribute to this task in aßdummyTurker
6591ß0.0ßcollective manner and they interact with historical prices to provide stock movement features.ßdummyTurker
6592ß0.0ßWe believe uncovering this is a very meaningful topic and will treat it in our future work on the topic.ßdummyTurker
6593ß0.0ß(1) We provide a detailed analysis of the effects of alpha in Section 5.3.ßdummyTurker
6594ß0.0ß(2) In existing research, the lag window is typically set between 3 and 10.ßdummyTurker
6595ß0.0ßAs introduced in Section 4.1, we treated trading days as basic units and 3 is too small since it cannot ensure the existence of more than one trading day in a lag (i.e.ßdummyTurker
6596ß0.0ßthe prediction for the movement of Monday).ßdummyTurker
6597ß0.0ßWe have also tried 7 and 10 but they did not show better performance than 5.ßdummyTurker
6598ß0.0ßWe thus kept using 5.ßdummyTurker
6599ß0.0ß(3) As to movement thresholds, we chose them to balance the two classes as explained in Section 3.ßdummyTurker
6600ß0.0ßIn fact, our initial choices are -0.5% and 0.5%.ßdummyTurker
6601ß0.0ßWe plotted the data distribution and found it was slightly skewed: there were more UP samples and less DOWN samples.ßdummyTurker
6602ß0.0ßTherefore we opted to move the positive threshold a bit rightward to 0.55 to balance the two classes.ßdummyTurker
6603ß0.0ßThere are in total 8 sectors in finance and we chose them all: Basic Materials, Consumer Goods, Healthcare, Services, Utilities, Conglomerates, Financial, Industrial Goods, and Technology.ßdummyTurker
6604ß0.0ßWe included them as well as all the target stocks in an appendix contained in our supplementary material.ßdummyTurker
6605ß0.0ßSee response (2) to Weakness 2.ßdummyTurker
6606ß0.0ßThese tweets were not thrown away.ßdummyTurker
6607ß0.0ßAs described in Section 4.1, trading-day alignment helps to organize tweets and prices for trading days in a lag.ßdummyTurker
6608ß0.0ßTherefore in most cases, tweetßdummyTurker
6609ß0.0ßs on weekends are aligned with tweets on Friday to jointly form a subcorpus for the movement prediction for the next Monday.ßdummyTurker
6610ß0.0ßYou are right and there are of course other options to balance classes.ßdummyTurker
6611ß0.0ßIn fact, our initial choices are -0.5% and 0.5%.ßdummyTurker
6612ß0.0ßWe plotted the data distribution and found it was slightly skewed: there were more UP samples than DOWN samples.ßdummyTurker
6613ß0.0ßTherefore we opted to move the positive threshold a bit rightward to 0.55 to balance the two classes.ßdummyTurker
6614ß0.0ßAs to the test set, we selected 01/10/2015 to 01/01/2016 since they are the last two months of the whole span 01/01/2014 to 01/01/2016.ßdummyTurker
6615ß0.0ßWe used earlier samples for training and latter samples for test, or we would end up predicting the past using the future.ßdummyTurker
6616ß0.0ßTo our best knowledge, there is no publicly-available large tweet dataset for stock movement prediction.ßdummyTurker
6617ß0.0ßThere is a Yahoo!ßdummyTurker
6618ß0.0ßMessage Board dataset released by a stock prediction paper published in EMNLP 2014.ßdummyTurker
6619ß0.0ßHowever, the Yahoo!ßdummyTurker
6620ß0.0ßMessage Board service was shut down by Yahoo!ßdummyTurker
6621ß0.0ßseveral years ago.ßdummyTurker
6622ß0.0ßInvestigating its dataset is of little practical significance since no more new data could be obtained for real-world applications.ßdummyTurker
6623ß0.0ßIn addition, it is a small dataset compared to our twitter dataset.ßdummyTurker
6624ß0.0ßIn Section 5.3 and 5.4, we devised four variants to test the effectiveness of these components.ßdummyTurker
6625ß0.0ßAccording to our experimental results, we believe VMD plays the most core role.ßdummyTurker
6626ß0.0ßMIE provides VMD with features from texts and prices and is thus more fundamental.ßdummyTurker
6627ß0.0ßAs to ATA, it captures temporal predictive dependency and enhances the variational framework further (but not in a strictly-linear manner, as discussed in Section 5.5).ßdummyTurker
6628ß0.0ßThe memory cost is fairly high: we need to feed into the model all tweets in batched samples at the same time.ßdummyTurker
6629ß0.0ßWe set the word embedding size to 50 instead of larger sizes make model training feasible on one single GPU (11G).ßdummyTurker
6630ß0.0ßAs introduced in Section 4.2.1, learning parameters are shared in the message embedding layer, leading to reasonable training efficiency.ßdummyTurker
6631ß0.0ßSee response to Comment 2.ßdummyTurker
6632ß0.0ßWe did an extensive literature survey, but we could not find anyone who has addressed this problem before.ßdummyTurker
6633ß0.0ßWe will use the extra page to add more explanation and comments about the figures and the tables.ßdummyTurker
6634ß0.0ßWe will rephrase the paper to clarify that our overall goal is to develop practical methods for estimating how much training data is required for a system to achieve an accuracy target.ßdummyTurker
6635ß0.0ßWe do this by developing methods for estimating a system's accuracy on a large training set from its results  on a small pilot set.ßdummyTurker
6636ß0.0ßThis paper investigates one simple problem in depth, but as the paper says, we suggest these kinds of experiments be conducted on many different tasks.ßdummyTurker
6637ß0.0ßWe will clarify in the text that we do not claim that any particular extrapolation method is best for all problems, but rather that extrapolation can be used to predict the system’s accuracy on a large dataset from small pilot sets.ßdummyTurker
6638ß0.0ßThe specific task we evaluated the extrapolation methods on is described in section 4.ßdummyTurker
6639ß0.0ßFull details of the task (including the data sets) will be released when the data when the paper is published.ßdummyTurker
6640ß0.0ßSince this is a new topic, we decided to study a fast classifier-based system in depth first.ßdummyTurker
6641ß0.0ßOur analysis involves training and testing around 900,000 different classifier configurations, which would not be feasible with more complex systems.ßdummyTurker
6642ß0.0ßThis enabled us to demonstrate the importance of hyperparameter optimisation on each different-sized training subset, for example, which prior work had not demonstrated.ßdummyTurker
6643ß0.0ßWe agree that the work should be extended to additional and more complex tasks.ßdummyTurker
6644ß0.0ßWe plan to address this in future work, as it is beyond the scope of a 4-page paper.ßdummyTurker
6645ß0.0ßWe will add more details of the methods and evaluation (including the hyperparameters).ßdummyTurker
6646ß0.0ßThe datasets (and train-test splits) are from Joulin et al (2016).ßdummyTurker
6647ß0.0ßWe used the 4 development corpora to find the best extrapolation method (biased power law with binomial item weights), and the 4 test corpora to evaluate the performance of the extrapolation method on different subsets of the training data.ßdummyTurker
6648ß0.0ßWe will add axes to the figures, and fix the color scheme (the scheme we used is supposed to be visible to the colorblind, but obviously isn't working).ßdummyTurker
6649ß0.0ßWe will also move the figures closer to the relevant text.ßdummyTurker
6650ß0.0ßMany thanks for your helpful reviews.ßdummyTurker
6651ß0.0ßIn about one-third of “loss” cases of win/tie/loss evaluation for OGM and S2S models, vague or non-committal responses (e.g., “So do I.” / “OK, I am sorry.”) from seq2seq models helped them to win over OGM, since those responses are appropriate to lots of utterances.ßdummyTurker
6652ß0.0ßBut OGM did not generate such kinds of responses.ßdummyTurker
6653ß0.0ßIf we take this factor into account, the performance gain with OGM is not small.ßdummyTurker
6654ß0.0ßMoreover, we noticed that the use of OR (opinion ranking) can promote subjective utterances and downgrade vague responses for S2S models.ßdummyTurker
6655ß0.0ßAnd OR hurts the performance of S2S since there are less vague responses to help S2S models.ßdummyTurker
6656ß0.0ßIt is consistent with our previous observation.ßdummyTurker
6657ß0.0ßThanks for your advice on this evaluation.ßdummyTurker
6658ß0.0ßWe conducted human judgement on high-frequency sentiment phrases from utterances in each speaker cluster id.ßdummyTurker
6659ß0.0ßWe found that speakers in some cluster ids tend to use negative words, e.g., “clumsy”/“lazy”/“ugly”;  speakers in some cluster ids tend to use positive words, e.g., “attractive”/“handsome”/“gentle”;  speakers in some cluster ids tend to use dirty words; and speakers in some cluster ids tend to use feminine expressions.ßdummyTurker
6660ß0.0ßWe will update the draft with these results later.ßdummyTurker
6661ß0.0ßMany thanks for your helpful reviews.ßdummyTurker
6662ß0.0ß1.Reply to weakness argument- “The approach used for ….”:ßdummyTurker
6663ß0.0ßThanks for pointing out missing references.ßdummyTurker
6664ß0.0ßThe focus of our work is different from argumentative dialogue systems or pursuation dialogue systems.ßdummyTurker
6665ß0.0ßIn this work, we focused on how to automatically learn personalized and self-consistent viewpoints from data without the requirement of predefining sentiment polarity (e.g., positive /negative view) towards each topic or target.ßdummyTurker
6666ß0.0ßBut argumentative or pursuation dialogue systems focused on selection of sequential arguments with a predefined dialogue goal (e.g., supportive  or non-supportive view towards a topic).ßdummyTurker
6667ß0.0ßThis work takes a step towards the aim of opinion modeling, or modeling of subjective expressions.ßdummyTurker
6668ß0.0ßIt is possible to extend this work to argument modeling and integrate it with argumentative or pursuation dialogue systems in the future.ßdummyTurker
6669ß0.0ßIt might help argumentative or pursuation dialogue systems to maintain self-consistency of viewpoints across argument topics.ßdummyTurker
6670ß0.0ß2.Reply to weakness argument-“The proposed model is compared …”:ßdummyTurker
6671ß0.0ßThe retrieval based baseline (Matsuyama  et.ßdummyTurker
6672ß0.0ßal.ßdummyTurker
6673ß0.0ß(2015)) was built for opinion generation.ßdummyTurker
6674ß0.0ßThey conducted opinion extraction/entity recognition/sentiment polarity classification for building a set of opinion candidates in retrieval system and they proposed various ranking functions for opinion retrieval.ßdummyTurker
6675ß0.0ßIn human communication, the content (e.g., sentiment phrase) of opinion expressions as responses usually depends on the contextual information, e.g.ßdummyTurker
6676ß0.0ßpreceding utterances.ßdummyTurker
6677ß0.0ßHowever, with respect to context information, only entity information from user input was used for opinion retrieval in Matsuyama  et.ßdummyTurker
6678ß0.0ßal.ßdummyTurker
6679ß0.0ß(2015).ßdummyTurker
6680ß0.0ßTheir system tended to generate the same opinion as a response to different utterances with the same entity, resulting in their worse performance.ßdummyTurker
6681ß0.0ßTo compensate for not-for-opinion framework of seq2seq and no manually labeled training data for seq2seq, we trained a seq2seq model on 4 million fact-opinion pairs( four times the size of data used for OGM training), denoted as S2S_{4m,fo}, for performance comparison.ßdummyTurker
6682ß0.0ßOur results showed that OGM performs better than this seq2seq model.ßdummyTurker
6683ß0.0ßWe conducted manual evaluation of response self-consistency on 30 message pairs built on test set.ßdummyTurker
6684ß0.0ßFor each pair, the two messages were selected from the test set and they shared similar meaning, e.g.ßdummyTurker
6685ß0.0ß“I am waiting to get off work.”/“Please open live video.ßdummyTurker
6686ß0.0ßIt's really boring to work.”.ßdummyTurker
6687ß0.0ßThen we run one system on the two messages in each pair and then checked if the two responses express similar personal preference or attitude.ßdummyTurker
6688ß0.0ßOur results showed that OGM can show self-consistency on 50% message pairs, S2S_{1m, fo} with a ratio of 33%,  S2S_{4m, fo} with a ratio of 40%, and S2S_{4m, mr} with a ratio of 30%.ßdummyTurker
6689ß0.0ßWe will update the draft with these results.ßdummyTurker
6690ß0.0ßThe symbol “e” in section 3.3 refers to an opinion target.ßdummyTurker
6691ß0.0ßFinally, we summarize the contribution of this work as follows:ßdummyTurker
6692ß0.0ß1.We studied the problem of opinion generation for chit-chat oriented dialogue systems.ßdummyTurker
6693ß0.0ßTo our knowledge, this is the first attempt on opinion generation for chit-chat oriented dialogue systems.ßdummyTurker
6694ß0.0ßPrevious works on opinion generation (Matsuyama et al.ßdummyTurker
6695ß0.0ß(2015) , Stajner et al.ßdummyTurker
6696ß0.0ß(2013) , and Cagan et al.ßdummyTurker
6697ß0.0ß(2017) ) were conducted in the context of QA systems/news comment systems, but not chit-chat systems.ßdummyTurker
6698ß0.0ß2.Furthermore, we focused on personalization for opinion generation, which is important for conversational agents or chit-chat oriented dialogue systems to maintain personality.ßdummyTurker
6699ß0.0ßIt may be viewed as endowing the agent with personalized viewpoints, helping generate specific subjective responses.ßdummyTurker
6700ß0.0ßTo our knowledge, it is not addressed in previous works.ßdummyTurker
6701ß0.0ß3.ßdummyTurker
6702ß0.0ßThe manually labeled dataset used in this work will be made publicly accessible for reproduction of research results.ßdummyTurker
6703ß0.0ßPlease see supplementary files with this submission for the dataset.ßdummyTurker
6704ß0.0ßIt does not mean that all the short texts are difficult to categorize.ßdummyTurker
6705ß0.0ßWhat we emphasize here is a statistical phenomenon of samples.ßdummyTurker
6706ß0.0ßIn Figure 3, the x-axis represents the length of the texts while the y-axis means the selected weights of samples corresponding to one of the three layers.ßdummyTurker
6707ß0.0ßThe samples with the same length will have the same selected weights.ßdummyTurker
6708ß0.0ßWe can see that the selected weights of short texts for intensive layer tend to be larger.ßdummyTurker
6709ß0.0ßWith the decreasing of samples’ length on almost all datasets, the weights to select intensive reading layer are gradually increasing while the ones of selectiong the extensive reading layer are gradually decreasing.ßdummyTurker
6710ß0.0ßWe believe this is due to thatßdummyTurker
6711ß0.0ßSome examples are so long that models cannot feed to GPU.We use a truncating method to deal with this problem and 85% of the texts are below this threshold.ßdummyTurker
6712ß0.0ßThis threshold value has nothing to do with the difficulty of a task.ßdummyTurker
6713ß0.0ßWe will explain and analyze the results in more details.ßdummyTurker
6714ß0.0ßThank you for pointing out this paper – it is clearly related to our work, but the results are inferior to ours, and we will cite it.ßdummyTurker
6715ß0.0ßAs shown in Figure 2, we conducted the comparison among relative errors referring to the (Zhang et al., NIPS 2015) paper.ßdummyTurker
6716ß0.0ßThat paper does not have statistical studies.ßdummyTurker
6717ß0.0ßBut it is a good suggestion.ßdummyTurker
6718ß0.0ßWe will do that.ßdummyTurker
6719ß0.0ßOur model contains encoder and decoder.ßdummyTurker
6720ß0.0ßThe reason for our importing skip connections is to extract different contexts to build multi-context representation.ßdummyTurker
6721ß0.0ßAt the same time, we use adaptive decoder to understand (classify) texts from diverse domains.ßdummyTurker
6722ß0.0ßThose two approaches are our contributions in text classification community.ßdummyTurker
6723ß0.0ßAnd our AHR achieves state-of-the-art performance on six of eight challenging benchmark datasets.ßdummyTurker
6724ß0.0ßWe conducted a comparison among relative errors referring to the (Zhang et al., 2015) paper.ßdummyTurker
6725ß0.0ßIn the right subfigure of Figure 2, we compared our model with the neural network-based models and found out that our model achieved the best result on each dataset respectively.ßdummyTurker
6726ß0.0ßThanks for your comments and suggestions.ßdummyTurker
6727ß0.0ßThanks for your comments and suggestions.ßdummyTurker
6728ß0.0ßReply to Question 1：ßdummyTurker
6729ß0.0ßA window size of 3 means the WSD algorithm only makes use of the context words near the target word, while a window size of 50 means the WSD algorithm can also makes use of the context words far away from the target word, and 50 is the maximum sentence length in the corpus.ßdummyTurker
6730ß0.0ßReply to Question 2：ßdummyTurker
6731ß0.0ßWe have 1607 sentences containing pun words.ßdummyTurker
6732ß0.0ßEach sentence contains only one pun word.ßdummyTurker
6733ß0.0ßSo we have 1607 pun words and 11633 non-pun words.ßdummyTurker
6734ß0.0ßTherefore, each of 9 splits of data contains 160 pun words and the last split of data contains 167 pun words.ßdummyTurker
6735ß0.0ßSince the sentences lengths are different, it is hard for us to make the non-pun words totally the same in each split of data.ßdummyTurker
6736ß0.0ßWe just randomly split 1607 sentences and get 1149, 1149, 1205, 1166, 1101, 1199, 1183, 1185, 1103, 1193 non-pun words for 10 splits of data.ßdummyTurker
6737ß0.0ßActually we pre-split the dataset in order to compare the performance of each model fairly.ßdummyTurker
6738ß0.0ßReply to Question 3：ßdummyTurker
6739ß0.0ßWe will add the feature details in the next edition.ßdummyTurker
6740ß0.0ßThanks for your comments and suggestions.ßdummyTurker
6741ß0.0ßWe list the results of the previous best systems in the table just for reference.ßdummyTurker
6742ß0.0ßWe will add special marks to indicate this more clearly.ßdummyTurker
6743ß0.0ßIt is also noted that Idiom Savant uses an unsupervised approach and thus the results on the test set are comparable, as mentioned in lines 332-336.ßdummyTurker
6744ß0.0ßWe will list the features used in the CRF model if extra page is allowed.ßdummyTurker
6745ß0.0ßThe Duluth system (Pedersen 2017) uses a heuristic method which identifies the last word which changed senses between different runs of the WordNet::SenseRelate::AllWords disambiguation algorithm.ßdummyTurker
6746ß0.0ßBut we do not use the WordNet-based WSD results.ßdummyTurker
6747ß0.0ßFurthermore, we do not heuristically identify the pun word but propose a neural model to achieve this goal.ßdummyTurker
6748ß0.0ßWe will describe the relationships between our method and that in (Pedersen 2017) more clearly in the next edition.ßdummyTurker
6749ß0.0ßWe thank you for the thoughtful comments!ßdummyTurker
6750ß0.0ßFor both train and test sets, meaning representations are derived automatically using the PredPatt framework, which is based on Universal Dependencies and has been shown to be fast and accurate in shallow semantic parsing (White et al.ßdummyTurker
6751ß0.0ß[1], Zhang et al.ßdummyTurker
6752ß0.0ß[2]).ßdummyTurker
6753ß0.0ßThe difference between train and test sets is that meaning representations in the train set are created on automatic UD annotations, and in the test set are created on gold UD annotations.ßdummyTurker
6754ß0.0ßWe’ll include more analysis of data generation in the next revision.ßdummyTurker
6755ß0.0ß[1] Aaron Steven White, Drew Reisinger, Keisuke Sakaguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme.ßdummyTurker
6756ß0.0ß2016.ßdummyTurker
6757ß0.0ßUniversal Decompositional Semantics on Universal Dependencies.ßdummyTurker
6758ß0.0ßIn Proceedings of EMNLP.ßdummyTurker
6759ß0.0ß[2] Sheng Zhang, Rachel Rudinger, and Benjamin Van Durme.ßdummyTurker
6760ß0.0ß2017.ßdummyTurker
6761ß0.0ßAn Evaluation of PredPatt and Open IE via Stage 1 Semantic Role Labeling.ßdummyTurker
6762ß0.0ßIn Proceedings of IWCS.ßdummyTurker
6763ß0.0ßThe resource assumption in this paper is only bitext.ßdummyTurker
6764ß0.0ßAs explained in the introduction, bitext is easier to get: it occurs often without researcher involvement (e.g., owing to a government decree), and even when not available, it may be easier to find bilingual speakers that can translate a text, than it is to find experts that will create in-language semantic annotations.ßdummyTurker
6765ß0.0ßIn addition, we are simply further along in being able to automatically understand English (using the PredPatt framework) than we are other languages, resulting from the bias in investment in English-rooted resources.ßdummyTurker
6766ß0.0ßTherefore, English as the target language is a reasonable expectation, and the source language could be any language as long as bitext is available.ßdummyTurker
6767ß0.0ßYes, we'll release the implementations and the scoring script.ßdummyTurker
6768ß0.0ßWe thank you for the valuable feedback!ßdummyTurker
6769ß0.0ßYes.ßdummyTurker
6770ß0.0ßUDS is a project, and PredPatt is both a formalism and a software tool.ßdummyTurker
6771ß0.0ßThe “flat” representation of PredPatt is motivated by and generally applicable to RMRS.ßdummyTurker
6772ß0.0ßIt's underspecified with respect to the deep representation of RMRS in the sense of scope and lexical information.ßdummyTurker
6773ß0.0ßSuch information is fully recoverable in the deeper processing.ßdummyTurker
6774ß0.0ßE.g., in the predicate constant “<by a storm surge_h>” from Fig.ßdummyTurker
6775ß0.0ß"2(b), the unscoped quantifier indicated by “a” can be raised to take scope over the graph, and the analysis of UDS properties about ""the storm surge"" can be incrementally added to the predicate."ßdummyTurker
6776ß0.0ßAnother distinction between RMRS and PredPatt is that PredPatt is cleanly linked to UD while (R)MRS is based on HPSG.ßdummyTurker
6777ß0.0ßWe'll make these details more clear in the next revision.ßdummyTurker
6778ß0.0ßThe first hypothesis is more likely to be the reason: the BLEU score for the MT model in the pipeline approach is 25.43, which means the MT model does a good job in translation, but the translated text interacts badly with the UD parser / PredPatt.ßdummyTurker
6779ß0.0ßWe'll confirm this point by running the experiment as suggested by the reviewer, and hopefully add it in the camera ready.ßdummyTurker
6780ß0.0ßApologies for not making details of the metric $S_{(\phi, \psi)}$ clear enough: what $\phi$ computes is the similarity of dotted arrows (as shown in Fig.ßdummyTurker
6781ß0.0ß2(b)), which belong to a special type of edges indicating instance relation.ßdummyTurker
6782ß0.0ßTherefore, both $\phi$ and $\psi$ measure edge similarity: $\phi$ measures the similarity of instance relation (i.e., dotted arrows), and $\psi$ measures the similarity of argument relation (i.e., solid arrows).ßdummyTurker
6783ß0.0ßThe metric $S_{(\phi, \psi)}$ sums over these two types of similarity to measure the graph similarity.ßdummyTurker
6784ß0.0ßFor the particular case in this paper, we set $\phi=BLEU$ such that partial credits are given to the instance relation similarity.ßdummyTurker
6785ß0.0ßWe agree with the reviewer that in this case $\phi$ can be considered as computing node scores too, and we think rather separating $\phi$ from the metric, adding an extra column in the paper to report weight aggregated node scores (i.e., lexical similarity) would be a better solution.ßdummyTurker
6786ß0.0ßHowever, for cases like AMR parsing where $\phi$ is set to $\delta$ and no longer measures lexical similarity, such an extra column is not necessary.ßdummyTurker
6787ß0.0ßThanks for the insightful comments.ßdummyTurker
6788ß0.0ßWhy A-corpus is much larger than H-corpus?]ßdummyTurker
6789ß0.0ßH-corpus represents the available training data annotated by humans.ßdummyTurker
6790ß0.0ßA-corpus refers to the corpus automatically generated by our proposed methods.ßdummyTurker
6791ß0.0ßHuman annotation will involve a time-consuming process, which leads to the difficulty for scaling up the corpus.ßdummyTurker
6792ß0.0ßTherefore, H-corpus is much smaller than A-corpus.ßdummyTurker
6793ß0.0ßThe difficulty in human annotation serves as the motivation of proposing methods for automatically generating annotated CSC corpus.ßdummyTurker
6794ß0.0ßThanks a lot for your great efforts on reviewing our paper and providing the encouraging comments.ßdummyTurker
6795ß0.0ßWe will add more evaluation results in our final version, including the cross-validation results and in-domain performance (as pointed out by other reviewers).ßdummyTurker
6796ß0.0ßWe agree, and we plan in our future work to extend the evaluated target sets by creating new datasets or adapting from others and conduct a full evaluation on every induced target pair.ßdummyTurker
6797ß0.0ßThe current work, however, already show the promise of the approach.ßdummyTurker
6798ß0.0ßThanks for this suggestion.ßdummyTurker
6799ß0.0ßWe will add the cross-validation results on all experiments in the final version.ßdummyTurker
6800ß0.0ßMany thanks for reviewing our paper and providing insightful comments.ßdummyTurker
6801ß0.0ßWe have the results for the in-domain evaluation as below (averaging over 10 runs)ßdummyTurker
6802ß0.0ßThe max length of a sentence (tweet) is set as 20, as we followed the same setup used in the MITRE system, which performed the best on this SemEval-2016 dataset in the shared task.ßdummyTurker
6803ß0.0ßThe max length of a target is also set as 20, although the length of the longest one, i.e., “climate change is concern” is 4 (we used masking in our code to handle the impact of padding).ßdummyTurker
6804ß0.0ßWe will clarify this point in our final version to avoid confusion.ßdummyTurker
6805ß0.0ßYes, it was our hypothesis and got confirmed.ßdummyTurker
6806ß0.0ßAnd this also explains the differences in the results with different domains.ßdummyTurker
6807ß0.0ßMany thanks for your support and the insightful comments to further improve our paper.ßdummyTurker
6808ß0.0ßThanks for pointing out this issue.ßdummyTurker
6809ß0.0ßWe have not done the significance test yet, but in our final version, we will conduct cross-validation on all the experiments and perform significance test on the produced results.ßdummyTurker
6810ß0.0ßThanks for this suggestion and the reference of a recent stance detection task in other languages.ßdummyTurker
6811ß0.0ßWe also think that testing our model (or an improved version) in a multilingual scenario would be a good way to examine its generalization ability (whether it can attend to useful domain-specific information in a new language) and multilingual scope.ßdummyTurker
6812ß0.0ßWe will try to do this in our future work.ßdummyTurker
6813ß0.0ßWe thank all the reviewers for their constructive suggestions.ßdummyTurker
6814ß0.0ßWe have conducted suggested experiments and address all the concerns of the reviewers.ßdummyTurker
6815ß0.0ßWe will add more descriptions in our revised version to make our paper much clearer.ßdummyTurker
6816ß0.0ßGood suggestion.ßdummyTurker
6817ß0.0ßThe superiority of EKD method is two-fold.ßdummyTurker
6818ß0.0ß"First, it mitigates the theoretical defect of ""feature noise"" in fix method and strengthens the task-related feature."ßdummyTurker
6819ß0.0ß"Second, it avoids the ""feature damage"" problem by fixing pre-trained word embedding."ßdummyTurker
6820ß0.0ßWe will make it more clear in the revised paper.ßdummyTurker
6821ß0.0ßDue to their reasonable network architecture, two baselines achieve very high accuracy on five datasets compared with related methods.ßdummyTurker
6822ß0.0ßAnd on the top of high baselines, the proposed method further gains consistent and competitive performance on five datasets.ßdummyTurker
6823ß0.0ßBy the comparison of these three methods on five text classification dataset, we think EDK is a more reasonable method for the transfer learning of word embedding.ßdummyTurker
6824ß0.0ßIt should be noted that fine-tune method will introduce a huge amount of parameters while EKD method does not.ßdummyTurker
6825ß0.0ßWe will go through the paper against grammar mistakes.ßdummyTurker
6826ß0.0ßThe datasets are class-balanced, so we adopt accuracy as evaluation protocol following prior work (Le et al., 2017; Conneau et al., 2017; Zhang et al., 2015).ßdummyTurker
6827ß0.0ßWe conduct experiment following the public protocol.ßdummyTurker
6828ß0.0ßDataset is divided into training and testing sets by its author.ßdummyTurker
6829ß0.0ßWe also notice that some testing sets are small (compared with training data), but we think the number of testing data is big enough to evaluate model performance.ßdummyTurker
6830ß0.0ßWe thank all the reviewers for their constructive suggestions.ßdummyTurker
6831ß0.0ßWe have conducted suggested experiments and address all the concerns of the reviewers.ßdummyTurker
6832ß0.0ßWe will add more descriptions in our revised version to make our paper much clearer.ßdummyTurker
6833ß0.0ßGood suggestion.ßdummyTurker
6834ß0.0ßDifferent from the prior definition of knowledge in NLP, the knowledge in our paper is defined as the degree of relevance between word embedding features and task-at-hand.ßdummyTurker
6835ß0.0ß"Our work seeks to strengthen the task-related feature and weaken less related feature to migrate ""feature noise"", yielding so-called ""embedding knowledge distilling""."ßdummyTurker
6836ß0.0ßWe will make it more clear in the revised paper.ßdummyTurker
6837ß0.0ßOur method is motivated by follows.ßdummyTurker
6838ß0.0ßFirst, we find fine-tune method (a common way to transfer learning word embeddings) could degrade performance in some datasets (line 118-124).ßdummyTurker
6839ß0.0ß"Then we propose theoretical disadvantages of fine-tune method and fix method, which we called ""feature damage"" and ""feature noise"", respectively (line 125-164)."ßdummyTurker
6840ß0.0ßTo alleviate these two disadvantages, we propose EDK method (line 165-181).ßdummyTurker
6841ß0.0ßThe working mechanism of the model goes bellow.ßdummyTurker
6842ß0.0ß"We fix pre-trained word embeddings to avoid ""feature damage"" problem."ßdummyTurker
6843ß0.0ßThe EKD and text classification network (TinyCNN) are trained jointly.ßdummyTurker
6844ß0.0ßThe overall loss is text classification loss.ßdummyTurker
6845ß0.0ßDuring training, EKD distills knowledge for TinyCNN and the classification loss of TinyCNN updates EKD through BP algorithm.ßdummyTurker
6846ß0.0ß"By the coordination between EDK and TinyCNN, the knowledge (task-related feature) can be distilled and noise feature can be weakened, which mitigates ""feature noise"" problem."ßdummyTurker
6847ß0.0ßWe will make it more clear and update Figure 2 in the revised paper.ßdummyTurker
6848ß0.0ßGood suggestion.ßdummyTurker
6849ß0.0ßWe will clearly introduce our method including motivation, working mechanism etc.ßdummyTurker
6850ß0.0ßWe will also modify bold-font places in the revised version.ßdummyTurker
6851ß0.0ßDuring rebuttal, we conduct experiment on 20 newsgroup.ßdummyTurker
6852ß0.0ß1) train.ßdummyTurker
6853ß0.0ßw2v.ßdummyTurker
6854ß0.0ßTinyCNN achieves 94.99%; 2) fix.ßdummyTurker
6855ß0.0ßw2v.ßdummyTurker
6856ß0.0ßTinyCNN gets 94.78%; 3).ßdummyTurker
6857ß0.0ßOur method (fix.ßdummyTurker
6858ß0.0ßEKD) gains 95.08%; 4) ShallowCNN (Siwei Lai et al., 2015) gets 94.79%.ßdummyTurker
6859ß0.0ßWe show that the proposed method is still effective on 20newsgroup.ßdummyTurker
6860ß0.0ßWe will add these results to the paper.ßdummyTurker
6861ß0.0ßWe thank all the reviewers for their constructive suggestions.ßdummyTurker
6862ß0.0ßWe have conducted suggested experiments and address all the concerns of the reviewers.ßdummyTurker
6863ß0.0ßWe will add more descriptions in our revised version to make our paper much clearer.ßdummyTurker
6864ß0.0ßThough the proposed method is simple, the motivation behind it is inspiring.ßdummyTurker
6865ß0.0ßFirst, we find that fine-tuning word embeddings brings disadvantage.ßdummyTurker
6866ß0.0ß"Second, we illustrate that there exist ""feature damage"" and ""feature noise"" theoretical disadvantage in fine-tuning and fixing word embeddings, respectively."ßdummyTurker
6867ß0.0ßTherefore, we propose EDK to distill knowledge in the fixed word embeddings.ßdummyTurker
6868ß0.0ßEDK gains stable and competitive performance on five datasets, which shows its effectiveness for the transfer learning of word embedding.ßdummyTurker
6869ß0.0ßMoreover, the proposed method also provides a perspective to understand pre-trained word embeddings (Section 4.4).ßdummyTurker
6870ß0.0ßa) It is understandable that fine-tuning word embeddings can create some undesirable distortion to the embedding space.ßdummyTurker
6871ß0.0ßDue to the meaning of word embedding is not intuitive, it may not possible to directly give a real example to show the undesirable distortion.ßdummyTurker
6872ß0.0ß"Besides, the ""feature damage"" we claimed here is a theoretical disadvantage in fine-tune method and we use Figure 1 to illustrate ""feature damage"" and ""feature noise""."ßdummyTurker
6873ß0.0ßb) Actually, our work does not hold such assumption.ßdummyTurker
6874ß0.0ßWe would like to make it clear that EKD handles each dimension of word embedding separately.ßdummyTurker
6875ß0.0ß"But it does not mean different dimensions of the word embedding space must ""independently"" represent different information."ßdummyTurker
6876ß0.0ßMoreover, in our preliminary experiment, we handle all dimensions together but the result is not good.ßdummyTurker
6877ß0.0ßc) Compared with two high-performance baselines, the superiority of EKD method is more obvious on large datasets, e.g., Yelp5, Yahoo, and Amazon.ßdummyTurker
6878ß0.0ßOur method gains consistent and competitive performance on five datasets compared with two baseline methods.ßdummyTurker
6879ß0.0ßThus, we think EDK is a more reasonable and effective method on the transfer learning of word embedding.ßdummyTurker
6880ß0.0ßOur model aims at augmenting the seq2seq model by additionally maximizing the mutual information not achieving the state-of-the-art performance.ßdummyTurker
6881ß0.0ßThere is no sense to utilize extra information, which is a completely different point of research.ßdummyTurker
6882ß0.0ßDue to the number of metrics we adopted and the similar behaviours within these two datasets, it is difficult and unnecessary to squeeze all results in the main content.ßdummyTurker
6883ß0.0ßMany previous papers also leave additional measures in the supplementary material.ßdummyTurker
6884ß0.0ßThanks for pointing outßdummyTurker
6885ß0.0ßSorry for the confusion.ßdummyTurker
6886ß0.0ßP_gamma in the supplementary material is p_phi in the main content.ßdummyTurker
6887ß0.0ßIn the proof we want to highlight gamma can be arbitrary parameter so we use a new symbol.ßdummyTurker
6888ß0.0ßWe use Dailydialog corpus mainly for its high quality and more precise wording.ßdummyTurker
6889ß0.0ßIt comes from training materials for real English learners thus is more suitable for the task.ßdummyTurker
6890ß0.0ßTwitter and Reddit have many informal, casual phrases and symbolic expressions.ßdummyTurker
6891ß0.0ßOpensubtitle has multiple speakers and contains too much noise.ßdummyTurker
6892ß0.0ßAs we would like to compare our model with others, so we also run experiments on Ubuntu.ßdummyTurker
6893ß0.0ßTwitter is not public due to its private policy and many tweets have already been deleted thus are not tractable anymore.ßdummyTurker
6894ß0.0ßThere is still not a public standard Reddit corpus.ßdummyTurker
6895ß0.0ßPeople collected their own data, it is hard to fairly compare models on them.ßdummyTurker
6896ß0.0ßThough Ubuntu is itself goal-oriented, the backbone of the conversation is effective and has been widely in previous papers.ßdummyTurker
6897ß0.0ßNexus-A will definitely outperform VHRED with respect to coherence as it maximizes the mutual information with the dialogue context.ßdummyTurker
6898ß0.0ßWe actually run more experiments on VHRED and it turns out VHRED cannot fully exploit the context information.ßdummyTurker
6899ß0.0ßRemoving the context vector when computing the posterior even slightly improves VHRED’s performance.ßdummyTurker
6900ß0.0ßNexus-A pays more attention to the context as it explicitly reconstruct the history vector.ßdummyTurker
6901ß0.0ßIt's not a trade-off thing.ßdummyTurker
6902ß0.0ßBoth NEXUS-A and NEXUS-B outperforms the seq2seq baseline, just the improvement is less significant than the other.ßdummyTurker
6903ß0.0ßFor example, NEXUS-B significantly improves the interestingness by considering future information, but NEXUS-A can also achieve this by avoiding trivial responses.ßdummyTurker
6904ß0.0ßTherefore, NEXUS can be viewed as combining strengths from both NEXUS-A and NEXUS-B.ßdummyTurker
6905ß0.0ßDue to the number of metrics we adopted and the similar behaviours within these two datasets, it is difficult and unnecessary to squeeze all results in the main content.ßdummyTurker
6906ß0.0ßMany previous papers also leave additional measures in the supplementary material.ßdummyTurker
6907ß0.0ßBut we agree Section 2 should be simplified for more space, thanks for the advice.ßdummyTurker
6908ß0.0ßWe use Dailydialog corpus mainly for its high quality and more precise wording.ßdummyTurker
6909ß0.0ßIt comes from training materials for real English learners thus is more suitable for the task.ßdummyTurker
6910ß0.0ßTwitter and Reddit have many informal, casual phrases and symbolic expressions.ßdummyTurker
6911ß0.0ßOpensubtitle has multiple speakers and contains too much noise.ßdummyTurker
6912ß0.0ßAs we would like to compare our model with others, so we also run experiments on Ubuntu.ßdummyTurker
6913ß0.0ßTwitter is not public due to its private policy and many tweets have already been deleted thus are not tractable anymore.ßdummyTurker
6914ß0.0ßThere is still not a public standard Reddit corpus.ßdummyTurker
6915ß0.0ßPeople collected their own data, it is hard to fairly compare models on them.ßdummyTurker
6916ß0.0ßThough Ubuntu is itself goal-oriented, the backbone of the conversation is effective and has been widely in previous papers.ßdummyTurker
6917ß0.0ßOur model is fundamentally different from the one using deep reinforcement learning.ßdummyTurker
6918ß0.0ßFrom the technical perspective our model is highly related to MMI and VHRED, but we can add an additional comparison in the revision.ßdummyTurker
6919ß0.0ßSee aboveßdummyTurker
6920ß0.0ßOptimizing Eq.ßdummyTurker
6921ß0.0ß7 guarantees F is accurate under the Gaussian distribution family.ßdummyTurker
6922ß0.0ßThe highest coherence and fluency scores also testify the sampling is effective.ßdummyTurker
6923ß0.0ßWe also compare the samples from the prior and posterior distributions and they share high overlap.ßdummyTurker
6924ß0.0ßEach case is evaluated by 3 participants and the score agreed by most is used.ßdummyTurker
6925ß0.0ßThe agreement rate is 39,  48 and  62 for pri, post and fluency respectively.ßdummyTurker
6926ß0.0ßThe task is to identify indicative tweetsßdummyTurker
6927ß0.0ßThe F-scores are the tweet level F-scoresßdummyTurker
6928ß0.0ßIt is a highlight identification and identification of indicative tweets taskßdummyTurker
6929ß0.0ßThe quoted 0.5 BLEU score improvement is one of several contributions.ßdummyTurker
6930ß0.0ßFurther, as shown in Table 2, by using the smaller model, the improvement over the baseline is ~1 BLEU point on the same task and this performance matches the performance of a much larger Transformer network (previous state-of-the-art) resulting in significant training and inference cost savings.ßdummyTurker
6931ß0.0ßBoth models have a linear layer following the self-attention layer.ßdummyTurker
6932ß0.0ßThe proposed model learns the combination of the attention heads, allowing for co-adaptation prevention, while the baseline approach concatenates them and treats them uniformly.ßdummyTurker
6933ß0.0ßRecent work by Morcos et.ßdummyTurker
6934ß0.0ßal.ßdummyTurker
6935ß0.0ß(ICLR 2018) points towards generalization being impacted by a single node (or branch) sensitivity which we feel also partially explains our success.ßdummyTurker
6936ß0.0ßWe argue that our work is novel given that we isolate the core component of the Transformer architecture and not only propose a simple and successful improvement but also analyze it in detail.ßdummyTurker
6937ß0.0ßWe believe that such a contribution is within the purview of papers at ACL.ßdummyTurker
6938ß0.0ßThe savings in computational costs is primarily due to reduced number of iterations required for convergence (described in Table 2).ßdummyTurker
6939ß0.0ßThe per-iteration cost of either networks is approximately the same.ßdummyTurker
6940ß0.0ßThrough our work, we showed that by revisiting the multi-head attention, all else kept the same, the performance of the entire Transformer can be improved.ßdummyTurker
6941ß0.0ßIf the performance of the baseline Transformer can be improved through hyperparameter tuning, we expect the same improvements would be seen for our proposed network.ßdummyTurker
6942ß0.0ßIndeed, the finding was surprising.ßdummyTurker
6943ß0.0ßThe performance of our system is however, not sensitive to kappa/alpha.ßdummyTurker
6944ß0.0ßWe had found robust results even with about 5-10% misspecification.ßdummyTurker
6945ß0.0ßThe equations describe the entire attention layer for the Transformer (we describe this in more detail in lines 271-275).ßdummyTurker
6946ß0.0ßWhile we don’t explicitly concatenate the bar(heads), the FFN(sum(bar(head))) performs the matrix multiplication in an equivalent way to concatenating first, and then multiplying.ßdummyTurker
6947ß0.0ßIn other words, we use the fact that concatenating before a matrix multiplication is equivalent to summing up several block-matrix multiplication results.ßdummyTurker
6948ß0.0ßHowever, we understand that this abuses the FFN(x) notation which we will clean up in the camera-ready version.ßdummyTurker
6949ß0.0ßWe believe that the interpretation is similar to that of Shake-Shake regularization; by treating the heads as branches and learning their interactions, we allow each to learn unique features that discourage co-adaptation.ßdummyTurker
6950ß0.0ßRecent work by Morcos et.ßdummyTurker
6951ß0.0ßal.ßdummyTurker
6952ß0.0ß(ICLR 2018) points towards generalization being impacted by a single node (or branch) sensitivity which we feel also partially explains our success.ßdummyTurker
6953ß0.0ßAs we clarify below, the residual connections are retained around the entire layer block as in the baseline approach.ßdummyTurker
6954ß0.0ßFurther, the proposed mechanism is applied on all layers, not just the source layer.ßdummyTurker
6955ß0.0ßWe understand how the figure has been a cause for confusion and will make the changes in the camera-ready version.ßdummyTurker
6956ß0.0ßWe agree and have clarified it below in Q3.ßdummyTurker
6957ß0.0ßWe will also add the details to the camera-ready version.ßdummyTurker
6958ß0.0ßThe goal of that subsection was to contrast the approach with one analogous to Shake-Shake regularization.ßdummyTurker
6959ß0.0ßThrough the experiment, we showed that randomly weighing branches leads to significant performance deterioration thus warranting the need for the alpha/kappa variables.ßdummyTurker
6960ß0.0ßHowever, we can certainly shorten this section and add more requisite details in the model description in the camera-ready version.ßdummyTurker
6961ß0.0ßYes, as in the original Transformer network, there is still a residual connection around the sub-layers (multi-head attention and FFN) of each layer.ßdummyTurker
6962ß0.0ßThe proposed modification to the attention layer is for all the attention layers in the network, not just the source layer.ßdummyTurker
6963ß0.0ßWe understand that the “Inputs” in the Figure 1 is the cause for confusion and will rectify this in the camera-ready version.ßdummyTurker
6964ß0.0ßWe followed exactly the scoring pattern (and evaluation code) of the baseline Transformer network.ßdummyTurker
6965ß0.0ßAs such, we used the Moses tokenizer outputs without any additional post-processing.ßdummyTurker
6966ß0.0ßThe conventional approach that uses word-level annotations is clearly a valid proven solution for the labels that have a word or phrase which appears in the utterance exactly.ßdummyTurker
6967ß0.0ßWhat is missing in this approach is that it is unable to handle the sentence-level annotations which doesn’t appear in the sentence in exact words (e.g.funny movie—>comedy).ßdummyTurker
6968ß0.0ßThe use of seq2seq for word-level annotations helps the user to address word-level annotation need while maintaining a single joint model for three tasks (intend prediction, slot filling, multi-labeling) that were explained in the paper.ßdummyTurker
6969ß0.0ßOtherwise, if our model would have supported only intend prediction and multi-labeling tasks, the user would have to maintain a separate model for word-level annotations (slot filling task).ßdummyTurker
6970ß0.0ßWe believe that we propose a novel approach and the page number of our paper is within the limits of ACL long paper.ßdummyTurker
6971ß0.0ßYet, we may shorten our paper a bit if it is accepted as a short paper as well.ßdummyTurker
6972ß0.0ßThe use of the term multi-tasking for our model is compliant with the definition and details of multi-task learning in the book of (Goodfellow, 2016) under the section 7.7 of Chapter 7(page 241).ßdummyTurker
6973ß0.0ß"The reviewer states that ‘’conversational agents is not an ""emerging"" field, they have been around for a long time’’."ßdummyTurker
6974ß0.0ßYet, we may revise the sentence with an expression as ‘emerging field in the industry’ or ‘popular field in the industry’, e.g.ßdummyTurker
6975ß0.0ßGoogle Assistant was released in ‎May 18, 2016.ßdummyTurker
6976ß0.0ßThe experimental results show the effectiveness of my model.ßdummyTurker
6977ß0.0ßIt could demonstrate that our works is meaningful.ßdummyTurker
6978ß0.0ßFirstly, there is no high quality labelled corpus for ORE task.ßdummyTurker
6979ß0.0ßOur automatically constructed dataset is not perfect, but it solves the problem to a large extent.ßdummyTurker
6980ß0.0ßIn order to build a high-quality corpus for models training, we are committed to the high-accuracy of extractions at the expense of the recall.ßdummyTurker
6981ß0.0ßWe think it is worthy.ßdummyTurker
6982ß0.0ßNext, the experimental results on the three manual annotation test sets, the Wikipedia, NYT, and Reverb data, show that the Recall of our model is better than that of traditional extractors.ßdummyTurker
6983ß0.0ßThus, in our opinion, Although the corpus is incomplete, it is an obvious improvement.ßdummyTurker
6984ß0.0ß“Tags sequence” is independent of each other.ßdummyTurker
6985ß0.0ßWe number tags sequence in Figure1 for the convenience of description and explanation.ßdummyTurker
6986ß0.0ßBut there is no need in the model to set the numbers of tags sequence for each sentence.ßdummyTurker
6987ß0.0ßIt is our negligence that we do not quote the paper (MT/IE- Cross-lingual Open Information Extraction with Neural Sequence-to-Sequence Models.ßdummyTurker
6988ß0.0ßZhang, Du, & Van Durme, 2017).ßdummyTurker
6989ß0.0ßThis article and ours are similar, but also very different.ßdummyTurker
6990ß0.0ßWe think there is no conflict with the innovation of our article.ßdummyTurker
6991ß0.0ß(1) In our paper, the Open Relation Extraction is considered as a relation sequence tagging task.ßdummyTurker
6992ß0.0ßAnd we only pay attention to the tags of the relation words, and the other words are ignored and labelled as “Other”.ßdummyTurker
6993ß0.0ßHowever, Zhang et al.ßdummyTurker
6994ß0.0ßproposed a seq2seq model, more like the Machine Translation mechanism.ßdummyTurker
6995ß0.0ßThis is used to maps Chinese sentence to English PredPatt representations (syntactic-tree).ßdummyTurker
6996ß0.0ßIt will be more cost-consuming.ßdummyTurker
6997ß0.0ß(2) We are committed to open relation extraction (ORE), which is to extract the relation phrase corresponding to a given candidate entity pair in the sentence.ßdummyTurker
6998ß0.0ßThe ORE is an independent core subtask in open information extraction (OIE).ßdummyTurker
6999ß0.0ßDifferent from ours, Zhang et al.ßdummyTurker
7000ß0.0ßwere to extract predicate-argument structure phrases.ßdummyTurker
7001ß0.0ßThe results of the experiment are all true and effective, and the test set generation is random, and there is no overlap data with the training set.ßdummyTurker
7002ß0.0ßAs mentioned in the article, the NSL dataset is built in the same way as building a training set.ßdummyTurker
7003ß0.0ßIt is the same distribution of data with training set.ßdummyTurker
7004ß0.0ßMost of the sequence labelling models can well learn the characteristics of the training set to extract the open relations.ßdummyTurker
7005ß0.0ßIt proves that the methods based on the neural sequence learning are effective for the open relation extraction task.ßdummyTurker
7006ß0.0ßDifferent from the NSL dataset, the Reverb dataset, Wikipedia dataset, and NYT dataset are created by handcraft.ßdummyTurker
7007ß0.0ßThe test sets may contain some samples that are absent from the training set, because the training set is a set of relation tuples with high accuracy but low recall.ßdummyTurker
7008ß0.0ßTherefore, on the one hand, all models will achieve the low-level performances in the three test sets.ßdummyTurker
7009ß0.0ßon the other hand, our proposed model may have better generalizing and learning ability, and it can get much better results on the other three small datasets.ßdummyTurker
7010ß0.0ßThe three extractors are excellent and popular.ßdummyTurker
7011ß0.0ßThey are all based on dependency-tree, but they use various methods to extract relations.ßdummyTurker
7012ß0.0ßBecause most of the open relation extraction methods are based on this, different architecture methods are lacking or inefficient.ßdummyTurker
7013ß0.0ßWe are sorry for the inconvenience caused by the mistakes in the article, and we will correct them seriously later.ßdummyTurker
7014ß0.0ßSame as the reply to weakness argument 2.ßdummyTurker
7015ß0.0ßWe didn’t do further error analysis, which is where we need to improve, thank you.ßdummyTurker
7016ß0.0ßIf the state-of-the-art neural POS taggers are adopted, the model effect could be improved to a certain extent.ßdummyTurker
7017ß0.0ßBut it will undoubtedly increase the complexity and difficulty of the model and increase the cost of the experiment.ßdummyTurker
7018ß0.0ßThe TreeTagger used for POS tagging is widely adopted.ßdummyTurker
7019ß0.0ßOn balance, we used the TreeTagger.ßdummyTurker
7020ß0.0ßThank you for your review.ßdummyTurker
7021ß0.0ßIn order to compare our result with “Language as a Latent Variable: Discrete Generative Models for Sentence Compression” by Miao et.ßdummyTurker
7022ß0.0ßal who also conduct semi-supervised experiments of text summarization on English Gigaword corpus, we choose to conduct experiments on the same corpus.ßdummyTurker
7023ß0.0ßIn fact, our model is also able to generate summaries from long text.ßdummyTurker
7024ß0.0ßFor another set of experiments on Chinese Gigaword, the document consists of several sentences.ßdummyTurker
7025ß0.0ßPlease refer to Table 2.ßdummyTurker
7026ß0.0ßWe are conducting the experiments on CNN/Diary Mail dataset.ßdummyTurker
7027ß0.0ßIf the paper fortunately gets accepted, we will report the results in the revised version.ßdummyTurker
7028ß0.0ßThank you for your review.ßdummyTurker
7029ß0.0ßYou are right.ßdummyTurker
7030ß0.0ßIn general, it’s a little bit unfair to call our model entirely unsupervised because in some settings we use summaries from the same dataset.ßdummyTurker
7031ß0.0ßHowever, in transfer learning, we used the summaries from another dataset.ßdummyTurker
7032ß0.0ßWe think it is appropriate to call it unsupervised in this setting.ßdummyTurker
7033ß0.0ßThank you for your review.ßdummyTurker
7034ß0.0ßIn this work, we not only try to achieve unparalleled text summarization, but also try to explore some possible ways for text generation by GAN.ßdummyTurker
7035ß0.0ßFrom a summarization perspective, our work try to explore text summarization in unparalleled, semi-supervised and transfer learning setting, which is a less explored direction in previous works.ßdummyTurker
7036ß0.0ßBecause most documents are not paired with summaries, we believe it is useful to explore text summarization in this direction.ßdummyTurker
7037ß0.0ßThank you for your reminder.ßdummyTurker
7038ß0.0ßWe will cite the papers mentioned in your review including noisy-channel model, text simplification system and discrete auto-encoders.ßdummyTurker
7039ß0.0ßThank you for your additional comment.ßdummyTurker
7040ß0.0ßWe will proofread our paper carefully.ßdummyTurker
7041ß0.0ßThank you for your review.ßdummyTurker
7042ß0.0ßWe don’t think our work is just an application of GAN on summarization.ßdummyTurker
7043ß0.0ßIn this work, unlike previous text summarization works which explore the model architecture of summarization task in supervised setting, we explore summarization in unsupervised setting by GAN.ßdummyTurker
7044ß0.0ßWe aim to propose a new training method on unsupervised setting, which may be useful when human or machines need to understand the core idea of text without any paralleled data available.ßdummyTurker
7045ß0.0ßTo achieve this, we used our proposed GAN method on language generation to constrain the latent representation in auto-encoder to be human readable.ßdummyTurker
7046ß0.0ßBy intuition, this human readable latent representations can be regarded as text summaries.ßdummyTurker
7047ß0.0ßFirst, we note that what constitutes slang is controversial in the linguistic community.ßdummyTurker
7048ß0.0ßTherefore right at the outset, we note and discuss competing views (in Lines 161-168).ßdummyTurker
7049ß0.0ßWe adopt the broad definition of slang which emphasizes several aspects including social, and novelty in addition to non-standard expressions (which is also adopted by Mattiello et.al).ßdummyTurker
7050ß0.0ßIt is always a challenge to work on areas where competing viewpoints and definitions present themselves and we hope that any current work and research (including ours) will aid convergence on such long-standing questions.ßdummyTurker
7051ß0.0ßWhere possible, we quantify the accuracies of any tools we used on relevant data.ßdummyTurker
7052ß0.0ßPlease see Footnotes 4 and 6.ßdummyTurker
7053ß0.0ßAs mentioned in Lines 563-566 and Lines 574-577 we exactly follow the method outlined in Bolukbasi et.al which we could not describe in the paper due to space constraints.ßdummyTurker
7054ß0.0ßWe will describe the method if given the opportunity.ßdummyTurker
7055ß0.0ßPlease see response to Argument 1 for Reviewer 2 below.ßdummyTurker
7056ß0.0ßWe use the state of art tagger for this.ßdummyTurker
7057ß0.0ßWe also analyzed the results qualitatively and justify our observation in Lines 504-509.ßdummyTurker
7058ß0.0ßWe do give the proportion of major categories.ßdummyTurker
7059ß0.0ßSee lines 545-549ßdummyTurker
7060ß0.0ßThis work belongs to the first direction (line 158-161) that improves QA model with data augmentation.ßdummyTurker
7061ß0.0ßOur work differs from previous works in this direction in that our QA task is semantic parsing (i.e.ßdummyTurker
7062ß0.0ßsequence-to-SQL generation), while they focus on machine reading comprehension (i.e.ßdummyTurker
7063ß0.0ßpointing text span from a passage).ßdummyTurker
7064ß0.0ßIn terms of the methodology of the QG model, our method differs from previous ones in that our model considers diversity of generated questions via incorporating latent variable.ßdummyTurker
7065ß0.0ßFrom Table 5, we can see that our QG model (s2s+cp+lv) that considers the diversity of generated questions performs better than other QG methods (s2s, s2s+cp).ßdummyTurker
7066ß0.0ßIn Table 1, the second column “Training Data” represents the proportion of WikiSQL training data we use for training QG model and semantic parser.ßdummyTurker
7067ß0.0ßThat is to say, STAMP +QG with 30% means that we sample 30% WikiSQL training data to train QG model, and then combine QG generated data and exactly the same 30% WikiSQL training data sampled before to train semantic parser.ßdummyTurker
7068ß0.0ßWe do not simply adding QG training data to semantic parser training data because they are exactly the same.ßdummyTurker
7069ß0.0ßWe will give more clarification here.ßdummyTurker
7070ß0.0ßWe give an overview of our approach in Figure 1.ßdummyTurker
7071ß0.0ßThe pipeline includes a QG model (described in Section 4) that generates natural language questions, and a semantic parser (described in Section 5) that is learnt from a combination of QG generated outcomes and a small number of supervised training data.ßdummyTurker
7072ß0.0ßThere are two models (QG model and a semantic parser) involved in this work.ßdummyTurker
7073ß0.0ßLet me briefly describe the motivations of modeling decisions of them, respectively.ßdummyTurker
7074ß0.0ßWe use sequence-to-sequence approach as the backbone of our QG model because it achieves strong performances on text generation tasks.ßdummyTurker
7075ß0.0ßWe use copying mechanism to replicate important words of low frequency in the source sequence.ßdummyTurker
7076ß0.0ßWe use latent variable to improve the diversity of generated questions.ßdummyTurker
7077ß0.0ßIn our semantic parser, we use three channels because this is summarized from the syntax of SQL language.ßdummyTurker
7078ß0.0ßWe use column-cell relation because SQL queries with mismatched column and cell are meaningless and could not be executed to the correct result.ßdummyTurker
7079ß0.0ßIn Section 6.5, we do a further experiment on another semantic parsing benchmark dataset (Pasupat and Liang, 2015), which is not question->query translation.ßdummyTurker
7080ß0.0ßResults show that our approach could also bring improvements on this semantic parsing task.ßdummyTurker
7081ß0.0ßThis result might give inspirations to related research on semantic parsing.ßdummyTurker
7082ß0.0ßWe believe that our approach could be also adapted to other semantic parsing tasks (such as question-> lambda calculus), which could further verify the scalability of the approach.ßdummyTurker
7083ß0.0ßWe leave this as a future work.ßdummyTurker
7084ß0.0ßWe use the same SQL sampler as Zhong et al.ßdummyTurker
7085ß0.0ß2017, which could be found at 210-211 lines.ßdummyTurker
7086ß0.0ßIn addition, we describe the details of the SQL sampler in supplementary (section C, Line 61-92).ßdummyTurker
7087ß0.0ßWe believe that conducting experiment on a dataset of the largest number of schemata would make the experiments more convincing.ßdummyTurker
7088ß0.0ßPlease kindly refer to our response to your weakness argument 1.ßdummyTurker
7089ß0.0ßAs in Figure 1, the workflow is top-down that consists of three components, SQL sampler, question generation and semantic parser.ßdummyTurker
7090ß0.0ßWe describe these components following this pipeline, therefore the QG model is being invoked before training semantic parser.ßdummyTurker
7091ß0.0ßWe only mentioned that we use the same SQL sampler as Zhong in 210-211 lines, and leave more details of the SQL sampler in supplementary (section C, Line 61-92).ßdummyTurker
7092ß0.0ßTable information is very important for our semantic parser which aims to generate SQL queries from natural language questions.ßdummyTurker
7093ß0.0ßReasons include (1) table cells act as condition values in WHERE clause, (2) column-cell relation could help to shrink number of valid candidate cells, and (3) column representation could be enhance with the representations of its correlated cells.ßdummyTurker
7094ß0.0ßWe will give more clarifications here.ßdummyTurker
7095ß0.0ßThank you very much for the valuable suggestions.ßdummyTurker
7096ß0.0ßWe will improve the writing and introduce our method much clearer.ßdummyTurker
7097ß0.0ßThank you very much for the valuable suggestions.ßdummyTurker
7098ß0.0ßWe investigated several ways to better study the incorporation of world alignment information.ßdummyTurker
7099ß0.0ßAs you pointed out, the proposed method contains some “engineered” solutions.ßdummyTurker
7100ß0.0ßIn the future, we will follow your suggestion and design a more generic method to automatically detect the troublesome words and improve their translation results.ßdummyTurker
7101ß0.0ß(Gu et al.ßdummyTurker
7102ß0.0ß2018, AAAI-2018) is a newly published work just several weeks before our submission.ßdummyTurker
7103ß0.0ßWe will follow your suggestion and add (Gu et al.ßdummyTurker
7104ß0.0ß2018, AAAI-2018)’s method as an additional baseline.ßdummyTurker
7105ß0.0ßWe appreciate you a lot for the valuable suggestions.ßdummyTurker
7106ß0.0ßOur work is indeed an extension to previous methods, among them, the most related is the work of Arthur et al.ßdummyTurker
7107ß0.0ß(2016).ßdummyTurker
7108ß0.0ßThere are some important differences between our work and Arthur’s: (a) defining the “troublesome” words, (b) considering the context of troublesome words and (c) designing a dynamic combination method for target word prediction.ßdummyTurker
7109ß0.0ß1-2 and 4-6, We will follow your suggestions and revise these descriptions.ßdummyTurker
7110ß0.0ß3, Equations 3-6: We describe the model of (Luong et al., 2015, EMNLP 2015), which is a deep NMT model.ßdummyTurker
7111ß0.0ßWe will follow your suggestion and introduce the model much clearer.ßdummyTurker
7112ß0.0ß7, Equation 11: To measure the similarity of different contexts, we try different methods.ßdummyTurker
7113ß0.0ßAmong them, the current feed-forward network (Equation 11) achieves the best result.ßdummyTurker
7114ß0.0ßIt is difficult to explain how it functions.ßdummyTurker
7115ß0.0ßFollowing your suggestions, we will conduct more experiments to analyze or visualize how this feed-forward network works.ßdummyTurker
7116ß0.0ß8, Equation 12: As you pointed out, the final result of the memory is not really a probability distribution.ßdummyTurker
7117ß0.0ßYour suggestion inspired us to analyze the effects during training.ßdummyTurker
7118ß0.0ßFurthermore, we will test the effects by normalizing Equation 12.ßdummyTurker
7119ß0.0ß9, Figure 6: We will follow your suggestion and report the results when the memory size exceeds 1000M.ßdummyTurker
7120ß0.0ß10, Table 4: As you pointed out, there also exist the cases that the original NMTßdummyTurker
7121ß0.0ßproduces the correct translation, which was then wrongly translated by the proposed method.ßdummyTurker
7122ß0.0ßWe will follow your suggestion and show the corresponding statistics.ßdummyTurker
7123ß0.0ßThanks for your advice.ßdummyTurker
7124ß0.0ßWe will conduct comparison experiments with PBMT and report the result in the refined paper.ßdummyTurker
7125ß0.0ßWe agree with you that low-resource language pairs can benefit from multilingual machine translation.ßdummyTurker
7126ß0.0ßWe will add detailed discussion and try to compare our method with it in the experiment.ßdummyTurker
7127ß0.0ßThanks very much for your kind suggestion.ßdummyTurker
7128ß0.0ßYou are right that OOV is not specially treated in our paper.ßdummyTurker
7129ß0.0ßWe think that the leveraging of BPE can further boost the performance, and IBM scores can still be used by calculating the scores on the BPE level or after restoring the sentences from BPE sequences.ßdummyTurker
7130ß0.0ßThank you very much for your fruitful comments.ßdummyTurker
7131ß0.0ßWe agree that PBMT should be compared in the experiments, and the result will be added in the refined paper.ßdummyTurker
7132ß0.0ßWe are sorry about this confusion.ßdummyTurker
7133ß0.0ßThe reason why BLEU scores of TA-NMT are lower than BackTrans in X=>Z and Y=> Z is that, the resources used by these two methods are different, as shown in Table 2.ßdummyTurker
7134ß0.0ßTo do back translation in two directions (e.g., X=>Z and Z=>X), we need monolingual data from both sides (e.g., X and Z), however, in TA-NMT, the monolingual data of Z is not necessary.ßdummyTurker
7135ß0.0ßTherefore, in the translation of X=>Z, BackTrans uses additional monolingual data of Z while TA-NMT does not, that’s why BackTrans outperforms TA-NMT.ßdummyTurker
7136ß0.0ßOur method can leverage back translation as good initialization, aka TA-NMT(GI) , and outperforms BackTrans on all translation directions.ßdummyTurker
7137ß0.0ßWe will explain this in more detail in our paper.ßdummyTurker
7138ß0.0ßYes, this is an interesting phenomenon that the BLEU scores of two translation directions (e.g., X=>Z and Z=>X) are different.ßdummyTurker
7139ß0.0ßThe reason we conjecture might be the inherent characteristics of different target languages.ßdummyTurker
7140ß0.0ßFor example, with less grammar rules and variants, English is easier than many other languages, so the translation to English might be easier than that from English.ßdummyTurker
7141ß0.0ßSorry about this unclear point.ßdummyTurker
7142ß0.0ßThe generation of pseudo bilingual data is described in the 5th and 9th steps in Algorithm 1, where (x,z’) and (y,z’) are the pseudo data.ßdummyTurker
7143ß0.0ßThe generation is based on the derivation of our EM algorithm.ßdummyTurker
7144ß0.0ßTake the direction of X=>Y for example.ßdummyTurker
7145ß0.0ßWe use gradient descend training to optimize both the E-step (Equation 5) and the M-step (Equation 7).ßdummyTurker
7146ß0.0ßBy taking derivatives w.r.t Theta_z|x and Theta_y|z, and the data used to train both X=>Z and Z=>Y are generated by model X=>Z, i.e., Z is sampled from p(Z|X).ßdummyTurker
7147ß0.0ßWe will clarify this in more details in our paper.ßdummyTurker
7148ß0.0ßOur method is combined with back translation by taking it as initialized models.ßdummyTurker
7149ß0.0ßThat is, we first train RNN Search models with bilingual data, and then we leverage back translation with additional monolingual data Z to improve these models.ßdummyTurker
7150ß0.0ßThe improved models are then used as the pre-trained ones for our TA-NMT training.ßdummyTurker
7151ß0.0ßFor the typoes in Figure 2, we will fix it immediately.ßdummyTurker
7152ß0.0ßThanks a lot.ßdummyTurker
7153ß0.0ßThanks for the advice.ßdummyTurker
7154ß0.0ßWe will change “re-writing” to “editing”.ßdummyTurker
7155ß0.0ßWe will also add previous re-writing papers as recommended into the related work.ßdummyTurker
7156ß0.0ßWe will summarize the experiment results in various settings more accurately in the abstract.ßdummyTurker
7157ß0.0ßWe trained and evaluated our re-writing approach with 1-7 iterations and the experimental results showed that the second iteration always produced the best results.ßdummyTurker
7158ß0.0ßThe reason may be as follows.ßdummyTurker
7159ß0.0ßThe attentive revision gate incorporates the knowledge from the paper title and the previous generated abstract.ßdummyTurker
7160ß0.0ßAs the rewriting process iterates, the knowledge pool will diverge since in each iteration the generated abstract may introduce some irrelevant information.ßdummyTurker
7161ß0.0ßEmpirically the second iteration achieved a good trade-off between good quality of generated abstract and consistency with topics in the title.ßdummyTurker
7162ß0.0ßWe will add more discussions and analysis about the impact of the number of iterations.ßdummyTurker
7163ß0.0ßWe will also add more failure cases in both of the paper and supplemental materials/corpora to share with the community.ßdummyTurker
7164ß0.0ßThat’s very true.ßdummyTurker
7165ß0.0ßWe are not aiming to use our current system to replace human on writing veridical paper abstracts or producing new scientific facts and results.ßdummyTurker
7166ß0.0ßBut we hope this approach can serve as an assistive technology for human to write paper abstracts more efficiently and professionally, by generating an initial draft for human’s further editing, correction and enrichment.ßdummyTurker
7167ß0.0ßRe: Please discuss about the potential of the method to generate grammatically and semantically sound content but wrong content with respect to the information it contains.ßdummyTurker
7168ß0.0ßWe presented an example of this type in (6) in the last paragraph in Section 3.3: “OurßdummyTurker
7169ß0.0ßsystem ranked first out of the participating teams in the field of providing such a distribution.” We will add more discussion and formally assess whether this kind of sentences can still provide a human writer initial drafts and templates in writing correct content.ßdummyTurker
7170ß0.0ßRe: In Section 3.2, it would have been nice to compare the proposed method also with respect to traditional language models and analyse their differences.ßdummyTurker
7171ß0.0ßWe will add a baseline generation approach based on traditional language models and analyze results.ßdummyTurker
7172ß0.0ß"Re: In Table 2, it is not clear how should be interpreted the ""top rated by human"" column."ßdummyTurker
7173ß0.0ßThe explanation about the methodology used for conducting the manual evaluation should be included in the paper.ßdummyTurker
7174ß0.0ßIn our experiments, we gave the human judges 4 abstracts (one generated abstract per method) and asked them to pick the best one by the criteria which include topical relevance, logical coherence, and conciseness as described in Section 3.2.ßdummyTurker
7175ß0.0ß“Top rated by human” for a given method refers to the number of times that the human judges pick the abstract generated by the method as the best.ßdummyTurker
7176ß0.0ßWe will make it clearer.ßdummyTurker
7177ß0.0ßRe: Why in the human evaluation, only 50 titles are analysed?ßdummyTurker
7178ß0.0ßthere is no justification for selecting such number of titles.ßdummyTurker
7179ß0.0ßFor each of the 50 title sets, a human needs to read N abstracts, compare them, and pick the one that is most likely to be generated by our system.ßdummyTurker
7180ß0.0ßThe larger N is, the more time it takes for a human judge to answer the question.ßdummyTurker
7181ß0.0ßWith a limited amount of time from human judges, we randomly selected 50 title sets for human evaluation, which should be sufficient to evaluate the performance of the system.ßdummyTurker
7182ß0.0ßRe: Table 4 is difficult to understand.ßdummyTurker
7183ß0.0ßIt should be better explained.ßdummyTurker
7184ß0.0ßThe Turing test is to ask human judges to pick the abstract generated by our system instead of a human generated abstract.ßdummyTurker
7185ß0.0ßTable 4 shows two types of Turing tests: 1) abstracts for different titles: each of these sets include 1 system generated abstract and N (N=1,4,9) human abstracts while these abstracts are for different titles.ßdummyTurker
7186ß0.0ß2) abstracts for the same title: for a given title, each set consists of 1 human abstract and N system generated abstracts (N=1 or 4).ßdummyTurker
7187ß0.0ßThe “passing rate” is the percentage of the sets where the system generated abstract deceived human.ßdummyTurker
7188ß0.0ßFrom Table 4, we can see that people with less domain knowledge are more easily deceived.ßdummyTurker
7189ß0.0ßAdditionally, the more abstracts that we provided to human judges, the easier it is to conceal the system generated abstract amongst human generated ones.ßdummyTurker
7190ß0.0ßWe will add more explanation into Section 3.3 for Table 4 and make it clearer.ßdummyTurker
7191ß0.0ßThanks for your suggestions!ßdummyTurker
7192ß0.0ßTopic information has been adopted into many NLP tasks; however, it is not well-studied in NMT.ßdummyTurker
7193ß0.0ßOur contributions consist of:ßdummyTurker
7194ß0.0ß1.In addition to traditional inter-dependent source word vector sequence, this work focuses on learning a novel non-dependent sentence-level topic vector sequence to capture sentence-level context for predicting translation.ßdummyTurker
7195ß0.0ß2.The proposed topic-attention can be estimated dynamically and trained jointly with the existing NMT architecture instead of being pre-computed.ßdummyTurker
7196ß0.0ß3.Extensive experiments on large-scale Chinese-to-English and English-to-German translation tasks show that the proposed method gains improvements over several exiting works.ßdummyTurker
7197ß0.0ßCompared with the traditional topic model learning symbolic topic distribution for each word, the proposed CNN (regarded as “Topic Model”) focuses on sentence-level context to learn topics included by the whole source sentence.ßdummyTurker
7198ß0.0ßTherefore, there is not a topic distribution for each source word in our CNN.ßdummyTurker
7199ß0.0ßIn comparison, the proposed topic-attention may be regarded as a distribution between these learned topic vectors and next predicted target word.ßdummyTurker
7200ß0.0ßInitially, the “latent topic representations” mainly considers that although a topic vector can represent a symbolic topic from source sentence, it is difficult to visually observe what the topic vector represents.ßdummyTurker
7201ß0.0ßWe are sorry that the abbreviation brings some unnecessary misunderstandings.ßdummyTurker
7202ß0.0ßWe think that “Sentence Topic Model” may be more appropriate for the proposed CNN.ßdummyTurker
7203ß0.0ßIn the experiment, we additionally design a comparison named DWCont model in which the proposed CNN is replaced by a standard LSTM, which is similar to the “ME-MD” method.ßdummyTurker
7204ß0.0ßThe results in Table 1 and 2 of Section 4.2 show that our LTANMT outperforms both AttNMT and DWCont.ßdummyTurker
7205ß0.0ßMeanwhile, the results also mean that the improvement comes from the proposed sentence-level source topic information, other than multiple encoders.ßdummyTurker
7206ß0.0ßWe have conducted significance test in Table 1 and 2 of Section 4.2.ßdummyTurker
7207ß0.0ßEmpirical results show that our method significantly outperformed the best-performed baseline.ßdummyTurker
7208ß0.0ßThanks for your comments!ßdummyTurker
7209ß0.0ßDue to four-pages short paper submission, translation examples are given in supplementary material.ßdummyTurker
7210ß0.0ßIn addition, we will transfer translation examples provided in the supplementary material into the main content and add other example analysis, such as virtualization word alignments, and virtualization topic alignments.ßdummyTurker
7211ß0.0ßIn Table 2, to verify that the improvement comes from the source topics rather than more neural network parameters, we designed a comparison named DWCont model in which the proposed CNN is replaced by a standard LSTM, which is similar to “ME-MD: An Effective Framework for NMT with Multiple Encoders and Decoders, IJCAI2017”.ßdummyTurker
7212ß0.0ßCompared the proposed LATNMT with DWCont, the results show that the LATNMT with sentence-level topic context gains an improvement over DWCont with the similar number of neural network parameters.ßdummyTurker
7213ß0.0ßThis indicates that the improvement is not from more neural network parameters.ßdummyTurker
7214ß0.0ßIn our existing experiments (ZH-EN), when the topic context itself is used to predict target word without the word context, there is a very severe performance degradation.ßdummyTurker
7215ß0.0ßThis indicates that source topic information should be an auxiliary role rather than a dominant role in translation.ßdummyTurker
7216ß0.0ßWhen topic context without gating scale weighting was directly used to predict target word together with word context, our LATNMT gained an improvement of 0.8 BLEU points on average over AttNMT (34.8).ßdummyTurker
7217ß0.0ßIn Table 1, LATNMT with gating scale outperforms the AttNMT on average by 1.2 BLEU points.ßdummyTurker
7218ß0.0ßIt indicates that the gating scale mechanism can effectively control the expected importance of topic context in translation.ßdummyTurker
7219ß0.0ßIn other words, the attention itself maybe not be adequately aware of the expected importance of topic context.ßdummyTurker
7220ß0.0ßFigure.2 shows the translation performance of different topic number on the dev set NIST02.ßdummyTurker
7221ß0.0ßThe number of source topics is set to 40 through comprehensively considering translation performance and training efficiency.ßdummyTurker
7222ß0.0ßThanks for your comments!ßdummyTurker
7223ß0.0ßDue to four pages limitation on short paper submission, we did our best to show our motivation, methods, and main experiments.ßdummyTurker
7224ß0.0ßWe will transfer translation examples in the supplementary material into the content in the revised version.ßdummyTurker
7225ß0.0ßWe think that topic-attention focuses on non-dependent topic vectors to capture sentence-level context while word-attention focuses on inter-dependent word vectors to capture word-level context.ßdummyTurker
7226ß0.0ßNaturally, both of word-based virtualization alignments and topic-based virtualization alignments will be added in the revised version.ßdummyTurker
7227ß0.0ßIn our existing experiments (ZH-EN), when the topic context itself is used to predict target word without the word context, there is a very severe performance degradation.ßdummyTurker
7228ß0.0ßThis indicates that source topic information should be an auxiliary role rather than a dominant role in translation.ßdummyTurker
7229ß0.0ßIn addition, we will have a preliminary attempt by vector concatenation (Sennrich et al., WMT2016) and context-aware smoothing (Chen et al., IJCNLP2017), and report the results in the revised version.ßdummyTurker
7230ß0.0ßIn short, the suggestions greatly help us to further improve our work!ßdummyTurker
7231ß0.0ßFollowing the method of (Koehn, 2004), significance test is an estimate of the degree, to which the true translation quality lies within a confidence interval around the measurement on the test sets.ßdummyTurker
7232ß0.0ßA commonly used level of reliability of the result is 95%, called p-value.ßdummyTurker
7233ß0.0ßWe thank the reviewer for their comments.ßdummyTurker
7234ß0.0ßWe thank the reviewer for their comments.ßdummyTurker
7235ß0.0ßWe were motivated to use MSE (Eq.ßdummyTurker
7236ß0.0ß2) instead of another similarity function (cosine, DKL) through preliminary experiments where we compared different similarity functions on our model.ßdummyTurker
7237ß0.0ßWe also experimented with a large number of architecture choices and found this to be the most stable.ßdummyTurker
7238ß0.0ßWhile we agree that MSE would not be the most straight forward choice, it performs well on our task.ßdummyTurker
7239ß0.0ßRegarding Eq.ßdummyTurker
7240ß0.0ß3, cross entropy loss is a standard loss function for sentiment classifiers.ßdummyTurker
7241ß0.0ßWe will report the results of these experiments in the final paper to justify our choice.ßdummyTurker
7242ß0.0ßWe thank the reviewer for their comments.ßdummyTurker
7243ß0.0ßFor the purposes of this paper, we do not consider the two upper bounds (MT & MONO) as baselines to beat, but rather as strong ceilings that we would be happy to reach.ßdummyTurker
7244ß0.0ßMONO is completely out of the realm of cross-lingual sentiment analysis and MT requires an order of magnitude more parallel training data.ßdummyTurker
7245ß0.0ßWhile we were able to use GoogleTranslate to get the machine translated data, this is a proprietary system and it is not clear what data they use to train it.ßdummyTurker
7246ß0.0ßWe are not aware of any English-to-Catalan or English-to-Basque data of sufficient size to construct a useful MT system and this is likely the case for most language pairs.ßdummyTurker
7247ß0.0ßFor this reason, we don't consider it.ßdummyTurker
7248ß0.0ßRegarding the ensemble method, we performed experiments with all other cross-lingual methods together with MT, respectively.ßdummyTurker
7249ß0.0ßThe ensemble with our method performs best and we will report this in the final paper.ßdummyTurker
7250ß0.0ßAn ensemble which consists of all methods together performs only non-significantly better than the ensemble with our method.ßdummyTurker
7251ß0.0ßTherefore, we conclude that our method contributes more over MT than the other methods.ßdummyTurker
7252ß0.0ßWe will make this more clear in the final version.ßdummyTurker
7253ß0.0ßFor the Mono upper bound baseline, we train the sentiment classifier on the same train, dev, test split that we use for all setups.ßdummyTurker
7254ß0.0ßThe difference lies in that the train and dev examples are from the same language and dataset in the Mono setting, e.g., En_train, En_dev, En_test.ßdummyTurker
7255ß0.0ßWith the other setups, the test is different, e.g., En_train, En_dev, Ca_test, but otherwise comparable.ßdummyTurker
7256ß0.0ßWe tune the C parameter on the development data.ßdummyTurker
7257ß0.0ß[This is the same answer as for Reviewer 2 Weakness 1]ßdummyTurker
7258ß0.0ßWe were motivated to use MSE (Eq.ßdummyTurker
7259ß0.0ß2) instead of another similarity function (cosine, DKL) through preliminary experiments where we compared different similarity functions on our model.ßdummyTurker
7260ß0.0ßWe also experimented with a large number of architecture choices and found this to be the most stable.ßdummyTurker
7261ß0.0ßWhile we agree that MSE would not be the most straight forward choice, it performs well on our task.ßdummyTurker
7262ß0.0ßWe will report the results of these experiments in the final paper to justify our choice.ßdummyTurker
7263ß0.0ßThe error analysis was conducted manually by one native speaker.ßdummyTurker
7264ß0.0ßThis person reported to us that the errors were clearly identifiable.ßdummyTurker
7265ß0.0ß"However, we did not check for ""interannotator error analysis agreement""."ßdummyTurker
7266ß0.0ßWe omitted these results in Table 4 because we found F-scores confusing when mixed with error numbers.ßdummyTurker
7267ß0.0ßHowever, given more space for the final version, we will try to include it.ßdummyTurker
7268ß0.0ßWe reference the Bing Liu dictionary on line 415.ßdummyTurker
7269ß0.0ß1) The information in Figure 2 is actually found in Table 3, but we thought it was useful to include a visual comparison as complimentary information to the rather dense table.ßdummyTurker
7270ß0.0ß2) The binary and 4-class distinction doesn't fall into the other error classes.ßdummyTurker
7271ß0.0ßThis section only describes how all of the models suffer when moving to a 4-class prediction problem.ßdummyTurker
7272ß0.0ßIt is true that its position in the paper doesn't make this obvious.ßdummyTurker
7273ß0.0ßWe will try to make the distinction clearer.ßdummyTurker
7274ß0.0ß3) Thank you for pointing out the two bold-faced values.ßdummyTurker
7275ß0.0ßThank you for your comments.ßdummyTurker
7276ß0.0ßUnfortunately, the practical application was not mentioned in the text due to the limited length of the text.ßdummyTurker
7277ß0.0ßHowever, we would like to supplement it.ßdummyTurker
7278ß0.0ßThank you for your comments.ßdummyTurker
7279ß0.0ßWe used a formal, i.e.ßdummyTurker
7280ß0.0ßnon-semantic, closed text analysis of linguistic data which depends on a correct definition of morphological categories of lexical units (e.g.ßdummyTurker
7281ß0.0ßword, punctuation or emoticon).ßdummyTurker
7282ß0.0ßFor this purpose, two distinctive process were used, namely lemmatization (i.e.ßdummyTurker
7283ß0.0ßthe identification of lexical units within the meaning of a dictionary entry) and disambiguation (i.e.ßdummyTurker
7284ß0.0ßthe removal of ambiguity in the interpretation of a specific word, i.e.ßdummyTurker
7285ß0.0ßhomonymy; e.g.ßdummyTurker
7286ß0.0ß"distinguishing the verb in ""A cold wind began to blow."""ßdummyTurker
7287ß0.0ßfrom the noun in “That was a big blow to me.”).ßdummyTurker
7288ß0.0ßDuring this process, morphological tags were assigned to each lexical unit of the textßdummyTurker
7289ß0.0ßThis is described in Current State of Knowledge section.ßdummyTurker
7290ß0.0ßWe presented a written text analysis only.ßdummyTurker
7291ß0.0ßThank you for your very careful review of our paper!ßdummyTurker
7292ß0.0ßWe fully agree with this view.ßdummyTurker
7293ß0.0ßWe just found some suitable alignment examples and would definitely include one or two of them in the final submission.ßdummyTurker
7294ß0.0ßWe would include some alignment examples and also meaningful analyses.ßdummyTurker
7295ß0.0ßThank you for your very careful review of our paper!ßdummyTurker
7296ß0.0ßYou are right, we have to compute softmax over the full vocabulary for each input position, which results in a slower decoding speed compared to attention-based models.ßdummyTurker
7297ß0.0ßBut in HMM marginalization of alignments is a required step.ßdummyTurker
7298ß0.0ßWe could try some heuristics such as using a threshold to the probabilities and only consider a set of most likely source positions instead of all positions.ßdummyTurker
7299ß0.0ßMaybe we could find a trade-off between speed and performance.ßdummyTurker
7300ß0.0ßI think the decoding complexity would be O(J*J*I) (J = source sentence length, I = target sentence length) compared to O(J*I) for attention-based models.ßdummyTurker
7301ß0.0ßThese are theoretical complexities of decoding on a CPU only considering source and target sentence lengths.ßdummyTurker
7302ß0.0ßIn practice, the size of the neural network must also be taken into account, and there are some optimized matrix multiplications for decoding on a GPU.ßdummyTurker
7303ß0.0ßIn general, the decoding speed of our model is about 3 times slower than that of the Sockeye attention model (1.07 sentences per second vs. 3.00 sentences per second) on a single GPU.ßdummyTurker
7304ß0.0ßWe agree that this is a major drawback of our approach.ßdummyTurker
7305ß0.0ßThis is still an initial decoder and we did not spend much time on accelerating its decoding yet.ßdummyTurker
7306ß0.0ßThe optimization of our decoder would be a promising future work, e.g.ßdummyTurker
7307ß0.0ßwe could use batch decoding.ßdummyTurker
7308ß0.0ß"We wrote ""a more modern and efficient way to use HMMs in machine translation"" just to express that here we use the HMM as an end-to-end translation system rather than just a word alignment model as before, and by means of neural networks the novel HMM could achieve a comparable performance with the state-of-the-art method."ßdummyTurker
7309ß0.0ßIt seems that this expression is a bit misleading and we will try to revise it.ßdummyTurker
7310ß0.0ßWe just found some suitable alignment examples and would definitely include one or two of them in the final submission.ßdummyTurker
7311ß0.0ßThank you for your constructive and useful feedback.ßdummyTurker
7312ß0.0ßHere is our response to your comments.ßdummyTurker
7313ß0.0ßWe will strive to make terminology clearer and more consistent.ßdummyTurker
7314ß0.0ßWe are aware that this aspect could be somewhat confusing.ßdummyTurker
7315ß0.0ßWe indeed assume that language regularities repeat across domains.ßdummyTurker
7316ß0.0ßAlthough this is not guaranteed, it was validated in multiple semantic parsing settings and multiple datasets where natural language in different domains was provided during training, e.g.ßdummyTurker
7317ß0.0ß: Su and Yan (2017), Herzig and Berant (2017), Fan et al.ßdummyTurker
7318ß0.0ß(2017) and most recently by Richardson et al.ßdummyTurker
7319ß0.0ß(2018).ßdummyTurker
7320ß0.0ßThank you for your constructive and useful feedback.ßdummyTurker
7321ß0.0ßHere is our response to your comments.ßdummyTurker
7322ß0.0ßResults for all domains in the Overnight dataset will be added to the camera ready version of the paper.ßdummyTurker
7323ß0.0ßWe are aware that this is a delicate point, and will revise the description of the KB lexicon such that its use is clear early in the paper's outline.ßdummyTurker
7324ß0.0ß* The similarity in CrossLexRep is defined as the cosine similarity (lines 467-468) between the embeddings of the generated KB constant c_1, and the candidate KB constant from theßdummyTurker
7325ß0.0ßtarget domain, c_2.ßdummyTurker
7326ß0.0ßWe will make this point clearer.ßdummyTurker
7327ß0.0ß*You are correct that the denotation accuracy (54.1%) is substantially higher than the exact match of the abstract structure of the top candidate in the beam (42.9%).ßdummyTurker
7328ß0.0ßHowever, we note that 42.9% is a lower bound for accuracy, as there are multiple reasons for having a correct denotation while the abstract structure does not exactly match the gold one, namely:ßdummyTurker
7329ß0.0ß-The candidate that is being lexicalized and returned by the parser could appear lower in the beam (i.e., not the top candidate).ßdummyTurker
7330ß0.0ß"-The exact match metric is much stricter than the correct denotation accuracy as the same logical form could be written in multiple ways (e.g., ""A and B"" is equal to ""B and A"" in its meaning)."ßdummyTurker
7331ß0.0ß-As you mention, there could be spurious logical forms, however we think this is less common in the setting we experimented with (full supervision) as opposed to training from denotations only.ßdummyTurker
7332ß0.0ß*Using the lexicalized natural language as another input to the structure mapper is an interesting experiment we can additionally perform.ßdummyTurker
7333ß0.0ßThe approach we currently took with the models we implemented was to make them domain independent as much as possible, where out-of-vacabulary tokens would not be encountered much.ßdummyTurker
7334ß0.0ßThank you for your helpful comments and suggestions.ßdummyTurker
7335ß0.0ßIn general, there are two main advantages of the proposed approach: (1) it is more robust to missing data and (2) it achieves top performance.ßdummyTurker
7336ß0.0ß(a) Matrix factorization (SVD++ etc.ßdummyTurker
7337ß0.0ß): we take into account user and item reviews as well as item images and metadata; this allows us to make rating predictions even for products without any reviews and ratings.ßdummyTurker
7338ß0.0ßFactorization based methods fail in this scenario.ßdummyTurker
7339ß0.0ß(b) Topic models (HFT etc.ßdummyTurker
7340ß0.0ß): we consider also the item metadata and images.ßdummyTurker
7341ß0.0ßDue to the robustness to missing data, our method also makes accurate predictions for items without any review text.ßdummyTurker
7342ß0.0ßTopic models do not work in this scenario.ßdummyTurker
7343ß0.0ßEmpirically, our method achieves also better results on the soft cold-start problem, that is, when only small numbers of reviews exist.ßdummyTurker
7344ß0.0ß(c) Deep learning models (DeepCoNN etc.ßdummyTurker
7345ß0.0ß): our approach works on several item modalities (reviews, metadata, images) simultaneously and is trained to be robust to missing data.ßdummyTurker
7346ß0.0ßExisting DL recommender systems are not trained to be robust to missing data and are not applicable to new products without any reviews/ratings.ßdummyTurker
7347ß0.0ßThank you for the helpful suggestion of including more qualitative results to more thoroughly understand the proposed approach.ßdummyTurker
7348ß0.0ßWe will provide more qualitative results in addition to Figures 6 and 7.ßdummyTurker
7349ß0.0ßSpecifically, we will include examples of items where data is missing and how the different existing methods compare to our approach.ßdummyTurker
7350ß0.0ßThanks again and we will re-clarify these advantages in a final version.ßdummyTurker
7351ß0.0ß1.ßdummyTurker
7352ß0.0ßWe appreciate your criticism and will attempt to explain better why we believe the proposed method is not ad-hoc:ßdummyTurker
7353ß0.0ß(a) Our work builds on recent developments (e.g.ßdummyTurker
7354ß0.0ßDeepCoNN (WSDM 2017), NARRE (WWW 2018)) to incorporate multimodal data to alleviate the cold-start problem.ßdummyTurker
7355ß0.0ßIt is therefore in line and grounded in recent progress in the recommender system community.ßdummyTurker
7356ß0.0ß(b) Contrary to existing methods, we view the cold-start problem as a missing data issue and adapt dropout to the multi-modal setting, making the model more robust to missing data.ßdummyTurker
7357ß0.0ßWe train our model on several modalities simultaneously, allowing it to learn dependencies between the modalities.ßdummyTurker
7358ß0.0ß2.ßdummyTurker
7359ß0.0ßThe proposed solution is novel as it is the first one that is robust to data that is completely missing.ßdummyTurker
7360ß0.0ß(a) While data-sparsity (user/item with only a few ratings/reviews) has been previously explored, missing data (user/item without ratings/reviews) in recommender systems is not well addressed in the literature.ßdummyTurker
7361ß0.0ß(b) The proposed modality dropout and multimodal seq-to-seq autoencoder are novel and increase robustness to missing data.ßdummyTurker
7362ß0.0ßAlthough our work is motivated by existing concepts of dropout and autoencoder, our multimodal learning method is not a straightforward combination.ßdummyTurker
7363ß0.0ß(a) Especially, the way we combine modalities is well motivated and it also significantly extends previous work.ßdummyTurker
7364ß0.0ßFor instance, to combine data modalities and learn shared representations (multimodal deep learning, ICML 2011), shared layers are used in recent state-of-the-art methods, e.g., NRT (SIGIR 2017) and DeepCoNN (WSDM 2017).ßdummyTurker
7365ß0.0ßOur method also uses a shared layer to learn the interaction across modalities.ßdummyTurker
7366ß0.0ßIn addition, it extends these approaches so as to make it more robust to missing data.ßdummyTurker
7367ß0.0ß(b) We conducted extensive experiments to evaluate the performance under different missing data regimes; these experiments show that our method achieves the state-of-the-art performance.ßdummyTurker
7368ß0.0ßA straightforward combination couldn't perform those tasks.ßdummyTurker
7369ß0.0ß"The ""correlations across modalities' means that the representation shared across different modalities (the output of the shared layer)."ßdummyTurker
7370ß0.0ßIt captures dependencies between different modality embeddings (e.g.ßdummyTurker
7371ß0.0ßuser embedding and item image embedding).ßdummyTurker
7372ß0.0ßIn our domain adaptation setting (e.g.ßdummyTurker
7373ß0.0ß"Sport->Movie ), we remove all ""Movie"" training examples and extract the modality-specific image feature, text feature and shared feature on ""Sport"" and generate recommendations on the test set of ""Movie""."ßdummyTurker
7374ß0.0ßTables 5 and 7 show the performance and examples.ßdummyTurker
7375ß0.0ßAgain, thank you for your feedback.ßdummyTurker
7376ß0.0ßWe will add these clarifications to the final version of the paper.ßdummyTurker
7377ß0.0ßThanks a lot for your comments.ßdummyTurker
7378ß0.0ßQ1.ßdummyTurker
7379ß0.0ßThere is a limited amount of discussion about the results.ßdummyTurker
7380ß0.0ßI would like to see more discussion about why certain features improve on certain tests.ßdummyTurker
7381ß0.0ßA1.ßdummyTurker
7382ß0.0ßDue to the limited size, we did sacrifice some space of discussion about the results.ßdummyTurker
7383ß0.0ßIn fact we actually prepared some quantitative analysis via two kinds of visualization (one for HWE-POS amd one for HWE-sense) but can not fill them in on the 4-page submission.ßdummyTurker
7384ß0.0ßWhen camera version/next version allows one more page, we will definitely add these parts.ßdummyTurker
7385ß0.0ßThanks a lot for your comments.ßdummyTurker
7386ß0.0ßQ1.ßdummyTurker
7387ß0.0ßFor instance, in the HWE-Sense setting target words predict their possible senses, but the corpus is not disambiguated.ßdummyTurker
7388ß0.0ßThat means that a word will always predict the same set of sense features, regardless of the context in which it appears.ßdummyTurker
7389ß0.0ßHow and why is this useful when creating the word embeddings?ßdummyTurker
7390ß0.0ßTo me, the main weakness of the paper lies in the fact that the authors do not explain the underlying reason for this method to work.ßdummyTurker
7391ß0.0ßA1.ßdummyTurker
7392ß0.0ßThanks for your comments.ßdummyTurker
7393ß0.0ßWe have three reasons for the setting of HWE-Sense:ßdummyTurker
7394ß0.0ß1.ßdummyTurker
7395ß0.0ßIn certain tasks, the contextual information are not available or difficult to utilize.ßdummyTurker
7396ß0.0ßIn our experiment of similarity task (table 1) and analogy task (table 2), no contextual information are provided, so WSD can not be performed.ßdummyTurker
7397ß0.0ß2.ßdummyTurker
7398ß0.0ßOn training phase, what HWE-sense does is to predict a given target's each sense.ßdummyTurker
7399ß0.0ßThe physical meaning of doing so is actually letting each sense/sense embedding can all contribute to the generation of the target word's embedding.ßdummyTurker
7400ß0.0ßWhen a word lacks sufficient occurrences in the corpus, its all correlated sense/sense embeddings might help supplement the information that word itself lacks, as the senses generally have more sufficient occurrences in the corpus.ßdummyTurker
7401ß0.0ßThis could be the reason why HWE-sense of dim - 300 can overperform the baseline skip-gram on document classification on Table 5.ßdummyTurker
7402ß0.0ß3.ßdummyTurker
7403ß0.0ßThe performances that the existing state-of-the-art WSD packages we collected so far are not very satisfying (all below accracy of 70%).ßdummyTurker
7404ß0.0ßIn the next/camera version, it could be worth investigating to apply one or two the best WSD packages to train and test HWE-sense.ßdummyTurker
7405ß0.0ßIndeed, due to the limited size, we sacrifice some space to explain the underlying reason of the model's success.ßdummyTurker
7406ß0.0ßWhen camera version/next version allows one more page, we will add the above discussion.ßdummyTurker
7407ß0.0ßIn fact we actually prepared someßdummyTurker
7408ß0.0ßquantitative analysis via two kinds of visualization (one for HWE-POS amd one for HWE-sense) but can not fill them in on the 4-page submission.ßdummyTurker
7409ß0.0ßWe will also add them on the camera version/next version.ßdummyTurker
7410ß0.0ß------------------------------ßdummyTurker
7411ß0.0ßQ2.ßdummyTurker
7412ß0.0ßFurthermore, the overall explanation of the system is very brief, and it is not clear, in particular, the features prediction,...It just means that the presented work can be described in four pages.ßdummyTurker
7413ß0.0ßA2.ßdummyTurker
7414ß0.0ßThe part of feature prediction (the P(f_wt|w_t) term in Eq.2) is approximated by negative sampling as in Mikolov et al.ßdummyTurker
7415ß0.0ß(2013).ßdummyTurker
7416ß0.0ßWe use stochastic gradient decent to compute the optimization.ßdummyTurker
7417ß0.0ßIndeed, it is important to enhance the overall explanation and the details of optimization.ßdummyTurker
7418ß0.0ßWe will enhance these parts on the camera version/next version.ßdummyTurker
7419ß0.0ßWe hope that our codes in the supplemented material along with this submission can somehow make up the insufficient details.ßdummyTurker
7420ß0.0ßThanks a lot for your comments.ßdummyTurker
7421ß0.0ßQ1.ßdummyTurker
7422ß0.0ßHWE is an extension of Skipgram.ßdummyTurker
7423ß0.0ßIn that model the resulting embeddings are extracted from the weights between the projection and the output layer.ßdummyTurker
7424ß0.0ß... Or including them as features in your model?ßdummyTurker
7425ß0.0ßA1.ßdummyTurker
7426ß0.0ßIndeed, putting new features as inputs is natural.ßdummyTurker
7427ß0.0ßActually TWE (Liu et al, 2015) adopts this strategy.ßdummyTurker
7428ß0.0ßIn their approach, the information between the target word and its features, both of which are inputs, are interacted or shared with each other in a more implicit fashion.ßdummyTurker
7429ß0.0ßOn the other hand, our approach lets the target word be inputs and let its features as output, which means that given a target word, the model aims to predict its features.ßdummyTurker
7430ß0.0ßBy this setting, the target word and its features are interacted or shared with each other in a more explicit fashion.ßdummyTurker
7431ß0.0ßOur experimental results, compared with TWE, indeed prove its advantage on both intrinsic and extrinsic tasks.ßdummyTurker
7432ß0.0ßCurrently the models in this paper are extended from Skip-gram.ßdummyTurker
7433ß0.0ßThanks to your prompt, it is worth investigating whether CBOW-based HWE is able to demonstrate the similar conclusion in the future.ßdummyTurker
7434ß0.0ß------------------------------ßdummyTurker
7435ß0.0ßQ2.ßdummyTurker
7436ß0.0ßAre the improvement significance?ßdummyTurker
7437ß0.0ßCould you include a chi-square test?ßdummyTurker
7438ß0.0ßA2.ßdummyTurker
7439ß0.0ßThanks for your suggestion.ßdummyTurker
7440ß0.0ßWe will include chi-square test in the next/camera version.ßdummyTurker
7441ß0.0ß------------------------------ßdummyTurker
7442ß0.0ßQ3.ßdummyTurker
7443ß0.0ßThe Word sense features are included without performing WSD right?ßdummyTurker
7444ß0.0ßSo, whether the context is, all the word senses are included as features?ßdummyTurker
7445ß0.0ßA3.ßdummyTurker
7446ß0.0ßYes, all the word senses are included as features.ßdummyTurker
7447ß0.0ßWe have three reasons for this setting:ßdummyTurker
7448ß0.0ß1.ßdummyTurker
7449ß0.0ßIn certain tasks, the contextual information are not available or difficult to utilize.ßdummyTurker
7450ß0.0ßIn our experiment of similarity task (table 1)ßdummyTurker
7451ß0.0ßand analogy task (table 2), no contextual information are provided, so WSD can not be performed.ßdummyTurker
7452ß0.0ß2.ßdummyTurker
7453ß0.0ßOn training phase, what HWE-sense does is to predict a given target's each sense.ßdummyTurker
7454ß0.0ßThe physical meaning of doing so is actually letting each sense/sense embedding can all contribute to the generation of the target word's embedding.ßdummyTurker
7455ß0.0ßWhen a word lacks sufficient occurrences in the corpus, its all correlated sense/sense embeddings might help supplement the information that word itself lacks, as the senses generally have more sufficient occurrences in the corpus.ßdummyTurker
7456ß0.0ßThis could be the reason why HWE-sense of dim - 300 can overperform the baseline skip-gram on document classification on Table 5.ßdummyTurker
7457ß0.0ß3.ßdummyTurker
7458ß0.0ßThe performances that the existing state-of-the-art WSD packages we collected so far are not very satisfying (all below accuracy of 70%).ßdummyTurker
7459ß0.0ßIn the next/camera version, it could be worth investigating to apply one or two the best WSD packages to train and test HWE-sense.ßdummyTurker
7460ß0.0ß------------------------------ßdummyTurker
7461ß0.0ßQ4.ßdummyTurker
7462ß0.0ßShould cite (Massimiliano et al, 2017).ßdummyTurker
7463ß0.0ßA4.ßdummyTurker
7464ß0.0ßThanks and we will cite it.ßdummyTurker
7465ß0.0ßWe did use these additional two metrics in our study.ßdummyTurker
7466ß0.0ßIt was observed that both nDCG and precision metrics are positively correlated with MAP metric.ßdummyTurker
7467ß0.0ßThe proposed DAZER still achieves the best performance in these two metrics.ßdummyTurker
7468ß0.0ßWe will include the results in terms of precision metric in the extra page.ßdummyTurker
7469ß0.0ßFor a fair comparison, we strictly follow the settings designed in these baseline methods.ßdummyTurker
7470ß0.0ßOnly the proposed model (DAZER) takes the average of the seed word embeddings as input.ßdummyTurker
7471ß0.0ßWe will include further description regarding this point in the final version.ßdummyTurker
7472ß0.0ßYes, we have tried fine-tuning in our study.ßdummyTurker
7473ß0.0ßHowever, the baslise models like K-NRM experience significant performance deterioration.ßdummyTurker
7474ß0.0ßFine-tuning would likely result in model overfitting over the training dataset.ßdummyTurker
7475ß0.0ßThanks for pointing out.ßdummyTurker
7476ß0.0ßWe will mention fine-tuning based results in our final version.ßdummyTurker
7477ß0.0ßThe seed word selection process follows the strategy used in the existing works for dataless text classification.ßdummyTurker
7478ß0.0ßThe topical words derived by the unsupervised topic model (LDA) are manually selected as seed words based on the meaning of each sentiment category.ßdummyTurker
7479ß0.0ßWe will consider the seed word analysis in the final version if the room is available.ßdummyTurker
7480ß0.0ßThe category information in terms of seed words is a necessity as input for the model.ßdummyTurker
7481ß0.0ßOtherwise, the model has no guidance about what is relevant or not.ßdummyTurker
7482ß0.0ßCategory independent hidden representation is an ideal condition here.ßdummyTurker
7483ß0.0ßWe utilize adversarial learning to prevent the model from memorizing the category-specific information seen during the training phase.ßdummyTurker
7484ß0.0ßWe can see from the experimental results that adversarial learning is indeed beneficial.ßdummyTurker
7485ß0.0ßOur experimental results show that probability of the ground truth category predicted by the category classifier is indeed reduced.ßdummyTurker
7486ß0.0ßDue to space limitation, we do not report the results.ßdummyTurker
7487ß0.0ßWe will include the discussion about this in the final version.ßdummyTurker
7488ß0.0ßThe existing non-neural techniques are all developed in the paradigm of supervised learning.ßdummyTurker
7489ß0.0ßThese approaches cannot handle document filtering for the unseen categories.ßdummyTurker
7490ß0.0ßHence, we omit the comparison.ßdummyTurker
7491ß0.0ßPlease check the reply to WA 1.ßdummyTurker
7492ß0.0ßPlease check the reply to WA 2.ßdummyTurker
7493ß0.0ßWe did consider CDSSM as a possible baseline.ßdummyTurker
7494ß0.0ßBut CDSSM requires that the input bears a sequential structure.ßdummyTurker
7495ß0.0ßIn contrast, each category is represented here by several seed words.ßdummyTurker
7496ß0.0ßThese seed words together convey the conceptual knowledge about the category.ßdummyTurker
7497ß0.0ßWe can only represent these seed words as a bag-of-word since no word order can be provided (and possibly meaningless).ßdummyTurker
7498ß0.0ßTherefore, CDSSM is not applicable here.ßdummyTurker
7499ß0.0ßLittle attention has been given for zero-shot learning over textual information.ßdummyTurker
7500ß0.0ßZero-shot techniques are mainly developed for image classification.ßdummyTurker
7501ß0.0ßThese works mainly focus on designing the mapping functions such that the novel categories can be well represented and compatible with the corresponding images in the semantic embedding space.ßdummyTurker
7502ß0.0ßBecause the image based features are extracted and used by these techniques, we cannot apply them over the text domain with minor adaptation.ßdummyTurker
7503ß0.0ßSpecifically, extracting the relevance signals directly from each hidden embedding feature could easily make the model memorizing the feature patterns seen during the training (i.e., model fitting).ßdummyTurker
7504ß0.0ßTo guarantee the model's generalization ability on unseen categories, we aim at modeling the relative information with respect to the category of interest, based on two kinds of element-wise interactions.ßdummyTurker
7505ß0.0ßDue to space limitation, we only make the description about this insight in Section 1.ßdummyTurker
7506ß0.0ßThe embedding-based relative information works similarly as the notion of relative distanceßdummyTurker
7507ß0.0ßWe will further polish the paper.ßdummyTurker
7508ß0.0ß"""rel(e_1,e_2)"" refers to the relatedness for two words in terms of their word embeddings."ßdummyTurker
7509ß0.0ßThe function g refers to the weight function for i-th dimension.ßdummyTurker
7510ß0.0ßFor inner product, function g(e_1,e_2,i) is 1; For cosine similarity, function g(e_1,e_2,i) is the normalization term.ßdummyTurker
7511ß0.0ß"This part can be understood by the preceding sentence ""An effective way to ... word embeddings."""ßdummyTurker
7512ß0.0ß"Also, as to ""approximately equal"", we count the number of the dimensions that two sign vector share the same value on as the similarity measure."ßdummyTurker
7513ß0.0ßWe will include more description about this point.ßdummyTurker
7514ß0.0ßPRF is an effective approach to enhance document retrieval.ßdummyTurker
7515ß0.0ßBy extracting more relevant words from the pseudo positive documents, a category can be more precisely represented.ßdummyTurker
7516ß0.0ßHowever, PRF can be used for the proposed DAZER model and all baseline methods.ßdummyTurker
7517ß0.0ßNote that PRF itself is a research topic that has attracted many efforts in the past years.ßdummyTurker
7518ß0.0ßThe experiments conducted in our work are fair since all methods in comparison do not exploit PRF in their models.ßdummyTurker
7519ß0.0ßThe purpose is to evaluate the filtering performance with limited number of seed words alone.ßdummyTurker
7520ß0.0ßWe will consider extending DAZER and baseline models with PRF in our future work.ßdummyTurker
7521ß0.0ßAlso, the IR baselines are quite strong here, as mentioned by the first reviewer as a strength argument.ßdummyTurker
7522ß0.0ßK-NRM, DRMM, DeepRank and DSSM are all state-of-the-art neural retrieval models published very recently in the top-tier avenues.ßdummyTurker
7523ß0.0ßPlease check the reply to WA 2.ßdummyTurker
7524ß0.0ßPlease check the reply to WA 1.ßdummyTurker
7525ß0.0ßWe will take into account this comment, and we are currently experimenting with prerequisite chains in hopes of adding it to our system.ßdummyTurker
7526ß0.0ßWhile our corpus presents an invaluable resource for teaching NLP, it large promotes research in the tasks of prerequisite chains, pedagogical functions, survey generation, resource recommendations, among others, thus making it a suitable submission.ßdummyTurker
7527ß0.0ßOur annotations for pedagogical function are analogous to those of Sheng et al.ßdummyTurker
7528ß0.0ß2017 while our annotations for prerequisite chains are analogous to those of Gordon et al.ßdummyTurker
7529ß0.0ß2016, which shows the appropriateness of our data to NLP research.ßdummyTurker
7530ß0.0ßAs stated in the paper, our pedagogical function data is 4x as large as Sheng et al.ßdummyTurker
7531ß0.0ßand our prerequisite annotations 2 orders of magnitude larger.ßdummyTurker
7532ß0.0ßAdditionally, our tasks of survey extraction as information retrieval and resource recommendation from abstract+title pairs are novel ways of framing the tasks of survey generation and reading list generation, which benefit from our corpus.ßdummyTurker
7533ß0.0ßWe plan to clarify this in the final version.ßdummyTurker
7534ß0.0ßSection 3.1 touches upon the collection briefly.ßdummyTurker
7535ß0.0ßTo clarify, the main goal in our decision process of what to include in our corpus has been the quality-control of resources which can be used for an educational purpose.ßdummyTurker
7536ß0.0ßIn its initial version, this corpus consisted of conference tutorials from conferences such as ACL, EMNLP as well as long survey papers and books, as these genres contain an inherent amount of quality-control.ßdummyTurker
7537ß0.0ßLater on, other online resources were added to the corpus.ßdummyTurker
7538ß0.0ßStudents as well as the professor examined resources which they encountered in their studies.ßdummyTurker
7539ß0.0ßThe resources were added to the corpus if deemed of good quality by the professor.ßdummyTurker
7540ß0.0ßThe quality of a resource is a subjective measure, but we aimed to find resources which would serve a pedagogical function to either students or researchers.ßdummyTurker
7541ß0.0ßIn reference to the effort that went into this procedure, this collection of resources and meta-data annotation has been done over multiple years, while this year we created the search engine and added other annotations mentioned in the paper.ßdummyTurker
7542ß0.0ßWe followed the three-point scale of Gordon et al.ßdummyTurker
7543ß0.0ß2016 for prerequisite chain annotations.ßdummyTurker
7544ß0.0ßFor our survey extraction annotation, we initially experimented with a five-point annotation scale, but we found that scale difficult to make guidelines for annotation, as the difference in a card being somewhat helpful vs. helpful, for example, is too unclear.ßdummyTurker
7545ß0.0ßThe two-point scale of usefulness for resource recommendation was motivated by viewing this problem as a binary classification problem.ßdummyTurker
7546ß0.0ßGordon et al.ßdummyTurker
7547ß0.0ß2016 report kappa values of 0.36 overall on their dependency (prerequisite) annotation, which is comparable to our agreement and shows the difficulty of this annotation and task.ßdummyTurker
7548ß0.0ßOur survey extraction annotation is on a similar scale.ßdummyTurker
7549ß0.0ßThe search engine was not made available in the supplementary material, as it in an online site which contains information which would make the submission non-anonymous.ßdummyTurker
7550ß0.0ßA link to the search engine will be included in the final version.ßdummyTurker
7551ß0.0ßThat is true, as we aimed to present the corpus and show the many uses of it in both pedagogical and research settings.ßdummyTurker
7552ß0.0ßThe goal of the experiment in section 6 was to show initial work in a new problem we formulate from our dataset -- recommending resources from abstract+title pairs.ßdummyTurker
7553ß0.0ßWe are planning to dedicate separate research and papers to each individual area.ßdummyTurker
7554ß0.0ßWe will clarify our corpus collection procedures and annotations in the final version (see above for some details on collection).ßdummyTurker
7555ß0.0ßWith regards to pedagogical function and prerequisite chain annotations, we have followed protocols analogous to Sheng et al.ßdummyTurker
7556ß0.0ß2017 and Gordon et al.ßdummyTurker
7557ß0.0ß2016.ßdummyTurker
7558ß0.0ßWe will clarify this as well as the annotations for survey extraction and our judging of resource recommendation (clarify what criteria these were judged according to).ßdummyTurker
7559ß0.0ßWe will also provide information describing the division of annotation work so that it will be reproducible.ßdummyTurker
7560ß0.0ßThank you for the additional stylistic comments.ßdummyTurker
7561ß0.0ßWe will take these into account.ßdummyTurker
7562ß0.0ßOther comments:ßdummyTurker
7563ß0.0ßHand-written would be better described as “human-written.” These just means that the surveys were written by human experts rather than created automatically.ßdummyTurker
7564ß0.0ßAs in Section 4 (Data Annotation), these students are 3 PhD students in NLP and 6 undergraduate CS students who have taken at least one course in NLP or AI.ßdummyTurker
7565ß0.0ßThank you for pointing out the missing reference.ßdummyTurker
7566ß0.0ßWhere we reference Table 4 in Section 4.1 should be a reference to Table 3, and we will add the required reference to Table 4 in Section 4.2.ßdummyTurker
7567ß0.0ßBy two orders we do mean a hundred times larger.ßdummyTurker
7568ß0.0ßThe order of prerequisite edges annotated in that paper is 10^2, while we annotate on the order of 10^4.ßdummyTurker
7569ß0.0ßThat is notable that the majority of the resources relate to deep learning, and this is somewhat related to the bias we mention in Section 3.1.1.ßdummyTurker
7570ß0.0ßWhile many new resources are being published related to deep learning and neural networks, our goal is to be able to teach the entirety of NLP, so we continue to add such resources.ßdummyTurker
7571ß0.0ßAdditionally, not all resources are automatically added to our corpus, so even though a resource may be on a hot-topic it may not be viable in terms of our quality measures.ßdummyTurker
7572ß0.0ßThank you for the detailed review and valuable comments!ßdummyTurker
7573ß0.0ßYes, at inference time the input of annotation type is always fixed to be “CTB”.ßdummyTurker
7574ß0.0ßAt training time, it is “CTB” for CTB instances and “PPD” for PPD instances.ßdummyTurker
7575ß0.0ßAccording to existing experiments (performance degradation on test sets with the annotation guidelines different from those of the training sets), the accuracy will also decrease if the training set is the union of CTB and PPD.ßdummyTurker
7576ß0.0ßWe have to conduct experiments to obtain a convincing conclusion.ßdummyTurker
7577ß0.0ßThank you for the instructive question！ßdummyTurker
7578ß0.0ßYes, this method would very probably work in the scenario of learning on different annotations.ßdummyTurker
7579ß0.0ßWe will validate it and conduct comparison with it.ßdummyTurker
7580ß0.0ßThank you!ßdummyTurker
7581ß0.0ßThank you for the detailed review and valuable comments!ßdummyTurker
7582ß0.0ßIn both training and inference we simply use the probability distribution predicted after the layer of the adaptor.ßdummyTurker
7583ß0.0ßAs you pointed out, it indeed suffers from the risk of giving labels out of expected label set.ßdummyTurker
7584ß0.0ß"Since the union of label sets remains the same, that is, [""b"", ""m"", ""e"", ""s""] for word segmentation, there is no problem in this specific scenario."ßdummyTurker
7585ß0.0ßFor scenarios with different label sets for different annotations, the union label set will be a larger set.ßdummyTurker
7586ß0.0ßThe dataset in our work is the same as in the previous work you mentioned.ßdummyTurker
7587ß0.0ßThe word count in the CTB training dataset is 493,939 (cited from Sun (2011)), which is about 0.47 in the unit of M. We use M as the unit to facilitate the comparison with the word count of PPD.ßdummyTurker
7588ß0.0ßWe are sorry for the confusion.ßdummyTurker
7589ß0.0ßThe CTB dataset for word segmentation is the same with the datasets used in the work you mentioned, the experimental results are comparable.ßdummyTurker
7590ß0.0ßFor mono-corpus learning we use the deep neural network model for sequence labeling as depicted in figure 2.ßdummyTurker
7591ß0.0ßWe are sorry for the carelessness.ßdummyTurker
7592ß0.0ßYes, the mono-corpus learning models are trained on only partial target corpora, with varying amounts of sentences in the original target corpus.ßdummyTurker
7593ß0.0ßIf the scale of PPD is analogous to that of CTB, similar improvement will be achieved when the source and target corpora are reversed.ßdummyTurker
7594ß0.0ßBut since PPD is much larger than CTB, we think that the improvement will be much less.ßdummyTurker
7595ß0.0ßWe will validate this assumption.ßdummyTurker
7596ß0.0ßThank you for the insightful comments.ßdummyTurker
7597ß0.0ßWe are sorry for the confusion.ßdummyTurker
7598ß0.0ßWe use the annotation type to indicate the name of the annotation guidelines of the corpora.ßdummyTurker
7599ß0.0ßIn our experiments the annotation type has two possible values, “PPD” and “CTB”.ßdummyTurker
7600ß0.0ßSorry for the confusion.ßdummyTurker
7601ß0.0ßWe mean the softmax operation after a linear layer.ßdummyTurker
7602ß0.0ßIt is indeed not a standard representation.ßdummyTurker
7603ß0.0ßOur goal is to verify if the semantic and syntactic information is embedded in a sentence vector.ßdummyTurker
7604ß0.0ßSo, we kept the auxiliary tasks simple to get decomposed unit information about what syntactic or semantic information is captured by the sentence embeddings.ßdummyTurker
7605ß0.0ßFor example, we considered identifying the existence of words of a specific part-of-speech tag in a sentence or if a sentence contains a prepositional phrase (PP) as a proxy to understand if and what syntactic information is encoded in the learned embeddings.ßdummyTurker
7606ß0.0ßWe did consider predicting parse trees based on sentence embeddings.ßdummyTurker
7607ß0.0ßHowever, our preliminary experiments showed that this particular task is too hard as even the sub-problems like predicting sentence length and word order are challenging.ßdummyTurker
7608ß0.0ßIn this work, we do not intend to achieve state-of-the-art performance on the source tasks.ßdummyTurker
7609ß0.0ßInstead, we aim to learn sentence encoders that can produce generalizable sentence vectors.ßdummyTurker
7610ß0.0ßPrevious work (Conneau et.ßdummyTurker
7611ß0.0ßal, 2017) has found that simple BiLSTM performs better than other complex architectures such as hierarchical ConvNet, self-attentive network, etc for this purpose.ßdummyTurker
7612ß0.0ßOur choice of simple neural network architectures is based on this previous finding.ßdummyTurker
7613ß0.0ßThe main intent of this paper is to show the advantages of multi-task learning for producing generic sentence representations.ßdummyTurker
7614ß0.0ßThe state-of-the-art architectures for the source tasks consider complex components that learn task specific information and may not transfer well in our setting.ßdummyTurker
7615ß0.0ßFor example, a self-attentive network has been shown to underperform in generating sentence representation (Conneau et.ßdummyTurker
7616ß0.0ßal, 2017).ßdummyTurker
7617ß0.0ßMoreover, some well-performed SNLI and Quora models consider modeling the interaction between words in two input sentences; therefore, their approaches do not generate sentence embedding and cannot be adopted in our setting.ßdummyTurker
7618ß0.0ßWe will include the discussion in the revision.ßdummyTurker
7619ß0.0ßThe auxiliary tasks are designed to evaluate the trained sentence embeddings and examine what syntactic and semantic information is captured by them.ßdummyTurker
7620ß0.0ßSince we observed that sentence embeddings learned via multitask learning was helpful for the transfer tasks, we also wanted to understand what type of information is embedded into the sentence vectors.ßdummyTurker
7621ß0.0ßTherefore, we analyze the sentence representations by evaluating them through the auxiliary tasks to see what syntactic and semantic properties are captured.ßdummyTurker
7622ß0.0ßWe omitted the detailed explanation for the STL system due to the page limit.ßdummyTurker
7623ß0.0ßWe use the exact BiLSTM-max pooling architecture used in Conneau et, al, 2017 as our baseline STL system.ßdummyTurker
7624ß0.0ßThank you for your suggestion.ßdummyTurker
7625ß0.0ßWe will use the additional page provided in camera-ready version to adjust and move few more information in the main body of the paper.ßdummyTurker
7626ß0.0ßThanks for recognizing our work as well as your efforts for reviewing our work.ßdummyTurker
7627ß0.0ßThanks for recognizing our work as well as your time and constructive comments.ßdummyTurker
7628ß0.0ßWe will add notations and labels in the figures and contexts to make the paper clearer.ßdummyTurker
7629ß0.0ßThe examples in Fig.ßdummyTurker
7630ß0.0ß3 denote the whole video from DiDeMo.ßdummyTurker
7631ß0.0ßWe sample one frame from each five-second chunk, resulting in 6 frames representing the whole video.ßdummyTurker
7632ß0.0ßSorry for not providing the grounding results of TGN in Fig.ßdummyTurker
7633ß0.0ß3.ßdummyTurker
7634ß0.0ßIn the revised version, we will provide more qualitative results.ßdummyTurker
7635ß0.0ßWe will make corresponding revisions to make the paper clearer.ßdummyTurker
7636ß0.0ßThanks for your careful comments.ßdummyTurker
7637ß0.0ßWe summarize and address your comments accordingly in the following.ßdummyTurker
7638ß0.0ßQ1: Missing references and writing.ßdummyTurker
7639ß0.0ß(Weaknesses 1a, 2a, 2b, Comments 1, 3)ßdummyTurker
7640ß0.0ßA1: Actually we have claimed that the baselines are from DiDeMo and CTRL paper as in lines (573-576) and lines (631-635) respectively.ßdummyTurker
7641ß0.0ßWe will add references in Table 1 and 2 to avoid misunderstanding.ßdummyTurker
7642ß0.0ßWe will cite and discuss the relevant works in action localization, natural language video retrieval, aligning text to video, and dense video captioning.ßdummyTurker
7643ß0.0ßWe will polish the writing and make the paper clearer.ßdummyTurker
7644ß0.0ßQ2: Fine-grained results.ßdummyTurker
7645ß0.0ß(Weaknesses 1b, Questions 1)ßdummyTurker
7646ß0.0ßA2: We show the results based on IoU from 1.0 to 0.1 with 0.1 as interval w.r.t.ßdummyTurker
7647ß0.0ßR@1/R@5(%).ßdummyTurker
7648ß0.0ßTACoS: 5.24/10.53, 7.49/13.78,  9.11/15.15,  9.59/15.95,  13.01/21.46,  18.9/31.02,  19.29/32.46,  21.77/39.06,  27.65/44.87,  41.87/53.4ßdummyTurker
7649ß0.0ßActivityNet: 8.94/24.44, 11.36/27.35,  14.97/29.91,  18.74/34.12,  24.25/38.03,  28.47/43.33,  37.05/49.54,  45.51/57.32,  56.13/66.2,  70.06/79.1ßdummyTurker
7650ß0.0ßQ3: Video segment retrieval task.ßdummyTurker
7651ß0.0ß(Weaknesses 1c, Questions 2)ßdummyTurker
7652ß0.0ßA3: This is a good suggestion.ßdummyTurker
7653ß0.0ßIn this paper, we mainly focus on the NSGV task, which needs to not only match one natural sentence and video segment, but also localize the starting and ending points of one video segment in an untrimmed video.ßdummyTurker
7654ß0.0ßAs such, NSGV is more challenging.ßdummyTurker
7655ß0.0ßIn the future, we will apply our proposed TGN on the video segment retrieval task.ßdummyTurker
7656ß0.0ßQ4: Video captioning as baseline.ßdummyTurker
7657ß0.0ß(Weaknesses 1d, 2c, Questions 3, Comments 2)ßdummyTurker
7658ß0.0ßA4: The idea of using video captioning model as baseline is interesting and inspiring.ßdummyTurker
7659ß0.0ßHowever, it may suffer from the efficiency problem.ßdummyTurker
7660ß0.0ßIn order to generate the TxK matrix, TxK video segments need to be fed into the captioning model, making the testing phrase very slow.ßdummyTurker
7661ß0.0ßFor example, for a video with T=300 timesteps (K=20 anchors considered), we need to go through video captioning model 300*20=6000 times.ßdummyTurker
7662ß0.0ßFor our proposed TGN, the whole video is only processed in one single pass.ßdummyTurker
7663ß0.0ßWe use the Yao Li’s work as the baseline video captioning model, which is pre-trained on MSR-VTT-10K dataset.ßdummyTurker
7664ß0.0ßDue to the heavy computations, we only provide the results based on randomly chosen 100 testing samples, which still costs more than 3 hours.ßdummyTurker
7665ß0.0ßWe show the results (video captioning and TGN) based on IoU from 1.0 to 0.1 with 0.1 as interval w.r.t.ßdummyTurker
7666ß0.0ßR@1/R@5(%).ßdummyTurker
7667ß0.0ßIt can be observed that captioning model can help tackle the NSGV task, which performs inferiorly to our proposed TGN.ßdummyTurker
7668ß0.0ßOne reason is that such video captioning based models does not consider the evolving fine-grained frame-by-word interactions.ßdummyTurker
7669ß0.0ßAnother reason is that the video captioning model is pre-trained on MSR-VTT-10K.ßdummyTurker
7670ß0.0ßMoreover, the video captioning based approaches suffer from the efficiency problem.ßdummyTurker
7671ß0.0ßActivityNet (Video Captioning): 2/3, 2/4, 3/7, 3/8, 5/13, 10/25, 11/28, 13/33, 18/40, 27/53ßdummyTurker
7672ß0.0ßActivityNet (our proposed TGN): 5/20, 11/25, 12/29, 18/30, 22/36, 24/40, 38/46, 42/55, 52/63, 69/75ßdummyTurker
7673ß0.0ßQ5: Human evaluation.ßdummyTurker
7674ß0.0ß(Weaknesses 1e, Questions 7)ßdummyTurker
7675ß0.0ßA5: This is a good suggestion.ßdummyTurker
7676ß0.0ßWe randomly select 100 testing samples from each dataset to perform human evaluation.ßdummyTurker
7677ß0.0ßEach participant is asked to identify whether the grounding results by our proposed TGN can well express the semantic meaning of the sentence or not, with 1 indicating a good expression and 0 indicating a bad expression.ßdummyTurker
7678ß0.0ßIn total, we recruited 5 subjects.ßdummyTurker
7679ß0.0ßAnd the average scores for DiDeMo/TACoS/ActivityNet are 0.66/0.32/0.58, respectively.ßdummyTurker
7680ß0.0ßThe prediction on DiDeMo/ActivityNet is acceptable while the performance on TACoS is relatively unsatisfactory.ßdummyTurker
7681ß0.0ßThe reason may be that the groundtruth grounding is very short compared with the video length in TACoS, which is more challenging.ßdummyTurker
7682ß0.0ßQ6: Effect of sampling rate.ßdummyTurker
7683ß0.0ß(Questions 4)ßdummyTurker
7684ß0.0ßA6: This is a good suggestion.ßdummyTurker
7685ß0.0ßThe selection of sampling rate is a trade-off between the time scale given in the dataset and the video length in the dataset (TACoS with 10,517 frame and ActivityNet with 3,384 frames on average).ßdummyTurker
7686ß0.0ßSmaller sampling rate may provide more precise boundary information with long timesteps, which will issues great challenges of the modeling capabilities of LSTM.ßdummyTurker
7687ß0.0ßTherefore, we sample every second for ActivityNet.ßdummyTurker
7688ß0.0ßIn the revised version, we will study the effects of sampling rate.ßdummyTurker
7689ß0.0ßThe fraction of the groundtruth segment to the full video clip on ActivityNet/TACoS/DiDeMo are 0.31/0.03/0.23.ßdummyTurker
7690ß0.0ßQ7: More baselines.ßdummyTurker
7691ß0.0ß(Questions 5)ßdummyTurker
7692ß0.0ßA7: The performance (%) of Random on ActivityNet isßdummyTurker
7693ß0.0ß11.64(R@1,0.5)/23.97(R@1,0.3)/42.68(R@1,0.1)/36.98(R@5,0.5)/56.73(R@5,0.3)/77.41(R@5,0.1).ßdummyTurker
7694ß0.0ßWe observe that Random on ActivityNet performs well on R@N,IoU=M when N is big (N=5) and M is small (M=0.1/0.3).ßdummyTurker
7695ß0.0ßThe reason can be attributed to that the fraction of groundtruth segment to the full video clip is very high on ActivityNet.ßdummyTurker
7696ß0.0ßThe TACoS dataset focuses on cooking, with 66 activities and 109 objects.ßdummyTurker
7697ß0.0ßHowever, ActivityNet contains a large amount of activities and objects.ßdummyTurker
7698ß0.0ßThe performance of Verb+Object baselines may not be good.ßdummyTurker
7699ß0.0ßIn the revised version, we will provide the corresponding results.ßdummyTurker
7700ß0.0ßQ8: Performance on InceptionV4.ßdummyTurker
7701ß0.0ß(Questions 6)ßdummyTurker
7702ß0.0ßA8: The results on ActivityNet are given in the paper (line:654).ßdummyTurker
7703ß0.0ßThe results (%) on DiDeMo areßdummyTurker
7704ß0.0ß28.66(R@1,1)/84.66(R@5,1)/41.34(mIoU)ßdummyTurker
7705ß0.0ßThe results (%) on TACoS areßdummyTurker
7706ß0.0ß19.14(R@1,0.5)/23.04(R@1,0.3)/39.57(R@1,0.1)/30.54(R@5,0.5)/40.68(R@5,0.3)/52.29(R@5,0.1)ßdummyTurker
7707ß0.0ßIt can be observed that more powerful CNNs can yield better performances.ßdummyTurker
7708ß0.0ßFor fair comparisons, in this paper, we demonstrate the results with the same image CNN as in MCN and CTRL.ßdummyTurker
7709ß0.0ßWe will provide the results with InceptionV4.ßdummyTurker
7710ß0.0ßQ9: Reproducibility.ßdummyTurker
7711ß0.0ß(Questions 8)ßdummyTurker
7712ß0.0ßA9: We will provide more detailed information about the parameters and training.ßdummyTurker
7713ß0.0ßMoreover, we will release the code and trained models.ßdummyTurker
7714ß0.0ßThis is a possibility, which we will acknowledge.ßdummyTurker
7715ß0.0ßHowever, an advantage of word2vec is that it can be trained relatively quickly on large amounts of data, compared to skip-thoughts.ßdummyTurker
7716ß0.0ßDue to the space limitations, we were not able to provide these details, however, they are available in the papers that are referenced.ßdummyTurker
7717ß0.0ßIf accepted, we will use the extra space allowed for the final version to include these details.ßdummyTurker
7718ß0.0ßThe tuning of the cost parameter for the SVM shows that both the word2vec and skip-thoughts approaches are sensitive to this parameter (with accuracies between 0.619 and 0.830, and 0.661 and 0.803, respectively).ßdummyTurker
7719ß0.0ßNeither the word2vec embeddings nor the skip-thoughts model were tuned for the task of idiomaticity prediction (e.g., tuning the window size or number of dimensions, in the case of word2vec).ßdummyTurker
7720ß0.0ßYes, the contributions do come from the analysis.ßdummyTurker
7721ß0.0ßWe agree that error analysis would strengthen the contributions in the paper.ßdummyTurker
7722ß0.0ßIf accepted, we will include this in the extra space allowed for the final version.ßdummyTurker
7723ß0.0ßWe have not considered randomly-assigned values for this feature, but this is a good suggestion.ßdummyTurker
7724ß0.0ßWe will add this analysis.ßdummyTurker
7725ß0.0ßWe did consider representing the canonical form information as a vector of 11 binary features, each representing a specific pattern.ßdummyTurker
7726ß0.0ßThis did not outperform the use of a single binary canonical form feature that is reported in the paper.ßdummyTurker
7727ß0.0ßWe did not test the word2vec model without stopword removal.ßdummyTurker
7728ß0.0ßThis is again a good suggestion, and we will also include it.ßdummyTurker
7729ß0.0ßIt will be particularly interesting to examine the impact of the canonical form feature on word2vec models with and without stopword removal.ßdummyTurker
7730ß0.0ßI think our model still has potential to improve performance by tuning network parameters and increasing network depth.ßdummyTurker
7731ß0.0ßThe strength of our method is showing a better balance in general perspective, while other baseline methods may only outperform in some specified data categories.ßdummyTurker
7732ß0.0ßConsidering different types of objectives, we handle the two modules respectively through a connection of word embeddings making sure for the transferability between two modules.ßdummyTurker
7733ß0.0ßWe omit some common descriptions and equations to save space including the optimization objectives.ßdummyTurker
7734ß0.0ßThis advice makes sense and we will fix this problem in a new version.ßdummyTurker
7735ß0.0ßAs replied to weakness 1, we handle the two modules respectively.ßdummyTurker
7736ß0.0ßThe summarization module training process are introduced in line 623 to 629, and we can supplement some more details if needed.ßdummyTurker
7737ß0.0ßReview 1ßdummyTurker
7738ß0.0ßWe would like to thank the reviewer for the very constructive and helpful feedback.ßdummyTurker
7739ß0.0ßOur method is significantly better than the following methods with p-value < 0.01:  (Larkey and Croft, 1996), (Franz et al., 2000), (Pestian et al., 2007), (Koopman et al., 2015), LET, HierNet, HybridNet, BranchNet, No-TLSTM, No-AL, No-IC, No-AM.ßdummyTurker
7740ß0.0ßOur method is significantly better than Bottom-up TLSTM with p-value = 0.03ßdummyTurker
7741ß0.0ßReview 2ßdummyTurker
7742ß0.0ßWe would like to thank the reviewer for the very constructive and helpful feedback.ßdummyTurker
7743ß0.0ßHere are a few patterns of errors.ßdummyTurker
7744ß0.0ß(1) Many codes lack sufficient training examples (less than 30).ßdummyTurker
7745ß0.0ßThe performance on these codes is typically low.ßdummyTurker
7746ß0.0ß(2) The difference between many codes is very subtle and hard to distinguish, e.g., “Nephrotic syndrome with lesion of proliferative glomerulonephritis“ and “Nephrotic syndrome with lesion of membranous glomerulonephritis”.ßdummyTurker
7747ß0.0ß(3) Codes with long description are prone to being incorrectly assigned, e.g., “Gastric ulcer, unspecified as acute or chronic, without mention of hemorrhage or perforation, without mention of obstruction”ßdummyTurker
7748ß0.0ßAccording to our medical coder collaborators, in a small experiment they did before, the Cohen's kappa coefficient (used for evaluating inter-annotator agreement) is 0.72.ßdummyTurker
7749ß0.0ßWe will apply the methods to other potential tasks such as textual entailment, synonym matching and entity linking.ßdummyTurker
7750ß0.0ßWe thank the reviewer for their supporting feedback.ßdummyTurker
7751ß0.0ßwe agree that ANOVA statistical significance testing methods are important.ßdummyTurker
7752ß0.0ßWe did not address them as our survey of ACL and TACL papers indicated that they are rarely used in NLP papers.ßdummyTurker
7753ß0.0ßOn the other hand - this is a good reason to expose the ACL audience to these methods.ßdummyTurker
7754ß0.0ßGiven the opportunity we will discuss these methods in the camera ready version.ßdummyTurker
7755ß0.0ßTesting for normality: we thank the reviewer for this very important comment.ßdummyTurker
7756ß0.0ßWhile we explored this issue with experts when preparing the paper, the comment suggests that we should look at this issue again.ßdummyTurker
7757ß0.0ßWe do not want to jump into conclusions in the short author response period but obligate that given the opportunity we will make sure we address this issue properly in the camera ready version.ßdummyTurker
7758ß0.0ßWe agree that t-test is rarely a good choice in NLP setups.ßdummyTurker
7759ß0.0ßWe will make this argument much stronger in the camera ready version.ßdummyTurker
7760ß0.0ßThank you very much for the precious feedback.ßdummyTurker
7761ß0.0ßThe number of domains explored in this work is not very small compared with previous work.ßdummyTurker
7762ß0.0ßFor example, Liu et al.ßdummyTurker
7763ß0.0ß(2015) investigated three domains, and Kim et al.ßdummyTurker
7764ß0.0ß(2017) investigated five domains.ßdummyTurker
7765ß0.0ß* Liu et al., Deep Contextual Language Understanding in Spoken Dialogue Systems, Interspeech2015.ßdummyTurker
7766ß0.0ß* Kim et al., ONENET: Joint Domain, Intent, Slot Prediction for Spoken Language Understanding, ASRU2017.ßdummyTurker
7767ß0.0ßThe five domains were selected because they are the most popular domains among ones supported by our intelligent assistant app.ßdummyTurker
7768ß0.0ßTherefore, this choice can be validated from the viewpoint of practical use.ßdummyTurker
7769ß0.0ßOther search domains were rarely specified in the course of data collection (footnote 5).ßdummyTurker
7770ß0.0ßAlthough users sometimes specified unsupported domains (e.g., shopping in Tab.ßdummyTurker
7771ß0.0ß5), such cases were also rare.ßdummyTurker
7772ß0.0ßWe appreciate if the reviewer could take the above into consideration to assess our experimental design.ßdummyTurker
7773ß0.0ßThe feed-forward neural network was followed by a soft-max layer to output a probability distribution.ßdummyTurker
7774ß0.0ßWe are very sorry if we misunderstood the point.ßdummyTurker
7775ß0.0ßUsing hand-coded rules, such as regular expression patterns, as a building block is not a bad idea.ßdummyTurker
7776ß0.0ßSome practical systems including recently proposed hybrid code networks (Williams et al., 2017) are designed based on such philosophy.ßdummyTurker
7777ß0.0ß* Williams et al., Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning, ACL2017.ßdummyTurker
7778ß0.0ßThe detection and extraction methods worked sufficiently well for the purpose of the data collection.ßdummyTurker
7779ß0.0ßThe fact that those methods are not technically elegant does not damage the significance of our work.ßdummyTurker
7780ß0.0ßThe proposed methods statistically significantly improved the non-trivial baseline (L646-649), and none of the reviewers have suggested using stronger baselines.ßdummyTurker
7781ß0.0ßWe appreciate if the reviewer could take this into account to evaluate our experimental results.ßdummyTurker
7782ß0.0ßThe weakness arguments in “the last half of 1,” 2, 4, and “the first half of 5” are about the writing problem.ßdummyTurker
7783ß0.0ßThose problems are not very serious and can be fixed easily, because the other reviewers did not pose similar concerns.ßdummyTurker
7784ß0.0ßWe are willing to follow your instructive comments to revise the draft.ßdummyTurker
7785ß0.0ßThank you for your advice.ßdummyTurker
7786ß0.0ßThank you very much for the detailed review.ßdummyTurker
7787ß0.0ßWe agree with the reviewer.ßdummyTurker
7788ß0.0ßWe are delighted to see that the reviewer nevertheless considers our work interesting overall.ßdummyTurker
7789ß0.0ßThank you for the precious comment.ßdummyTurker
7790ß0.0ßAsking clarification questions is common in human-human conversations.ßdummyTurker
7791ß0.0ßTherefore, we believe that such a dialogue strategy is worth exploring for developing human-like chat bots that can naturally converse with us.ßdummyTurker
7792ß0.0ßWe admit that further research efforts, such as user satisfaction study, are required to strengthen our claim.ßdummyTurker
7793ß0.0ßPaired bootstrap resampling was used (L678).ßdummyTurker
7794ß0.0ßPlease refer to, for example, (Koehn, 2004).ßdummyTurker
7795ß0.0ß* Koehn, Statistical Significance Tests for Machine Translation Evaluation, EMNLP2004.ßdummyTurker
7796ß0.0ßJoulin et al.’s method learns document embeddings while learning document classification model.ßdummyTurker
7797ß0.0ßWe used their method to learn query embeddings by regarding a query as a document (made up of a single word) just for the sake of convenience.ßdummyTurker
7798ß0.0ßThe threshold was determined so that the given question ratio is achieved on the test data.ßdummyTurker
7799ß0.0ßWe have an experience in releasing logs of the same intelligent assistant for research purposes.ßdummyTurker
7800ß0.0ßBased on the previuos experience, we consider that there are no serious obstacles to the data release, while we cannot detail the procedure here for space limitation.ßdummyTurker
7801ß0.0ßThank you very much for this comment.ßdummyTurker
7802ß0.0ßMost NMT systems adopt the encoder-decoder framework with a long information processing procedure, which causes the word embeddings at both ends to be not directly related.ßdummyTurker
7803ß0.0ßTheoretically and formally analyze deep neural networks from the perspective of information theory is still an open and challenging problem.ßdummyTurker
7804ß0.0ßCurrently there are some studies on this in the deep learning community.ßdummyTurker
7805ß0.0ßWe are also interested in this especially with respect to neural machine translation.ßdummyTurker
7806ß0.0ßWe’d like to leave it to our future work.ßdummyTurker
7807ß0.0ßIn this paper, we try to provide empirical evidences to support our hypothesis.ßdummyTurker
7808ß0.0ßWe have analyzed NMT translations and found that there are many word alignment errors.ßdummyTurker
7809ß0.0ß"Taking the stop symbol ""eos"" as an example, 50% of the 'eos' alignments are incorrect."ßdummyTurker
7810ß0.0ßOur experiments show that shortening the information flow distance is helpful for both word alignments and translation quality.ßdummyTurker
7811ß0.0ßThank you for your suggestion.ßdummyTurker
7812ß0.0ßWe will further analyze the results of our experiment to show the strengths and weaknesses of the proposed method.ßdummyTurker
7813ß0.0ßOne of weaknesses is the introduction of the around 5% additional parameters (discussed in Section 3.2), which causes a very small extra overhead.ßdummyTurker
7814ß0.0ßThank you very much for this great suggestion.ßdummyTurker
7815ß0.0ßWe’d like to try this method as a baseline for languages with overlapped source and target vocabularies.ßdummyTurker
7816ß0.0ßThe additional 5% parameters account for a very small proportion and cause a slightly extra overhead according to our analysis.ßdummyTurker
7817ß0.0ßWe will provide detailed analysis on this in the new version.ßdummyTurker
7818ß0.0ßThanks a lot.ßdummyTurker
7819ß0.0ßThank you very much for this great comment.ßdummyTurker
7820ß0.0ßThe reason of using pre-training strategy is that the embedding loss in the direct bridging model requires well-trained word alignments as a starting point.ßdummyTurker
7821ß0.0ßWe will try to train the direct bridging model from the scratch to figure out the impact of such pretraining on the direct bridging model.ßdummyTurker
7822ß0.0ßThanks a lot for this suggestion.ßdummyTurker
7823ß0.0ßThe pretraining is only used for the direct bridging model.ßdummyTurker
7824ß0.0ßWe will provide the comparison results in the new version following your great suggestion.ßdummyTurker
7825ß0.0ßThanks a lot.ßdummyTurker
7826ß0.0ßThanks a lot for this great suggestion.ßdummyTurker
7827ß0.0ßWe performed model training on sentences of length of less than 50 words, considering that it will shorten the training time.ßdummyTurker
7828ß0.0ßIn future work, we will try to use corpora with longer sentences for training with length normalization as you suggest.ßdummyTurker
7829ß0.0ßThank you very much for this suggestion.ßdummyTurker
7830ß0.0ßWe actually had some positive experiment results on other language pairs after submission.ßdummyTurker
7831ß0.0ßWe will provide these results in the new version.ßdummyTurker
7832ß0.0ßDue to the space limit, we do not explain these concepts.ßdummyTurker
7833ß0.0ßThanks a lot for pointing this out.ßdummyTurker
7834ß0.0ßWe will explain them in the new version.ßdummyTurker
7835ß0.0ßThanks a lot for this great suggestion.ßdummyTurker
7836ß0.0ßWe will definitely try to build a new model to fuse these three models in the future.ßdummyTurker
7837ß0.0ßIn line 466,s.ßdummyTurker
7838ß0.0ßThe authors calaim that It would prefer a “good” generated sentence y if the values of Cd(target|dT , y) and Cs(1|yT, y) are large.ßdummyTurker
7839ß0.0ßThis hypothesis is ideally, the model cannot garantee this----concating label for generation cannot garantee that the generated response accord with the label.ßdummyTurker
7840ß0.0ßThis makes the adversarial traing not solid.ßdummyTurker
7841ß0.0ßAs stated in sec 3.4 where we introduce a text-similarity critic and a domain critic to guarantee, as much as possible, that the generated sentences resemble the sentences drawn from the target domain.ßdummyTurker
7842ß0.0ßUsing latent variable without analysising it making the result, to some extent, ambiguous.ßdummyTurker
7843ß0.0ßWe take your valuable comments to further report the latent variable analysis such as KL cost and latent variable visualization.ßdummyTurker
7844ß0.0ßThere has been several work which uses variational autoencoder for dialog generation.ßdummyTurker
7845ß0.0ßAnd it is CVAE precisely.ßdummyTurker
7846ß0.0ßThe authors has no nessasary writing a lot for this part.ßdummyTurker
7847ß0.0ßThank you very much for pointing out the current related works.ßdummyTurker
7848ß0.0ßWe consider them to revise the paper to be more concise.ßdummyTurker
7849ß0.0ßThank you very much for your valuable comments.ßdummyTurker
7850ß0.0ßWe consider your comments to make the paper more concise and to ensure a basic fairness.ßdummyTurker
7851ß0.0ßHow exactly did the domain adaptation without critics (sec 1 in Table 2) work?ßdummyTurker
7852ß0.0ßIf I'm not mistaken, this isn't explicitly described anywhere.ßdummyTurker
7853ß0.0ßThe table title described the process in which we trained the VRALSTM model, which is the model without critics, in adaptation scenario.ßdummyTurker
7854ß0.0ßThank you for making a good point.ßdummyTurker
7855ß0.0ßIn short, the motivation of Mem2Seq is that we expect our model to generate natural and correct responses without a lot of human effort, NLP tools, and pattern engineering.ßdummyTurker
7856ß0.0ßTo the best of our knowledge, dialog management modules depend on state trackers to summarize dialog history into state features (Williams and Young, 2007; Hori et al., 2009, Lee et al., 2009; Griol et al., 2008; Young et al., 2013; Li et al., 2014).ßdummyTurker
7857ß0.0ßThis process requires specialized designs and labelling and may have low coverage that cannot flexibly deal with variable linguistic phenomena.ßdummyTurker
7858ß0.0ßIn particular, when dealing with multi-domain scenarios, additional domain classifiers are needed before applying state trackers.ßdummyTurker
7859ß0.0ßThe same issues have been also pointed out by other recent researchers (Williams et al., 2017; Bordes and Weston, 2017; Eric et al., 2017; Eric and Manning, 2017; He et al., 2017).ßdummyTurker
7860ß0.0ßTherefore, similar to (Bordes and Weston, 2017; Eric et al., 2017; Eric and Manning, 2017; He et al., 2017), our work aims to learn a single model that is able to deal with all of the task-oriented pipelines.ßdummyTurker
7861ß0.0ßIdeally, our model embeds a “hidden” state tracker using the MemNN encoder.ßdummyTurker
7862ß0.0ßOne main contribution of our work is incorporating the copying mechanism from pointer network into MemNN, which can further learn a better* state tracker, like a “hidden” slot-filling mechanism.ßdummyTurker
7863ß0.0ßMoreover, Mem2Seq also learns a better* natural language generator simultaneously, which learns to interpret KB results and generate responses without additional NLG modules.ßdummyTurker
7864ß0.0ßWe designed our model in this way as we believe the words in a natural response may be generated from a vocabulary or by copy relevant entities from the dialog history and the corresponding KB.ßdummyTurker
7865ß0.0ß(* Mem2Seq can be trained 5 times faster and achieve state-of-the-art results in several task-oriented dialog datasets)ßdummyTurker
7866ß0.0ßThank you for your suggestion and we will definitely consider it to further strengthen our work.ßdummyTurker
7867ß0.0ßIn this paper, we mainly followed other works (Williams et al., 2017; Bordes and Weston, 2017; Eric et al., 2017; Eric and Manning, 2017), which solely focus on unsupervised evaluation metrics (accuracy, BLEU, F1 score) to evaluate the model performance.ßdummyTurker
7868ß0.0ßThank you for the good question and we also share your concern.ßdummyTurker
7869ß0.0ßOur work included BLEU score in order to compare our model to existing methods and baselines.ßdummyTurker
7870ß0.0ßHowever, we agree that BLEU may not be the best evaluation metric, as we have also argued in the discussion section.ßdummyTurker
7871ß0.0ßWe discussed that entity F1-score seems to be a better evaluation metric for task-oriented dialog.ßdummyTurker
7872ß0.0ßWe also agree that including the task-completion rate is important, so we will consider to include this measure for bAbI dialog and DSTC2 (ex: In bAbI dialog, we achieve around 90% task-completion rate based on 98.3% per-response accuracy).ßdummyTurker
7873ß0.0ßMoreover, unsupervised evaluation of dialog systems is another open research question.ßdummyTurker
7874ß0.0ßWe want to highlight that humans have a low BLEU score despite their correctness because there may not be a large n-gram overlap between the given response and the expected one.ßdummyTurker
7875ß0.0ßHowever, this does not imply that there is no correlation between BLEU score and human evaluation.ßdummyTurker
7876ß0.0ßIn addition, unlike open domain dialogs where BLEU score does not correlate with human evaluation (Liu et al., 2016), in task-oriented dialogs, the answers are constrained to particular entities and recurrent patterns.ßdummyTurker
7877ß0.0ßThus, we believe BLEU score still can be considered as a relevant measure.ßdummyTurker
7878ß0.0ßLastly, as we mentioned above, we are willing to conduct a human evaluation to further strengthen our work.ßdummyTurker
7879ß0.0ßThank you for review.ßdummyTurker
7880ß0.0ßWe list two points that make us believe that Mem2Seq is an interesting technical contribution:ßdummyTurker
7881ß0.0ß1) Mem2Seq does not need the attention weights computed from the encoder.ßdummyTurker
7882ß0.0ßNamely, we can encode only dialog history, not the whole KB information.ßdummyTurker
7883ß0.0ßThat is the main reason why our model can achieve 5x speedup compared to Seq2Seq with attention.ßdummyTurker
7884ß0.0ß2) Multi-hop attention mechanism in the decoding stage in combination with pointers is novel, and it helps in learning correlations between memories, which a typical attention mechanism cannot do so.ßdummyTurker
7885ß0.0ßReply to weakness argument 2ßdummyTurker
7886ß0.0ßThank you for your review.ßdummyTurker
7887ß0.0ßWe agree that some parts of our paper are not self-contained enough and will try our best to include a further explanation.ßdummyTurker
7888ß0.0ßIn particular, as you mentioned, we will create a separated section for the Sentinel.ßdummyTurker
7889ß0.0ßThank you for making a good point and we share your concerns.ßdummyTurker
7890ß0.0ßWe also agree that including the task-completion, in particular in Task 5, is important, so we will consider to include this measure for bAbI dialog and DSTC2.ßdummyTurker
7891ß0.0ßMoreover, we tried to evaluate correctness, especially for KB query and results, by using the entity F1-score of the gold entity.ßdummyTurker
7892ß0.0ßThis unsupervised evaluation metric should represent how correct the generated entities are.ßdummyTurker
7893ß0.0ßHowever, measuring consistency is still very hard.ßdummyTurker
7894ß0.0ßTherefore, as suggested by another reviewer, we are willing to conduct a human evaluation if it is suitable.ßdummyTurker
7895ß0.0ßYou raise a good point.ßdummyTurker
7896ß0.0ßWe will compare Mem2Seq with KVR (Eric et al (2017) model the same data pre-processing method (as we mentioned in our paper, a simplified canonical form is used in their original paper.ßdummyTurker
7897ß0.0ßHowever, with the copy mechanism used be Mem2Seq, we still believe our model can outperform KVR model.ßdummyTurker
7898ß0.0ßWe agree that the MemNN part of our paper is not self-contained enough and will try our best to further explain it in details.ßdummyTurker
7899ß0.0ßThis is a good question.ßdummyTurker
7900ß0.0ßWe will make it clearer in the paper.ßdummyTurker
7901ß0.0ßIn all the dataset we used, the KB information for each dialog is pre-defined by the data provider.ßdummyTurker
7902ß0.0ßTherefore, we do not load the “whole” KB information we have, but only the KB information they provided for the specific dialog.ßdummyTurker
7903ß0.0ßThis is essential because some KB information changes over time and place, like weather and schedule.ßdummyTurker
7904ß0.0ßWe think there are three novel points:(1)Statutes recommendation itself is novel;(2)The idea to recommend so many statutes is new.ßdummyTurker
7905ß0.0ßUsing content-based methods to find close judgment documents to recommend costs too much time.ßdummyTurker
7906ß0.0ßIt results in a oversized dimension in collaborative filtering and excessive categories in classification models;(3)The method can recommend new statutes easily, which just needs to learn again on the original parameters, and as the training data increases, the model will become powerful.ßdummyTurker
7907ß0.0ßThough the models themselves are not new, our paper explores a new way to make accurate recommendations on the original recommendation results.ßdummyTurker
7908ß0.0ßWe utilize SVM classifier to solve the top k problem.ßdummyTurker
7909ß0.0ßIt can be applied to other recommender systems.ßdummyTurker
7910ß0.0ßIn fact, we did topic model compared experiments.ßdummyTurker
7911ß0.0ßWe utilized topic model to select the most similar judgment documents and then recommended the cited statutes.ßdummyTurker
7912ß0.0ßBut it was time-costing and was abandoned because of the dead line.ßdummyTurker
7913ß0.0ßTo our best known, there is only one paper studying predicting statutes.ßdummyTurker
7914ß0.0ßThey dealt with statutes recommendation as a classification problem and used TF-IDF+SVM.ßdummyTurker
7915ß0.0ßSo we mainly compare our method with theirs.ßdummyTurker
7916ß0.0ßJudgment documents often cite some very common statutes.ßdummyTurker
7917ß0.0ßThese statutes are cited by almost 90% judgment documents.ßdummyTurker
7918ß0.0ßSo there are excessive common statutes samples in training data, which causes data imbalance.ßdummyTurker
7919ß0.0ßThe SVM classifier are more likely to bias these data.ßdummyTurker
7920ß0.0ßJust recommending these common statutes can get very high precision.ßdummyTurker
7921ß0.0ßIt is why the TF-IDF+SVM precision is better than ours.ßdummyTurker
7922ß0.0ßBut our method has higher F1 score and recall.ßdummyTurker
7923ß0.0ßAnd the training data is not enough for our model.ßdummyTurker
7924ß0.0ßWe think there are three novel points:(1)Statutes recommendation itself is novel;(2)The idea to recommend so many statutes is new.ßdummyTurker
7925ß0.0ßUsing content-based methods to find close judgment documents to recommend costs too much time.ßdummyTurker
7926ß0.0ßIt results in a oversized dimension in collaborative filtering and excessive categories in classification models;(3)The method can recommend new statutes easily, which just needs to learn again on the original parameters, and as the training data increases, the model will become powerful.ßdummyTurker
7927ß0.0ßThough the models themselves are not new, our paper explores a new way to make accurate recommendations on the original recommendation results.ßdummyTurker
7928ß0.0ßWe utilize SVM classifier to solve the top k problem.ßdummyTurker
7929ß0.0ßIt can be applied to other recommender systems.ßdummyTurker
7930ß0.0ßIn fact, we did topic model compared experiments.ßdummyTurker
7931ß0.0ßWe utilized topic model to select the most similar judgment documents and then recommended the cited statutes.ßdummyTurker
7932ß0.0ßBut it was time-costing and was abandoned because of the dead line.ßdummyTurker
7933ß0.0ßTo our best known, there is only one paper studying predicting statutes.ßdummyTurker
7934ß0.0ßThey dealt with statutes recommendation as a classification problem and used TF-IDF+SVM.ßdummyTurker
7935ß0.0ßSo we mainly compare our method with theirs.ßdummyTurker
7936ß0.0ßThe first experiment is designed to confirm that applying SVM to our statute2vec is better than the original.ßdummyTurker
7937ß0.0ßThe second experiment is not simply exchanged statute2vec with TF-IDF.ßdummyTurker
7938ß0.0ßThese two SVMs are different.ßdummyTurker
7939ß0.0ßThe first is a multi-label classifier.ßdummyTurker
7940ß0.0ßThe second is a binary classifier and used to decide which statute is applicable.ßdummyTurker
7941ß0.0ßAnd their training methods are different.ßdummyTurker
7942ß0.0ßWe use wrong statutes and right statutes in the top k statutes to train the second SVM.ßdummyTurker
7943ß0.0ßJudgment documents often cite some very common statutes.ßdummyTurker
7944ß0.0ßThese statutes are cited by almost 90% judgment documents.ßdummyTurker
7945ß0.0ßSo there are excessive common statutes samples in training data, which cause data imbalance.ßdummyTurker
7946ß0.0ßThe SVM classifier are more likely to bias these data.ßdummyTurker
7947ß0.0ßJust recommending these common statutes can get very high precision.ßdummyTurker
7948ß0.0ßIt is why the TF-IDF+SVM precision is better than ours.ßdummyTurker
7949ß0.0ßBut our method has higher F1 score and recall.ßdummyTurker
7950ß0.0ßAnd the training data is not enough for our model.ßdummyTurker
7951ß0.0ßThank you for mentioning this, we will add the Kappa statistics to the camera-ready version.ßdummyTurker
7952ß0.0ß"""The novelty of the proposed method is limited"":"ßdummyTurker
7953ß0.0ßThe main contribution of this paper is the dataset that we gathered.ßdummyTurker
7954ß0.0ßGathering a dataset is hard and time-consuming task and many researchers and developers can't afford to spend that much time and energy.ßdummyTurker
7955ß0.0ßIn this paper, we gathered a dataset and implemented and tested a high accuracy model.ßdummyTurker
7956ß0.0ßThe machine learning model that is proposed is intended to be an exemplary model for those researchers or developers who want to implement this model for their application.ßdummyTurker
7957ß0.0ßWe also want to publicly share the dataset and source code on GitHub in order for them to use the dataset and source code, so our results will be a huge help for them.ßdummyTurker
7958ß0.0ß"""Although constructing dataset is a time-consuming job, the special character of the Persian should be given"":"ßdummyTurker
7959ß0.0ßWe have mentioned the different characteristics of the Persian language in the paper at line 385.ßdummyTurker
7960ß0.0ßWe have presented a list of features and showed that they will work for the Persian language text readability assessment, and we didn't intend to bring a whole new set of features for the Persian language.ßdummyTurker
7961ß0.0ßThese features will be a helpful reference for researchers and developers who want to implement such models.ßdummyTurker
7962ß0.0ßWe agree with you, we are refining the paper.ßdummyTurker
7963ß0.0ßWe also will trim the related works section in the camera-ready version.ßdummyTurker
7964ß0.0ßSome of this differences are addressed at line 385.ßdummyTurker
7965ß0.0ßWe have used some features that are rarely used in other languages' text readability assessment such as POS model frequency variance.ßdummyTurker
7966ß0.0ßYou can take look at our source code, which is uploaded too.ßdummyTurker
7967ß0.0ßFeatures are separately extracted using some python code and then are fed to the classifier.ßdummyTurker
7968ß0.0ßThank you for your detailed review.ßdummyTurker
7969ß0.0ßWe agree with you.ßdummyTurker
7970ß0.0ßThis details will be added to the camera-ready version.ßdummyTurker
7971ß0.0ßFirst of all, we have recorded each annotator's label separately in the dataset so future users can use their own inter-annotator agreement method.ßdummyTurker
7972ß0.0ßAnd secondly, in our implementation, we have used texts that got an 80 percent agreement on a label.ßdummyTurker
7973ß0.0ßWe tried to show that there is a meaningful relationship between the dataset texts and their labels by showing the high accuracy of multiple classifiers on the dataset.ßdummyTurker
7974ß0.0ßWe didn't mention in the paper that we have explained the easy, medium and hard class to the annotators.ßdummyTurker
7975ß0.0ßWe have explained to them that they should be looking at the number of unfamiliar words, the length of sentences and text, the grammatical complexity and so on.ßdummyTurker
7976ß0.0ßWe also approximately defined each class using the above criteria, so that each user knows what to look for.ßdummyTurker
7977ß0.0ßThank you for mentioning this.ßdummyTurker
7978ß0.0ßThis analysis can be done easily and this information will be added to the camera-ready version.ßdummyTurker
7979ß0.0ßWe agree with you that these features are language independent.ßdummyTurker
7980ß0.0ßIt seems that we didn't clearly express our purpose and we have caused some misunderstandings.ßdummyTurker
7981ß0.0ßit will be corrected in the camera-ready version.ßdummyTurker
7982ß0.0ßIf you consider the feature set that is used for other languages in the field of text readability assessment, some of our features are rarely used in them.ßdummyTurker
7983ß0.0ßIn fact, they are some commonly used features but not for text readability assessment.ßdummyTurker
7984ß0.0ßFor example, you can rarely see the use of POS model frequency variance in the text readability assessment field but this feature is important for the Persian language.ßdummyTurker
7985ß0.0ßWe agree with the standardness of our machine learning approach.ßdummyTurker
7986ß0.0ßThe purpose of our paper is first to introduce a Persian dataset for text readability assessment for NLP researchers.ßdummyTurker
7987ß0.0ßAnd second to bring a high accuracy model for developers and researchers who need a text readability module in their work.ßdummyTurker
7988ß0.0ßFor example, a researcher who is working on a text simplification system needs a text readability module and our dataset and source code can be really helpful.ßdummyTurker
7989ß0.0ßWe didn't mention in the paper that we have explained the easy, medium and hard class to the annotators.ßdummyTurker
7990ß0.0ßWe have explained to them that they should be looking at the number of unfamiliar words, the length of sentences and text, the grammatical complexity and so on.ßdummyTurker
7991ß0.0ßWe also approximately defined each class using the above criteria, so that each user knows what to look for.ßdummyTurker
7992ß0.0ßWe will add this information to the camera-ready version.ßdummyTurker
7993ß0.0ßWe have mentioned this in table 1 and in line 368.ßdummyTurker
7994ß0.0ßWe choose texts which more than 80 percent of their labels were the same.ßdummyTurker
7995ß0.0ßwe will mention this in the camera-ready version.ßdummyTurker
7996ß0.0ß"""The authors do not provide enough details that convince the reader that the collection is reliable and reusable"""ßdummyTurker
7997ß0.0ßWe agree with you, one of the main reasons that we have created models using multiple classifiers was to show that the dataset is usable and there is a meaningful relationship between the labels and texts.ßdummyTurker
7998ß0.0ßYour idea is really interesting but Persian language agglutinativeness is not tokenizable because the agglutinated word has a different meaning and usage and its hardness can't be measured by looking at the separated parts so tokenization can have no effect.ßdummyTurker
7999ß0.0ßA professional is proofreading the paper.ßdummyTurker
8000ß0.0ßThe importance of our work is that there are no prior datasets or models available for Persian text readability assessment and our work can be helpful for Persian NLP community.ßdummyTurker
8001ß0.0ßThank you for your comments and careful review.ßdummyTurker
8002ß0.0ßActually, the relation extraction method for Weston is the same as HRERE-basic, and was included for a fair comparison.ßdummyTurker
8003ß0.0ßThe only difference between them is whether to use KBE or not.ßdummyTurker
8004ß0.0ßIn addition, the improvement brought by HRERE-full over HRERE-base is quite significant according to Table 2: 5.26% for P@10%, 9.27% for P@30% and 12.19% for P@50% (averaging 8.9% of the three).ßdummyTurker
8005ß0.0ßWe decided to plot them separately for increased readability, but will attempt to plot all methods in a single figure as suggested.ßdummyTurker
8006ß0.0ßWe agree that our method is simple.ßdummyTurker
8007ß0.0ßHowever, it is also the first to unify the learning of linguistic relations (relation extraction) with the learning of knowledge relations (knowledge base embeddings) in a principled way within the same neural training framework.ßdummyTurker
8008ß0.0ßAs pointed out by reviewer#1 our approach is very flexible, and we believe it can be further improved as well as applied in other NLP tasks where one learner can nudge another away from overfitting.ßdummyTurker
8009ß0.0ßThank you for your detailed and careful review.ßdummyTurker
8010ß0.0ßWe have addressed your comments and performed a thorough review of the text.ßdummyTurker
8011ß0.0ßYes, HRERE-naive does not uses $J_D$ (eq.ßdummyTurker
8012ß0.0ß15).ßdummyTurker
8013ß0.0ßAll your comments have been addressed.ßdummyTurker
8014ß0.0ßYes, it’s supposed to link to the following paragraph.ßdummyTurker
8015ß0.0ßWe will reword this part to make it clear in our paper.ßdummyTurker
8016ß0.0ßIn fact we have tested with $\alpha=0.0$, but the performance is extremely poor (falling out of the chart, in fact).ßdummyTurker
8017ß0.0ßWe have added a mention to that in the paper.ßdummyTurker
8018ß0.0ßFixed.ßdummyTurker
8019ß0.0ß“Conclusion could be more substantial and explanatory.”ßdummyTurker
8020ß0.0ßWe agree that the conclusion section is brief, which is due to space reasons.ßdummyTurker
8021ß0.0ßWe are happy to extend it in the final version of the paper, as the current conclusion very much focuses on one angle (tri-training), which is only one of the aspects we studied in this work.ßdummyTurker
8022ß0.0ß“It is claimed that the proposed multi-task tri-training is more efficient in time and space than the original tri-training.ßdummyTurker
8023ß0.0ßIs it possible to expand on that and give numbers?”ßdummyTurker
8024ß0.0ßThank you for the question.ßdummyTurker
8025ß0.0ßIn brief, multi-task tri-training (MT-Tri) is ~3x more space-efficient than regular tri-training (Tri) as tri-training stores one set of parameters for each of the three models, while we only store one set of parameters (we use three output layers, but these make up a comparatively small part of the total parameter budget).ßdummyTurker
8026ß0.0ßRegarding time-efficiency, tri-training first requires to train each of the models from scratch and then requires to run tri-training, which takes about the same time as training from scratch and requires a separate forward pass for each model, effectively training three independent models at the same time.ßdummyTurker
8027ß0.0ßIn contrast, multi-task tri-training only requires one forward pass as well as the evaluation of the two additional output layers (which takes a negligible amount of time) and requires about as many epochs as tri-training for convergence (see Table 3, second column).ßdummyTurker
8028ß0.0ßIn our experiments, MT-Tri thus trained about 5-6x faster than traditional tri-training.ßdummyTurker
8029ß0.0ßThese are good points, we will elaborate on it in the camera-ready version.ßdummyTurker
8030ß0.0ß“Although the contributions are interesting and useful, there is no major contribution.”ßdummyTurker
8031ß0.0ßThank you for all your useful comments and praising our paper, emphasizing that it is also a really good survey.ßdummyTurker
8032ß0.0ßRather than focusing on just one method, we wanted to study the contribution of several methods in the light of a large body of existing methods---both recent and traditional---to obtain a better understanding of how novel methods---including our own---fare.ßdummyTurker
8033ß0.0ßWe believe that reclaiming traditional approaches and studying methods in this context and from a diverse set of angles provides more insight to the community than proposing a new method that outperforms existing approaches on one dataset.ßdummyTurker
8034ß0.0ß“The authors fail to address the asymmetry in the results between some of the domain transfer pairs.ßdummyTurker
8035ß0.0ßE.g.ßdummyTurker
8036ß0.0ßin Fig.ßdummyTurker
8037ß0.0ß2ßdummyTurker
8038ß0.0ßThis is a great question!ßdummyTurker
8039ß0.0ßWe guess that this has to do with properties of the data and that the domains are relatively far apart, e.g., in terms of A-distance (Blitzer et al., 2007), thus some asymmetry is to be expected.ßdummyTurker
8040ß0.0ßIn fact, asymmetry in these domains is already reflected in the results of the original SCL paper (Blitzer et al., 2007), and is corroborated in the results for asymmetric tri-training (Saito et al., 2017) and our method.ßdummyTurker
8041ß0.0ßWe will elaborate on this in the final version of the paper.ßdummyTurker
8042ß0.0ßBlitzer, J., Dredze, M., & Pereira, F. (2007).ßdummyTurker
8043ß0.0ßBiographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.ßdummyTurker
8044ß0.0ßAnnual Meeting-Association for Computational Linguistics, 45(1), 440. https://doi.org/10.1109/IRPS.2011.5784441ßdummyTurker
8045ß0.0ßSaito, K., Ushiku, Y., & Harada, T. (2017).ßdummyTurker
8046ß0.0ßAsymmetric Tri-training for Unsupervised Domain Adaptation.ßdummyTurker
8047ß0.0ßIn ICML 2017.ßdummyTurker
8048ß0.0ßRetrieved from http://arxiv.org/abs/1702.08400ßdummyTurker
8049ß0.0ßYes, our approach is inspired by semantic parsing works that utilize grammar and KB schema to generate logical forms.ßdummyTurker
8050ß0.0ßThis work differs from them in that SQL grammar is used here.ßdummyTurker
8051ß0.0ßCompared to slot-filling based models that restrict target SQL queries to fixed forms of “select-aggregator-where”, our model is less tailored.ßdummyTurker
8052ß0.0ßWe believe that it is easy to expand our model to generate nested SQL queries or JOIN clauses, which could also be easily trained with back-propagation if enough training instances of these SQL types are available.ßdummyTurker
8053ß0.0ßFor example, we could incorporate a hierarchical “value” channel to handle nest queries.ßdummyTurker
8054ß0.0ßLet us suppose our decoder works horizontally that next generated token is at the right hand of the current token.ßdummyTurker
8055ß0.0ßInspired by Chunk-based Decoder for Neural Machine Translation (Ishiwatari et al.ßdummyTurker
8056ß0.0ß2017, ACL), we could increase the depth of the “value” channel to generates tokens of a nested WHERE value along the vertical axis.ßdummyTurker
8057ß0.0ßDuring inference, an addition gating function might be necessary to determine whether to generate a nested query, followed by the generation of WHERE value.ßdummyTurker
8058ß0.0ßAn intuitive way that extends our model to handle JOIN clauses is to add the 4th channel, which predicts a table from a collection of tables.ßdummyTurker
8059ß0.0ßTherefore, the decoder should learn to select one of the four channels at each time step.ßdummyTurker
8060ß0.0ßAccordingly, we need to add “from” as a new SQL keyword in order to generate SQL queries including “from xxxTable”.ßdummyTurker
8061ß0.0ßThanks.ßdummyTurker
8062ß0.0ßSupposing the question is “How many schools did player number 3 play at?” and the SQL query is “Select count School\Club Team where No.ßdummyTurker
8063ß0.0ß= 3”.ßdummyTurker
8064ß0.0ßWe could see that the column names ‘School\Club Team’ and ‘No.’ are different from their corresponding utterances ‘schools’, ‘number’ in natural language question.ßdummyTurker
8065ß0.0ßWe will add this example.ßdummyTurker
8066ß0.0ß"""the number of question words embodied in the cell"" means the number of cell words occurring in the question."ßdummyTurker
8067ß0.0ßCompared to slot-filling based models that restrict target SQL queries to fixed forms of “select-aggregator-where”, our model is less tailored.ßdummyTurker
8068ß0.0ßWe believe that it is easy to expand our model to generate nested SQL queries or JOIN clauses, which could also be easily trained with back-propagation if enough training instances of these SQL types are available.ßdummyTurker
8069ß0.0ßFor example, we could incorporate a hierarchical “value” channel to handle nest queries.ßdummyTurker
8070ß0.0ßLet us suppose our decoder works horizontally that next generated token is at the right hand of the current token.ßdummyTurker
8071ß0.0ßInspired by Chunk-based Decoder for Neural Machine Translation (Ishiwatari et al.ßdummyTurker
8072ß0.0ß2017, ACL), we could increase the depth of the “value” channel to generates tokens of a nested WHERE value along the vertical axis.ßdummyTurker
8073ß0.0ßDuring inference, an addition gating function might be necessary to determine whether to generate a nested query, followed by the generation of WHERE value.ßdummyTurker
8074ß0.0ßAn intuitive way that extends our model to handle JOIN clauses is to add the 4th channel, which predicts a table from a collection of tables.ßdummyTurker
8075ß0.0ßTherefore, the decoder should learn to select one of the four channels at each time step.ßdummyTurker
8076ß0.0ßAccordingly, we need to add “from” as a new SQL keyword in order to generate SQL queries including “from xxxTable”.ßdummyTurker
8077ß0.0ßIn terms of the syntax of SQL, the grammar we used in this work could be regarded as shallow syntax, such as three channels and column-cell relation.ßdummyTurker
8078ß0.0ßWe do not use deep syntax, such as the sketch of SQL language utilized in some slot-filling models, because incorporating them would make the model clumpy.ßdummyTurker
8079ß0.0ßInstead, we let the model to learn the sequential and compositional relations of SQL queries automatically from data.ßdummyTurker
8080ß0.0ßEmpirical results show that our model learns these patterns well.ßdummyTurker
8081ß0.0ßWe do this additional experiment to test whether the learnt model from WikiSQL would perform well on another semantic parsing dataset.ßdummyTurker
8082ß0.0ßDespite the numbers are unsurprisingly lower than state-of-the-art model due to the difference between the two datasets, data augmentation brings improvement.ßdummyTurker
8083ß0.0ßWe also believe that leveraging domain adaptation for semantic parsing is a promising direction to work on.ßdummyTurker
8084ß0.0ßThe vector of each cell is calculated with GRU RNN, the inputs of which are words in cell.ßdummyTurker
8085ß0.0ßThe last hidden state is regarded as cell vector.ßdummyTurker
8086ß0.0ßThese cell vectors are not only used for predicting WHERE values in the “value” channel, they are also used to enhance column vector through weighted sum.ßdummyTurker
8087ß0.0ßThe weights of cells are question-aware, which are calculated based on the word occurrence between cell words and question words.ßdummyTurker
8088ß0.0ßIn this short paper, our main contribution is to propose an effective Ancient-Modern Chinese clause alignment approach and build a large high-quality Ancient-Modern Chinese dataset.ßdummyTurker
8089ß0.0ßIn addition, we report the performances of the SMT model and various NMT based models on the dataset.ßdummyTurker
8090ß0.0ßAs for proposing a new model to tackle this task, we leave it as our future work.ßdummyTurker
8091ß0.0ßThe Ancient-Modern Chinese has some characteristics that are different from other language pairs.ßdummyTurker
8092ß0.0ßFor example, ancient and modern Chinese are both written in Chinese characters, but ancient Chinese is highly concise and its syntactical structure is different from modern Chinese.ßdummyTurker
8093ß0.0ßThe traditional methods do not take these characteristics into account.ßdummyTurker
8094ß0.0ßBased on the idea of lexical-based and statistical-based approaches, we proposed some new measuring factors (eq.ßdummyTurker
8095ß0.0ß1 and 3) for this task.ßdummyTurker
8096ß0.0ß"The concept of ""Ancient Chinese"" in this paper almost refers to the cultural/historical notion of 'literary Chinese' (文言文)."ßdummyTurker
8097ß0.0ßIn order to ensure that the article style approximate and representative, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 200BC- 1000BC) and articles from celebrities of that era.ßdummyTurker
8098ß0.0ßThey use plain and accurate words to express what happened at that time, thus ensure the generality of the translated materials.ßdummyTurker
8099ß0.0ßThe clause is different from the sentence in Chinese, because a sentence may be split into multiple clauses.ßdummyTurker
8100ß0.0ßIn the experiment, a sentence had been split into clauses when we meet comma, semicolon, period or exclamation mark, so clause alignment is more fine-grained than sentence alignment.ßdummyTurker
8101ß0.0ßAs described in the 169 to 175 lines of our paper, we considered 6 align modes which cover 99.4% sentence disorder samples in the test set.ßdummyTurker
8102ß0.0ßIn the experiment, we found that proper nouns could bring some difficulties to translation.ßdummyTurker
8103ß0.0ßOur next work will focus on this issue.ßdummyTurker
8104ß0.0ß"The concept of ""Ancient Chinese"" in this paper almost refers to the cultural/historical notion of 'literary Chinese' (文言文)."ßdummyTurker
8105ß0.0ßMore specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 200BC-1000BC) and articles written by celebrities of that era.ßdummyTurker
8106ß0.0ßBecause of China's respect for traditional culture, there are some professional websites that allow ancient literature lovers to obtain more relevant information on ancient literature.ßdummyTurker
8107ß0.0ßThus, we search the articles from websites such as http://www.gushiwen.org/ and http://wyw.5156edu.com/.ßdummyTurker
8108ß0.0ßAfter manually cleaning the unaligned bilingual articles, we got an article-aligned corpus and the work of paragraph alignment is manually completed.ßdummyTurker
8109ß0.0ßBoth of the paper mentioned in this part experiment on the small (the dataset in [1] only involves 205 Chinese Ancient-Modern paragraph pairs, the dataset in [2] only involves one history book), non-public datasets.ßdummyTurker
8110ß0.0ßThe scale of data sets above is actually not enough for machine translation.ßdummyTurker
8111ß0.0ßBefore submitting this paper, we still did not find the dataset larger than ours in the existing work in our limited knowledge.ßdummyTurker
8112ß0.0ßThanks for your advice, we will add more details about the existing work in Ancient-Modern Chinese MT and data sets in the next version.ßdummyTurker
8113ß0.0ßWe will present some examples in the appendix of the next version.ßdummyTurker
8114ß0.0ßIn addition, we will release the dataset and source code upon acceptance.ßdummyTurker
8115ß0.0ß"Yes, ""exact match"" is better, we will consider to use this word in the next version."ßdummyTurker
8116ß0.0ßThe Ancient-Modern Chinese has some characteristics that are different from other language pairs.ßdummyTurker
8117ß0.0ßFor example, Ancient and Modern Chinese are both written in Chinese characters, but ancient Chinese is highly concise and its syntactical structure is different from modern Chinese.ßdummyTurker
8118ß0.0ßThe traditional methods do not take these characteristics into account.ßdummyTurker
8119ß0.0ßBased on the idea of lexical-based and statistical-based approaches, we proposed some new measuring factors (eq.ßdummyTurker
8120ß0.0ß1 and 3) for this task.ßdummyTurker
8121ß0.0ßWe used the Jieba Chinese text segmentation which is a popular Python-based Chinese word segmentation module (https://github.com/fxsjy/jieba).ßdummyTurker
8122ß0.0ßIt is the third step, we will correct this typo.ßdummyTurker
8123ß0.0ßThank you for your suggestion, we plan to reorganize our paper and provide more details about the dataset in the camera-ready version.ßdummyTurker
8124ß0.0ßThe dictionary we used come from the website http://guwen.xiexingcun.com/, which collects explanations of ancient Chinese characters in classical Chinese, poetry, and lexicon.ßdummyTurker
8125ß0.0ß"The concept of ""Ancient Chinese"" in this paper almost refers to the cultural/historical notion of 'literary Chinese' (文言文)."ßdummyTurker
8126ß0.0ßMore specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 200BC- 1000BC) and articles written by celebrities of that era.ßdummyTurker
8127ß0.0ßThey use plain and accurate words to express what happened at that time, thus ensure the generality of the translated materials.ßdummyTurker
8128ß0.0ßThe work of paragraph alignment is manually completed.ßdummyTurker
8129ß0.0ßWe will release the dataset and source code upon acceptance.ßdummyTurker
8130ß0.0ßAs we wrote in the 370 line of our paper, the size of the data set before the augmentation is 0.46M.ßdummyTurker
8131ß0.0ßThe size of augmented data is 1.02M.ßdummyTurker
8132ß0.0ßWe found that the augmented data can provide more alignment information for the translation model and increase the generalization ability of the model.ßdummyTurker
8133ß0.0ßThanks for your advice, we did the experiment as soon as we saw it.ßdummyTurker
8134ß0.0ßWe used an SMT model (add aug) and an best NMT model to complete the experiment.ßdummyTurker
8135ß0.0ßBy using the Moses tool (bootstrap-hypothesis-difference-significance.pl) and in 95% confidence interval, we found that their BLEU scores are almost the same (only 0.003 difference), but their NIST scores are different (NMT model is 0.3 higher than SMT model).ßdummyTurker
8136ß0.0ßWe will add more experimental results and details about this part in the next version.ßdummyTurker
8137ß0.0ßThe paper https://arxiv.org/abs/1803.01557 was published later than our submission, we didn't know this paper before.ßdummyTurker
8138ß0.0ßThanks for your advice on the references.ßdummyTurker
8139ß0.0ßWe will add the references and talk more about the difference between them and our proposed approach in the following version of this manuscript.ßdummyTurker
8140ß0.0ßThanks for your careful review.ßdummyTurker
8141ß0.0ß"""the necessary"" is a writing error."ßdummyTurker
8142ß0.0ß"It should be ""the necessity""."ßdummyTurker
8143ß0.0ßAs you known, Amazon Product Review is a benchmark dataset for domain adaptation.ßdummyTurker
8144ß0.0ßMany published works have tested their approaches on this dataset.ßdummyTurker
8145ß0.0ßAnd most of them used the bag-of-words unigrams and bigrams feature.ßdummyTurker
8146ß0.0ßWe follow this protocol to make it comparable to previous works.ßdummyTurker
8147ß0.0ßWe agree that the mean value and standard deviation may disturb the reader.ßdummyTurker
8148ß0.0ßWe will perform significance test between the proposed model and its most comparative baseline (e.g., CoCMD v.s.ßdummyTurker
8149ß0.0ßCMD-ft for B->D task).ßdummyTurker
8150ß0.0ßWe hope that the significance test can ease your burden to get more intuitive information from the result table.ßdummyTurker
8151ß0.0ßThe similar method may be applied in computer vision, but to the best of our knowledge, we have not seen the idea used in NLP area.ßdummyTurker
8152ß0.0ßFrom the perspective of application, we  just apply the GAN into standard models.ßdummyTurker
8153ß0.0ßHowever, different from previous papers, we propose a new idea that generates embeddings directly as well.ßdummyTurker
8154ß0.0ßGenerally speaking,  the method we propose is more effective in the relatively smaller dataset.ßdummyTurker
8155ß0.0ßIt is mainly because when the dataset is large, the standard model can extract enough features.ßdummyTurker
8156ß0.0ßBut when the dataset is small, some features are missing and the GAN part facilitates generating those missing features.ßdummyTurker
8157ß0.0ßWe mentioned the CNN baseline in Section 4.3, the structure is the same with (Kim, 2014).ßdummyTurker
8158ß0.0ßThe model consists of a convolutional layer and a max-pooling layer.ßdummyTurker
8159ß0.0ßSome parameters (like tne number of filters, the size of filters) are shown in the Section 4.3.ßdummyTurker
8160ß0.0ßThe parameters of GAN-CNN are the same with CNN except the GAN part.ßdummyTurker
8161ß0.0ßGenerally speaking, the method we propose is more effective in the relatively smaller dataset.ßdummyTurker
8162ß0.0ßThe results can be found from Table 2-4.ßdummyTurker
8163ß0.0ßThe possible reasons please refer to Reviewer#1(3).ßdummyTurker
8164ß0.0ßWe hold that the CNN or GRU is effective enough, as shown in many papers.ßdummyTurker
8165ß0.0ßWhat we want to show is that our proposed  method  is more effective than standard model.ßdummyTurker
8166ß0.0ßTherefore, we only choose the commonest model in the text classification.ßdummyTurker
8167ß0.0ßThe suggestion of more datasets is  reasonable, we will experiment on more datasets in the next version.ßdummyTurker
8168ß0.0ßThe proportion=0.2, 0.4, ... denotes the proportion of dataset we used in the experiment respectively, which is randomly divided.ßdummyTurker
8169ß0.0ßIt was mentioned in the Section 4.1.ßdummyTurker
8170ß0.0ßSince the limited width of paper, we used the decimal instead, which is the same with the percentage in Section 4.1.ßdummyTurker
8171ß0.0ßThank you for reminding.ßdummyTurker
8172ß0.0ßWe will cite them in the next version.ßdummyTurker
8173ß0.0ßIt is proper to consider more evaluation metrics to fully evaluate the model.ßdummyTurker
8174ß0.0ßSince some papers mentioned in Section 2.2 (Wang et al., 2017; Ma et al., 2017) only take accuracy as the evaluation metric, therefore, we only consider accuracy like other papers.ßdummyTurker
8175ß0.0ßWe thank the reviewers for their valuable feedback.ßdummyTurker
8176ß0.0ßWe are aware of the mentioned review paperßdummyTurker
8177ß0.0ßThe challenge corpus is mainly released with visual and audio information only, and the test set used in the challenge was never released.ßdummyTurker
8178ß0.0ßWe have added text information by requesting the challenge organisers.ßdummyTurker
8179ß0.0ßThe transcriptions were obtained by using a professional human transcription service (http://www.rev.com) to ensure maximum quality for the ground truth annotations.ßdummyTurker
8180ß0.0ßWe thank the reviewers for their valuable feedback.ßdummyTurker
8181ß0.0ß1.ßdummyTurker
8182ß0.0ß[Evaluation] and 4.ßdummyTurker
8183ß0.0ß[Novelty] The challenge corpus is mainly released with visual and audio information, and the test set used in the challenge was never released.ßdummyTurker
8184ß0.0ßThe challenge also did not concentrate on multimodal analysis, but mostly on video analysis, with the same methods often applied to audio without adaptation.ßdummyTurker
8185ß0.0ßWe have also included text information by sending request to the chalenge organisers to obtain the transcriptions.ßdummyTurker
8186ß0.0ßThe main aim of our research is to investigate the effect of adding different modalities in the personality recognition task, where this analysis was never performed before.ßdummyTurker
8187ß0.0ß2.ßdummyTurker
8188ß0.0ß[Video] We have selected a random video frame in order to consider only appearance effect in personality recognision, and not movement information.ßdummyTurker
8189ß0.0ßThere is also very little movement in the videos since they most portrait video-bloggers with a fixed posture.ßdummyTurker
8190ß0.0ß3.ßdummyTurker
8191ß0.0ß[Suitability to ACL] We decided to submit this paper to ACL as speech (together with language, analyzed in the trancriptions) and multimodality were topics mentioned in the call for papers.ßdummyTurker
8192ß0.0ß5.ßdummyTurker
8193ß0.0ß[grammatical errors and typos].ßdummyTurker
8194ß0.0ßIn Fig.ßdummyTurker
8195ß0.0ß1, after concateneting layers with 64, 64, and 512 outputs the output is of dimension 64+64+512=640.ßdummyTurker
8196ß0.0ßIt may require finishing the whole sequence to know that the policy BABABA can be a good choice.ßdummyTurker
8197ß0.0ßSequence level training such as MIXER (Ranzato et al., 2016) is more suitable to deal with this case.ßdummyTurker
8198ß0.0ßOur method belongs to step-level training and most step-level training methods are not proposed to deal with this kind of cases.ßdummyTurker
8199ß0.0ßWhen facing the oracle actions are ABABABA, our method prefers to constrain the model to give the answer ABABABA.ßdummyTurker
8200ß0.0ßRather than exploring other good policy out of the training data, our motivation is to make our trained model also fit the training data well when working in the mode of DoP, as it would have to work in this mode during inference.ßdummyTurker
8201ß0.0ßIn this way, we alleviate the impact of the change in working mode at inference which is the issue of exposure bias.ßdummyTurker
8202ß0.0ßOur method may not perform well facing with this kind of whole sequence level case, so we try to combine with other sequence-level training approaches.ßdummyTurker
8203ß0.0ßIn page 8 of our paper, our experiments showed that the performance can be further improved by combing other sequence-level training approaches with our approach.ßdummyTurker
8204ß0.0ßCurriculum learning is one of the auxiliary learning approaches to help model training under our proposed framework.ßdummyTurker
8205ß0.0ßOur framework also works under the parasitic Learning approach, where the regularizer is applied at the beginning.ßdummyTurker
8206ß0.0ßBesides, DAD(scheduled sampling) as we mentioned in the paper followed the ideas from Dagger (Ross et al.ßdummyTurker
8207ß0.0ß2011).ßdummyTurker
8208ß0.0ßBoth of DAgger and DAD introduce curriculum learning to help training.ßdummyTurker
8209ß0.0ß"Please see the ""Reply to weakness argument 2"" for Review#2."ßdummyTurker
8210ß0.0ßThank you very much!ßdummyTurker
8211ß0.0ß(Wiseman and Rush, 2016) proposed to extend sequence prediction model to learn global sequence scores, which can be an alternative to MIXER.ßdummyTurker
8212ß0.0ßThis kind of sequence-level training methods always requires the word-level training as an auxiliary.ßdummyTurker
8213ß0.0ßIn Section 5.4 of our paper, experiments showed that the performance can be further improved by combining other sequence-level training approaches such as MIXER with our approach.ßdummyTurker
8214ß0.0ßIn (Wiseman and Rush, 2016), the results of XENT and DAD on the IWLST dataset is referenced from (Ranzato et al., 2016) directly.ßdummyTurker
8215ß0.0ßBesides, its model structure differed from that in (Ranzato et al., 2016).ßdummyTurker
8216ß0.0ßWe use the codes identical to (Ranzato et al., 2016), which are provided by the paper itself, and the experimental results are shown in Table 4.ßdummyTurker
8217ß0.0ßAs we mentioned at the beginning of Section 5, we use a held-out validation set to search over the values of hyper-parameters.ßdummyTurker
8218ß0.0ßFor example, for the hyperparameter \epsilon_t, we carefully try different schedule listed in the (Bengio et al., 2015), and finally use a linear decay schedule for \epsilon_t for curriculum learning, and run grid-search on the proper decay rate using a validation set.ßdummyTurker
8219ß0.0ßDue to the limitation of pages, we didn’t describe more details about the model architecture, but we quoted the related paper using the same structure.ßdummyTurker
8220ß0.0ßBesides, we provided the codes and hyper-parameters of the experiments in the supplementary material.ßdummyTurker
8221ß0.0ßAs page 2 in our paper, we point out the limitation of DAD, that forcing the model to predict the ground truth token in the mode of DoP during training may lead to a possibly incorrect sequence and is an indirect way to reduce the variance of model brought by the input distribution mismatch.ßdummyTurker
8222ß0.0ßDifferently, we directly constrain the distance in the outputs of the same model in these two modes (DoP and DoG), while continuously optimizing the standard log-likelihood of DoG.ßdummyTurker
8223ß0.0ßIn this way, both the bias and variance of the model are reduced.ßdummyTurker
8224ß0.0ßBesides, our proposed framework is proved able to achieve improvements without curriculum learning approach.ßdummyTurker
8225ß0.0ßThe difference between DoG and DoP brings an obvious input distribution mismatch for decoder part between training and inference.ßdummyTurker
8226ß0.0ßThis input distribution mismatch could be seen as a special covariate shift that only the input distribution changes.ßdummyTurker
8227ß0.0ßIn this paper, we try to reduce the variance of model brought by this input distribution mismatch and help the model perform better at inference.ßdummyTurker
8228ß0.0ßThe main idea of our method is to measure the gap between one model’s behaviors in DoP and DoG and add it to the loss as a regularization.ßdummyTurker
8229ß0.0ßIt is important to note that in our framework, DoP and DoG share all parameters, and we keep optimizing the result of DoG using standard log-likelihood loss.ßdummyTurker
8230ß0.0ßBased on this, the gap is used to regulate the optimization process of DoG to avoid the model overfitting in the mode of DoG, which is the reason why we name it exposure bias regularization.ßdummyTurker
8231ß0.0ßFor the measure of the gap, we consider comparing their outputs.ßdummyTurker
8232ß0.0ßTheoretically, the constraint on the distance of two outputs is equal to reduce the variance in predictions from the different input distributions caused by the change of mode.ßdummyTurker
8233ß0.0ßSince the output of the sequence generation model is discrete and it is hard to directly optimize the distance between tokens, we chose to minimize the distance of two logits outputs, following the ideas from (Zhou et al., 2017; Ba and Caruana, 2014).ßdummyTurker
8234ß0.0ßOur work is motivated by the exposure bias problem in the sequence prediction model, which is a common issue in the current neural model for sequence prediction.ßdummyTurker
8235ß0.0ßTherefore, we pay more attention to whether our methods can achieve improvements from this kind of models.ßdummyTurker
8236ß0.0ßBesides, our experiments is implemented based on the open source code OpenNMT which keeps updating their codes to maintain state-of-the-art accuracy and we use a held-out validation set to search over the values of hyper-parameters.ßdummyTurker
8237ß0.0ßWe run grid search on hyper-parameters using a validation set and report the results on the test set.ßdummyTurker
8238ß0.0ßAlthough the improvement is not particularly significant(24.57(EB-reg) vs 24.15(DAD) vs 23.35(base)), we achieve improvements on the three tasks, which can verify the effectiveness of our methods.ßdummyTurker
8239ß0.0ßWe believe that with the detailed performance evaluation provided in Table 2, the effectiveness of the features and overall framework can be appreciated by readers.ßdummyTurker
8240ß0.0ßWe compared with conventional statistical methods which are not able to model linguistic phenomena prevalent in opinionated textual content.ßdummyTurker
8241ß0.0ßWe agree that there are other qualitative methods that can be employed to show the effectiveness and quality of the features, so we are working on an extended version of this paper to address this important aspect of the research.ßdummyTurker
8242ß0.0ßFurthermore, the purpose of the gender-based pattern analysis was to briefly present a use-case showcasing the type of analysis that is possible with the emotion lexicon, which is mainly intended for researchers interested in adopting our resources for their own analysis.ßdummyTurker
8243ß0.0ßOverall, we strongly believe that the pattern-based emotion lexicon is an important contribution to the emotion recognition research and other related domains as the reviewer help to point out.ßdummyTurker
8244ß0.0ßThe main motivation behind the graph-based extraction method is to discover latent, emotion-rich patterns that are ignored by conventional statistical methods (e.g., n-grams and bag of words) and other methods that rely on hand-crafted linguistic rules.ßdummyTurker
8245ß0.0ßIn comparison to statistical approaches, our dynamic method captures syntactic patterns that can properly model linguistic phenomena found in the opinionated text, such as character repetitions in words, shorthand, slang, misspelling, etc.ßdummyTurker
8246ß0.0ßThrough structural description, graph-based methods are more efficient in capturing word co-occurrence and word contiguity than the conventional statistical approaches.ßdummyTurker
8247ß0.0ßIn addition, graph-based methods are more flexible in that they allow for various sizes of representation as opposed to fixed sizes of statistical methods.ßdummyTurker
8248ß0.0ßKindly note that in Table 4 we provided results obtained from combining the enriched pattern-based features with a Bi-GRNN model.ßdummyTurker
8249ß0.0ßThe results obtained with the Bi-GRNN model are similar to the DeepEmo model, which helps to show that the improvements are not only dependent on the multilayered CNN model.ßdummyTurker
8250ß0.0ßWe also obtained comparable performance with other conventional machine learning models (e.g., SVM and Multinomial naive Bayes) though not reported.ßdummyTurker
8251ß0.0ßWe believe deep learning offers tremendous opportunities in affective computing, therefore, we focused on demonstrating its applicability to the problem.ßdummyTurker
8252ß0.0ßWe overlooked clarifying this part in the experiment but we believe this can be addressed with minor modification of the manuscript.ßdummyTurker
8253ß0.0ßWe agree that the overall readability and flow of the paper can be improved by strengthening the motivation behind specific design choices and experiments.ßdummyTurker
8254ß0.0ßBriefly speaking, the graph-based extraction mechanism was adopted because we aimed to efficiently model informality prevalent in opinionated content.ßdummyTurker
8255ß0.0ßThroughout the experiments, we show that the proposed method is a more reliable approach for emotion recognition than several conventional statistical methods and that the emotion lexicon provided can be used for further emotion analysis and interpretation.ßdummyTurker
8256ß0.0ßSimilar to Weakness 2, we believe this can be addressed with minor modification of the manuscript.ßdummyTurker
8257ß0.0ßWe would like to clarify that the dataset used for feature extraction differs from the dataset used for training.ßdummyTurker
8258ß0.0ßIn other words, up to the “Basic Pattern Extraction” section two datasets were used (subjective tweets S and objective tweets O).ßdummyTurker
8259ß0.0ßA separate distinct dataset was collected (via noisy labels) and used for weighing the patterns and training the models.ßdummyTurker
8260ß0.0ßWe chose this approach mainly to avoid any bias or dependencies that may arise from the datasets used for feature extraction.ßdummyTurker
8261ß0.0ßAs it stands, the generated emotion lexicon (patterns) can be directly used for training emotion classifiers and can also be reweighted using a distinct dataset.ßdummyTurker
8262ß0.0ßWe enriched the patterns via a word embedding approach.ßdummyTurker
8263ß0.0ßAll of these capabilities show that the patterns are somewhat flexible, portable, and adaptable.ßdummyTurker
8264ß0.0ßAt the time of preparing the manuscript, we were only able to report the F1 scores obtained by the test sets used in each respective method.ßdummyTurker
8265ß0.0ßThis is due to the unavailability of resources (e.g., lexicons and different emotion corpora) which were required to reproduce the methods.ßdummyTurker
8266ß0.0ßTo offer a fair comparison, we modified our emotion model to compare directly with their results.ßdummyTurker
8267ß0.0ßRecent related works also used a similar approach for comparison and have clarified this issue.ßdummyTurker
8268ß0.0ßAs mentioned in our paper, going forward, we aim to alleviate these issues by providing public access to all datasets, lexicons, and code for reproducibility of all experiments and analysis.ßdummyTurker
8269ß0.0ßYou are right.ßdummyTurker
8270ß0.0ßEmploying arguments for event detection is not a novel idea, which has used in previous work (e.g., Hong et al., 2011; Li et al., 2013).ßdummyTurker
8271ß0.0ßLiu’s model used annotated arguments to train attention mechanism and it only capture existed combination of trigger and arguments, ignoring other possible combination of triggers and arguments.ßdummyTurker
8272ß0.0ßCompared with Liu et al.ßdummyTurker
8273ß0.0ß(2017), our method of argument prediction can enumerate all possible arguments and provide more argument semantic information.ßdummyTurker
8274ß0.0ßBesides, we will add Liu’s work to the related work and give the difference.ßdummyTurker
8275ß0.0ßThe authors should compare the proposed model with Liu’s method.ßdummyTurker
8276ß0.0ßAnswer: Due to the performance of Liu’s method is lower than HNN (71.7 vs 73.4 in F1-score), which is a baseline in our paper, we did not use it as a baseline.ßdummyTurker
8277ß0.0ßwe used python programming language and NLTK, We will add these to our revised version.ßdummyTurker
8278ß0.0ßWe first propose a method to use predicted candidate arguments to a Hybrid neural network to detect events.ßdummyTurker
8279ß0.0ßYou are right, our model is not novel, our contribution is that we use the predicting candidate arguments and triggers as the event representation to detect events.ßdummyTurker
8280ß0.0ßThe ACE 2005 English corpus is the most popular corpus use in event detection and it is licensed by LDC, so we cannot provide this corpus without LDC’s permission.ßdummyTurker
8281ß0.0ßFollowing previous work, we randomly selected 40 articles from newswire as test set, and 30 articles for development set, and remaining 529 articles for training set.ßdummyTurker
8282ß0.0ßFor example, event type Meet achieves a high recall rate, which reached 87.2%, but its precision is very low, only has 57.1%.ßdummyTurker
8283ß0.0ßSo we believe our model can improve recall and endure relatively low precision.ßdummyTurker
8284ß0.0ßFor example, the sentence “she founded the China polar museum foundation with a view to consolidating scientific research findings about polar exploration from all over the world”, the list of entity types [she/PER, (China polar museum foundation)/ORG, world/LOC], the candidate trigger founded belongs to Start-Org event type, Start-Org has two roles Agent and Org, while can fill entities whose type are PER/ORG/GPE and ORG, respectively.ßdummyTurker
8285ß0.0ßHence the list of entity types [she, China polar museum foundation] can act as event roles.ßdummyTurker
8286ß0.0ßThere are 1037 trigger words in the training set.ßdummyTurker
8287ß0.0ß85.6% only refer to one event type and 11.7% belong to two distinct event types.ßdummyTurker
8288ß0.0ßwe used python programming language and NLTK, We will add these to our revised version.ßdummyTurker
8289ß0.0ßWe have calculated the recalls of the Attack, Transport and Meet event types, which can reach 87.5%, 78.8% and 87.2%, respectively.ßdummyTurker
8290ß0.0ßThanks for your review.ßdummyTurker
8291ß0.0ßThe Equation 3 is adopted to collect the information of K-neighbors in document graph.ßdummyTurker
8292ß0.0ßThis intution comes from that GCN is a local graph model which can achieve this goal.ßdummyTurker
8293ß0.0ßIn this paper, we just compare our model with other graph-based models in our work.ßdummyTurker
8294ß0.0ßSo we have not compared with these metioned models.ßdummyTurker
8295ß0.0ßGood review.ßdummyTurker
8296ß0.0ßA reason for that is we made a mistake in our experimentsßdummyTurker
8297ß0.0ßAs for the hyperparameters \alpha and \lambda, a fact is that only fine-tuning the \lambda consumes 4 days.ßdummyTurker
8298ß0.0ß\alpha is a hyperparameters related to the testing phase, so fine-tuning \alpha is not time-consuming.ßdummyTurker
8299ß0.0ßThank you for your advice.ßdummyTurker
8300ß0.0ßThe word embedding size in our experiments is 100, and 20 is the position embedding size.ßdummyTurker
8301ß0.0ßwe develop a theoretical model of dialogue.ßdummyTurker
8302ß0.0ßWe focus on the direct question/answer pairs.ßdummyTurker
8303ß0.0ßWe think it is necessary to have formally valid models that are linguistically justified in order to understand human language and provide higher quality implementations.ßdummyTurker
8304ß0.0ßthank you for this insight, we will take it into account.ßdummyTurker
8305ß0.0ßthe question of the implementation is a very important and interesting one.ßdummyTurker
8306ß0.0ßWe want first to build a formal model before the implementation.ßdummyTurker
8307ß0.0ßDue to the length constraints, we only focused in this paper on the formal side.ßdummyTurker
8308ß0.0ßWe are currently working on an implementation using ACGs.ßdummyTurker
8309ß0.0ßsee argument 1.ßdummyTurker
8310ß0.0ßsee argument 3.ßdummyTurker
8311ß0.0ßtrue, we do not consider implicature nor sarcasm.ßdummyTurker
8312ß0.0ßThe model is focused on direct questions and answers.ßdummyTurker
8313ß0.0ßWhat is left out of the model for now should be considered in further work.ßdummyTurker
8314ß0.0ßaccording to us, rhetorical questions are taken into consideration during the pretreatement phase of the lexical data, as the type of information they bring is very different in nature from the one brought by direct questions.ßdummyTurker
8315ß0.0ßsee argument 1.ßdummyTurker
8316ß0.0ßthe list of thematic roles can be found in the reference, the page number will be added in a revised version.ßdummyTurker
8317ß0.0ßtrue, our wh-word to feature pairing doesn't provide a full coverage (as pointed out in the argument), yet it comes as a result of a will to cover phenomena encountered in data.ßdummyTurker
8318ß0.0ßwe will fix this in after revision.ßdummyTurker
8319ß0.0ßas explained in Section 3, we take (Ginzburg and Sag, 2000) as a starting point for our study of direct questions.ßdummyTurker
8320ß0.0ßthank you for this insight, we will take it into account.ßdummyTurker
8321ß0.0ßyes , these are obvious mistakes that have been overlooked.ßdummyTurker
8322ß0.0ßit will be rectified in camera ready paper if selectedßdummyTurker
8323ß0.0ßyes, there could be more discussion on the significance of the resultsßdummyTurker
8324ß0.0ßyes, a syntactic tree is ideal but bracketing label was adopted for limited space issues.ßdummyTurker
8325ß0.0ßa tree would have taken more space and there are many examples we wanted to illustrate.ßdummyTurker
8326ß0.0ßlicensing condition is delicate as this is health data.ßdummyTurker
8327ß0.0ßbut a parser trained on this health data will be available in the near futureßdummyTurker
8328ß0.0ßPenntreebank contains 325 FRAGs out of 49000 sents.ßdummyTurker
8329ß0.0ßOther treebanks' F1 score on different parser engines as reported by Min Jiang et al (2015) (referred in our paper)ßdummyTurker
8330ß0.0ßBest results were obtained by combining MiPACQ corpus with WSJ corpus, and that wereßdummyTurker
8331ß0.0ßWith Stanford parser, F1 = 84.15ßdummyTurker
8332ß0.0ßWith Bikel parser,       F1 = 78.03ßdummyTurker
8333ß0.0ßWith Charniak parser F1 = 83.59ßdummyTurker
8334ß0.0ßThis is certainly an interesting avenue to explore.ßdummyTurker
8335ß0.0ßWe have done a cursory examination of the output and we find that the problems arise from (1) the translation quality of the pivot.ßdummyTurker
8336ß0.0ß(2) The choice of compression length.ßdummyTurker
8337ß0.0ßWe have considered approaches which relax the length requirement when generating, allowing for more fluid compressions (see the two-stage approach described in Section 2.3).ßdummyTurker
8338ß0.0ßOur approach using multiple pivots more heavily emphasises re-ordering and insertions.ßdummyTurker
8339ß0.0ßHowever, the most common rewriting operation performed by humans is deletion (this can be seen in Table 2).ßdummyTurker
8340ß0.0ßEnsuring high-quality output from crowdsourcing is a challenging task, which we hoped would be mitigated by having multiple (5) references and by manual inspection of the dataset.ßdummyTurker
8341ß0.0ßWe did not correct typos or post-process the data in any way, we only removed compressions which were done maliciously by the users.ßdummyTurker
8342ß0.0ßWe agree automatic evaluation methods for sentence compression are problematic, which is in part why we perform human evaluations.ßdummyTurker
8343ß0.0ßTo mitigate the problems with ROUGE we follow the recommendations of Toutanova et al, (2016), who compared automatic metrics against human judgements.ßdummyTurker
8344ß0.0ßKristina Toutanova, Chris Brockett, Ke M. Tran, and Saleema Amershi.ßdummyTurker
8345ß0.0ß2016.ßdummyTurker
8346ß0.0ßA dataset and evaluation metrics for abstractive compression of sentences and short paragraphs.ßdummyTurker
8347ß0.0ßIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 340–350, Austin, TexasßdummyTurker
8348ß0.0ßWe provide DUC-2004 numbers to show that our pivot based model is competitive against models which use large amounts of supervised headline generation data.ßdummyTurker
8349ß0.0ßOur approach requires no such data, which is particularly useful for the many languages where no headline datasets exist.ßdummyTurker
8350ß0.0ßOn the MOSS dataset, we compare our system with two state-of-the art modelsßdummyTurker
8351ß0.0ßModels like ABS and RAS will probably do better on DUC since they are trained on compression specific data.ßdummyTurker
8352ß0.0ßOur aim is not to show that we have the best compression system on DUC, rather that a model trained on out-of-domain data without having access to long-short parallel sentences can do reasonably well on several languages not just English.ßdummyTurker
8353ß0.0ßAs explained in Section 5, last paragraph, differences among systems are statistically significant with regard to grammaticality but not importance.ßdummyTurker
8354ß0.0ßOur 100-sentence sample was randomly selected, there was no reason to believe that it was biased in some way.ßdummyTurker
8355ß0.0ßHowever, increase the power of the experiment by adding more sentences or having more Turkers per task.ßdummyTurker
8356ß0.0ßWe leave modeling document-level compression to future work.ßdummyTurker
8357ß0.0ßFor producing high-quality compressions, we feel that access to the entire document is critical, since the notion of importance can be discourse-specific.ßdummyTurker
8358ß0.0ßThank you for the suggestion.ßdummyTurker
8359ß0.0ßSeeing how human compressions differ from system output would be very interesting and we will add this analysis in a revised version.ßdummyTurker
8360ß0.0ßNote that we provide analysis on how humans compress in Section 3.ßdummyTurker
8361ß0.0ßKikuchi et al.ßdummyTurker
8362ß0.0ß(2016) compared four different approaches to controlling output length, two of which modify beam search and two of which modify the decoder.ßdummyTurker
8363ß0.0ßLength_init (the approach we used) achieved the best performance.ßdummyTurker
8364ß0.0ßYuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura.ßdummyTurker
8365ß0.0ß2016.ßdummyTurker
8366ß0.0ßControlling output length in neural encoder-decoders.ßdummyTurker
8367ß0.0ßIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1328–1338, Austin, Texas.ßdummyTurker
8368ß0.0ßPlease see our reply to identified weakness 1.ßdummyTurker
8369ß0.0ßWe hypothesise that the translating German, due to word order changes, is a richer task, which allows for more flexible learning.ßdummyTurker
8370ß0.0ßThis flexibility then leads to better compressions.ßdummyTurker
8371ß0.0ßThe Gold row in Table 6 refers to an upper bound which measures how well humans agree with each other; we  treat one (randomly selected) reference as system output and compute how it agrees with the rest of the human-produced compressions.ßdummyTurker
8372ß0.0ßThanks to efficient embedding algorithms, graph embedding is a relatively quick process.ßdummyTurker
8373ß0.0ßThe transformation can also be computed at high speeds for large embedding spaces, given the linear algorithm.ßdummyTurker
8374ß0.0ßThe gained benefit from the merger does justify the additional effort, particularly in domain-specific settings.ßdummyTurker
8375ß0.0ßWe will try to extend the analysis using the additional page.ßdummyTurker
8376ß0.0ßTo our knowledge, this is the first time that an attempt has been made to align heterogeneous spaces (specifically, KB and word spaces) belonging to the same language; we believe this is one of our contributions.ßdummyTurker
8377ß0.0ßPrevious work has mostly used CCA (or other techniques such as least squares) for the transformation of word embedding spaces for different languages, mainly to compute cross-lingual semantic spaces.ßdummyTurker
8378ß0.0ßWe have carried out experiments for other transformation techniques (which we could not report due to space limitations), specifically, least squares and non-linear neural-based transformations.ßdummyTurker
8379ß0.0ßWe can use the additional page to report some of these results.ßdummyTurker
8380ß0.0ßAccording to our analysis (Footnote 1), more than 2/3rd of the words in a large corpus (Wikipedia) tend to have appeared less than or equal to 20 times.ßdummyTurker
8381ß0.0ßGiven the superiority of our system in reliable computing of embeddings for such infrequent words (results in Table 2), we do not believe that these settings are unrealistic.ßdummyTurker
8382ß0.0ßThe system of Herbelot and Baroni (2017) is in fact based on that of Lazaridou et al.ßdummyTurker
8383ß0.0ß(2017) for which we have presented results in Table 2 as “Additive” (for more information, please see Section 3 in page 3 of Herbelot’s paper).ßdummyTurker
8384ß0.0ßWe will clarify this in the paper.ßdummyTurker
8385ß0.0ßThanks for mentioning Pinter et al (2017).ßdummyTurker
8386ß0.0ßWe should have included it in the related work and we will do so in the final version.ßdummyTurker
8387ß0.0ßPlease note that as a representative for the sub-word models we report results for FastText which is the best performing system of the type and can be considered as an “upper bound” (quoted from Pinter et al, 2017) for these models.ßdummyTurker
8388ß0.0ßIn fact, FastText outperforms MIMICK (Pinter et al, 2017) by more than 20 points (0.47 vs. 0.27) on the Stanford Rare Word dataset.ßdummyTurker
8389ß0.0ßPlease note the unavailability of evaluation benchmarks for the task.ßdummyTurker
8390ß0.0ßIn fact, many of the existing work have sufficed to do a mere evaluation on word similarity (and that mainly on the Stanford Rare Word similarity dataset).ßdummyTurker
8391ß0.0ßIn this paper, we have gone beyond this by performing a range of experiments: (i) generic (on Stanford dataset as well as several other downsampled word similarity datasets), (ii) extensive downstream experiments on several datasets across two popular classification tasks.ßdummyTurker
8392ß0.0ßAs for the downstream evaluation, we agree that this might not be the most realistic evaluation.ßdummyTurker
8393ß0.0ßHowever, this is the best experimental setup we could think of.ßdummyTurker
8394ß0.0ßWe considered many factors to mimic a scenario which is close to a realistic downstream setting.ßdummyTurker
8395ß0.0ßThanks for the suggestion.ßdummyTurker
8396ß0.0ßWe will report F1 and include more baselines.ßdummyTurker
8397ß0.0ßWe leave the reverse exploration of alignment for the improvement of WN representations to future work.ßdummyTurker
8398ß0.0ß[FastText, 0.44 vs 0.473]ßdummyTurker
8399ß0.0ßWe trained FastText on the Wikipedia corpus to make it comparable to other models in Table 1 (which use the same training data).ßdummyTurker
8400ß0.0ßThe difference in the training corpora may explain the difference in the reported results.ßdummyTurker
8401ß0.0ßPlease also note that we provide results for the Stanford Rare Word dataset only for the sake of completeness (which is also acknowledged by R1).ßdummyTurker
8402ß0.0ßWe believe that the dataset is not a very reliable benchmark (see our analysis in Section 3.1.2), hence we carried out further experiments.ßdummyTurker
8403ß0.0ß[comparison to FastText]ßdummyTurker
8404ß0.0ßAs also mentioned in the paper, we believe that “FastText benefits from the advantage of having access to all plural forms of these (originally frequent) downsampled words in the training dataset, which might not establish a fair comparison”... “However, the sub-word backoff strategy might not be effective for single-morpheme words and exocentric compounds, which in a real-world scenario account for the most frequent cases of unseen words and can be effectively handled by our model.”ßdummyTurker
8405ß0.0ßWe agree that having included these statistics would have better helped in the analysis and justification of the approach.ßdummyTurker
8406ß0.0ßWe'll clarify these in the paper.ßdummyTurker
8407ß0.0ßWe will try to use the additional page to provide an in-depth analysis.ßdummyTurker
8408ß0.0ßThe random walk is insensitive to relation types, and to relation directions.ßdummyTurker
8409ß0.0ßOur preliminary experiments showed that viewing WordNet as a non-typed non-directional semantic network would result in better synset embeddings, as measured on word similarity benchmarks.ßdummyTurker
8410ß0.0ßWe agree that having an analysis on this would have been interesting.ßdummyTurker
8411ß0.0ßBut, we could not include this for space limitations.ßdummyTurker
8412ß0.0ßThe words were randomly sampled and dropped.ßdummyTurker
8413ß0.0ßThe source of variance is due to random initializations of network parameters as well as the stochastic nature of the optimizer used for training the network (Adam).ßdummyTurker
8414ß0.0ßThis hypothesis may be reasonable.ßdummyTurker
8415ß0.0ßWe will do more analysis later.ßdummyTurker
8416ß0.0ßThanks for your valuable advice.ßdummyTurker
8417ß0.0ßThanks for your valuable advice.ßdummyTurker
8418ß0.0ßWe will explain in more detail to make them clearer.ßdummyTurker
8419ß0.0ßThanks for your valuable comments.ßdummyTurker
8420ß0.0ßThe data statistics and some ACs are presented in Table 1.ßdummyTurker
8421ß0.0ßAnd, we actually use about 13 ACs in this paper, which will be stated clearer in the final version.ßdummyTurker
8422ß0.0ßNobody can deny that our method does have some flaws.ßdummyTurker
8423ß0.0ßFor example, it cannot properly predict the sentiment polarities of a sentence that contains ‘but also’.ßdummyTurker
8424ß0.0ßHowever, our method still achieves better results compare than baselines.ßdummyTurker
8425ß0.0ßThis is mainly due to the fact that sentences containing ‘but also’ are rare, as mentioned in Section 4.5.ßdummyTurker
8426ß0.0ßWe will consider this topic in our future work.ßdummyTurker
8427ß0.0ßFor the conjunction omitted in the text, the regular term in the loss function is set to zero, which means that ACR is not used.ßdummyTurker
8428ß0.0ßCompared with Qian et al.ßdummyTurker
8429ß0.0ß2017, we only used one rule but achieved comparable results.ßdummyTurker
8430ß0.0ßThe experimental results prove that our method is effective and ACR does play a key role in aspect-level sentiment analysis.ßdummyTurker
8431ß0.0ßWe will make revisions on our paper according to your valuable writing advice.ßdummyTurker
8432ß0.0ßThanks for your insightful comments.ßdummyTurker
8433ß0.0ßTo handle the proposed case, we can add more regular terms in the objective function and employ more linguistic knowledge.ßdummyTurker
8434ß0.0ßCombining ellipses and ‘though’ is actually a linguistic rule, which is really an interesting idea.ßdummyTurker
8435ß0.0ßAnd, our method can be extended to incorporate such rule.ßdummyTurker
8436ß0.0ßYour suggestions are worth studying in the future work.ßdummyTurker
8437ß0.0ßWe use 13 ACs extracted based on the observation of the corpus.ßdummyTurker
8438ß0.0ßIt is an insightful idea that pulls rules from an off-the-shelf ontology.ßdummyTurker
8439ß0.0ßWe will make further analysis of your advice in the future works.ßdummyTurker
8440ß0.0ßThanks for your additional comments.ßdummyTurker
8441ß0.0ßThere indeed are some format issues because of our carelessness.ßdummyTurker
8442ß0.0ßWe will make revisions to them carefully.ßdummyTurker
8443ß0.0ßAnd we will add the explanations of the dataset in Table 1 to make it clear.ßdummyTurker
8444ß0.0ßMany thanks for your carefully checking and great suggestion, we will adjust the table size in the camera-ready version.ßdummyTurker
8445ß0.0ßWe appreciate the comparison suggestions about the mentioned works!ßdummyTurker
8446ß0.0ßWe did not perform the comparison in the submitted version as most of them evaluate on knowledge complement task (usually on link prediction) and use only KBs as the dataset (such as FB15K[Bordes et al., 2013]), while we focus on distantly supervised relation extraction and use textual mentions as datasets.ßdummyTurker
8447ß0.0ßSo we didn't compare with them because of the difference in task and dataset.ßdummyTurker
8448ß0.0ßBack to your suggestion, both these works and ours jointly infer facts utilizing the texts and KG, so we will compare them in the new version.ßdummyTurker
8449ß0.0ßThanks!ßdummyTurker
8450ß0.0ßThe main contribution of our work is to explore the interaction between the two sub-networks.ßdummyTurker
8451ß0.0ßAs you mentioned in question 2, we proposed several modules to improve previous methods, which are experimentally confirmed as follows:ßdummyTurker
8452ß0.0ß"For the ""External Rule Knowledge"", we reported the improvement in Figure 5 and Table 2; for the ""Bi-directional Knowledge Distillation"", we reported the improvement in Figure 4 and Section 3.4; for the ""Adaptive Imitation Rate"", we reported the improvement in Figure 6; for the ""Dynamic Ensemble Prediction"", we reported the improvement in Table 4."ßdummyTurker
8453ß0.0ßThanks for your helpful suggestion!ßdummyTurker
8454ß0.0ßWe will sample some noisy sentences which are low attention weight and inconsistent predictions, and include them in the revision.ßdummyTurker
8455ß0.0ßWe really appreciate your great review comments!ßdummyTurker
8456ß0.0ßSorry for the confusion in Section 2.2.ßdummyTurker
8457ß0.0ßThe basic idea is to project the prediction of KG-Net into a rule-constrained subspace by solving a posterior regularization optimization problem, in which we expect to find the optimal predictions that are determined by both rules and the predictions of KG-Net.ßdummyTurker
8458ß0.0ßWe will add more detailed explanations in the revision.ßdummyTurker
8459ß0.0ßThanks for your suggestion, we will add the comparison for NA prediction in the new version.ßdummyTurker
8460ß0.0ßFor the submitted version, the baselines and many other distant supervision works exclude the NA predictions in testing, we follow the setting for fair comparison.ßdummyTurker
8461ß0.0ßThanks for your encouraging comments.ßdummyTurker
8462ß0.0ßThe datasets we used in this paper can also be regarded as benchmark datasets.ßdummyTurker
8463ß0.0ßTo answer your concerns, we have performed the evaluation on the SimLex dataset.ßdummyTurker
8464ß0.0ßWe don’t get worse results.ßdummyTurker
8465ß0.0ßFor example, the score of skip-gram in SimLex on Newsir corpus is 0.3198 while our method achieves 0.3241.ßdummyTurker
8466ß0.0ßWe will report the results of this dataset in the final version.ßdummyTurker
8467ß0.0ßIn ontology population The process is done fully automatically and can be easily plugged into different domains and applications.ßdummyTurker
8468ß0.0ßIn order to populate the ontology with papers first we collect all papers presented in conferences of ACL Anthology from 2000 to 2017.ßdummyTurker
8469ß0.0ßWe add papers as instances of publications to the ontology and make “is_related_to” relation between papers and their related concepts in the ontologyßdummyTurker
8470ß0.0ßFinally the ontology is populated with about 36000 papers and 32000 authors.ßdummyTurker
8471ß0.0ß"To find related concepts for each paper to instantiate  ""is_related_to” relations, we create a training dataset."ßdummyTurker
8472ß0.0ßWe have used our training data to construct the tf-idf vector for each concept of the ontology.ßdummyTurker
8473ß0.0ßWe construct the same vector for all 36000 papers and then we calculate cosine similarity between each paper's vector and the vectors of each concepts.ßdummyTurker
8474ß0.0ßWe consider the threshold 0.11 for the calculated similarities.ßdummyTurker
8475ß0.0ßSo the most similar ones were chosen as the related concepts to the specified paper and is_related_to relation was created between them.ßdummyTurker
8476ß0.0ßthe main aim of this work is the ontology itself, not its building procedure.ßdummyTurker
8477ß0.0ßIt means that although we have used a combination of automatic and manual methods to build the ontology and populate it, we are not going to claim about the novelty of each individual method we used.ßdummyTurker
8478ß0.0ßAlthough there may be a novelty in the whole methodology and combining the individual methods, we just talk about the result which is a novel NLP ontology.ßdummyTurker
8479ß0.0ßwe will add examples in the final versionßdummyTurker
8480ß0.0ßWeakness5: Thanks for the comment.ßdummyTurker
8481ß0.0ßWe will revise the whole paper for its English in the final version and add another figure (besides Figure 1 ) to show more detailsßdummyTurker
8482ß0.0ßYes, the NLP ontology will be open source after publishing the paper about it.ßdummyTurker
8483ß0.0ßIn this table, the precision is calculated upon all the papers in the test sets.ßdummyTurker
8484ß0.0ßupon the reviewer’ question we repeat the test for each class separately and calculate the average recall for different classes which is %64.4.ßdummyTurker
8485ß0.0ß3-	Semiautomatic construction of the ontology (the schema-level)ßdummyTurker
8486ß0.0ß4-	Automatic population of the ontology (instance level)ßdummyTurker
8487ß0.0ßThe first phase has manual evaluation and refinement during its lifecycle but the second phase is done fully automatically.ßdummyTurker
8488ß0.0ßIn the first phase, we check the accuracy of the ontology manually.ßdummyTurker
8489ß0.0ßThis way we make sure that the ontology is appropriate enough to be used in an expert finding system.ßdummyTurker
8490ß0.0ßActually the main goal of the work is the ontology itself, not its building method.ßdummyTurker
8491ß0.0ßBut in the second phase which is ontology population, we add a large scaled knowledge base (more than 381000 elements: 68000 instances and 313000 relations) which on one hand cannot be manually checked.ßdummyTurker
8492ß0.0ßweakness3: The expert is someone who teaches NLP in university and is familiar with the domain very well.As there is no data set with which automatic evaluation can be done, we have to evaluate the results of expert finding system manually.ßdummyTurker
8493ß0.0ßFor this purpose for each query we have to examine the first 5 recommendations of the system by examining the papers of the author for which she/he is selected as an expert.ßdummyTurker
8494ß0.0ßWithout the ontology the expert finding system works as an information retrieval system using keyword matching.ßdummyTurker
8495ß0.0ßThus using the ontology increases the recall of the expert finding system obviously.ßdummyTurker
8496ß0.0ßOn the other hand as the ontology is bilingual the system considers the authors of Persian papers as well as English papers when the query is in English and vice versa.ßdummyTurker
8497ß0.0ßTaking a subset of the ontology, we miss some semantic information.ßdummyTurker
8498ß0.0ßweakness4: We annotate all concepts in the ontology with their Persian translation by “PersianTitle” tag.ßdummyTurker
8499ß0.0ßSo all capabilities of the ontology are also available in PersianßdummyTurker
8500ß0.0ßThanks for your suggestions.ßdummyTurker
8501ß0.0ßWe will improve on our exposition of the reinforcement learning (RL) discussion, if accepted.ßdummyTurker
8502ß0.0ßWe do have additional experiments, validation and analysis on RL, but we have specifically focused on just the Sequicity framework for the scope of this paper.ßdummyTurker
8503ß0.0ßOur apologies for the confusion.ßdummyTurker
8504ß0.0ßIn the example, “Chinese_unk” will be labeled as a slot.ßdummyTurker
8505ß0.0ßSince we employ a copy mechanism, the word “Chinese_unk” will be copied into the belief span, which is then used directly to match entities in the KB.ßdummyTurker
8506ß0.0ßWe may not even necessarily know what the constraint semantically represents it is.ßdummyTurker
8507ß0.0ßSequicity proposes a completely different dialogue architecture to achieve the same output as which the traditional pipeline architecture(as R3 outlines) can achieve.ßdummyTurker
8508ß0.0ßAs such, there are some components that do not have exact correspondence to the traditional architecture.ßdummyTurker
8509ß0.0ßHowever, there is some similarity.ßdummyTurker
8510ß0.0ßFor example, the encoder functions as an intent classifier(Wen et al., 2017a) and the belief span functions as the belief tracker.ßdummyTurker
8511ß0.0ßThe second-stage decoder functions as the traditional natural language generation component.ßdummyTurker
8512ß0.0ßWhile holistic, our framework is extendable as each component can be individually partwise improved, benefiting the entire pipeline (e.g., improving the encoder for better intent classification)ßdummyTurker
8513ß0.0ßWe thank the reviewer for the chance to clarify these aspects.ßdummyTurker
8514ß0.0ßWe address these issues sequentially.ßdummyTurker
8515ß0.0ßInjecting new domain knowledge and constraints: Our method is domain-independent and learns actions from training data.ßdummyTurker
8516ß0.0ßFor a new domain, we only need additional training data.ßdummyTurker
8517ß0.0ßIf the training data is enough, we can train a new model without changing the model structure.ßdummyTurker
8518ß0.0ßIf the training data is little, domain adaptation could be performed (J.Jiang&C.Zhai, 2007).ßdummyTurker
8519ß0.0ßWhy the information from immediate previous turn is sufficient: We generate belief span based on the belief span (B_{t-1}) and the machine response (R_{t-1}) of the immediate turn and the user input of the current turn (U_t).ßdummyTurker
8520ß0.0ßIt is quite a common solution to generate belief states at current turn based on those three information sources (Wen et al., 2017a; Mrksic, 2017).ßdummyTurker
8521ß0.0ßOur belief span here actually acts as a traditional belief tracker.ßdummyTurker
8522ß0.0ßHandling OOV: As stated in lines 723~728, our model learns sentence patterns.ßdummyTurker
8523ß0.0ßIt can capture OOV slots specified by users and generates them in the current belief span.ßdummyTurker
8524ß0.0ßOnce the belief span obtained, we can do knowledge base search.ßdummyTurker
8525ß0.0ßSorry, we will improve the reinforcement learning(RL) part with more details if this paper is accepted.ßdummyTurker
8526ß0.0ßBriefly, we follow existing methods (Wen et al., 2017b) to demonstrate the potential of Sequicity framework to be optimized by RL.ßdummyTurker
8527ß0.0ßSuch method adopts common practices in seq2seq models: using hidden vectors as states and defining word choices as actions (Ranzato et al., 2016).ßdummyTurker
8528ß0.0ßThe standard policy gradient (Eq.ßdummyTurker
8529ß0.0ß11 in line 491) encourages our model to generate responses of high reward: containing user requested tokens but being short (lines 496-498).ßdummyTurker
8530ß0.0ßIt’s a good question.ßdummyTurker
8531ß0.0ßWe had a fruitful discussion on this but omitted this due to page limitation; Indeed, it is common for users to change slots during task-oriented dialogue.ßdummyTurker
8532ß0.0ßWe currently consider three scenarios:ßdummyTurker
8533ß0.0ß1.ßdummyTurker
8534ß0.0ßThe user totally changes his mind.ßdummyTurker
8535ß0.0ßE.g., the user requests a Japanese restaurant first and says “I don’t want Japanese food anymore, I'd like French now.” Then, all the slot activated before should be invalid now.ßdummyTurker
8536ß0.0ßThe slot annotated for this turn is only “French”.ßdummyTurker
8537ß0.0ßSequicity can learn this pattern, as long as it is annotated in the training set.ßdummyTurker
8538ß0.0ß2.ßdummyTurker
8539ß0.0ßA user requests cannot be found in the KB(e.g., “Japanese food”).ßdummyTurker
8540ß0.0ßThen the system should respond like “Sorry, there is no Japanese food...”.ßdummyTurker
8541ß0.0ßConsequently, the user can choose a different option: “OK, then French food.” The activated slot “Japanese” will be replaced as “French”, which our system can learn.ßdummyTurker
8542ß0.0ßTherefore, an important pattern is the machine-response (e.g., “there is no [XXX constraint]”) in the immediate previous utterance.ßdummyTurker
8543ß0.0ß3.ßdummyTurker
8544ß0.0ßIn few cases where our model dose not know which slot to replace, it will generate both slots.ßdummyTurker
8545ß0.0ßWe run a simple post-processing script at each turn to maintain the belief span.ßdummyTurker
8546ß0.0ßThe script detects whether two slot values have the same slot name(e.g., FOOD_TYPE) by a pre-defined slot name-value table and only remain the one in the current user utterance.ßdummyTurker
8547ß0.0ßWe believe Sequicity performs consistently on both datasets.ßdummyTurker
8548ß0.0ßThe key difference between two datasets is that KVRET is larger, with three domains.ßdummyTurker
8549ß0.0ßDue to this diversity, Sequicity performs worse on KVRET.ßdummyTurker
8550ß0.0ßAlso we had inadvertently swapped our reporting of the training time on Att-Rnn (17.5 min) and TSCP\RL (23 min) on KVRET.ßdummyTurker
8551ß0.0ßThis error will be correct if the work is accepted.ßdummyTurker
8552ß0.0ßReferencesßdummyTurker
8553ß0.0ßJ.Jiang&C.Zhai.ßdummyTurker
8554ß0.0ß2007.ßdummyTurker
8555ß0.0ßInstance weighting for domain adaptation in nlp.ßdummyTurker
8556ß0.0ßACL.ßdummyTurker
8557ß0.0ßT-H.Wen, et al.ßdummyTurker
8558ß0.0ß2017b.ßdummyTurker
8559ß0.0ßA networkbased end-to-end trainable task-oriented dialogue system.ßdummyTurker
8560ß0.0ßEACL.ßdummyTurker
8561ß0.0ßN.Mrksic, et al.ßdummyTurker
8562ß0.0ß2017.ßdummyTurker
8563ß0.0ßNeural belief tracker: Data-driven dialogue state tracking.ßdummyTurker
8564ß0.0ßACL.ßdummyTurker
8565ß0.0ßM.Ranzato, et al.ßdummyTurker
8566ß0.0ß2016.ßdummyTurker
8567ß0.0ßSequence Level Training with Recurrent Neural Networks.ßdummyTurker
8568ß0.0ßICLR.ßdummyTurker
8569ß0.0ßT-H.Wen, et al.ßdummyTurker
8570ß0.0ß2017b.ßdummyTurker
8571ß0.0ßLatent Intention Dialogue Models.ICML.ßdummyTurker
8572ß0.0ßThis question is important.ßdummyTurker
8573ß0.0ßWe will clarify it if accepted.ßdummyTurker
8574ß0.0ßFirst, it is impossible that all information variables collected but multiple match still happens.ßdummyTurker
8575ß0.0ßOur system uses all possible attributes for disambiguation, including name, address, etc.ßdummyTurker
8576ß0.0ßGenerally, our paper is towards building a system to learn to handle various situations directly from the training data with less manual intervention.ßdummyTurker
8577ß0.0ßThe most salient problem encountered in our system is that a user might change her/his constraints, also brought up by R3(Please refer to our response to weakness4 of R3).ßdummyTurker
8578ß0.0ßWe will report all metrics for OOV if accepted.ßdummyTurker
8579ß0.0ßWe believe it's important to catch the incorrect input first.ßdummyTurker
8580ß0.0ßWe also believe our method of handling incorrect query is satisfactory: if an OOV is an error, which produces no result in KB, Sequicity will generate responses such as “sorry, we don’t have XXX”.ßdummyTurker
8581ß0.0ßOur OOV handling has additional advantages in the industry setting where users' requests can vary a lot.ßdummyTurker
8582ß0.0ßFor traditional classification-based belief tracker, the only solution is re-training the model using the OOV as a classification label.ßdummyTurker
8583ß0.0ßJudging from the comments you made on our paper, it is reasonable for us to conclude that you haven't read our paper carefully.ßdummyTurker
8584ß0.0ßWe are expected to treat our peers' work with respect and create a healthy environment in the research of NLP.ßdummyTurker
8585ß0.0ßIt is for this reason that your unprofessionalism cannot be allowed.ßdummyTurker
8586ß0.0ßWe have reported to the chair regarding your improper behavior.ßdummyTurker
8587ß0.0ßHere we will refute some of your comments so that you get an idea of how irresponsible or even absurd they are.ßdummyTurker
8588ß0.0ß"It Why would you think ""it's impossible to tell how this is constructed, how it is used...""."ßdummyTurker
8589ß0.0ß"The ""impossible"" here is really confusing."ßdummyTurker
8590ß0.0ßThe construction, representation and incorporation of the feature matrix have already been illustrated in Section 3.ßdummyTurker
8591ß0.0ßIf it was unclear, please tell us why our demonstration is unclear!ßdummyTurker
8592ß0.0ßAs for the figures, Figure 3 is actually very common in many papers, articles or blogs about Bi-LSTM-CRF model, e.g.ßdummyTurker
8593ß0.0ßZhang et al., 2016 in our references.ßdummyTurker
8594ß0.0ß"Why did you say ""none of the figures are clear""?"ßdummyTurker
8595ß0.0ß"The ""insights"" we stated in page 8 are supported by the experiment results we showed in Figure 4."ßdummyTurker
8596ß0.0ßIt also confuses us why you think our writing is weak.ßdummyTurker
8597ß0.0ßWe would also want to know which sentences made no sense to you.ßdummyTurker
8598ß0.0ßHere, we take your comments as an example of weak writing with the reasons given as followsßdummyTurker
8599ß0.0ß1.The first letter in Contribution 1, Strengthen argument 1 and Weakness argument 1 should be capitalized;ßdummyTurker
8600ß0.0ß"2.The usage of second and forth ""the"" in Contribution 1 is wrong;"ßdummyTurker
8601ß0.0ß3.The period of Strength argument 1 is missing;ßdummyTurker
8602ß0.0ß"4.There are two successive ""the"" in Weakness argument 1;"ßdummyTurker
8603ß0.0ß5.The usage of comma in Weakness argument 3 is wrong;ßdummyTurker
8604ß0.0ß"6.There should be a comma after ""More generally"" in Weakness argument 5."ßdummyTurker
8605ß0.0ßJudging from the quality of writing you have demonstrated in your comments, we should conclude that you are not the right person to say someone's writing as weak.ßdummyTurker
8606ß0.0ßWe give the reason about why we choose a baseline just after the name of it in Section 5.2.1 as well as the beginning of Section 5.3.1.ßdummyTurker
8607ß0.0ßThe purpose of example sessions in Figure 1 is to illustrate what confusable entity names are and how they trouble command understanding in SDS.ßdummyTurker
8608ß0.0ßAnd, the importance of our task has been sufficiently illustrated in Section 1.ßdummyTurker
8609ß0.0ßLast but not Least, have you actually read our paper carefully?ßdummyTurker
8610ß0.0ßAs there is no other public dataset for command understanding, the only extra dataset for us to illustrate the effectiveness of our method is ATIS, the most popular data set for SLU in the past two decades.ßdummyTurker
8611ß0.0ßMoreover, the main problem we focus on in this work is the confusable entity names and it is particularly serious in music domain.ßdummyTurker
8612ß0.0ßTherefore, we believe the experiments on these two datasets are sufficient enough to illustrate the effectiveness of our method.ßdummyTurker
8613ß0.0ßAs for the baseline systems, the models we choose for CR are all famous classification methods and the models we choose for SF are all recent and well-known models for SF in SLU.ßdummyTurker
8614ß0.0ßTherefore, we believe it is unnecessary to explain the details of these model.ßdummyTurker
8615ß0.0ßBesides, we have added the citations on the compared baselines.ßdummyTurker
8616ß0.0ßIt is easy to get the details of these models by the cited papers.ßdummyTurker
8617ß0.0ß"As for the statements of ""more features"", as the Chinese word segmentation is a prerequisite of POS tagging, it is impossible to add any syntactic features like POS to the method of us or of Zhang, et al."ßdummyTurker
8618ß0.0ß2018.ßdummyTurker
8619ß0.0ßBesides, various syntactic features including POS are actually extremely inefficient for oral short sentences, especially for Chinese oral short sentences.ßdummyTurker
8620ß0.0ßAccording to our previous attempt, various pre-processing techniques, such as dependency analysis, Part-of-Speech tagging and word segmentation, import more noises than effective features.ßdummyTurker
8621ß0.0ßAs stated in Section 3.1, the feature matrix is initialized as a zero matrix and only the elements of matched words would be set to specific positive integers.ßdummyTurker
8622ß0.0ßTherefore, when there is no entity or template can be matched for an utterance, which is almost impossible, the feature matrix would still be a zero matrix.ßdummyTurker
8623ß0.0ßOur method is to build an additional feature matrix for an utterance.ßdummyTurker
8624ß0.0ßTherefore, it can be generalized to any sentences that can be matched with entities or templates.ßdummyTurker
8625ß0.0ßAs stated in Section 5.1, we extract all the utterances from logs of a commercial system and their distribution in our dataset keeps the same as that of real utterances of users.ßdummyTurker
8626ß0.0ßOur method achieves 97.25% at F1-score, which is definitely not minor for a dataset whose state-of-the-art model has already achieved 96.89%.ßdummyTurker
8627ß0.0ßIn the past several years, the F1-score on ATIS just improved from ~93% to ~96.5%.ßdummyTurker
8628ß0.0ßMoreover, from the perspective of error rate, our method reduces >10% error rates compared with the previous state-of-the-art.ßdummyTurker
8629ß0.0ßThank you very much for your nice and constructive comments, we will revise our paper according to your comments.ßdummyTurker
8630ß0.0ß"In my paper, we said that ""Hits@10 can reflect a KGE method’s performance much clearer than Mean Rank""."ßdummyTurker
8631ß0.0ßThis is because that the range of Hits@10 is [0,1].ßdummyTurker
8632ß0.0ßWith it, one can clearly know the state-of-the-art performance of KGE methods.ßdummyTurker
8633ß0.0ßBesides, one can easily compare different KGE methods by Hits@10.ßdummyTurker
8634ß0.0ßWhile for Mean Rank, in theory, its range is [1,infinity].ßdummyTurker
8635ß0.0ßEven on the same dataset, its difference among different methods is usually more huge than expected.ßdummyTurker
8636ß0.0ßFor example, in ConvE(Dettmers, 2018, Table3), on WN18, the range of Mean Rank for different methods is from 352 to 902; and on FB15k, the range of Mean Rank for different methods is even from 64 to 1897.ßdummyTurker
8637ß0.0ßSo given a Mean Rank score, sometimes it is very difficult for someone to know the true performance level of a method.ßdummyTurker
8638ß0.0ßWe don't evaluate our method on the mentioned datasets for the following reasons.ßdummyTurker
8639ß0.0ß1)The experimental results reported in ConvE(Dettmers, 2018) show that in most of times, it is completely impossible to achieve state-of-the-art performance by simply taking advantage of the mentioned test leakage.ßdummyTurker
8640ß0.0ßFrom Table5 of ConvE we can see that on YAGO3-10, the Hits@10 of a reverse model (a simple model proposed in ConvE to take advantage of the mentioned test leakage) is only 0.02, which can be completely ignored compared with the Hits@10 achieved by ConvE (on YAGO3-10,ConvE's Hits@10 is 0.66, more than 30 times larger than the inverse model).ßdummyTurker
8641ß0.0ßFurthermore, from Table3 of ConvE we can see that on FB15k, the Hits@10 of a reverse model is far less than state-of-the-art results (Before ConvE, the best Hits@10 is 88, which is reported by TransG).More importantly, none of existing methods(including our method) take any prior considerations to take advantage of the mentioned test leakage.ßdummyTurker
8642ß0.0ßThat's why this problem is first noted in 2015(Toutanova and Chen), but almost all of the researchers continue using WN18 and FB15k for evaluation without addressing this issue at all.ßdummyTurker
8643ß0.0ßEven in ConvE, this issue only accounts for a very small proportion of their experiments.ßdummyTurker
8644ß0.0ß2)WN18 and FB15k are two most widely used datasets for evaluating KGE methods.ßdummyTurker
8645ß0.0ßThey can complement each other during evaluation and can provide cross validation for the actual performance of a KGE method.ßdummyTurker
8646ß0.0ßWithin the page length limitation, it is sufficient enough to gauge the actual performance of a method by using them together.ßdummyTurker
8647ß0.0ßFor example, from Table3 in ConvE we can clearly see that even a reverse model achieves state-of-the-art results on WN18, it does much worse on FB15k compared with state-of-the-art methods.ßdummyTurker
8648ß0.0ßIn a word, by using WN18 and FB15k together, one can clearly gauge the actual performance of a method.ßdummyTurker
8649ß0.0ßThat's another reason why almost all of the researchers use WN18 and FB15k for KGE evaluation.ßdummyTurker
8650ß0.0ßIn fact, the experimental results reported in Table2 and 3 clearly response these concerns.ßdummyTurker
8651ß0.0ß(1)We analyzed the effectiveness of multi-source descriptions in Table2(page7).ßdummyTurker
8652ß0.0ßThe performance differences among MCnnE-DB/MCnneWiki/MCnnE show the contributions of different kinds of descriptions and their combination.ßdummyTurker
8653ß0.0ß"(2)We reported the ""Detailed Results"" in Table3."ßdummyTurker
8654ß0.0ßBy comparing with other baselines like TransG, one can clearly see which kinds of relations are actually helped with the addition of the extra knowledge.ßdummyTurker
8655ß0.0ß(3)attention scores over text descriptions are some real numbers, which is used to capture the most useful information among different descriptions.ßdummyTurker
8656ß0.0ß1)The number of hyperparameters in our method is similar to that of in other neural networks based methods like ProjE, ConvE, etc.ßdummyTurker
8657ß0.0ßTotally, there are 8 hyperparameter in our method, 9 in ProjE, and 7 in ConvE, etc.ßdummyTurker
8658ß0.0ßAlmost all of the methods (for example, all the methods listed in Table2 of our paper) select hyperparameters via a grid search on the validation set.ßdummyTurker
8659ß0.0ß"As the hyperparameter tuning is out the scope of a KGE method, none of existing methods (for example, all the methods listed in Table2 and the methods introduced in ""related work"") reported the variance of experimental results according to different hyperparameter settings."ßdummyTurker
8660ß0.0ß2)The performance of baselines is higher and higher, thus the improvements over baselines are smaller and smaller.ßdummyTurker
8661ß0.0ßTable2 of our paper clearly show the improvements of recent methods over previous baselines.ßdummyTurker
8662ß0.0ßMore importantly, from Table2, one can clearly find that none of recent methods could achieve better results on both FB15k and WN18.ßdummyTurker
8663ß0.0ßFor example, ITransF(2017) and ConvE(2018), two of the best and latest methods, compared with TransG(2016), they both achieve better results on WN18, but worse results on FB15k.ßdummyTurker
8664ß0.0ßHowever, our method achieves best results on both FB15k and WN18.ßdummyTurker
8665ß0.0ßFor your concern, our opinion is that since Lin et al.ßdummyTurker
8666ß0.0ß(2017)can even find multi-lingual descriptions for a relation extraction task, where the involved entities are more specific than the entities in a common KG, we think that even for specific domains, with the rapid development of web resources, we still could find some mapping resources and extract some textual descriptions.ßdummyTurker
8667ß0.0ßWe don't report the evaluation of the Hits@1 and Hits@3 because we want to make clear comparisions with other methods: almost all of the state-of-the-art methods report hits@10 results, but only a few of them report Hits@3 and Hits@1 results.ßdummyTurker
8668ß0.0ßHowever, it is very easily for us to add the results of Hits@1 and Hits@3.ßdummyTurker
8669ß0.0ßWe will revise our paper according to your constructive comments.ßdummyTurker
8670ß0.0ßThe major challenge was the availability of the data for comparison and evaluation.ßdummyTurker
8671ß0.0ßWe are not aware of any open source data for dialectal Arabic.ßdummyTurker
8672ß0.0ßFor such, we opted using the Bible versions which were recently made available.ßdummyTurker
8673ß0.0ßOther relevant work could be Zayyan et.ßdummyTurker
8674ß0.0ßal (2016) who used LDC Egyptian CallHome which is not available without a fee (LDC membership).ßdummyTurker
8675ß0.0ßTheir Multi-lexical levels statistical based approach achieved a WER of 16.8% and DER of 11.7% of the ECH.ßdummyTurker
8676ß0.0ßAn earlier work, Harrat et al.ßdummyTurker
8677ß0.0ß(2013), they built a collection of 4k sentences of Algiers dialect and manually diactized it.ßdummyTurker
8678ß0.0ßThis data was not released or available for cross-examination.ßdummyTurker
8679ß0.0ßThe results obtained for the dialect collection were very low WER 56.5% and DER of 20.5%ßdummyTurker
8680ß0.0ß@inproceedings{zayyan2016crosslingual,ßdummyTurker
8681ß0.0ßtitle={Crosslingual automatic diacritization for Egyptian Colloquial Dialect},ßdummyTurker
8682ß0.0ßauthor={Zayyan, Ayman A and Elmahdy, Mohamed and Husni, Husniza Binti and Yousf, Shahrul Azmi},ßdummyTurker
8683ß0.0ßbooktitle={Computer Systems and Applications (AICCSA), 2016 IEEE/ACS 13th International Conference of},ßdummyTurker
8684ß0.0ßpages={1--6},ßdummyTurker
8685ß0.0ßyear={2016},ßdummyTurker
8686ß0.0ßorganization={IEEE}ßdummyTurker
8687ß0.0ß}ßdummyTurker
8688ß0.0ß@inproceedings{harrat2013diacritics,ßdummyTurker
8689ß0.0ßtitle={Diacritics restoration for Arabic dialect texts.ßdummyTurker
8690ß0.0ß},ßdummyTurker
8691ß0.0ßauthor={Harrat, Salima and Abbas, Mourad and Meftouh, Karima and Smaili, Kamel},ßdummyTurker
8692ß0.0ßyear={2013}ßdummyTurker
8693ß0.0ß}ßdummyTurker
8694ß0.0ßThis is a very valid argument.ßdummyTurker
8695ß0.0ßIn the absence of sizable and diacritized data, it is hard to know all the challenges and issues.ßdummyTurker
8696ß0.0ßWe hope that this effort will allow to bootstrap further work in the domain.ßdummyTurker
8697ß0.0ßAlthough the data used is from a specific domain -religious, but still it cover a significant amount of vocabulary in the MOR and TUN dialects.ßdummyTurker
8698ß0.0ßThe absence of standard orthography have been studied thoroughly in tweeter data and news comments for example.ßdummyTurker
8699ß0.0ß- Exploring the problem of dialectal diacritization and showing how best to build a dialectal diacritizerßdummyTurker
8700ß0.0ß- Comparing multiple approaches that work at character levelßdummyTurker
8701ß0.0ß- Showing that a DNN can implicitly identify subdialect to properly diacritize text without specifically specifying the dialect.ßdummyTurker
8702ß0.0ßWe hope that this effort will allow to capitalize of this success and use this system as a bootstrap to diacritize for example large amount of Twitter data and dialectal comments found in news outlets or youtube comments.ßdummyTurker
8703ß0.0ßWe will report the results from the relevant literature even though they are obtained on different testsets.ßdummyTurker
8704ß0.0ßWe hope if we can find additional resources to use to expand the training and evaluation of our system.ßdummyTurker
8705ß0.0ßThis includes the books and chapters titles.ßdummyTurker
8706ß0.0ßThe titles are also diacritized.ßdummyTurker
8707ß0.0ßIn total was collected 8219 segment/sentence.ßdummyTurker
8708ß0.0ßWe will clarify this.ßdummyTurker
8709ß0.0ß- الْفَصْلْ الْخَامْسْ الْخُطْبَة دْيَالْ يَسُوعْ فُوقْ الجّْبَلْ  (MOR)ßdummyTurker
8710ß0.0ß- الوَعْظَة فِي الجْبَلْ ( لُوقَا ‏6‏:‏20‏-23 )  (TUN)ßdummyTurker
8711ß0.0ß-  الموعظة على الجبل - التطويبات  (MSA1)ßdummyTurker
8712ß0.0ß- تَعليمُ يَسُوع  (MSA2)ßdummyTurker
8713ß0.0ß- Jesus Gives Blessings  (ENG)ßdummyTurker
8714ß0.0ßThank you for your valuable suggestions and constructive feedback!ßdummyTurker
8715ß0.0ßW1: Compared with lambda calculus in related works: AnnaParser not only supports strong type constraints but also has other constraints (e.g.ßdummyTurker
8716ß0.0ßlogic validity) which can help reduce search space.ßdummyTurker
8717ß0.0ßCompared with traditional logic forms, the deduction rule in AnnaParser is more flexible (e.g.ßdummyTurker
8718ß0.0ßorder-insensitive).ßdummyTurker
8719ß0.0ßBy recursively applying Equation 1,2,3,4 in Section 3.3,  a set of logic forms are generated and Figure 4 shows an example of AnnaParser’s parsing tree.ßdummyTurker
8720ß0.0ßWe will explain it more in our paper.ßdummyTurker
8721ß0.0ßW2: We will explain the preconditions of grammar rules in detail and give examples to show how they work.ßdummyTurker
8722ß0.0ßThank you for your valuable suggestions and helpful feedback!ßdummyTurker
8723ß0.0ßW1: Firstly, we extend the traditional chart-parsing algorithm to support raising grammars.ßdummyTurker
8724ß0.0ßSecondly, during the designing of the whole deduction system, when we added a new grammar, we use circle detection algorithm to detect a loop in our deduction system.ßdummyTurker
8725ß0.0ßOnce we find the loop, there are several ways to eliminate the loop: 1) introduce a new symbol, 2) add pre-conditions to grammar rules.ßdummyTurker
8726ß0.0ßWe will follow this suggestion to add more examples.ßdummyTurker
8727ß0.0ßW2: Given more space in the camera-ready, we will move important parts into our paper from supplementary materials to make it more self-contained.ßdummyTurker
8728ß0.0ßW3: Yes, most of them should be called “grammar rule” or “deduction rule”.ßdummyTurker
8729ß0.0ßWe will revise them accordingly.ßdummyTurker
8730ß0.0ßThank you for your valuable suggestions and feedback!ßdummyTurker
8731ß0.0ßWe will carefully revise our paper in the camera-ready version accordingly.ßdummyTurker
8732ß0.0ßW1: Given more space in the final version, we will move important information to our paper from supplementary materials to make it self-contained.ßdummyTurker
8733ß0.0ßThe supplementary material includes the full version of our symbols and grammars, detailed information of data abstraction, our presented dataset (e.g.ßdummyTurker
8734ß0.0ßCommQuery), and the code.ßdummyTurker
8735ß0.0ßHowever, they are automatically compressed by the HotCRP system.ßdummyTurker
8736ß0.0ßW2: We will carefully revise our paper and clarify the vague parts you mentioned with more illustrative examples.ßdummyTurker
8737ß0.0ß1) Our goal is beyond WikiSQL.ßdummyTurker
8738ß0.0ßSince WikiSQL is simple, we thus collected some complex SQL queries (such as superlative or break-down operations) to evaluate our model.ßdummyTurker
8739ß0.0ßAs seen in Sec.ßdummyTurker
8740ß0.0ß4.3, the advantage of our approach is more significant in CommQuery.ßdummyTurker
8741ß0.0ß2) Furthermore, as shown in Figure 5, with grammar rules, we have much better performance on smaller data which is exactly a desired property of a practical parser in realistic scenarios.ßdummyTurker
8742ß0.0ß3) Our semantic grammar rules are designed to be generic for multiple languages and broad domains.ßdummyTurker
8743ß0.0ßWe believe building the grammar once for multiple languages and broad domains are more worth doing.ßdummyTurker
8744ß0.0ß1) We did consider using WikiTableQuestions for evaluation.ßdummyTurker
8745ß0.0ßHowever, we finally believe WTQ dataset is not in the scope of our paper.ßdummyTurker
8746ß0.0ßOur NL2SQL task is for data analysis, but WikiTableQuestion is mainly designed for Table-based Q&A (e.g., aggregating through slicing/dicing is not the main goal).ßdummyTurker
8747ß0.0ßTo the best of our knowledge, WikiSQL is the largest open NL2SQL dataset which is most suitable.ßdummyTurker
8748ß0.0ß2) We tried to reimplement the model of Pasupat et.al (2015) on WikiSQL.ßdummyTurker
8749ß0.0ßHowever, we found that their model needs re-engineering on both grammar rules and feature construction for WikiSQL, which is hardly the original approach.ßdummyTurker
8750ß0.0ßOur semantic parsing approach meet the following three practical requirements: data agnostic, easy language expansion, and quick-start with small training set, which have never been systematically addressed before.ßdummyTurker
8751ß0.0ßOne of our principles is to incorporate semantic logics in data analysis domain with grammar rules as much as we can, and thus let neural model only learn the mapping between spans and grammar rules.ßdummyTurker
8752ß0.0ßNowadays, end-to-end neural models are prevalent, but we want to prove that carefully building grammar rules and finding a good way to combine them together is still much more valuable in practice.ßdummyTurker
8753ß0.0ßFrom the experimental results, we can get three conclusions on which parts of our approach that are actually better than some counterparts.ßdummyTurker
8754ß0.0ß1) Our approach outperforms other neural methods because our grammar with rich semantic knowledge and strong pre-conditions can avoid huge search space and better handle complex queries (Figure 5 and Table 7).ßdummyTurker
8755ß0.0ß2) Compared with floating parsers, our span-based parser parallels constituency parsing which has better performance, since it can use span information and reduce the search space (Table 3).ßdummyTurker
8756ß0.0ß3) Compared to log-linear model with sparse features, our neural based model has better performance and can reduce efforts of constructing hand-crafted features (Table 3).ßdummyTurker
8757ß0.0ßW6: We will give the details about our presented dataset.ßdummyTurker
8758ß0.0ßMany thanks for the detailed comments on the cases where our paper content is not clear enough.ßdummyTurker
8759ß0.0ßWe will take the following steps to make the paper clear.ßdummyTurker
8760ß0.0ß1) As our answer for “not self-contained” comments, we will make our paper self-contained without the supplemental files.ßdummyTurker
8761ß0.0ßFor example, we will refine section 2.2 by explaining the full set of grammar rules.ßdummyTurker
8762ß0.0ßWe will refine section 3.2 by explaining the details about data abstraction to improve the replicability.ßdummyTurker
8763ß0.0ß2) We will clarify the vague points by making the claims more concrete, adding examples:ßdummyTurker
8764ß0.0ßa) We will refine section 2.1 by giving a detailed explanation of the symbols, including the properties, semantics, and the design purpose.ßdummyTurker
8765ß0.0ßb) In section 2.2, we will explain the preconditions in detail and give examples to show how they work.ßdummyTurker
8766ß0.0ßc) In section 2.2.1, we will explain how Lazy-Interpreting Mechanism works using examples.ßdummyTurker
8767ß0.0ßd) In section 3.4 between line 447-449. we will explain how we encode a span.ßdummyTurker
8768ß0.0ß3) As our answer for “no proper wording” comments, we will correct them.ßdummyTurker
8769ß0.0ßFor example, we will change “grammar” to “grammar rule”.ßdummyTurker
8770ß0.0ß"We will change  ""specifies the logic of the deduced symbol"" to “is a logic predicate assigned to the deduced symbol”."ßdummyTurker
8771ß0.0ßWe really appreciate your careful reading and extensive constructive comments, which will help us revise our paper for the final version.ßdummyTurker
8772ß0.0ßBecause our text generation is applied in the Chinese domain and currently used in realistic scenarios.ßdummyTurker
8773ß0.0ßThis technology has already well served many users.ßdummyTurker
8774ß0.0ßWe deeply apologize for the unpleasant reading experience caused by translation.ßdummyTurker
8775ß0.0ßThis is the direction we will strive to optimize.ßdummyTurker
8776ß0.0ßOur experimental data are collected from some E-commerce scenarios, which are produced by human.ßdummyTurker
8777ß0.0ßThere are very few data sets published in the main text generation area.ßdummyTurker
8778ß0.0ßWe have already used this model in real business, so we use our actual data to experiment.ßdummyTurker
8779ß0.0ßCNN and RNN are the most important structures in the DNN domains.ßdummyTurker
8780ß0.0ßThere are indeed many ways to combine them.ßdummyTurker
8781ß0.0ßHowever, CNN and RNN are basically used independently or in series in NLP.ßdummyTurker
8782ß0.0ßWe hope to make RNN and CNN take advantage of each other in parallel.ßdummyTurker
8783ß0.0ßThen we further discussed the role of CNN and RNN in NLP in the experiment.ßdummyTurker
8784ß0.0ßCNN is to capture the high level feature of topic for each feature map, at the same time RNN is more able to express the relationship between sequences.ßdummyTurker
8785ß0.0ßI am very sorry that our writing and grammatical problems have brought a bad experience to reading.ßdummyTurker
8786ß0.0ßThis is also the direction of our follow-up efforts.ßdummyTurker
8787ß0.0ßThe most famous CNN related work is <Convolutional neural networks for sentence classification> and .ßdummyTurker
8788ß0.0ßThese are the ways to independently use CNN to build NLP models.ßdummyTurker
8789ß0.0ßWe tried to use CNN independently, whose BLEU is lower than which uses RNN independently.ßdummyTurker
8790ß0.0ßThe use of CNN and RNN in parallel is better than using the RNN model independently.ßdummyTurker
8791ß0.0ßItem recommendations in this paper are product descriptions that enable the user to create a desire to buy.ßdummyTurker
8792ß0.0ßThe input is the information of the product, including the participle of the title of the product, attribute information, including category, subject, etc.ßdummyTurker
8793ß0.0ßThe output are product descriptions that enable the user to create a desire to buy.ßdummyTurker
8794ß0.0ßI am very sorry that our writing and grammatical problems have brought a bad experience to reading.ßdummyTurker
8795ß0.0ßThis is also the direction of our follow-up efforts.ßdummyTurker
8796ß0.0ßThis structure allows RNN-encoder and CNN-encoder connected in parallel, then use attention mechanical to combine both information together to generate new attention weights.ßdummyTurker
8797ß0.0ßIn this model structure, all source and target inputs will first look up the embedding tables to retrieve the corresponding word representations, then we feed the word representation vectors into both RNN and stacked CNN.ßdummyTurker
8798ß0.0ßOur experimental data are collected from some E-commerce scenarios, which are produced by human.ßdummyTurker
8799ß0.0ßOur data contains input and output.ßdummyTurker
8800ß0.0ßThe input is the information of the product, including the participle of the title of the product, attribute information, including category, subject, etc.ßdummyTurker
8801ß0.0ßThe output are product descriptions that enable the user to create a desire to buy.ßdummyTurker
8802ß0.0ßHuman evaluation is indeed very important.ßdummyTurker
8803ß0.0ßWe use the BLEU as an evaluation because we mainly want to compare our optimization results horizontally on such a general structure.ßdummyTurker
8804ß0.0ßinput(the information of the product, including the participle of the title of the product, attribute information, including category, subject, etc.ßdummyTurker
8805ß0.0ß):flounced,oversize,lazy,autumn,sweater,female,stitching,fake two-piece,loose,Korean version,2018,fashionßdummyTurker
8806ß0.0ßoutput(product descriptions that enable the user to create a desire to buy): Two pieces of fake design make your figure more lengthy and loose version makes you comfortable.ßdummyTurker
8807ß0.0ßThis dress can suit a variety of different body shapes, fashionable and can match a variety of styles, so that people can easily wear a stylish effect.ßdummyTurker
8808ß0.0ßOur experimental data are collected from some E-commerce scenarios, which are produced by human.ßdummyTurker
8809ß0.0ßOur data contains input and output.ßdummyTurker
8810ß0.0ßThe input is the information of the product, including the participle of the title of the product, attribute information, including category, subject, etc.ßdummyTurker
8811ß0.0ßThe output are product descriptions that enable the user to create a desire to buy.ßdummyTurker
8812ß0.0ßThank you for your comments.ßdummyTurker
8813ß0.0ßWe agree in principle, but please take into account that this first presentation of the model is already very dense.ßdummyTurker
8814ß0.0ßIt is true that there are other methods than TF-IDF to model the relationship of textual features with the entities, but what we wanted at this stage was first and foremost to show that textual features are indeed helpful to mainstream tasks.ßdummyTurker
8815ß0.0ßFor this we chose to use TF-IDF, since it’s quite standard method and generally more expressive than binary alternatives.ßdummyTurker
8816ß0.0ßThe analysis you are suggesting is very interesting though, and it will be certainly part of future work on the model.ßdummyTurker
8817ß0.0ßWe think this comment is a bit unfairßdummyTurker
8818ß0.0ßWe have experiments on WordNet, the most well-known linguistic resource (120,000 entities); on SNOMED, a well-known medical knowledge base (320,000 concepts); and on CORA (a document network of 2,880 documents).ßdummyTurker
8819ß0.0ßThe three resources we use are quite diverse, and we think representative in showing the potential of the model.ßdummyTurker
8820ß0.0ßAgain, please take into account that, given the space limitations of a conference paper, we had to achieve a certain balance between the theoretical and the experimental aspects of our system.ßdummyTurker
8821ß0.0ßActually, several supplementary results were left out by necessity.ßdummyTurker
8822ß0.0ßThank you for your comments.ßdummyTurker
8823ß0.0ßIt is our belief that the current paper does much more than present an incremental improvement.ßdummyTurker
8824ß0.0ßAt a higher level, the paper provides a novel methodology for addressing an interesting and relatively unexplored task.ßdummyTurker
8825ß0.0ßAt a lower level, the multi-sense LSTM is a technical concrete contribution which seems to work very well in practice.ßdummyTurker
8826ß0.0ßFurther, it promotes an interesting shift in the evaluation of multi-prototype systems; it is certainly time to leave behind subjective intrinsic tasks such as word similarity datasets and move to real-world end tasks.ßdummyTurker
8827ß0.0ßYou are right that such a baseline would be extremely weak, since it would convert the system to a classifier trained on a dataset where the classes are as many as the inputs.ßdummyTurker
8828ß0.0ßNote this means that none of the “classes” in the test data would have any presence in the training set, and the system would deliver random results (unless we employ some form of one-shot learning).ßdummyTurker
8829ß0.0ßBut experimenting with other types of target spaces is an interesting idea and something we plan to do in the future.ßdummyTurker
8830ß0.0ßThe word embeddings are updated during training, but the entity embeddings are pre-trained according to the procedure of Section 3.1 and remain fixed, since they provide the gold references for the supervised learning.ßdummyTurker
8831ß0.0ßIn order to convert the model to a fully unsupervised one, as you are suggesting, we would probably need to use a form of autoencoder or a siamese architecture with negative sampling.ßdummyTurker
8832ß0.0ßIt is surely feasible, but we feel the current simpler setting is more robust for our purposes.ßdummyTurker
8833ß0.0ßWe would certainly like to properly compare the two alternatives in the future.ßdummyTurker
8834ß0.0ßThat is a good suggestion; we will add this baseline in the final version.ßdummyTurker
8835ß0.0ßThis is only because of the space limitationsßdummyTurker
8836ß0.0ßSee response in “Weakness argument 3” above.ßdummyTurker
8837ß0.0ßAs mentioned in the paper, we were able to extract 21,000 concepts (from a total of 320,000), using a simple string matching strategy for associating a SNOMED concept with a BabelNet concept.ßdummyTurker
8838ß0.0ßWe are very grateful for your insightful and valuable comments.ßdummyTurker
8839ß0.0ßThanks for reminding us of the previous work on error generation for NMT.ßdummyTurker
8840ß0.0ßWe will address and discuss these studies in the revised version.ßdummyTurker
8841ß0.0ßThe combination process is not complicated.ßdummyTurker
8842ß0.0ßWe first use the AMU16 system to correct the raw sentences.ßdummyTurker
8843ß0.0ßThen, we use the output of AMU16 as the input to our GEC system; in other words, we edit on top of the output of AMU16, which is very effectiveßdummyTurker
8844ß0.0ßWe are very grateful for your insightful and valuable comments.ßdummyTurker
8845ß0.0ßWe will give more in-depth analyses for the performance improvement by error types.ßdummyTurker
8846ß0.0ßWe will thoroughly and carefully proofread this paper.ßdummyTurker
8847ß0.0ßAs Algorithm 3 demonstrates, in dual-boost learning, the number of training examples in an epoch becomes twice the size of the original training examples.ßdummyTurker
8848ß0.0ßFor example, without the non-public Lang-8 data, we have about 2.5 million original training examples, which means that there are about 5 million training examples in an epoch in dual-boost learning.ßdummyTurker
8849ß0.0ßIn this case, given that our batch size is 128, the model takes about 40,000 iterations during one epoch.ßdummyTurker
8850ß0.0ßFor 40 training epochs, it takes about 1.6 million iterations.ßdummyTurker
8851ß0.0ßThe hyper-parameters such as parameter initialization, the number of layers, hidden and vocabulary size and dropout rate are empirically set, according to previous empirical studies on GEC and NMT as well as our experiments on the development set.ßdummyTurker
8852ß0.0ßSorry for the confusion of the highlights (bold fonts).ßdummyTurker
8853ß0.0ßThe highlighted results are the best scores in different settings (e.g., single/ensemble, with/without non-public Lang-8 data).ßdummyTurker
8854ß0.0ßWe will make it clearer.ßdummyTurker
8855ß0.0ßAccording to our analysis, performance on common error types (e.g., subject-verb agreement and noun and verb forms) is shown to be better than previous approaches, since most errors generated by our approaches for training are of these types.ßdummyTurker
8856ß0.0ßIn conclusion section, we claim that our approaches largely improve normal seq2seq, which is statistically significant, according to Wilcoxon Signed-Rank Test.ßdummyTurker
8857ß0.0ßIn addition, our best systems' performance is also tested to be statistically significantly better than the state-of-the-art systems (AMU16 and NUS17) on CoNLL-2014 benchmark.ßdummyTurker
8858ß0.0ßWe will fix the reference formats to make them consistent.ßdummyTurker
8859ß0.0ßThank you very much for your insightful review.ßdummyTurker
8860ß0.0ßWe see your points and will consider them in the revision (if the paper will be accepted) or in the future.ßdummyTurker
8861ß0.0ßMAE and F1 scores in the personality have two different meanings, the first refer to the regression results and the second to the classification results where the threshold is the median score.ßdummyTurker
8862ß0.0ßTherefore it is possible for a sample to exhibit a very high or very low output score, with an high difference from the reference, but be indeed classified correctly.ßdummyTurker
8863ß0.0ßAmong emotion (and personality) languages there are many differences related to the dataset sizes and nature (spontaneous vs acted speech, number and gender of speakers, way and amount they stress the emotions, etc.ßdummyTurker
8864ß0.0ß).ßdummyTurker
8865ß0.0ßWe believe that the spectrogram features are more limited than the raw waveform, with the latter allowing the network to learn more low and high level features when more data is available.ßdummyTurker
8866ß0.0ßOne of the possible reason might have been identified in weakness 1.ßdummyTurker
8867ß0.0ßReviewer #1ßdummyTurker
8868ß0.0ßThank you very much for your insightful review.ßdummyTurker
8869ß0.0ßWe will be grateful if the reviewer can expand on how to improve the baselines for the camera-ready.ßdummyTurker
8870ß0.0ßWe would like to point out that ensembling our method with IR results in improved performance.ßdummyTurker
8871ß0.0ßWe are unsure what paper by Cardie (we assume Claire Cardie) the reviewer refers to and thus are unable to properly respond.ßdummyTurker
8872ß0.0ßYes, we tried it and DocQN outperforms DQN at navigation accuracy when combined with TF-IDF by 0.5-1 point.ßdummyTurker
8873ß0.0ßWe do not claim that our approach is more efficient than IR.ßdummyTurker
8874ß0.0ßWe do claim that our approach is more flexible and opens a path to research on question answering where reasoning is necessary and there is low lexical overlap between the question and document.ßdummyTurker
8875ß0.0ßWe are thankful for your constructive comments and feedback.ßdummyTurker
8876ß0.0ß"""Ablation studies on test set"""ßdummyTurker
8877ß0.0ßWe conducted an ablation study on the test dataset to show the effectiveness of each component.ßdummyTurker
8878ß0.0ßWe are indeed planning ablation study on the dev set, the result of which can be included in the final version.ßdummyTurker
8879ß0.0ß"""Lack of comparison of the number of parameters of the proposed model"""ßdummyTurker
8880ß0.0ßTo address the Reviewer1's concern about lack of comparison with respect to the number of parameters, we will add a comparison result into the experimental section.ßdummyTurker
8881ß0.0ß"""Is the proposed network bigger or smaller than the previous state of the art?"""ßdummyTurker
8882ß0.0ßOur recurrent and co-attentive features of each layer are connected using the concatenation operation like Densenet (Huang et al., 2017).ßdummyTurker
8883ß0.0ßHowever, unlike the original Densenet, we don't use the bottleneck layer because the depth of ours is not too deep like convnets.ßdummyTurker
8884ß0.0ßAs mentioned in the weakness argument 2, we will compare the number of parameters.ßdummyTurker
8885ß0.0ßWe are thankful for pointing it out, and we'd like to study the effect of the bottleneck layer in very deep recurrent layers for our future work.ßdummyTurker
8886ß0.0ßWe greatly appreciate your thorough review of this submission.ßdummyTurker
8887ß0.0ßPlease let us provide some clarification to your concerns and questions.ßdummyTurker
8888ß0.0ß"""Chen et al's model"""ßdummyTurker
8889ß0.0ßChen et al’s model used some conventional features like exact match flag, POS and NER tags.ßdummyTurker
8890ß0.0ßAnd Gong et al’s model used also exact match flag and POS tags for syntactical features.ßdummyTurker
8891ß0.0ßWe tried to avoid using language-dependent tools as much as possible, so we only applied exact match flags which could be effectively used without any of those tools.ßdummyTurker
8892ß0.0ß"""Nie and Bansal's NLI model"""ßdummyTurker
8893ß0.0ßNie and Bansal's NLI model used concatenated shortcuts only for the recurrent features, however we used both recurrent and co-attentive features.ßdummyTurker
8894ß0.0ßCo-attentive features are used for the local interaction with each other, and these are well-semanticized over multiple layers through the concatenation operation.ßdummyTurker
8895ß0.0ßThe biggest difference is that we use all features of the preceding layers as well as word embedding features for the global interaction, which plays an important roll as a collective knowledge, while Nie and Bansal's model only use the hidden features of the uppermost layer for the global interaction.ßdummyTurker
8896ß0.0ßIn learning semantics, different levels of linguistic combination are needed and our word-level embeddings and different levels of abstraction and granularity of deep RNNs are modeled regarding linguistic semantics.ßdummyTurker
8897ß0.0ß"""Parikh et al's model"""ßdummyTurker
8898ß0.0ßThe main difference between Parikh et al's model and ours is that we keep the concatenated features unchanged.ßdummyTurker
8899ß0.0ßHowever, Parikh et al's model uses these concatenated features as an input of the feed forward network.ßdummyTurker
8900ß0.0ßSo their concatenated features are not preserved intact.ßdummyTurker
8901ß0.0ß"""What is the problem of semantic gap?"""ßdummyTurker
8902ß0.0ßSemantic gap is the discrepancy between two objects.ßdummyTurker
8903ß0.0ßHigh-level semantics like a human's interpretation and low-level features like a machine's information can be one example of semantic gap.ßdummyTurker
8904ß0.0ßAnd two different words can be another example of semantic gap, especially in the conventional natural language tasks because the words are considered as atomic symbols.ßdummyTurker
8905ß0.0ßMinimizing this semantic gap is important in learning semantics.ßdummyTurker
8906ß0.0ßRecently, word embedding, attention mechanism and deep models are helpful to overcome the semantic gap problem.ßdummyTurker
8907ß0.0ß"""Why does the summation operation impede the flow of information?"""ßdummyTurker
8908ß0.0ßDue to the summation operation, the features cannot be preserved intact.ßdummyTurker
8909ß0.0ßConsider two sets of features in opposite directions are added.ßdummyTurker
8910ß0.0ßIn this case, the original directional information is lost by the summation operation.ßdummyTurker
8911ß0.0ßThus, compared with the concatenation operation, the summation operation may impede the information flow.ßdummyTurker
8912ß0.0ßThank you for your positive evaluation of our work as well as your thoughtful and detailed comments.ßdummyTurker
8913ß0.0ß"""I'm curious to see how its efficiency compared to existing models"""ßdummyTurker
8914ß0.0ßLike Reviewer3's thoughtful question, efficiency-related problems are substantially important in practice and we need a comparison study about the speed and the number of parameters of the model.ßdummyTurker
8915ß0.0ßWe will do comparative experiments and add the results in the final version.ßdummyTurker
8916ß0.0ß"""I'd like to see a discussion why a complex encoder performed better than a complex interaction layer, and whether there is a chance to combine strengths of both."""ßdummyTurker
8917ß0.0ßWe use two well-semanticized vectors for the global interaction, and it seems relatively simpler than others' complex interaction layers.ßdummyTurker
8918ß0.0ßHowever, the co-attentive information in our encoding step can also be considered as an interaction, more specifically a local interaction.ßdummyTurker
8919ß0.0ßThese local interactive informations in each recurrent layer, made of an attention mechanism, are well preserved by dense connections and become a part of collective knowledge.ßdummyTurker
8920ß0.0ßNevertheless, your insightful comment of the method to combine our encoder with a better and more complex interaction is well worth a further study.ßdummyTurker
8921ß0.0ß"""How is the model performing on negations?"""ßdummyTurker
8922ß0.0ßWe couldn't investigate the linguistic phenomena such as pronouns, negation, existentials and modals, etc.ßdummyTurker
8923ß0.0ßAnd this work will be very helpful in understanding semantics of language.ßdummyTurker
8924ß0.0ßWe will make sure to carry out in-depth research for our future work.ßdummyTurker
8925ß0.0ßWe thank the reviewer for the insightful and detailed feedback.ßdummyTurker
8926ß0.0ßLet us address the weaknesses in turn:ßdummyTurker
8927ß0.0ß* While we agree that comparison to work using treebank syntax would be interesting, we believe that our unsupervised set-up is valuable on its own.ßdummyTurker
8928ß0.0ßEffective syntactic parsers are available only for a couple of dozen languages, they are brittle when applied out-of-domain and are relatively computationally expensive.ßdummyTurker
8929ß0.0ßAlso, in our work we rely on sub-word units (BPE), as standard in modern NMT, whereas treebank grammars are defined on the word level.ßdummyTurker
8930ß0.0ßSo there is no straightforward and fair way to compare the two, at least in the context of a short paper.ßdummyTurker
8931ß0.0ß* While our data sets are not huge, they are of good quality (news and scientific text) and of similar size to the largest data set used by Hashimoto & Tsuruoka.ßdummyTurker
8932ß0.0ßAlso noticed that we did not polish the data in any sense (we limited ourselves to very basic common-practice pre-processing).ßdummyTurker
8933ß0.0ßIn particular, we believe there has been a misunderstanding about the vocabulary size.ßdummyTurker
8934ß0.0ßThe numbers we indicate are after pre-processing the data into sub-words (BPEs and SentencePieces) for which we used a vocabulary of about 16k sub-words (per language).ßdummyTurker
8935ß0.0ßThis is a reasonable choice, see Wu et al.ßdummyTurker
8936ß0.0ß(2016): https://arxiv.org/abs/1609.08144 for a discussion.ßdummyTurker
8937ß0.0ß* We observed that deviations (in BLEU) across different seeds were small.ßdummyTurker
8938ß0.0ßWe could not afford rerunning all the experiments multiple times, but for En->De (CNN) we did and observed that standard deviation in BLEU was below 0.14.ßdummyTurker
8939ß0.0ßWe also verified results using the paired bootstrap resampling method (Koehn, EMNLP’14).ßdummyTurker
8940ß0.0ßAll differences between the Emb./CNN baselines and their latent-graph counterparts are significant with p<0.05, except for CNN vs Latent-graph-CNN on De-En.ßdummyTurker
8941ß0.0ßThank you for the suggestions.ßdummyTurker
8942ß0.0ßActually, we have a task definition in the Section 3, we will make it more clear in the revision.ßdummyTurker
8943ß0.0ßWe will add a more comprehensive analysis about the results as suggested by you.ßdummyTurker
8944ß0.0ßThe baselines are directly generated by the tools, like sumy.ßdummyTurker
8945ß0.0ßIn the future, we will design stronger baselines to evaluate the performance of our method.ßdummyTurker
8946ß0.0ßWe will add a more clear justification in the revision.ßdummyTurker
8947ß0.0ßWe assume that only one article would not be enough for describing a complicated formula.ßdummyTurker
8948ß0.0ßIn the revision, we will compare our results with the descriptions that are constructed from top-1 ranked article.ßdummyTurker
8949ß0.0ßMeanwhile, we will add more analyses about why we regard this task as a multiple-document summarization task.ßdummyTurker
8950ß0.0ßWe sort the descriptions which are generated by different methods, and then calculate the percentage of the description which is ranking in top-n position.ßdummyTurker
8951ß0.0ßWe will design stronger baselines to evaluate the performance of our system as your suggestion.ßdummyTurker
8952ß0.0ßThank you very much!ßdummyTurker
8953ß0.0ßThanks for your comments.ßdummyTurker
8954ß0.0ßWe make a revision about this problem.ßdummyTurker
8955ß0.0ßWe will evaluate the method by more conventional metrics as suggested.ßdummyTurker
8956ß0.0ßThanks, we will add more comprehensive analyses, meanwhile, we will pick up two kinds of casesßdummyTurker
8957ß0.0ßWe hypothesis that the more recent and complex documents are based on the classical and simple documents.ßdummyTurker
8958ß0.0ßOrdering the sentences by a chronological fashion will make the difficulty of description gradually increase, we think that be helpful for understanding.ßdummyTurker
8959ß0.0ßWe hypothesis that the more complex and recent documents can evolve from the simple and classical documents.ßdummyTurker
8960ß0.0ßThe descriptions of formulae which are described from simple to complex can easier to be understood by students.ßdummyTurker
8961ß0.0ßWe will describe the extraction more rigorously as suggested.ßdummyTurker
8962ß0.0ßActually, we extracted the surrounding text similar as the method which are proposed in literature ” Yuehan Wang, Liangcai Gao, Simeng Wang, Zhi Tang, Xiaozhong Liu, and Ke Yuan.ßdummyTurker
8963ß0.0ß2015.ßdummyTurker
8964ß0.0ßWikimirs 3.0ßdummyTurker
8965ß0.0ßThanks for your comments and support to our work!ßdummyTurker
8966ß0.0ßWe will endeavor here to discuss theßdummyTurker
8967ß0.0ßWeaknesses  and answer the questions。ßdummyTurker
8968ß0.0ßFrom our literature review, most of works on act classification are based on two datasets, namely Switchboard Act Corpus and MapTask Dialogue Act Corpus, thus our proposed model is first verified on these two datasets.ßdummyTurker
8969ß0.0ßIn our recent work, we also apply our model into a real world medical dataset which contains 200,000 training samples with 35 labels.ßdummyTurker
8970ß0.0ßOur model can also achieve a state-of-the-art performance of 85% accuracy.ßdummyTurker
8971ß0.0ßFirstly, we counted frequencies of each label in two datasets.ßdummyTurker
8972ß0.0ßSecondly, in training datasets, we continue to record the conversation segments with rare labels (The two datasets contain several files, each file records one conversation contains several segments, every segment consists of some utterances).ßdummyTurker
8973ß0.0ßThe ‘up sampling’ in our experiments is to repeat sampling the recorded segments as a whole several times.ßdummyTurker
8974ß0.0ßAs we observe the confusion matrix, rare labels such as ‘ad’, ‘sv’,’qyd’, are harder to predict.ßdummyTurker
8975ß0.0ßHere we demonstrate the confusion matrix for SwDA for 10 DA class labels, where the row denotes the true labels and the column denotes the predicted labels.ßdummyTurker
8976ß0.0ßWe show it in an anonymous website: http://mypaper.droppages.com/.ßdummyTurker
8977ß0.0ßActually, we have employed pre-trained word vectors such as Word2Vec and GloVe to initialize the encoder of our model, while from our experimental results, we find that the pre-trained vectors has little effect on the final performance, so we don’t report the results in this paper.ßdummyTurker
8978ß0.0ßThank you for your elaborative comments.ßdummyTurker
8979ß0.0ßWe carefully to discuss the weaknesses you point out.ßdummyTurker
8980ß0.0ßAlthough our model is a combination if dynamic memory networks and adversarial learning, there are still some merits that cannot be ignored.ßdummyTurker
8981ß0.0ßTo the best of our knowledge, it is the first time that we use dynamic memory networks (DMN) for the task of dialogue act classification.ßdummyTurker
8982ß0.0ßFurthermore, in the DMN, we replace the Bi-GRU with a pyramid encoder, which can boost the computational efficiency and increase the accuracy.ßdummyTurker
8983ß0.0ßIn addition, we also propose a unified framework of DMN and adversarial learning for Dialogue Act Classification, which would give some insights to other researchers who are conducting similar tasks such multiclass classification.ßdummyTurker
8984ß0.0ßActually, reasoning is the capability of the dynamic memory networks.ßdummyTurker
8985ß0.0ßThe episode memory module combining with the question module iterates over the input evidences representations multiple times to update the memory.ßdummyTurker
8986ß0.0ßThis process is reasoning.ßdummyTurker
8987ß0.0ßIn DA task, reasoning refers to in episode memory module, we first calculate contextual vector $c_t$ and then update the memory $m_t$ according to the previous memory $m_{t-1}$, $c_t$ and question vector $q$.ßdummyTurker
8988ß0.0ß$c_t$ contains the input evidences information and combines $m_{t-1}$ with $q$ to generate the new $m_t$ßdummyTurker
8989ß0.0ßThe experiment results show that the two-layer pyramid encoder achieves the highest accuracy and the three-layer result is between one-layer's and two-layer’s.ßdummyTurker
8990ß0.0ßWe suppose that in English two neighboring words can structure a phrase which makes sense the two-layer pyramid encoder performing best.ßdummyTurker
8991ß0.0ßWhen we use three-layer encoder, every output of the encoder corresponds to four neighboring words, however, combining the four words may have no sense.ßdummyTurker
8992ß0.0ßSo even if three-layer encoder indeed compacts the information, they may be meaningless for attention mechanism soft alignment.ßdummyTurker
8993ß0.0ßActually, we have an experiment analysis on the influence of adversarial learning.ßdummyTurker
8994ß0.0ßThe experimental results show that adversarial learning can really boost the performance of act classification.ßdummyTurker
8995ß0.0ßWe attribute this success to two factors: A) adversarial training provides a means of regularizing supervised learning algorithms.ßdummyTurker
8996ß0.0ßB) Adversarial learning can improve robustness to adversarial examples and generalization performance for original examples.ßdummyTurker
8997ß0.0ßThank you so much for your insightful comments and great suggestions.ßdummyTurker
8998ß0.0ßA: The coverage is not our core contribution, so we did not include it in current version.ßdummyTurker
8999ß0.0ßWe conducted additional experiments by incorporating a standard word coverage mechanism with the proposed models on German-English corpus.ßdummyTurker
9000ß0.0ßThe table below shows the results.ßdummyTurker
9001ß0.0ßThe performance of Gates_Shared_Decoder with coverage acquired the best result.ßdummyTurker
9002ß0.0ßWe will add detailed discussions in our final version.ßdummyTurker
9003ß0.0ßWe agree that Chen et al.ßdummyTurker
9004ß0.0ß(2017) is an important and successful prior work that we will extensively reference.ßdummyTurker
9005ß0.0ßModels                                  Coverage                   German-English(Test)ßdummyTurker
9006ß0.0ßRNNSearch                             O/word                              23.80/24.63ßdummyTurker
9007ß0.0ßIndependen_Decoder             O/word                             24.45/25.34ßdummyTurker
9008ß0.0ßGates_Shared_Decoder	        O/word                              25.04/25.72ßdummyTurker
9009ß0.0ßFully_Shared_Decoder            O/word                             25.27/25.60ßdummyTurker
9010ß0.0ßA: Although the improvement of our proposed model is not that huge, we still outperform all other baselines (more than 0.5 BLEU).ßdummyTurker
9011ß0.0ßMore importantly, we believe that our proposed architecture makes a significant methodological contribution to the research on NMT and NLG along two directions: 1, The POS-tag is proved to be a useful feature for decoder in seq2seq architecture.ßdummyTurker
9012ß0.0ß2.ßdummyTurker
9013ß0.0ßOur approach explores various ways of sharing decoding information in a multi-task learning framework.ßdummyTurker
9014ß0.0ßA: We added the coverage experiments as shown in weakness argument 1.ßdummyTurker
9015ß0.0ßA: We used 1.25M training set (same as Zhang et al.ßdummyTurker
9016ß0.0ß(2017) [http://www.aclweb.org/anthology/P17-1139], Wang et al.ßdummyTurker
9017ß0.0ß(2017) [http://www.aclweb.org/anthology/D17-1149]).ßdummyTurker
9018ß0.0ßThank you so much for your insightful comments and great suggestions.ßdummyTurker
9019ß0.0ßA: Thanks for pointing out the important missing references.ßdummyTurker
9020ß0.0ßWe will add these references and do more careful literature study.ßdummyTurker
9021ß0.0ßA: We did not modify the beam search in the fully shared decoder.ßdummyTurker
9022ß0.0ßThe beam search for predicting word sequence and POS tag sequence were completely separate.ßdummyTurker
9023ß0.0ßWe did not combine the embeddings of the POS tag and word.ßdummyTurker
9024ß0.0ßWe will make our claim more precise and clearer in the final version.ßdummyTurker
9025ß0.0ßA: Although the improvement of our proposed model is not that huge, we believe that our proposed architecture makes a significant methodological contribution to the research on NMT and NLG along two directions: 1, The POS-tag is proved to be a useful feature for decoder in seq2seq architecture.ßdummyTurker
9026ß0.0ß2.ßdummyTurker
9027ß0.0ßOur approach explores various ways of sharing decoding information in a multi-task learning framework.ßdummyTurker
9028ß0.0ßPlease see weakness argument 2.ßdummyTurker
9029ß0.0ßPlease see weakness argument 2.ßdummyTurker
9030ß0.0ßA: We will rephrase these lines and make our claim clearer.ßdummyTurker
9031ß0.0ßThank you so much for your insightful comments and great suggestions.ßdummyTurker
9032ß0.0ßA: Dalvi et al.ßdummyTurker
9033ß0.0ß(2017) is an important and successful prior work that we will extensively reference and appropriately contrast with in our final version.ßdummyTurker
9034ß0.0ßAlthough morphological tagging is very similar to POS tagging, our paper proved that POS tagging is also a useful feature for NMT decoder like CCG supertagging or morphological tagging.ßdummyTurker
9035ß0.0ßMoreover, our approach explores various ways of sharing decoding information in a multi-task learning framework.ßdummyTurker
9036ß0.0ß(1)	As we know, the LSTM is a special form of RNNs with three gated units, input, output and forget.ßdummyTurker
9037ß0.0ßTaking the shared input gate as an example, the input gate formula is it = σ (Wi・[ht-1, xt] + bi), where the parameters of Wi and bi are shared for predicting word sequence and POS tag sequence.ßdummyTurker
9038ß0.0ßAll other parameters are task-independent.ßdummyTurker
9039ß0.0ß(2)	We will add experimental results and analysis about sharing different gates in our final version.ßdummyTurker
9040ß0.0ßMoreover, our approach explores how to share decoding information in a multi-task learning framework.ßdummyTurker
9041ß0.0ßA: For the first issue, we fully agree that weighing the contribution of the two loss function is an important baseline that we will appropriately contrast with in our final version.ßdummyTurker
9042ß0.0ßFor the second issue, we actually conducted this comparison experiment.ßdummyTurker
9043ß0.0ßHowever, due to the restrictions on the number of pages, we did not report.ßdummyTurker
9044ß0.0ßAs the table below shows, the results were better than interleaving words and CCG supertagging.ßdummyTurker
9045ß0.0ßModels								Chinese-English(Ave)	German-English(Test)ßdummyTurker
9046ß0.0ßInterleaving_words_and_CCG_supertags*		32.37					25.50ßdummyTurker
9047ß0.0ßInterleaving_words_and_POS_tags*			32.61					25.63ßdummyTurker
9048ß0.0ßFully_Shared_Decoder						32.95					25.27ßdummyTurker
9049ß0.0ßFully_Shared_Decoder*						32.01					26.05ßdummyTurker
9050ß0.0ßA: In Nadejde et al.ßdummyTurker
9051ß0.0ß(2017) work, they also incorporated source-side syntax information.ßdummyTurker
9052ß0.0ßWe reported results of Nadejde et al.ßdummyTurker
9053ß0.0ß(2017), which also contains source-side syntax features.ßdummyTurker
9054ß0.0ßAlthough we will release the code, we agree that the hyperparameter is very important and we will clarify the parameter settings in our final version.ßdummyTurker
9055ß0.0ßWe limit vocabulary size to 30K for both languages.ßdummyTurker
9056ß0.0ßThe word and POS tag embedding dimension is set to 512.ßdummyTurker
9057ß0.0ßThe dimension of hidden layer is set to 1,024.ßdummyTurker
9058ß0.0ßIn training, the batch size is set to 80.ßdummyTurker
9059ß0.0ßWe use the Adam (α=0.001, β1=0.9, β2=0.999, ε=0.999) for optimizing model parameters.ßdummyTurker
9060ß0.0ßIn decoding, the beam size is set to 10 for word sequence and 5 for POS tagging sequence.ßdummyTurker
9061ß0.0ßA: Yes, please see weakness argument 3.ßdummyTurker
9062ß0.0ßA: Yes, we evaluated the quality of the POS tagging task in the developed methods.ßdummyTurker
9063ß0.0ßHowever, due to the restrictions on the number of pages, we did not report.ßdummyTurker
9064ß0.0ßWe will add them and corresponding analysis.ßdummyTurker
9065ß0.0ßMoreover, we will add a weight parameter to the joint loss and contrast with in our final version.ßdummyTurker
9066ß0.0ßA: Yes, I agree with your colleague.ßdummyTurker
9067ß0.0ßOn the other hand, this also shows that the output of our model is closer to the reference translation.ßdummyTurker
9068ß0.0ßThe problem lies in the reference, not the model.ßdummyTurker
9069ß0.0ßA: We compared different ways of interleave tasks and found that training the POS tagging model first and then training the NMT model can achieve the best results.ßdummyTurker
9070ß0.0ßIn the future version, we will make our claim more precise.ßdummyTurker
9071ß0.0ßA: We will fix grammatical errors and make our claim clearer.ßdummyTurker
9072ß0.0ßWe will provide a manual analysis of a subsets of the terms extracted by our approach so that difference between the annotations can be compared and differences can be spotted.ßdummyTurker
9073ß0.0ßThere is a large body of work in linguistics addressing the analysis of  co-occurrence, but it has never been used to extract terms in the way we are proposing.ßdummyTurker
9074ß0.0ßWe will report the statistical significance of the comparison between our results and the baselines.ßdummyTurker
9075ß0.0ßWe will clarify certain aspects of our methodology, add details to the materials section, and carry-out a more detailed analysis of the results.ßdummyTurker
9076ß0.0ßOur response is detailed below.ßdummyTurker
9077ß0.0ßThe second rank is applied to all candidate terms on the basis of their co-occurrence with the terms from the first quartile.ßdummyTurker
9078ß0.0ßWe will clarify this on the new version.ßdummyTurker
9079ß0.0ßPOS-tagging is used to lemmatize and to match term-like morpho-syntactic patterns.ßdummyTurker
9080ß0.0ßQ3: Unfortunately, Termostat was not available online at the time of submission, but we will inquiry to the developers to see if they will let us use the program.ßdummyTurker
9081ß0.0ßQ4: According to our analysis of corpus data, both terms exist.ßdummyTurker
9082ß0.0ßOne designates a kind of event and the other a kind of device.ßdummyTurker
9083ß0.0ßQ5: As explained in Section 3.1., the reference corpus D is a 5 billion-token sample from patents released by the USPTO patent office, downloaded from United States Patent and Trademark Office Bulk Downloads.ßdummyTurker
9084ß0.0ßQ6: We used the most recent version of the program.ßdummyTurker
9085ß0.0ßOur criticism was not directed towards this tool, but to our own use of it in a specialized corpus, which is not the scenario it was trained for.ßdummyTurker
9086ß0.0ßQ7: The reviewer is right.ßdummyTurker
9087ß0.0ßPatents can be consider as a genre, but not a domain.ßdummyTurker
9088ß0.0ßThe patents we used pertain to different technical domains (chemistry, biotechnology, green energy, etc.).ßdummyTurker
9089ß0.0ßThis clarification will be addressed.ßdummyTurker
9090ß0.0ßwe prefer to break down our reply to each of the reviewer's comments.ßdummyTurker
9091ß0.0ß1.ßdummyTurker
9092ß0.0ßAppropriate references will be added in the Introduction.ßdummyTurker
9093ß0.0ß2.ßdummyTurker
9094ß0.0ßYes, the notation in the paper is incorrect.ßdummyTurker
9095ß0.0ßWe will correct it as indicated by the reviewer.ßdummyTurker
9096ß0.0ß3.ßdummyTurker
9097ß0.0ßWe will reduce the space devoted to background, but we would prefer not to eliminate the entire section, as many readers may need it to understand the motivation for this work.ßdummyTurker
9098ß0.0ßOne must bear in mind that there will be one extra page in the final version of the paper, so we will have enough space to provide more details on the methodology without the need to sacrifice the background section.ßdummyTurker
9099ß0.0ß4.ßdummyTurker
9100ß0.0ßThe missing information will be added.ßdummyTurker
9101ß0.0ßAs for evidence of our assessment of the ideal size of the input document, that is precisely the raison d'etre of our strategy, which is text-based.ßdummyTurker
9102ß0.0ßThe ideal size is that of a scientific paper.ßdummyTurker
9103ß0.0ßA short text, like an abstract, will not generate enough co-occurrences, and a whole textbook can cover many topics and this will blur the association between terms, as there would potentially be too much distance between the occurrence of one term and the other.ßdummyTurker
9104ß0.0ßTwo terms are semantically related if they consistently co-occur in the same texts with significant frequency.ßdummyTurker
9105ß0.0ßIf we have enough space, we could provide evidence of this by comparing the results of the extraction of a patent vs. a fragment of this patent.ßdummyTurker
9106ß0.0ß5.ßdummyTurker
9107ß0.0ßThe text is the input text.ßdummyTurker
9108ß0.0ßWe say our system is text-based because it analyses one text (document) at a time.ßdummyTurker
9109ß0.0ßIn the reported experiments, a text is a patent.ßdummyTurker
9110ß0.0ß6.ßdummyTurker
9111ß0.0ßWe appreciate this suggestion, and terms distributions by n-grams will be reported for the list produced by our system and for the gold-standard.ßdummyTurker
9112ß0.0ßWe will report the n-gram term distribution in the gold-standard and systems outputs to draw some light on this point.ßdummyTurker
9113ß0.0ß7.ßdummyTurker
9114ß0.0ßWe see now that there was not enough detail.ßdummyTurker
9115ß0.0ßThis is partly due to the fact that our treatment of candidates in this initial phase of the processing is pretty standard in the terminology industry.ßdummyTurker
9116ß0.0ßPrepositions such as the example mentioned are not eliminated when they appear between two valid components of a multi-word term.ßdummyTurker
9117ß0.0ßCandidates are eliminated when they include one such preposition in initial or final position.ßdummyTurker
9118ß0.0ß8.ßdummyTurker
9119ß0.0ßParameters are selected by convenience (R), by convention (u) or empirically (l).ßdummyTurker
9120ß0.0ßWe will try our best to justify their selection and, if space allows, to offer examples of what results would be obtained if these parameters are changed.ßdummyTurker
9121ß0.0ß9.ßdummyTurker
9122ß0.0ßYes, we totally agree with this comment.ßdummyTurker
9123ß0.0ßWe will try to group the baselines as suggested by the reviewer.ßdummyTurker
9124ß0.0ßOur criterion for their selection was that they are statistical methods and thus comparable to our method.ßdummyTurker
9125ß0.0ßJudea's results were not reported because they pertain to a different family (machine learning algorithms).ßdummyTurker
9126ß0.0ß10.ßdummyTurker
9127ß0.0ßThis is normal when the extraction is made by human annotators.ßdummyTurker
9128ß0.0ßThey tend to omit many terms.ßdummyTurker
9129ß0.0ßThis is partly one of the reasons why automatic means are needed.ßdummyTurker
9130ß0.0ß11.ßdummyTurker
9131ß0.0ß"""the authors are human beings and detected the chargeable battery"": After careful re-reading of this comment, we were unfortunately incapable of understanding the point being raised."ßdummyTurker
9132ß0.0ß12.ßdummyTurker
9133ß0.0ßWe will add the analysis of P@K.ßdummyTurker
9134ß0.0ß13.ßdummyTurker
9135ß0.0ßMissing information in references: This was an accident: in these three cases, a dash was used instead of a sequence of two hyphens, in the “pages” field of the bibtex file.ßdummyTurker
9136ß0.0ßAs a consequence, the latex compiler ignored the rest of the information in those entries.ßdummyTurker
9137ß0.0ßWhen we realised this, it was too late.ßdummyTurker
9138ß0.0ßBut the problem is fixed now.ßdummyTurker
9139ß0.0ßThanks for the valuable feedback.ßdummyTurker
9140ß0.0ßWe are grateful for your insightful comments and thoughtful suggestions.ßdummyTurker
9141ß0.0ßWe totally agree with your point that the performance might be further improved if we took some sophisticated active learning models (Zhe et al, 2008; Yang et al, 2015; Ramirez-Loaiza et al, 2017) for use.ßdummyTurker
9142ß0.0ßIn this paper, we focus on verifying whether the methodological framework does work.ßdummyTurker
9143ß0.0ßOur method can be fairly compared with the work of Rutherford and Xue (2015), Liu et al (2016) and Wu et al (2017).ßdummyTurker
9144ß0.0ßIt is not only because the same test data is used but all the work proposes an outstanding expansion method to enlarge the training data set.ßdummyTurker
9145ß0.0ßWe promise to illustrate, compare and analyze the binary classification performance of the work, by buying an additional page.ßdummyTurker
9146ß0.0ßThanks much for your valuable suggestions.ßdummyTurker
9147ß0.0ßImplicit relations are essentially different from explicit relations.ßdummyTurker
9148ß0.0ßBesides the omitted connectives, the syntaxes, sentiments and coreferences of the implicitly-related arguments are most probably different from that of the explicitly-related ones.ßdummyTurker
9149ß0.0ßWe will provide some as concrete examples as possible to exhibit the difference and define the task more clearly.ßdummyTurker
9150ß0.0ßThanks much for your thoughtful suggestions on the presentation.ßdummyTurker
9151ß0.0ßWe will follow the suggestions to improve the paper.ßdummyTurker
9152ß0.0ßWe are grateful for your insightful comments on the weakness.ßdummyTurker
9153ß0.0ßThank much for your thoughtful suggestions.ßdummyTurker
9154ß0.0ßWe will expand the discussion of AL, including that on the workflow, as well as the instructions of sampling and batch selection functions.ßdummyTurker
9155ß0.0ßThe model usually treats the instances as informative which obtain a higher uncertainty score (about 1) during the active learning process.ßdummyTurker
9156ß0.0ßSuch an uncertainty score reveals a fact that the definite relation of an instance (argument pair) cannot be determined by the baseline model at a higher level of confidence.ßdummyTurker
9157ß0.0ßAnd thus, if it is used as an additional example in the training set, the baseline model most probably learns much from it.ßdummyTurker
9158ß0.0ßTherefore, we call them the informative instances.ßdummyTurker
9159ß0.0ßBy contrast, we filter the instances which obtain a much lower uncertainty score which are either definitely positive or negative.ßdummyTurker
9160ß0.0ßThe baseline model have had the capacity of recognizing the relations of such kind of instances.ßdummyTurker
9161ß0.0ßTherefore, using such instances to expand the training data will not improve the ability of the baseline model for relation recognition.ßdummyTurker
9162ß0.0ßTherefore they are regarded as less informative.ßdummyTurker
9163ß0.0ßWe promise to revise the tables and captions which confuse the understanding of the performance advantages.ßdummyTurker
9164ß0.0ßThe statistical significant verification (p < 0.05; one-tailed t-test) will be presented in the main text but not the footnote of the tables.ßdummyTurker
9165ß0.0ßWe will provide an additional table to clarify the numbers of instances in the training set as well as the distribution over different relation types.ßdummyTurker
9166ß0.0ßReply to question 2 (parameter settings and time expenditure): In AL strategy, the batch size and other hyper-parameters are fine-tuned in the validation set.ßdummyTurker
9167ß0.0ßWe took a couple of weeks to tune the parameters, laboriously, using different batch sizes.ßdummyTurker
9168ß0.0ßWe are grateful for your insightful comments on the weakness of this paper.ßdummyTurker
9169ß0.0ßReply to weakness argument 1 (difference from the previous work): Rutherford and Xue (2015) proposed an attracting criterion for determine which connectives can be directly removed or not.ßdummyTurker
9170ß0.0ßThe explicitly-related arguments whose connectives can be safely removed appear to be effectively used as the qualified candidates of implicitly-related arguments.ßdummyTurker
9171ß0.0ßTherefore, we totally agree with your point that the proposed method in this paper is theoretically like the work of Rutherford and Xue (2015).ßdummyTurker
9172ß0.0ßThe difference primarily lies in the use of active learning.ßdummyTurker
9173ß0.0ßThe active learning framework allows an expansion method to take into consideration of a variety of factors for the quality measurement, not only redundancy (paraphrasing problem) and noisy examples (pragmatic problems), but also entailment, domain-independent or specific cues, topic relevance, frame semantics, etc.ßdummyTurker
9174ß0.0ßMore importantly, there are a variety of active learning approaches can be used in practice.ßdummyTurker
9175ß0.0ßThey can also be combined with feature selection (such as PSO) to enable more sophisticated learning models to come into being.ßdummyTurker
9176ß0.0ßIn this paper, we employ a simple active learning to expand the training data, which improves the supervised model and yields relatively substantial improvements for some of the main relation types.ßdummyTurker
9177ß0.0ßIt appears to be promising to achieve better performance when the state-of-the-art active learning methods will be used.ßdummyTurker
9178ß0.0ßWe also appreciate your interest in the comparison between Rutherford and Xue (2015)’s work and ours.ßdummyTurker
9179ß0.0ßTherefore, we will try our best to reproduce their work and present the performance comparison by using an additional page.ßdummyTurker
9180ß0.0ßWe promise to provide a case study and thus present systemic analysis on the real noises.ßdummyTurker
9181ß0.0ßReply to weakness argument 3 (justification for the use of CNN): The examples selected from the external resources are the explicitly-related arguments.ßdummyTurker
9182ß0.0ßThey are originally connected with a connective.ßdummyTurker
9183ß0.0ßAfter removing the connectives, the unbroken sentences will become broken.ßdummyTurker
9184ß0.0ßIf will use a RNN to encode the sentences.ßdummyTurker
9185ß0.0ßThe garbled word orders in the sentences will mislead language modeling.ßdummyTurker
9186ß0.0ßBy contrast, CNN doesn’t suffer from the problem severely.ßdummyTurker
9187ß0.0ßTherefore, we use CNN to constitute the relation recognizer.ßdummyTurker
9188ß0.0ßWe fine-tune the hyperparameters to make Liu et al’s CNN model to achieve a performance no less than the reported.ßdummyTurker
9189ß0.0ßWe aim to avoid suspicion that the performance gain attributes to a weaker baseline or the failure of reproducing the baseline.ßdummyTurker
9190ß0.0ßReply to question 2&3 (definition and presentation): We will define the terminology more clearly by exhibiting concrete examples.ßdummyTurker
9191ß0.0ßAnd we will follow the thoughtful suggestions to improve the presentation, so as to reduce confusion.ßdummyTurker
9192ß0.0ßWe thanks the reviewer for the comments.ßdummyTurker
9193ß0.0ßPlease find our responses below:ßdummyTurker
9194ß0.0ßWe agreed that LTCM did achieve a similar performance to LV-S2S models in the paper, which is mainly because that LV-S2S tends to generate duplicated responses.ßdummyTurker
9195ß0.0ßIn the human evaluation, duplicated responses were removed and account for only one score.ßdummyTurker
9196ß0.0ßSince LV-S2S avoids to be evaluated multiple times (we evaluate based on 5 generated sentences, line 676-677) by generating duplicated, safe choices, it directly puts LTCM in a weaker position because of its diverse nature of the output.ßdummyTurker
9197ß0.0ßDue to the space limit, it is only discussed at line 737-740.ßdummyTurker
9198ß0.0ßWe will improve the content of the final version of the paper.ßdummyTurker
9199ß0.0ßThis is because we considered that KL and lowerbound are less important than the others.ßdummyTurker
9200ß0.0ßHowever, we agree with the reviewer and would highlight the best-performing systems for the two metrics shall the paper be accepted.ßdummyTurker
9201ß0.0ßWe thanks the reviewer for the comments.ßdummyTurker
9202ß0.0ßPlease find our responses below:ßdummyTurker
9203ß0.0ßWe agreed that LTCM did achieve a similar performance to LV-S2S models in the paper, which is mainly because that LV-S2S tends to generate duplicated responses.ßdummyTurker
9204ß0.0ßIn the human evaluation, duplicated responses were removed and account for only one score.ßdummyTurker
9205ß0.0ßSince LV-S2S avoids to be evaluated multiple times (we evaluate based on 5 generated sentences, line 676-677) by generating duplicated, safe choices, it directly puts LTCM in a weaker position because of its diverse nature of the output.ßdummyTurker
9206ß0.0ßDespite this, LTCM is still considered to be better than the other baselines in the corpus-based and human evaluation.ßdummyTurker
9207ß0.0ßMoreover, one thing worth to note is that LV-S2S with scheduled annealing is quite hard to train (Bowman et al 2015).ßdummyTurker
9208ß0.0ßOn the other hand, LTCM training is rather straightforward and the result is more interpretable.ßdummyTurker
9209ß0.0ßWe don't want to introduce additional decoding tricks in the experiments mainly because we want to focus on evaluating the intrinsic quality (how good they can generate without relying on auxiliary helps) of the trained models.ßdummyTurker
9210ß0.0ßThis tells us more about the quality of the trained model itself.ßdummyTurker
9211ß0.0ßAs mentioned in argument 4 above, we avoided using any advance decoding algorithms so that the experiment can better assess the intrinsic model quality.ßdummyTurker
9212ß0.0ßTherefore, we only used greedy decoding or random sampling in our evaluation.ßdummyTurker
9213ß0.0ßRandom sampling picks the next word randomly with a probability equals to the model prediction.ßdummyTurker
9214ß0.0ßAlthough the suggestion makes a lot of sense, the reason why we excluded them is as mentioned in argument 4 that we didn't want to introduce advance decoding heuristics to undermine the comparison of intrinsic model quality.ßdummyTurker
9215ß0.0ßHowever, the suggestion makes and we will add more results related to this to the final version.ßdummyTurker
9216ß0.0ß"The examples shown in the results were picked from examples in ""A Neural Conversational Model, Vinyals and Le 2015"": https://arxiv.org/abs/1506.05869"ßdummyTurker
9217ß0.0ßretaining unk: The vocabulary we trained our models on is relatively small (30K) but conversation datasets, in general, contain a lot of function words.ßdummyTurker
9218ß0.0ßThis usually removes a lot of interesting name entities from the vocabulary.ßdummyTurker
9219ß0.0ßRetaining unk at least makes sure that during inference time their occurrences are well represented.ßdummyTurker
9220ß0.0ßasterisk and edits: we will double check our result and if there is indeed a missing we will include it in the final version.ßdummyTurker
9221ß0.0ßR2-W1: The purpose of extracting location names from texts is to support spatiotemporal studies and disaster response.ßdummyTurker
9222ß0.0ßGeotagged tweets (which are rarely available -- only around 1% of tweets on average and 0% of our Chennai tweets) cannot always achieve the same purpose as extracting location mentions from tweets.ßdummyTurker
9223ß0.0ßFor example, the text of geotagged tweets might refer to a remote location or a remote event not about the current location of a tweeter.ßdummyTurker
9224ß0.0ßOur larger goal is to extract and aggregate information associated with the mentioned locations, e.g., “x is flooded”, and “y road is closed”.ßdummyTurker
9225ß0.0ßR2-W2: We have included a comparison with general purpose NER tools for coverage.ßdummyTurker
9226ß0.0ßThe tools proposed for comparison use supervised techniques requiring training data.ßdummyTurker
9227ß0.0ßIn contrast, our work is effective in situations where we do not have such data available, like in a disaster setting where fast disaster response and situational awareness are critical.ßdummyTurker
9228ß0.0ßTable 3 shows the comparative performance of all tools on each of the three datasets, underscoring the fact that supervised approaches can perform poorly on unseen datasets (due to the data sparsity problem).ßdummyTurker
9229ß0.0ßAdditionally, Stanford NER still beats other taggers on CONLL-2003, reaching 93.0 F1 [1] while Chiu and Nichols, Ma and Hovy, and Lample et al.ßdummyTurker
9230ß0.0ßall reach 91.62,  91.21, and 90.94 F1 respectively.ßdummyTurker
9231ß0.0ß[1] https://nlp.stanford.edu/software/crf-faq.shtml#kßdummyTurker
9232ß0.0ßR2-W3: We will take care of that in the revised paper.ßdummyTurker
9233ß0.0ßR2-W4: We used LaTex template but will double check and fix in the revised paper.ßdummyTurker
9234ß0.0ßR2-Q1: The language model function is a binary function used to test if an n-gram is valid or not.ßdummyTurker
9235ß0.0ßWe do not take into consideration the probability of an n-gram.ßdummyTurker
9236ß0.0ßIn the revised paper, we will update the text and algorithm to reflect this.ßdummyTurker
9237ß0.0ßAdditionally, since we are linking locations with their geographical metadata, we do not have to match with a location name that is not in our augmented gazetteer.ßdummyTurker
9238ß0.0ßWe changed Line 11 to the following boolean function (called from the pipeline-- not included in this algorithm):ßdummyTurker
9239ß0.0ßprocedure VALID-N-GRAM (string = s): booleanßdummyTurker
9240ß0.0ßw_1^n = (w_1 , .ßdummyTurker
9241ß0.0ß.ßdummyTurker
9242ß0.0ß.ßdummyTurker
9243ß0.0ß, w_n ) ← tokenize(s);ßdummyTurker
9244ß0.0ßreturn P (w_1^n ) > 0ßdummyTurker
9245ß0.0ßR3-W1: Our choice of OpenStreetMap is supported by the fact that it has a strong volunteer foundation such as hotosm.org which maps thousands of locations during a disaster.ßdummyTurker
9246ß0.0ßOpenStreetMap is regarded as the Wikipedia of maps, and since our tool design was motivated by the spatiotemporal and context-aware computing use cases, we will need a gazetteer regardless of the technique to get the metadata of locations (lat/long and location types).ßdummyTurker
9247ß0.0ßWe agree with the reviewer, but linking location mentions to gazetteer records is required even if we use any other delimitation technique, making it a limitation of knowledge-bases in general.ßdummyTurker
9248ß0.0ßR3-W2: the tool is currently in use to extract locations from millions of tweets and other texts, and we have not faced any problems in these realistic settings.ßdummyTurker
9249ß0.0ßR3-W3: Our simplistic approach was motivated by the unavailability of ample training data for deep learning and other techniques.ßdummyTurker
9250ß0.0ßThe proposed technique allowed us to handle stream processing and is very fast in practice.ßdummyTurker
9251ß0.0ßAdditionally, using an n-gram-based model allowed us to overcome the lexical variations and other challenges which is desirable and empirically proved to be very valuable.ßdummyTurker
9252ß0.0ß1 and 2: We will fix inadvertent typos in the revised version.ßdummyTurker
9253ß0.0ß3: “Houston is a cesspool this time of year” is an example metonymßdummyTurker
9254ß0.0ß4: Unigrams do not have conditional probabilities.ßdummyTurker
9255ß0.0ßAlso, please refer to the answer to R2-Q1.ßdummyTurker
9256ß0.0ß5: Our approach uses the longest sequence of words that corresponds to a location as the extracted location.ßdummyTurker
9257ß0.0ßIf necessary, we break the ties using frequency counts (see Fig.ßdummyTurker
9258ß0.0ß1).ßdummyTurker
9259ß0.0ß6: We do not remove acronyms, we remove things like “historical” or “private road”.ßdummyTurker
9260ß0.0ßWe retain acronyms as alternative names of a location to improve recall.ßdummyTurker
9261ß0.0ß7: please refer to the answer to R2-Q1ßdummyTurker
9262ß0.0ß8: The percentage of location names in the gazetteers with location names longer than bigram for Chennai, Louisiana, and Houston are 51%, 45%, and 54%, respectively.ßdummyTurker
9263ß0.0ß9: we are using the LaTex template but will double check in the final version.ßdummyTurker
9264ß0.0ß10: Thank you, yes, we will fix that.ßdummyTurker
9265ß0.0ßWe thank all the reviewers for the time, expertise, and their invaluable feedback that has led to important clarifications and improvements in the revised paper.ßdummyTurker
9266ß0.0ßBelow we give specific details and responses to reviewer identified weaknesses that we hope will ally their concerns.ßdummyTurker
9267ß0.0ßDue to the space constraint, we did omit some details.ßdummyTurker
9268ß0.0ßR1-W1: The differences in performance of all tools on all of the three datasets show that even though the datasets are disaster-related, they still pose different challenges.ßdummyTurker
9269ß0.0ßFor example, our three datasets are in English but from different countries (India vs. the USA).ßdummyTurker
9270ß0.0ßR1-W2: Even though our tool is designed to work with the ill-formed and challenging social media text, there is no reason why it would fail for other texts.ßdummyTurker
9271ß0.0ßOn the contrary, we expect better performance on clean unstructured/grammatical text.ßdummyTurker
9272ß0.0ßOur presentation obscured some important facets of the method, the tool was designed with targeted streams in mind to partially handle the geo/geo ambiguities since it also links the mentions to gazetteer records after delimitation.ßdummyTurker
9273ß0.0ß"R1-Q1: “City College of New York” is not going to be treated since, we require the last token of the location name to be a location category name, and since “York"" is not a location category name the location name is not going to be augmented."ßdummyTurker
9274ß0.0ßR1-Q2: We gave the following definition in the paper: “We define a targeted stream as a set of tweets that has the potential to satisfy an event-related information need (Piskorski and Ehrmann, 2013) crawled using keywords and hashtags, to contextualize the event”.ßdummyTurker
9275ß0.0ß"Piskorski and Ehrmann (2013) define targeted streams as the streams of text that are “usually constructed by filtering tweets with user-defined selection criteria (e.g., tweets published by users from a selected region, or tweets that match one or more predefined keywords)""."ßdummyTurker
9276ß0.0ßR1-Q3: Even though eliminating some words from the gazetteer would, in theory, reduce recall, we observed a very small effect.ßdummyTurker
9277ß0.0ßIn fact, our performance evaluation of the tool on the development dataset extracted many false positives harming precision, not recall significantly, due to the presence of highly ambiguous location names in the gazetteers, such as “People”, a shop in Chennai, India, and “Cut Off”, a town in Louisiana.ßdummyTurker
9278ß0.0ßActually all papers have reported results with same format.ßdummyTurker
9279ß0.0ßI have maintained consistency here.ßdummyTurker
9280ß0.0ßPlease look at comparison on mentioned paper in this table.ßdummyTurker
9281ß0.0ß:) Interesting since for first 2 years 4-5 publications improved only 1.6% from first paper.ßdummyTurker
9282ß0.0ßI would expect even smaller improvement in subsequent years since head room of improvement will be reducing by each paper.ßdummyTurker
9283ß0.0ßI have highlighted some areas e.g.ßdummyTurker
9284ß0.0ßtable understanding, we can't answer win-loss query without understanding meaning of 2-3 in goals table.ßdummyTurker
9285ß0.0ßimproving ranking could be another one : Match@N  is 6% more than match@1.ßdummyTurker
9286ß0.0ßI agree that there are different ways to segment queries.ßdummyTurker
9287ß0.0ßBut I didn't find any one which builds up segmentation from only annotations.ßdummyTurker
9288ß0.0ßMost segmentation works with n gram model.ßdummyTurker
9289ß0.0ßIn this paper, segment is made from annotations only not query text.ßdummyTurker
9290ß0.0ßI focused more on work in question answering field.ßdummyTurker
9291ß0.0ßScores are important for ranking.ßdummyTurker
9292ß0.0ßSo absolute score doesn't matter.ßdummyTurker
9293ß0.0ßIn general score of segment was num_tokens * segment multiplier.ßdummyTurker
9294ß0.0ßWe added some weight for perfect match.ßdummyTurker
9295ß0.0ßSynonyms were mostly common sense e.g.ßdummyTurker
9296ß0.0ßmonth names, (noun, adjective) from country names etc.ßdummyTurker
9297ß0.0ß(apr, april), (jan, january), (country, nation), (french, france), (spanish, spain), (1st, first) etc.ßdummyTurker
9298ß0.0ßWe used this as direct match e.g.ßdummyTurker
9299ß0.0ß"""spain"" and ""spanish"" was considered as token match for annotation."ßdummyTurker
9300ß0.0ß---ßdummyTurker
9301ß0.0ßThanks for the pointers of other papers.ßdummyTurker
9302ß0.0ßWhile they talk about Web tables, those techniques will have limitation on semantic meaning for numeric, comparative and superlative segments.ßdummyTurker
9303ß0.0ß"We need semantic meaning for ""greater than 10""."ßdummyTurker
9304ß0.0ßParikh et all is interesting and may be added in related work, but in this paper, we use annotation and segments for matching rather than n consecutive terms.ßdummyTurker
9305ß0.0ßWe do have comparative data for bag of the words model.ßdummyTurker
9306ß0.0ßThank you for your comments.ßdummyTurker
9307ß0.0ßThere are indeed two kinds of multiplicative recurrent connections in LSTMs, one from the simple RNN (content layer) and one from the gates.ßdummyTurker
9308ß0.0ßWe will clarify this in the final version.ßdummyTurker
9309ß0.0ßOur experiments show that the recurrence from content layer is not necessary, and giving it up doesn’t sacrifice performance across a variety of NLP tasks.ßdummyTurker
9310ß0.0ß"The evaluation metric only adopts ""ROUGE-L""."ßdummyTurker
9311ß0.0ßAnswer: We adopted the “ROUGE-L” metric because the results generated by the Orator can be viewed as a summarization of the original sentences, whereas the “ROUGE-L” metric was widely adopted in text summarization researches.ßdummyTurker
9312ß0.0ßThe adopted “ROUGE-L” metric is an overall evaluation of precision and recall.ßdummyTurker
9313ß0.0ßWe actually had the result for “BLEU” metric which showed even noticeably larger improvements; nevertheless, we still preferred to present ROUGE-L results.ßdummyTurker
9314ß0.0ßWe are more than happy to make space to add the BLEU results in the final version.ßdummyTurker
9315ß0.0ßNo significant analysis of the results of the experiment.ßdummyTurker
9316ß0.0ßAnswer: Our main contribution is to show that Logician@dual substantially improves Logician.ßdummyTurker
9317ß0.0ßTake precision result as an example, the numbers are 0.494 versus 0.449.ßdummyTurker
9318ß0.0ßWith 4188 test examples, based on a simple t-test, the difference is highly significant.ßdummyTurker
9319ß0.0ßOf course, we all know simple t-test is not that precise and hence we did not report the p-value.ßdummyTurker
9320ß0.0ßThe sample size (4188) is fairly large.ßdummyTurker
9321ß0.0ß…In my opinion, IN is may use, but OIN is useless.ßdummyTurker
9322ß0.0ßAnswer: We think the IN and OIN tasks belong to different information processing pipelines with different pre-conditions:ßdummyTurker
9323ß0.0ß1)	Text -> IE -> KB -> IN -> Text, given an existing KB;ßdummyTurker
9324ß0.0ß2)	Text -> OIE -> Open Domain Facts -> OIN -> Text, without KB or Schema system.ßdummyTurker
9325ß0.0ßThe second pipeline eliminates the prerequisite condition of an existing KB.ßdummyTurker
9326ß0.0ßIt is valuable since constructing a KB is a complicated and costly task.ßdummyTurker
9327ß0.0ßA typical situation is the open domain KBQA, where OIN agent can assemble the resulting facts into more fluent and more informative answers.ßdummyTurker
9328ß0.0ßIt is also possible to be extended to fit similar tasks (such article writing) wherever open domain facts sever as an important intermediate layer.ßdummyTurker
9329ß0.0ß… it seems that … not substantially improve the performance.ßdummyTurker
9330ß0.0ßWe can see from Table 4 that dual learning actually improve the baseline (Logician) quite substantially (with 4188 test examples).ßdummyTurker
9331ß0.0ßIn Table 4, we added the result of our intermediate work R-Logician, which probably obscures the improvement.ßdummyTurker
9332ß0.0ßR-Logician is also our contribution.ßdummyTurker
9333ß0.0ßIn Table 3, we presented a very strong baseline Orator (which is part of the contribution of this paper, as the first work on OIN tasks).ßdummyTurker
9334ß0.0ßIn addition, Table 5 reveals for noisy open-domain facts generated by OIE algorithms, the Orator@dual exhibits better performance than R-Orator and Orator, which means that it is more stable when confronting real problems.ßdummyTurker
9335ß0.0ß… why they use ROUGE-L ….ßdummyTurker
9336ß0.0ßAnswer: We adopted the “ROUGE-L” metric because the results generated by the Orator can be viewed as a summarization of the original sentences, whereas the “ROUGE-L” metric was widely adopted in text summarization researches.ßdummyTurker
9337ß0.0ßThe “ROUGE-L” metric is a balanced evaluation of precision and recall, which is a suitable reward to guide the learning system.ßdummyTurker
9338ß0.0ßThank you for your kind suggestions, we will accordingly comply.ßdummyTurker
9339ß0.0ß… how you train/initialize word embedding.ßdummyTurker
9340ß0.0ßAnswer: The word embedding vectors are randomly initialized.ßdummyTurker
9341ß0.0ßWe will add the information to the final version of the paper.ßdummyTurker
9342ß0.0ßWhat is ISA in Table 2?ßdummyTurker
9343ß0.0ßAnswer: We will re-write ISA to be IS-A.ßdummyTurker
9344ß0.0ßThank you for pointing this out.ßdummyTurker
9345ß0.0ßBasically, it means that an entity is an instance of a concept.ßdummyTurker
9346ß0.0ß"…why the SAOKE dataset ""significantly …""."ßdummyTurker
9347ß0.0ßAnswer: When we are considering the OIN problem, we first consult a natural approach that uses OIE algorithms to build a quality collection of <facts, sentence> pairs.ßdummyTurker
9348ß0.0ßHowever, the facts extracted traditional OIE algorithms may only preserve 30%-40% information of the sentence, which makes the learning task much difficult.ßdummyTurker
9349ß0.0ßSAOKE dataset is naturally a qualified corpus of <facts, sentence> pairs.ßdummyTurker
9350ß0.0ßIn our observation, the facts generally preserve 70%-80% information of the sentence.ßdummyTurker
9351ß0.0ßIt is also guaranteed that the facts can be reasonably expressed in one natural language sentence.ßdummyTurker
9352ß0.0ßThus in this sense, SAOKE dataset resolve the problem.ßdummyTurker
9353ß0.0ßThank you for your comments and suggestions.ßdummyTurker
9354ß0.0ß[WEAKNESS-1]: Thanks for the valuable suggestion.ßdummyTurker
9355ß0.0ßIn the current version, we focus on introducing the overall framework and adversarial training strategy and ignore some details due to space limitation.ßdummyTurker
9356ß0.0ßWe will present more details in our revision.ßdummyTurker
9357ß0.0ßThe source code will also be released with the revised paper to provide more concrete information.ßdummyTurker
9358ß0.0ß[WEAKNESS-2]: Great suggestion!ßdummyTurker
9359ß0.0ßIn the revision, we will conduct the suggested comparison to demonstrate the effectiveness of the split strategy.ßdummyTurker
9360ß0.0ßThe analysis will then be presented in the appendix to make our conclusion more solid.ßdummyTurker
9361ß0.0ßThank you for your comments and suggestions.ßdummyTurker
9362ß0.0ß[WEAKNESS-1]: We disagree with this claim in general.ßdummyTurker
9363ß0.0ßIn our framework, there are actually parameters to be learned in the sampler components (Line:307-316), and the two components (discriminator and sampler) are actually optimized in an iterative scheme (Line:489-495).ßdummyTurker
9364ß0.0ßMore importantly, our framework is indeed adversarial, instead of self-training.ßdummyTurker
9365ß0.0ßIn order to make the idea of our framework more clear, we give the following detailed explanation.ßdummyTurker
9366ß0.0ßWe design an adversarial scheme between the discriminator and the sampler: The sampler tries to make sentences in the unconfident set but similar to those in the confident set keep high probability.ßdummyTurker
9367ß0.0ßThe discriminator selects sentences with the probability distribution learned by the sampler as its negative instances to reduce their probability.ßdummyTurker
9368ß0.0ßIn this adversarial framework (Line:274-282), the sampler and discriminator learn their parameters (encoders, separating hyperplane and classifiers) respectively.ßdummyTurker
9369ß0.0ß[WEAKNESS-2]: We disagree with the claim that the evaluation setting in our paper is incorrect.ßdummyTurker
9370ß0.0ßThe dataset used in our paper is originally developed by Mintz et al.ßdummyTurker
9371ß0.0ß[1], and Riedel et al.ßdummyTurker
9372ß0.0ß[2] improve it in 2010 (named as NYT-10).ßdummyTurker
9373ß0.0ßThe NYT-10 dataset is widely used in recent neural-based works [4,5,6,7].ßdummyTurker
9374ß0.0ßIn EMNLP 2017, nearly half of RE papers adopt this dataset.ßdummyTurker
9375ß0.0ßHence, we select NYT-10 in our paper.ßdummyTurker
9376ß0.0ßThe dataset and baseline models used in our paper are all open-source [8,9].ßdummyTurker
9377ß0.0ßThese baseline models are all state-of-the-art.ßdummyTurker
9378ß0.0ßIn experiments, we follow the dataset and experimental setting used in the previous works.ßdummyTurker
9379ß0.0ßWe appreciate the suggestion of adopting more datasets such as NYT-13[3].ßdummyTurker
9380ß0.0ßWe will evaluate our models with this dataset in the future.ßdummyTurker
9381ß0.0ßHowever, we don't find the evidence that the work [3] addresses the noise issue in distant supervision.ßdummyTurker
9382ß0.0ß[WEAKNESS-3]: We follow the same dataset and experimental setting used in the previous works [4,5,6,7,8,9].ßdummyTurker
9383ß0.0ß[WEAKNESS-4]: Thanks for the insightful suggestion!ßdummyTurker
9384ß0.0ßIn this paper, we refer MIL to those MIL methods for entity-pair-level relation extraction.ßdummyTurker
9385ß0.0ßIn the revision, we will revise our statements to avoid ambiguity.ßdummyTurker
9386ß0.0ßOur models and other baseline MIL models actually adopt the same sentence encoders and similar classifiers, and the key difference is the denoising mechanisms (Line:575-581).ßdummyTurker
9387ß0.0ßBy setting the same parameters for sentence encoders and classifiers, the model results are mainly determined by their denoising mechanisms.ßdummyTurker
9388ß0.0ßHence, the comparison between these models is convincing.ßdummyTurker
9389ß0.0ß[QUESTION-1]: In the scenario in section-4.3, we only keep one or two sentences for each entity pair.ßdummyTurker
9390ß0.0ßIf each entity pair has enough corresponding sentences, existing entity-pair-level denoising mechanisms can effectively select informative sentences for final extraction.ßdummyTurker
9391ß0.0ßHowever, it is difficult to train entity-pair-level denoising mechanisms when each entity pair only has few sentences.ßdummyTurker
9392ß0.0ßFollowing the previous works [5,6], we report P@N (100,200,300), which means the precision of the top-N relation recommendations (Table-3).ßdummyTurker
9393ß0.0ßIt is completely different from the precision in the various recalls (Table-2).ßdummyTurker
9394ß0.0ß[1] http://www.aclweb.org/anthology/P09-1113ßdummyTurker
9395ß0.0ß[2] https://link.springer.com/chapter/10.1007/978-3-642-15939-8_10ßdummyTurker
9396ß0.0ß[3] http://www.aclweb.org/anthology/N13-1008ßdummyTurker
9397ß0.0ß[4] http://www.aclweb.org/anthology/D15-1203ßdummyTurker
9398ß0.0ß[5] http://www.aclweb.org/anthology/P16-1200ßdummyTurker
9399ß0.0ß[6] http://www.aclweb.org/anthology/D17-1189ßdummyTurker
9400ß0.0ß[7] http://www.aclweb.org/anthology/D17-1187ßdummyTurker
9401ß0.0ß[8] https://github.com/thunlp/NREßdummyTurker
9402ß0.0ß[9] https://github.com/jxwuyi/AtNREßdummyTurker
9403ß0.0ßThank you for your comments and suggestions.ßdummyTurker
9404ß0.0ßWe really appreciate that you point out inappropriate expressions and writing issues.ßdummyTurker
9405ß0.0ßWe will extensively revise our paper to avoid those misleading and confusing expressions.ßdummyTurker
9406ß0.0ß[WEAKNESS-1]: We agree that adversarial models are not new, but the idea has less been explored for relation extraction(RE).ßdummyTurker
9407ß0.0ßIn this paper, we successfully adopt instance-level adversarial training(AT) to alleviate the wrong labeling problem for distantly supervised RE models.ßdummyTurker
9408ß0.0ß[WEAKNESS-2,3,5]: We really appreciate your valuable suggestion!ßdummyTurker
9409ß0.0ßSince our proposed framework involves both RE and AT, there are many related works.ßdummyTurker
9410ß0.0ßDue to the space limitation, we have to highlight the model introduction and concisely explain the background information.ßdummyTurker
9411ß0.0ßThe benchmark datasets and baseline models used in this paper are all open-source projects that are completely free to use [1].ßdummyTurker
9412ß0.0ßIn the revision, we will present more background details about our work.ßdummyTurker
9413ß0.0ß[WEAKNESS-4] and [QUESTION-3]: Insightful suggestion!ßdummyTurker
9414ß0.0ßThe focus of this paper is to adopt AT to alleviate the wrong labeling problem in distant supervision, and don't dive deep into the mechanism analysis.ßdummyTurker
9415ß0.0ßWe simply apply a straightforward mechanism: After every few training epochs, an instance that can confuse the discriminator at every epoch will be selected to enrich the confident set.ßdummyTurker
9416ß0.0ßWe don't expect to propose a flawless denoising filter at once.ßdummyTurker
9417ß0.0ßOur experimental results have demonstrated adopting AT is promising.ßdummyTurker
9418ß0.0ßIn the revision, we will give a more detailed description about the selection process and make further quantitative analysis.ßdummyTurker
9419ß0.0ß[QUESTION-1]: Position embedding is proposed by Zeng et al.ßdummyTurker
9420ß0.0ß[2].ßdummyTurker
9421ß0.0ßWe represent each word's relative distances to the two entities into two k_p-dimensional vectors, and concatenate the vectors to get the position embedding.ßdummyTurker
9422ß0.0ßThese embeddings are all learnable parameters.ßdummyTurker
9423ß0.0ß[QUESTION-2]: Our models and baselines adopt the same encoders, which are the bottlenecks for parameter learning.ßdummyTurker
9424ß0.0ßHence these models' time complexity is comparable.ßdummyTurker
9425ß0.0ß[QUESTION-4]: As explained in previous replies, we will release our source codes with the revision after acceptance.ßdummyTurker
9426ß0.0ß[1] NRE.ßdummyTurker
9427ß0.0ßhttps://github.com/thunlp/NREßdummyTurker
9428ß0.0ß[2] http://www.aclweb.org/anthology/C14-1220ßdummyTurker
9429ß0.0ßThis paper focuses on designing a framework for better political forecasting based on a specific NLP task, sentiment analysis in this case (sentiment analysis of influential social media messages).ßdummyTurker
9430ß0.0ßMany previous works have relied on volume of data related to the candidate for political forecasting.ßdummyTurker
9431ß0.0ßWe investigated the impact of sentiment analysis of influential messages to produce an accurate prediction.ßdummyTurker
9432ß0.0ßThe original component in this framework is influential message identification.ßdummyTurker
9433ß0.0ßContrary to the methods focusing on the volume of data related to candidates, sentiment analysis of influential messages would reduce the noise not only based on positive messages but also based on influential positive messages.ßdummyTurker
9434ß0.0ßTable 3 illustrates the results when sentiment analysis of influential messages performs better than SA and message count.ßdummyTurker
9435ß0.0ßWhile social media messages are influencing voters, analyzing influential messages would allow to accurately predict the results that will eventually be applied.ßdummyTurker
9436ß0.0ßMost of the influential messages should be posted by specialized peoples aiming to direct the results towards a particular candidate.ßdummyTurker
9437ß0.0ßHence, the more positive influential messages the candidate has, the more he is supposed to win.ßdummyTurker
9438ß0.0ßIncluding non-presidential elections should drive a richer evaluation.ßdummyTurker
9439ß0.0ßWhile we are the first working on sentiment analysis of political influential messages, first promising results would encourage researchers to take this direction in order to provide better-explained forecasts.ßdummyTurker
9440ß0.0ßIncluding non-presidential elections should be among the focus of our future work.ßdummyTurker
9441ß0.0ßComparing influence classifier to some appropriate baseline would be interesting.ßdummyTurker
9442ß0.0ßWhile we are the first focusing on influential messages, it was hard to find an appropriate baseline.ßdummyTurker
9443ß0.0ßTo justify the impact of influential messages detection, we conducted the forecast with and without filtering messages by influence (Table 3).ßdummyTurker
9444ß0.0ßResults showed that sentiment analysis of influential messages would reduce the noise and lead to a more accurate forecast.ßdummyTurker
9445ß0.0ßThis argument is useful to make features description clearer.ßdummyTurker
9446ß0.0ßLife cycle feature is calculated based on time of the last response to the message.ßdummyTurker
9447ß0.0ßThe ideal would be that the message still receives responses until the data mining time.ßdummyTurker
9448ß0.0ßA message posted before the election day may receive responses after it (until the data mining time).ßdummyTurker
9449ß0.0ßThe time of last response should draw near the time of data mining.ßdummyTurker
9450ß0.0ßIn contrast, reaction time feature is calculated based on time of the message and time of the first response.ßdummyTurker
9451ß0.0ßThe ideal would be that the message receives responses rapidly.ßdummyTurker
9452ß0.0ßTime of the first response should be the maximum nearest from message time and the furthest from the time of data mining.ßdummyTurker
9453ß0.0ßNormalizing using a (DataMiningTime–MessagePublicationTime) denominator means that each Feature is rated relative to its ideal value.ßdummyTurker
9454ß0.0ßIn table 3 the period column shows the time intervals in which messages are taken into account.ßdummyTurker
9455ß0.0ßEach interval is in the following arrangement: [poll starting date, election date].ßdummyTurker
9456ß0.0ß"For example, in case of IMS, the 3 months row is showing results when all positive influential messages between ""3 months"" before the election up until election day are taken into account."ßdummyTurker
9457ß0.0ßAs the original component in the proposed framework is influential message identification, the evaluation aimed to show the effectiveness of this component.ßdummyTurker
9458ß0.0ßSentiment analysis of influential messages would reduce the noise not only based on positive messages but also based on influential positive messages (Table 3).ßdummyTurker
9459ß0.0ßResults showed that influential positive messages perform better than positive messages and messages count.ßdummyTurker
9460ß0.0ßFirst promising results would encourage researchers to replicate and adapt our approach to non-presidential elections, parliament elections for example.ßdummyTurker
9461ß0.0ßPast works in this area are not taking into account the influential nature of the messages in a social network.ßdummyTurker
9462ß0.0ßThe detection was based on six features reflecting a user’s behavior when he tends to be influenced by the message.ßdummyTurker
9463ß0.0ßThose features were designed based on the domain-experts recommendations (section 4.2).ßdummyTurker
9464ß0.0ßFuture work should consider using linguistic features to detect the influential nature.ßdummyTurker
9465ß0.0ßThis would lead to the construction of lexicons containing terms commonly used in politics to influence social media users.ßdummyTurker
9466ß0.0ßProvide application of SA would help to present works recommending SenticNet rather than citing only2 (p2 line106).ßdummyTurker
9467ß0.0ßPresenting in detail the current state-of-the-art in SA on social media would encourage future works to provide better-explained forecasts based on sentiment analysis rather than volume data simplistic methods.ßdummyTurker
9468ß0.0ßIt would be also better if we have discussed largely works by Gayo Avello etal.ßdummyTurker
9469ß0.0ßrather than considering only one work (p4.line336) because they carefully studied electoral predictions using Twitter and focused on its limits.ßdummyTurker
9470ß0.0ßThere are a lot of ways that one can think of in terms of combining sentiment and influence.ßdummyTurker
9471ß0.0ßThe select of positive influential messages would reduce well data noise.ßdummyTurker
9472ß0.0ßWe cannot count a negative message as a vote for the competing candidate.ßdummyTurker
9473ß0.0ßWe succeed to get 34 experts: 10political associations members, 10politician bloggers, 4politician journalists, and 10students researchers in psychology.ßdummyTurker
9474ß0.0ß34expertsßdummyTurker
9475ß0.0ß500 messages were labeled manually by associations’ members, the politician bloggers, and the students researchers.ßdummyTurker
9476ß0.0ßCases of disagreement (84) were relabeled by politician journalist.ßdummyTurker
9477ß0.0ßFuture work should consider using linguistic features to detect the influential nature of social media messages.ßdummyTurker
9478ß0.0ßFurther, this would lead to the construction of lexicons containing terms and expressions commonly used in politics to influence social media users.ßdummyTurker
9479ß0.0ßWe used tenfold cross-validation to improve generalization and avoid over-fitting.ßdummyTurker
9480ß0.0ßRandom Forest shows the best performance to correctly predict the Influential class with minimal false Positives.ßdummyTurker
9481ß0.0ßSenticNet use sub-symbolic methods such as deep neural networks and multiple kernel learning to infer syntactic patterns from data.ßdummyTurker
9482ß0.0ßThe model selection approach involved both the SVM-based classifiers and the ELM-based predictors.ßdummyTurker
9483ß0.0ßThe work of Chalkidis et al.ßdummyTurker
9484ß0.0ßconsidered extracting contract elements (e.g., contractor names, dates, amounts), a task which is similar to named entity recognition, i.e., a classification decision is needed per token.ßdummyTurker
9485ß0.0ßThe task we consider is sentence classification, i.e., a classification decision is needed per sentence, and we show that the best (hierarchical) method considers both the sequence of words in each sentence and the sequence of sentences.ßdummyTurker
9486ß0.0ßHence, the tasks and datasets of Chalkidis et al.ßdummyTurker
9487ß0.0ßare very different than ours.ßdummyTurker
9488ß0.0ßIndeed, measuring the agreement between the law student annotators and the expert would not make much sense, since the expert had access to (corrected) the annotations of the students.ßdummyTurker
9489ß0.0ßWe would like to point out, however, as in our reply to the first reviewer, that the expert manually annotates obligations and prohibitions as a core part of his/her main job based on strict guidelines of the company that provided the data.ßdummyTurker
9490ß0.0ßHence, the gold labels of the dataset are reasonably reliable.ßdummyTurker
9491ß0.0ßRegarding the classes, the deontic classes vary per work (and company), but obligations and prohibitions are the most common ones, as noted in the introduction (paragraph 2).ßdummyTurker
9492ß0.0ßRegarding more classical classifiers, O’ Neil et al.ßdummyTurker
9493ß0.0ß(2017) showed that BILSTMs outperform SVMs, Logistic Regression, AdaBoost, and Random Forests in legal sentence classification, as also noted in the introduction (same paragraph).ßdummyTurker
9494ß0.0ßThese can be easily fixed in the final version of the paper.ßdummyTurker
9495ß0.0ßThe 50k English contracts that were used to obtain POS tag embeddings (after replacing tokens by their POS tags) are a subset of the 750k contracts that were used to obtain word embeddings, but we see no reason why the relation between the two datasets should matter when pre-training word and POS tag embeddings.ßdummyTurker
9496ß0.0ßWe also note that the dataset of annotated contracts we experimented with has no overlap with the 750k (and 50k) contracts that were used to obtain word and POS tag embeddings.ßdummyTurker
9497ß0.0ßAs noted in our reply to the second reviewer, O’ Neil et al.ßdummyTurker
9498ß0.0ß(2017) showed that BILSTMs outperform SVMs, Logistic Regression, AdaBoost, and Random Forests in legal sentence classification, as also noted in the introduction (paragraph 2).ßdummyTurker
9499ß0.0ßAs noted in our replies to the previous two reviewers, inter-annotator agreement was not measured (nor could it have been measured, given that the expert had access to the annotations of the law students), but the expert manually annotates obligations and prohibitions as a core part of his/her main job based on strict guidelines of the company that provided the data.ßdummyTurker
9500ß0.0ßHence, the gold labels of the dataset are reasonably reliable.ßdummyTurker
9501ß0.0ßWe started with 9,400 annotated sections from 100 randomly selected English service agreement contracts.ßdummyTurker
9502ß0.0ßWe aimed to use 20% (1,880 sections) of the 9,400 sections as the test set, and 80% (7,520 sections) for training and development.ßdummyTurker
9503ß0.0ßWe also wanted, however, to avoid having test sections that would be very similar to training/development sections (e.g., because of boilerplate -similar- clauses from older contracts).ßdummyTurker
9504ß0.0ßWe therefore clustered all the 9,400 sections by agglomeratively clustering, iteratively placing in the same cluster pairs of sections with Levenshtein distance lower than 500.ßdummyTurker
9505ß0.0ßWe then partitioned the resulting clusters to test clusters and training/development clusters, trying to keep approximately the 80%-20% ratio of the resulting test and training/development sections.ßdummyTurker
9506ß0.0ßBecause of the granularity of the clusters, this led to 1,420 (instead of 1,880) test sections and 7,980 (instead of 7,520) training/development sections.ßdummyTurker
9507ß0.0ßWe repeated the same process to divide the 7,980 sections to training and development sections, again aiming at a 80%-20% split, but because of the granularity of the clusters, we ended up having 6,385 (instead of 6,016) training sections and 1,595 (instead of 1,504) development sections.ßdummyTurker
9508ß0.0ßThe imbalance in the number of sentences and number of sentences per class mentioned by the reviewer was caused by the fact that the number of sentences and their classes vary per section.ßdummyTurker
9509ß0.0ßWe believe that the imbalance actually adds to the credibility of our results, in the sense that most classifiers perform better with balanced data.ßdummyTurker
9510ß0.0ßUnfortunately lack of space did not permit us to discuss these details further.ßdummyTurker
9511ß0.0ßThe vocabulary size was 514,369, as in the work of Chalkidis et al.ßdummyTurker
9512ß0.0ß; this is the number of vocabulary words we had embeddings for (we reused the word embeddings of Chalkidis et al.).ßdummyTurker
9513ß0.0ßNo trigger words were used during annotation; the annotators considered the entire sections that were given to them.ßdummyTurker
9514ß0.0ßThe annotators labeled only sentences expressing obligations and prohibitions; sentences (e.g., containing “shall”) expressing permissions were not labeled.ßdummyTurker
9515ß0.0ßThese points can be easily clarified.ßdummyTurker
9516ß0.0ßThese declines in the performance of list item classes of BILSTM-ATT and X-BILSTM-ATT (and also of BILSTM) are due to the fact that these methods have no (or only limited, in the case of X-BILSTM-ATT) view of the previous sentences, which often clarify if a following list item is an obligation or prohibition (see, for example, examples 4-6 in Table 1).ßdummyTurker
9517ß0.0ßLacking this information, these methods tend to classify all list items as obligation list items (which are more frequent than None list items and prohibition list items), reducing the precision of obligation list items.ßdummyTurker
9518ß0.0ßThis is an interesting point that can be included in the paper.ßdummyTurker
9519ß0.0ßWe do not see any decline in the obligation list begin class.ßdummyTurker
9520ß0.0ßFurther comments will be considered in the final version of the paper.ßdummyTurker
9521ß0.0ßThe DLM approach referred in the paper is the combination of the two stages.ßdummyTurker
9522ß0.0ßUsing two stages approach reduces data sparsity.ßdummyTurker
9523ß0.0ßIn the first stage the choice is among the whole vocabulary, but in the second stage, the choice is among the most probable top ten words, so we deal with much less sparse space.ßdummyTurker
9524ß0.0ßThe compared Neural readers also have these two stages implicitly where the first stage extracts and scores features and the second stage focuses its attention on the prediction.ßdummyTurker
9525ß0.0ßThe text does not clarify this definition in the introduction and conclusion properly and we will make the appropriate changes in the next version.ßdummyTurker
9526ß0.0ßOur claim is not that these features could not be learned by the neural network.ßdummyTurker
9527ß0.0ßThe neural reader probably learns similar features automatically, but it is hard to know and scrutinize what kind of features are learned and how much each feature affect the result.ßdummyTurker
9528ß0.0ßIn contrast, in DLM it is much easier to manipulate and dig into these features and augment missing information to the current model.ßdummyTurker
9529ß0.0ßWe could also elaborate that the extensive meta-parameter tuning done on neural nets is not transferable but those well-known models used in this paper can generalize and be used for different tasks.ßdummyTurker
9530ß0.0ßn is the total number of words in the whole text.ßdummyTurker
9531ß0.0ßWe kept the same notation of original paper of the method for Γ,  n, and m but we have to remove the original formulas because of the lack of space.ßdummyTurker
9532ß0.0ßThis final modification is just an extra penalty for the frequent words which are not interesting in LAMBADA task.ßdummyTurker
9533ß0.0ßWe will rework these suggestions in the next version.ßdummyTurker
9534ß0.0ßWe agree with the reviewer that LAMBADA is not end-task or an established subtask, but it casts language modeling as a word prediction task and allows different models to be compared cheaply.ßdummyTurker
9535ß0.0ßThis language modeling task is central to many NLP applications like conversational agents, question answering, and summarization.ßdummyTurker
9536ß0.0ßNeural networks with their implicit and explicit memorizing capabilities have performed well on such tasks.ßdummyTurker
9537ß0.0ßOur paper provides a contrarian view on this subject where this memorization can be performed using feature engineering and still perform better than neural networks on this language modeling task.ßdummyTurker
9538ß0.0ßAn important fallout of this study is that long-term memorization can still be advanced with the help of feature engineering and not just with neural networks.ßdummyTurker
9539ß0.0ßMoreover, even if the neural reader learns similar features automatically, it is hard to scrutinize what kind of features are learned and how much each feature affect the accuracy.ßdummyTurker
9540ß0.0ßIn contrast, in DLM it is much easier to manipulate and dig into the features.ßdummyTurker
9541ß0.0ßIn this short paper, we focused on LAMBADA as an example, because it shows best the capability of current models on broad context sentence completion.ßdummyTurker
9542ß0.0ßIn future, we plan to investigate this method on other tasks.ßdummyTurker
9543ß0.0ßMost of the features used in our work are widely used in other NLP applications, except the models which learn the possible last words in a sentence and similarity of word candidates to important words.ßdummyTurker
9544ß0.0ßHowever, these features have not been used in the context of word prediction pipelines, which is what we focussed on this paper.ßdummyTurker
9545ß0.0ßAlso to our knowledge, these variants of across sentence boundary n-grams for capturing long-term dependencies are never used in any NLP task.ßdummyTurker
9546ß0.0ßThis clarification needs to be presented in the next version.ßdummyTurker
9547ß0.0ßThe other two datasets in Dhingra et al.ßdummyTurker
9548ß0.0ßare datasets for question answering, However, LAMBADA is a data set for sentence completion requiring broad context.ßdummyTurker
9549ß0.0ßThe instances in LAMBADA dataset are manually filtered so that the target words are 100% guessable by humans when given the larger context but not at all when only last sentence is given.ßdummyTurker
9550ß0.0ßThe other datasets don’t have these properties.ßdummyTurker
9551ß0.0ßAlso, these datasets require models to perform different kinds of inferences over broader passages.ßdummyTurker
9552ß0.0ßFor example for CNN, models must be able to summarize the articles, in order to make sense of the sentence containing the missing word, whereas in LAMBADA the last sentence is not a summary of the broader passage, but a continuation of the same story.ßdummyTurker
9553ß0.0ßAs the concentration of my work was sentence completion requiring broad context, we focused on LAMBADA task.ßdummyTurker
9554ß0.0ßUsing two stages approach reduces data sparsity.ßdummyTurker
9555ß0.0ßIn the first stage the choice is among the whole vocabulary, but in the second stage, the choice is among the most probable top ten words, so we deal with much less sparse space.ßdummyTurker
9556ß0.0ßAlso in the first stage, a fixed vector of weights is used for the features, which is learned by a grid search over the development set.ßdummyTurker
9557ß0.0ßThe grid search is a hard-earned and time-consuming process and it is not possible to try all combination of weights with small changes.ßdummyTurker
9558ß0.0ßHowever, in the second stage, the neural network classifier decides on the weight of each feature.ßdummyTurker
9559ß0.0ßSome of the proposed features could be used for other tasks.ßdummyTurker
9560ß0.0ßFor example, the model for the last word prediction could be applied in any sentence completion, or across sentence boundary n-grams would be useful in any broad context language modeling.ßdummyTurker
9561ß0.0ßIn this short paper, we focused on LAMBADA as an example, because LAMBADA shows best the capability of current models on broad context sentence completion.ßdummyTurker
9562ß0.0ßIn future, we plan to investigate this method on other tasks.ßdummyTurker
9563ß0.0ßWe agree that the paper may benefit from a stronger motivation.ßdummyTurker
9564ß0.0ßHence, if it is accepted to the conference we plan to use the additional space provided by the organizers to further strengthen this aspect of our work.ßdummyTurker
9565ß0.0ßWe also believe that while our models may not be novel in absolute terms since they are inspired by other fields, they are to be considered novel for the keyphrase extraction field, hence they should be of great interest to the KE community.ßdummyTurker
9566ß0.0ßWe believe that the weakness of the CNN approach is due to the intrinsic limitation of convolutional neural networks, in particular, due to the necessity of applying max pooling layers to reduce the network size, which are known to lead to a loss of information.ßdummyTurker
9567ß0.0ßOn the other hand, when using LSTM layers, there is no such loss, justifying their superior performance in our task.ßdummyTurker
9568ß0.0ßTF-IDF is an usual baseline when evaluating keyphrase extraction algorithms, hence we included it in our work.ßdummyTurker
9569ß0.0ßRegarding other unsupervised methods, we included them because of their scientific relevance, since many of them (e.g.ßdummyTurker
9570ß0.0ßTopicRank) are among the most cited papers in the keyphrase extraction field.ßdummyTurker
9571ß0.0ßIn short, as we already briefly mentioned in the paper, we believe that our approaches benefit from using an internal representation of the document, in the form of what we call a 'document vector'.ßdummyTurker
9572ß0.0ßThis mimics how humans understand a document.ßdummyTurker
9573ß0.0ßIn fact, a technique that is often used in order to get a full understanding of a document is to briefly read it to have a general picture of the contents, then to read it again thoroughly and focus on the most important concepts.ßdummyTurker
9574ß0.0ßIntuitively, this is the inspiration for our networks.ßdummyTurker
9575ß0.0ßIn fact, first a LSTM builds the document vector to have a general idea of the document.ßdummyTurker
9576ß0.0ßIn the KL approach, we use such vectors in what we can imagine as a 'second read' of the document to label the most important words only.ßdummyTurker
9577ß0.0ßIn the KS approach, the document vector is instead used to understand if a phrase is relevant enough with respect to the document content to be considered a keyphrase.ßdummyTurker
9578ß0.0ßWe're not sure on what you mean with the remark that 'the RNN model was not used in the twin architectures'.ßdummyTurker
9579ß0.0ßIn fact, all our architectures but the one described in Section 7.2 are based on bidirectional LSTMs, which are a variant of RNNs.ßdummyTurker
9580ß0.0ßRegarding Section 7.2, there we don't use LSTMs but CNNs in order to obtain a lightweight architecture and test if still performs well enough.ßdummyTurker
9581ß0.0ßIf this still does not answer your concerns, please let us know and we'll be happy to address them in the final version of the paper.ßdummyTurker
9582ß0.0ßWe agree that the paper is fairly dense.ßdummyTurker
9583ß0.0ßHowever, we think that many details are necessary to display a rigorous experimental setting as the one we have built.ßdummyTurker
9584ß0.0ßNevertheless, if the paper is accepted to the conference, we plan to use the additional space provided by the organizers to further strengthen the motivation behind our work.ßdummyTurker
9585ß0.0ßWe want to stress that we believe that our detailed descriptions of the architectures, and our plans to release the code, are necessary to guarantee the reproducibility of our work.ßdummyTurker
9586ß0.0ßIn this sense, since we notice you assigned us an average 'replicability' score, if possible, we would like to hear more details on what you think we should've improved in this aspect.ßdummyTurker
9587ß0.0ßThanks for your review.ßdummyTurker
9588ß0.0ßWe agree that the paper may benefit from a stronger motivation.ßdummyTurker
9589ß0.0ßHence, if it is accepted to the conference we plan to use the additional space provided by the organizers to further strengthen this aspect of our work.ßdummyTurker
9590ß0.0ßIt's true that the paper contains many details about the network architectures, but we choose this path, together will the release of the source code, to ensure the maximum reproducibility of our experiments.ßdummyTurker
9591ß0.0ßSee Question #2.ßdummyTurker
9592ß0.0ßTo the best of our knowledge, the possibility of using keyphrases not appearing in the input document to train the algorithm is a novelty in the supervised keyphrase extraction field.ßdummyTurker
9593ß0.0ßFor this reason, this is a big advantage of the KS family of algorithms with respect to the KL ones.ßdummyTurker
9594ß0.0ßHence, we believe that this contribution deserves to be highlighted with an entire Section.ßdummyTurker
9595ß0.0ßMoreover, the first approach still has to be described, because one still must use it at evaluation time.ßdummyTurker
9596ß0.0ßIf the paper is accepted to the conference, we will use the additional space to better explain our motivation of each architecture.ßdummyTurker
9597ß0.0ßHowever, for reasons of space, we're sorry that we can't provide such explanations here.ßdummyTurker
9598ß0.0ßThe concatenation of the document vector to the word vector allows the network to exploit contextual information in order to understand the relevance of a single word with the whole document.ßdummyTurker
9599ß0.0ßIn fact, in a regular LSTM, at the first time step (i.e.ßdummyTurker
9600ß0.0ßthe first word of the document) the network knows nothing about the content of the document.ßdummyTurker
9601ß0.0ßWith our approach the 'second' LSTM is immediatly able to use a neural representation of the document, hence it's able to better evaluate the relevance of each single word in the whole context.ßdummyTurker
9602ß0.0ßThanks for your insightful suggestions for improving our paper.ßdummyTurker
9603ß0.0ßWe check out the results generated by pure directly optimizing CIDEr with self-critical.ßdummyTurker
9604ß0.0ß"We find out after deleting the ""bad ending"", the end of all sentences are valid n-grams."ßdummyTurker
9605ß0.0ßSo the results in Table 2 with blue color is also the baseline result reviewer 2 wants.ßdummyTurker
9606ß0.0ßIn addition, we also try another mechanism that we remove unseen n-gram at the end of the sentence in self-critical training as reviewer 2 suggested.ßdummyTurker
9607ß0.0ßAnd we find that the algorithm converges almost three times slower than ours and still achieve a lower CIDEr score compared with Ours-Att-4-gram.ßdummyTurker
9608ß0.0ßYes, we did not use any smoothing or backoff algorithms in this version.ßdummyTurker
9609ß0.0ßThis simple treatment has already give us comparable CIDEr and much faster convergence, which already verified the assumptions in the paper.ßdummyTurker
9610ß0.0ßWe will add standard smoothing/backoff results in the revision, and will investigate more advanced approaches in future work.ßdummyTurker
9611ß0.0ßThe main contributions of this paper are two-foldßdummyTurker
9612ß0.0ßThe goal of this paper is to develop a simple algorithm that is efficient and wildly appreciable to a wide range of RL-based learning.ßdummyTurker
9613ß0.0ßThe work falls to the direction of developing new training algorithms, which is orthogonal to some state-of-the-art methods that focus more on network structure design.ßdummyTurker
9614ß0.0ßGiven the availability of the open-sourced RL-based algorithm and the purpose of our work, we select Ranzato et.al,  Ren et.ßdummyTurker
9615ß0.0ßal.2017 and  Rennie et al., 2017. because these works all release their codes and pay more attention to RL training strategy.ßdummyTurker
9616ß0.0ßThank you for your positive and constructive comments.ßdummyTurker
9617ß0.0ßAll our RL-setting experiments use the same warm start model for fair comparison.ßdummyTurker
9618ß0.0ßAll our experimental results are repeated for over five times and implemented with a GTX1080 GPU.ßdummyTurker
9619ß0.0ß(1) For attention warm start training, it takes almost 270k iterations which cost almost 600 minutes.ßdummyTurker
9620ß0.0ßDuring RL training, Ours-Att-4-gram takes 30k iterations with 1,300 minutes and Ours-Att-trigram takes 25k iterations with 1,200 minutes.ßdummyTurker
9621ß0.0ßAtt-SC takes 555k iterations with 2,300 minutes.ßdummyTurker
9622ß0.0ß(2) For ED warm start training, it takes 200k iterations with 400 minutes.ßdummyTurker
9623ß0.0ßOurs-ED-4-gram takes 62k iterations with 400 minutes and Ours-ED-trigram takes 15k iterations with 350 minutes.ßdummyTurker
9624ß0.0ßED-SC takes 95k iterations with 1,050 minutes.ßdummyTurker
9625ß0.0ßWe test Ours-Att-4-gram on the percentage of unique captions (5000 images in validation set) generated that is no repetitions from the training for over 5 times.ßdummyTurker
9626ß0.0ßThe unique rate (doesn't seen in training set) are 76.86%, 78.88%, 77.26%, 77.54% and 78.62%.ßdummyTurker
9627ß0.0ßThe results show that our algorithm can generate various image description instead of repetitions from the training dataset.ßdummyTurker
9628ß0.0ßOurs-Att-trigram has a similar result with 86.14%, 85.86%, 88.84%,  85.06% and 87.18%.ßdummyTurker
9629ß0.0ßWe will add human evaluation on a subset of the test set in the revision.ßdummyTurker
9630ß0.0ßThank you for your comments.ßdummyTurker
9631ß0.0ßWe submit three versionßdummyTurker
9632ß0.0ßOurs-Att-4-gram----CIDEr-C5: 99.41  CIDEr-C40: 104.70  BLEU4-C5: 30.53  BLEU4-C40: 59.75ßdummyTurker
9633ß0.0ßOurs-Att-trigram-----CIDEr-C5: 98.69  CIDEr-C40: 103.43  BLEU4-C5: 30.6  BLEU4-C40: 60.45ßdummyTurker
9634ß0.0ßAtt-SC----CIDEr-C5: 105.23  CIDEr-C40: 109.25  BLEU4-C5: 32.41  BLEU4-C40: 61.86  (The higher performance might come from the bad endings)ßdummyTurker
9635ß0.0ßThe main contributions of this paper are two-foldßdummyTurker
9636ß0.0ßWe will add human evaluation on a subset of the test set in the revision.ßdummyTurker
9637ß0.0ßWe will cite these two papers in revision and discuss the relations.ßdummyTurker
9638ß0.0ßHill et al.ßdummyTurker
9639ß0.0ß(2016) trained a denoising auto-encoder on a seq2seq network architecture for training sentence and paragraph representations from the output of the intermediate layers.ßdummyTurker
9640ß0.0ßThe trained model can then encode novel word sequences into distributed representations.ßdummyTurker
9641ß0.0ßThey did not modify the input (manipulate the denoising process) in a way that the input looks like the input of their task.ßdummyTurker
9642ß0.0ßI disagree.ßdummyTurker
9643ß0.0ßThe result is surprising as the translation data is fully out-of-domain.ßdummyTurker
9644ß0.0ßFurther, this experiments should demonstrate the advantage of an unsupervised system by including either any unlabeled data or even multilingual data.ßdummyTurker
9645ß0.0ßIn a supervised data it would be very complicated to just used available translation data to build an multilingual NLG system.ßdummyTurker
9646ß0.0ßWe have not tried to remove the attention mechanism from the model.ßdummyTurker
9647ß0.0ßBut, we fully agree that it is very likely not needed.ßdummyTurker
9648ß0.0ßFurther, we think that a much easier network structure would also do a good job for the unsupervised NLG task.ßdummyTurker
9649ß0.0ßBut, the seq2seq architecture with attention is critical for the multilingual settings.ßdummyTurker
9650ß0.0ßThe multilingual setting is more an extra experiment than a base experiment of the paper.ßdummyTurker
9651ß0.0ßWe are e not planing to do a human evaluation also for the multilingual experiments.ßdummyTurker
9652ß0.0ßBut, there are also few examples where the output is not good, we can add these examples to show that the multilingual settings still need more attention/ different data to reach high quality.ßdummyTurker
9653ß0.0ßWe added a section to the paper that addresses the limitations of our model.ßdummyTurker
9654ß0.0ßOur approach will not work for every NLG problem!ßdummyTurker
9655ß0.0ßFor many NLG problems, we would first need many rules/templates to modify the MRs into words that we also want to have in the target.ßdummyTurker
9656ß0.0ßEven still possible that would take to much effort and the work can be similar to that of building a template system.ßdummyTurker
9657ß0.0ßWe are currently running a human evaluation for the best unsupervised system versus the two supervised baselines.ßdummyTurker
9658ß0.0ßWe will add the numbers.ßdummyTurker
9659ß0.0ßHuman also score the output based on missing information or additional information and fluency.ßdummyTurker
9660ß0.0ßThe human evaluation took us much longer then expected.ßdummyTurker
9661ß0.0ßWe are using exactly the seq2seq model of Badhanau et al.ßdummyTurker
9662ß0.0ßThere is not a single modification of the network structure.ßdummyTurker
9663ß0.0ßWe will make this more clear in the paper.ßdummyTurker
9664ß0.0ßOh thanks!ßdummyTurker
9665ß0.0ßThats a typo.ßdummyTurker
9666ß0.0ßShould be 72.0ßdummyTurker
9667ß0.0ßThanks for the comment.ßdummyTurker
9668ß0.0ßWe will add significance tests in the next version of this paper.ßdummyTurker
9669ß0.0ßWe make a backup of the index on the original tensor for each word, so we can still find their original indices even if the dimension changes.ßdummyTurker
9670ß0.0ßThe beam search component can proceed as usual but on a smaller set of candidates and it needs to notice the original indices of each word on the smaller tensor.ßdummyTurker
9671ß0.0ßThanks for the comment.ßdummyTurker
9672ß0.0ßWe will add significance tests in the next version of this paper.ßdummyTurker
9673ß0.0ßWe will proofread the paper and double-check the writing and formatting.ßdummyTurker
9674ß0.0ßDue to the space limitation, we do not illustrate the summarization examples.ßdummyTurker
9675ß0.0ßBut we submit our source code and the data to the conference as supplementary materials.ßdummyTurker
9676ß0.0ßThus this can be replicated by running the code on the corresponding dataset.ßdummyTurker
9677ß0.0ßWe take this suggestion, and prepare to add more analysis in the submission.ßdummyTurker
9678ß0.0ßCurrently, Table 4-Table 7 illustrates the analysis of robustness in other domains, and the difference from the upper-bound (supervised method) performance.ßdummyTurker
9679ß0.0ßFigure 6 illustrates a qualitative study to demonstrate whether the mapped feature vectors are densely overlapped.ßdummyTurker
9680ß0.0ßFigure 7 illustrates the analysis of lambda in Eq.ßdummyTurker
9681ß0.0ß1 and the feature dimensions.ßdummyTurker
9682ß0.0ßWe will add more detailed analysis in future submissions.ßdummyTurker
9683ß0.0ßThe target domain is the dialog domain, which is relevant to the focused application.ßdummyTurker
9684ß0.0ßThe idea of forcing feature vectors of two domains to have similar distributions is indeed employed from DAN (we mentioned this in line 320) in designing the transfer learning framework, but it is not the main contribution and the main idea of this paper.ßdummyTurker
9685ß0.0ßThe main challenge that previous DAN cannot solve is the zero-shot labels.ßdummyTurker
9686ß0.0ßLet us illustrate an example.ßdummyTurker
9687ß0.0ßSuppose word A1 and A2 are two words in the news domain; word B1 and B2 are two words in the dialog domain.ßdummyTurker
9688ß0.0ßIf simply employing the idea of DAN, in the common feature space, the distribution of A1 A2 and the distribution of B1 B2 are indeed similar.ßdummyTurker
9689ß0.0ßHowever, the semantic information between B1 and B2 will be lost.ßdummyTurker
9690ß0.0ßFor example, if we change the position of B1 and B2, the distribution will also be similar with A1 and A2.ßdummyTurker
9691ß0.0ßThe key challenge is, on one hand, we need to have similar distributions to let the shared encoder/decoder be effective in summarization; on the other hand, we need to let the mapped feature vectors can still represent the meaning of the sentence.ßdummyTurker
9692ß0.0ßFor zero-shot labels, DAN can guarantee the former, but cannot model the latter.ßdummyTurker
9693ß0.0ßBy the proposed shared auto-encoder framework, we solve this problem.ßdummyTurker
9694ß0.0ßThe learned bidirectional mapping functions and the learned encoder/decoder parameters can guarantee that the sentence can be generated from the common feature space, besides having similar feature distributions.ßdummyTurker
9695ß0.0ßThus the semantic information is retained.ßdummyTurker
9696ß0.0ßThe insight contribution is to learn the mapping between the zero-shot label’s feature distributions and the training label’s feature distributions, through the sharing of auto-encoder parameters.ßdummyTurker
9697ß0.0ßIn experimental session, we compare the proposed framework and the DAN approach, as shown in Table 2 and Table 3, in abstractive methods, our method outperforms DAN by 3.1% in RG-1 and 6.5% in RG-2; in the extractive methods, our method outperforms DAN by 5.9% in RG-1 and 5.8% in RG-2.ßdummyTurker
9698ß0.0ßFrom table 4-7, we can approximate the upper-bound of improvement (supervised methods) is 15% (supervised over DAN) in abstractive methods and 20% (supervised over DAN) in extractive methods.ßdummyTurker
9699ß0.0ßTherefore, the proposed method gains significant improvements over DAN.ßdummyTurker
9700ß0.0ßWe are very sorry to mislead the readers.ßdummyTurker
9701ß0.0ßThe novel application is to emphasize that customer service dialog summarization has not been investigated previously and no training data are available.ßdummyTurker
9702ß0.0ßThe customer service dialog summarization task does not have major differences to other dialog summarization tasks.ßdummyTurker
9703ß0.0ßSince no training data can be used in this application, transfer learning methods are investigated in this paper.ßdummyTurker
9704ß0.0ßThis suggestion is helpful and we will certainly take it.ßdummyTurker
9705ß0.0ßIn abstractive summarization, we have tried the one without the copy mechanism, the results are significantly worse.ßdummyTurker
9706ß0.0ßWe will add these results in the next submission.ßdummyTurker
9707ß0.0ßx^(t) in line 431-432 refers to token in the reference sequence.ßdummyTurker
9708ß0.0ßy refers to the generated output of the model.ßdummyTurker
9709ß0.0ßAlthough auto-encoder is an existing technique, how to utilize it in zero-shot neural text summarization is nontrivial.ßdummyTurker
9710ß0.0ßThere are two key challenges in designing the transfer learning mechanism, (1) how auto-encoder and summarization work together, and (2) how two-domain data work together.ßdummyTurker
9711ß0.0ßIn the proposed framework, auto-encoder and summarization share the same bidirectional feature mapping functions, and the two-domain data share the same auto-encoder and summarization model parameters.ßdummyTurker
9712ß0.0ßBy this design, the performance of the zero-shot text summarization, which has not been investigated in previous work, is significantly improved.ßdummyTurker
9713ß0.0ßDAN is the state-of-the-art method in neural transfer learning.ßdummyTurker
9714ß0.0ßAs introduced in the response of the first review expert, it cannot model the zero-shot labels well.ßdummyTurker
9715ß0.0ßOur contribution is to provide a mechanism when zero-shot labels are confronted in practice.ßdummyTurker
9716ß0.0ßWe feel very sorry for the presentation unclearness.ßdummyTurker
9717ß0.0ßWe accept all the suggestions.ßdummyTurker
9718ß0.0ßThe review expert’s suggestions are in detail and very helpful.ßdummyTurker
9719ß0.0ßWe are sorry again for the inappropriate style of related work.ßdummyTurker
9720ß0.0ßThis is surely a good suggestion.ßdummyTurker
9721ß0.0ßThe insight assumption of the paper is the semantic structure of two domains are closely related to the task of summarization.ßdummyTurker
9722ß0.0ßFor example, in both domains, some words are more expressive than others, some words often co-occur with other words, etc.ßdummyTurker
9723ß0.0ßThus even the meanings of the words in two domains are irrelevant, which seems “contradictious” as said by the review expert, the semantic structure is the real treasure to be utilized in the transfer learning framework.ßdummyTurker
9724ß0.0ßExperimental results also demonstrates its effectiveness.ßdummyTurker
9725ß0.0ßFigure 3 is a framework, Fig.ßdummyTurker
9726ß0.0ß4 and Fig.ßdummyTurker
9727ß0.0ß5 are implementations with abstractive and extractive models.ßdummyTurker
9728ß0.0ßV stands for the state transition matrix in the summarization model, distinguished from W for auto-encoder.ßdummyTurker
9729ß0.0ßThanks for the reviewing our paper.ßdummyTurker
9730ß0.0ß"Those more complex linguistic features are all pairwise features, and therefore all of them are included in the ""+pairwise"" results in Table4."ßdummyTurker
9731ß0.0ß"As the results show, ""+pairwise"" performs worse than the baseline (top-pairs model with no linguistic feature)."ßdummyTurker
9732ß0.0ßYes, those features are not invented by EPM.ßdummyTurker
9733ß0.0ßAs mentioned in Section 5.2, we can use EPM as a data-driven approach for confirming or refining earlier linguistic findings, e.g.ßdummyTurker
9734ß0.0ßUryupina (2007).ßdummyTurker
9735ß0.0ßUryupina (2007) claims are based on the analysis on the MUC dataset.ßdummyTurker
9736ß0.0ßThe EPM results show that those findings are still valid for the current larger dataset, i.e.ßdummyTurker
9737ß0.0ßCoNLL.ßdummyTurker
9738ß0.0ßWe incorporate features of Section 5.2 as the base features for EPM, i.e.ßdummyTurker
9739ß0.0ßwe run EPM on features of Section 5.1+5.2.ßdummyTurker
9740ß0.0ßIn the end, we only incorporate feature-values that are included in the EPM mined patterns.ßdummyTurker
9741ß0.0ß"Features of Section 5.2 are incorporated in ""+EPM"" experiments only if they are contained in at least one informative pattern."ßdummyTurker
9742ß0.0ß"Therefore, ""base"", ""all"" and ""EPM"" experiments are exactly done the way that you requested them to be."ßdummyTurker
9743ß0.0ß"Could you please read ""Reply_to_weakness_argument_3"" of reviewer one?"ßdummyTurker
9744ß0.0ßThe best performing coreference resolvers are all now using deep neural networks.ßdummyTurker
9745ß0.0ßWith deep NNs, there is no need for using combinatorial features, that is one of the things that NNs are well capable of.ßdummyTurker
9746ß0.0ßWe only intend to use discriminative pattern mining approaches for selecting useful values of linguistic features.ßdummyTurker
9747ß0.0ßSince we cannot determine useful feature-values in isolation, we explore all combinations of feature-values up to a certain length.ßdummyTurker
9748ß0.0ßTherefore, we explore informative patterns in order to get to the informative feature-values.ßdummyTurker
9749ß0.0ßNevertheless, the use of patterns themselves in a linear classifier is reported in Table1.ßdummyTurker
9750ß0.0ßThanks for the reviewing our paper.ßdummyTurker
9751ß0.0ßWe have the results in which we use a 2-degree polynomial SVM as the baseline in Table1.ßdummyTurker
9752ß0.0ßThe incorporation of informative patterns still significantly improves the results of SVM-2 for both MPP and EPM patterns.ßdummyTurker
9753ß0.0ßUsing a linear SVM that doesn't perform feature combination itself better indicates the quality of patterns.ßdummyTurker
9754ß0.0ßOtherwise, one may claim that improvements are an effect of combining those patterns instead of patterns themselves.ßdummyTurker
9755ß0.0ßWe can briefly mention this point in the paper.ßdummyTurker
9756ß0.0ßWe choose CoNLL for training because it is the biggest coreference data (>2000 documents) while Wikicoref only includes 30 documents.ßdummyTurker
9757ß0.0ßIt is more of a test rather than a training data.ßdummyTurker
9758ß0.0ßWe would report the ratio of informative feature-values of CoNLL that can also be mined from WikiCoref.ßdummyTurker
9759ß0.0ßWe only use the CoNLL training data for EPM mining and we didn't perform any coreference evaluation for setting the parameters of EPM.ßdummyTurker
9760ß0.0ßWe set the parameters based on the length, number, and the quality (coreference probability) of mined patterns themselves.ßdummyTurker
9761ß0.0ßWe add statistics of what would change for differenr parameters in the Supplementary material.ßdummyTurker
9762ß0.0ßThanks for the suggested analysis.ßdummyTurker
9763ß0.0ßWe will include it in the paper.ßdummyTurker
9764ß0.0ß1) The bi-directional LSTM encoder uses different parameters for both directions.ßdummyTurker
9765ß0.0ß2) The sketch token embeddings are initialized by the default method in PyTorch.ßdummyTurker
9766ß0.0ßThe number of training epoches is determined by early stopping.ßdummyTurker
9767ß0.0ßWe will describe this in Section 5.ßdummyTurker
9768ß0.0ßWe will add error analysis for the coarse-to-fine model.ßdummyTurker
9769ß0.0ß"y_{t-1} is determined by a_k"" means there is a one-to-one alignment between y_{t-1} and a_k."ßdummyTurker
9770ß0.0ßFor WikiSQL, the table schema serves as input in addition to natural language.ßdummyTurker
9771ß0.0ßWe therefore modify our input encoder in order to render it table-aware.ßdummyTurker
9772ß0.0ßThe pointer mechanism in the decoder is used to generate table columns and condition values for SQL queries.ßdummyTurker
9773ß0.0ßWe will rephrase the sentence.ßdummyTurker
9774ß0.0ß"We will cite ""[OOPSLA17] SQLizer"ßdummyTurker
9775ß0.0ßWe thank the reviewers for their useful reviews, which will further improve its quality.ßdummyTurker
9776ß0.0ßWe will address all suggestions in the extra page provided for the final submission if the paper is accepted.ßdummyTurker
9777ß0.0ßBelow we address the reviewers' concerns in detail.ßdummyTurker
9778ß0.0ßReview #2ßdummyTurker
9779ß0.0ßWhich of the features added in each category leads to most significant gains?ßdummyTurker
9780ß0.0ßGiven the short space we did not include ablation studies at feature level.ßdummyTurker
9781ß0.0ßWe have done this and we can include it taking advantage of the extra page in the camera ready version.ßdummyTurker
9782ß0.0ßWe can also perform the study for each individual category as requested.ßdummyTurker
9783ß0.0ßGeneralization to other models: It is non-trivial to transform our feature set to neural models, plus is not the focus of this work.ßdummyTurker
9784ß0.0ßWe picked linear CRFs (successfully used in NER) for its simplicity and explainability, which are both attributes that cannot be provided by neural models.ßdummyTurker
9785ß0.0ßMultilinguality: Our methodology is general enough to be language-independent.ßdummyTurker
9786ß0.0ßWe have performed experiments for Spanish and German that can be included in the camera ready version using the extra space.ßdummyTurker
9787ß0.0ßWe thank the reviewers for their useful reviews, which will further improve its quality.ßdummyTurker
9788ß0.0ßWe will address all suggestions in the extra page provided for the final submission if the paper is accepted.ßdummyTurker
9789ß0.0ßBelow we address the reviewers' concerns in detail.ßdummyTurker
9790ß0.0ßReview #1ßdummyTurker
9791ß0.0ßOnly one NER approach is studied: We picked linear CRFs for its simplicity and explainability.ßdummyTurker
9792ß0.0ßMore complex CRFs have been used (Luo et al.ßdummyTurker
9793ß0.0ßJoint Entity Recognition and Disambiguation), more recently Neural approaches.ßdummyTurker
9794ß0.0ßBut these models, especially the latter, are hard to trace.ßdummyTurker
9795ß0.0ßSo for the purpose of understandability, we argue that using linear CRFs is a suitable approach.ßdummyTurker
9796ß0.0ßOther external knowledge sources apart from Wikipedia do not add much: In principle, all resources we used originate in Wikipedia, the difference is the degree of knowledge we use: from a list of entity names (i.e., Wikipedia title pages) to the named entities themselves (with their categories and set of possible names).The novelty in this work does not lie in the use of Wikipedia as a knowledge resource but more generally in the principled modularization of the knowledge classes.ßdummyTurker
9797ß0.0ßThank you for the reviews.ßdummyTurker
9798ß0.0ßThanks to the reviewers' professional comments, in fact, the data I gave in this paper are the results of repeating Chen et al.ßdummyTurker
9799ß0.0ß(2016) in my computer environment.ßdummyTurker
9800ß0.0ßBecause the initialization setting and the computing environment are different, I do not use the data in the original text.ßdummyTurker
9801ß0.0ßThank You.ßdummyTurker
9802ß0.0ßI repeat the experiments on Chen et al.ßdummyTurker
9803ß0.0ß(2016) (the source code and the dataset are proposed by Chen),  I can't obtain the results proposed by Chen.ßdummyTurker
9804ß0.0ßSo I use the results that I repeat in my computing environment.ßdummyTurker
9805ß0.0ßOur model is not a multiway attention model, the main contribution in my paper is the multi-glance reading which modeling the behavior of reading.ßdummyTurker
9806ß0.0ßMoreover,  the reason that Ma et al.ßdummyTurker
9807ß0.0ß(2017) 's results are stronger than my results is though we use the same datasets e.g.ßdummyTurker
9808ß0.0ßYelp and IMDB, I just use the rankings and the text in the datasets, while Ma et al.ßdummyTurker
9809ß0.0ß(2017) also adopt other features in the datasets, e.g.ßdummyTurker
9810ß0.0ßuser and product information.ßdummyTurker
9811ß0.0ßGenerally speaking, Ma et al.ßdummyTurker
9812ß0.0ß(2017) is a user product attention model, our model is new multi-glance model.ßdummyTurker
9813ß0.0ßThank You.ßdummyTurker
9814ß0.0ßI have compared my model to traditional RNN+ATT model in section4.6, our model achieves a better performance.ßdummyTurker
9815ß0.0ßMoreover, algorithm acceleration and model lightweight design is my future work, afterwards, our model would have same parameters comparing to traditional RNN model.ßdummyTurker
9816ß0.0ßThank You.ßdummyTurker
9817ß0.0ßThank you for the reviews.ßdummyTurker
9818ß0.0ßWe didn't use pre-trained embedding.ßdummyTurker
9819ß0.0ßActually, we tried it during experiments, but it didn't improve our model's performance.ßdummyTurker
9820ß0.0ßTherefore, in the end, we didn't choose to use pre-trained embedding.ßdummyTurker
9821ß0.0ß"Reply to ""Please define ""1-MAE"" and its difference to MAE."ßdummyTurker
9822ß0.0ß""": It should be subtracting MAE from one."ßdummyTurker
9823ß0.0ßThanks for the suggestion.ßdummyTurker
9824ß0.0ßWe will further conduct significance tests.ßdummyTurker
9825ß0.0ßWe reported MRR and HITS@n to make our approach directly comparable to previous work (note that on WN18 and FB15K we directly copied the results reported in the original literature for our baselines to avoid re-implementation bias).ßdummyTurker
9826ß0.0ßWe think recall might not be that suitable for the specific link prediction task, in particular the filtered setting, where each test case gets only a single positive ground-truth (for a test triple (e_i, r_k, e_j) with e_i missing, only e_i will be taken as the correct answer).ßdummyTurker
9827ß0.0ßBut we will try anyway, maybe in other settings.ßdummyTurker
9828ß0.0ßNon-negativity of entity representations is a pre-requisite for our approach to model relation entailment.ßdummyTurker
9829ß0.0ßOnly when both the non-negativity constraints (equation (2)) and the entailment constraints (equation (4)) are given can our approach model universally quantified entailment (equation (3), see the appendix for a detailed proof).ßdummyTurker
9830ß0.0ßSo we cannot use the entailment constraints alone without introducing non-negativity on entities.ßdummyTurker
9831ß0.0ßThanks for the insightful suggestion.ßdummyTurker
9832ß0.0ßWe will further investigate the generality of our approach.ßdummyTurker
9833ß0.0ßSpecifically, we will introduce the non-negativity and approximate entailment constraints into other basic embedding models, and to see whether these constraints can still bring improvements.ßdummyTurker
9834ß0.0ßPlease kindly note that the non-negativity constraints are imposed on entity representations.ßdummyTurker
9835ß0.0ßWhen applying this kind of constraints, we don't need to decide negative properties (we guess a negative property means an inverse of a relation).ßdummyTurker
9836ß0.0ßInverse relations, actually, may come with the approximate entailment constraints (see Table 1 for examples).ßdummyTurker
9837ß0.0ßIn this work, we just introduce a pseudo inverse for each relation, e.g., child^{-1} for child.ßdummyTurker
9838ß0.0ßThis can be achieved trivially, e.g., by reformulating every triple (e_i, child, e_j) as (e_j, child^{-1}, e_i).ßdummyTurker
9839ß0.0ßWe don't need to explicitly specify which pre-existing relation is the inverse of another one.ßdummyTurker
9840ß0.0ß"That means, we don't need prior knowledge such as ""parent is the inverse of child"" (child^{-1} = parent) during learning."ßdummyTurker
9841ß0.0ß(As you may notice, we need such prior knowledge only in the visualization phase (Figure 3).ßdummyTurker
9842ß0.0ßIn this phase, inversion is detected by mutual entailment (see Section 4.2 for details).)ßdummyTurker
9843ß0.0ß"Yes, in the _cons.txt file the ""minus"" sign before a relation indicates its inverse."ßdummyTurker
9844ß0.0ßPlease refer to the reply to question 2.ßdummyTurker
9845ß0.0ßWe will try to clarify all the points mentioned.ßdummyTurker
9846ß0.0ßThe strict entailment r_p -> r_q states that if relation r_p holds then relation r_q must also holds, but not vice versa.ßdummyTurker
9847ß0.0ßThis entailment can be roughly modeled by requiring \phi(e_i, r_p, e_j) <= \phi(e_i, r_q, e_j) for every two entities e_i and e_j, i.e.ßdummyTurker
9848ß0.0ßequation (3), where \phi is the score for a triple predicted by the embedding model.ßdummyTurker
9849ß0.0ßThe interpretation is, for any e_i and e_j, if the triple (e_i, r_p, e_j) is a true fact whose score is \phi(e_i, r_p, e_j), then the triple (e_i, r_q, e_j) with an even higher score should also be predicted as a true fact by our embedding model (coincides with r_p -> r_q).ßdummyTurker
9850ß0.0ßThe requirement of equation (3) can be met just by imposing the entailment constraints onto the relation representations, i.e.ßdummyTurker
9851ß0.0ßequation (4).ßdummyTurker
9852ß0.0ßThat means, we can force the learned scores to satisfy equation (3) by imposing the constraints of equation (4).ßdummyTurker
9853ß0.0ßThis is actually the reason why we design entailment constraints on relation representations.ßdummyTurker
9854ß0.0ßNote we further relaxed strict entailments into approximate ones.ßdummyTurker
9855ß0.0ßThe approximate entailment between two relations was known beforehand (identified automatically by rule mining systems).ßdummyTurker
9856ß0.0ßIn equation (7), the first term of the objective function is a typical logistic loss widely used in previous work.ßdummyTurker
9857ß0.0ßThis loss enforces positive triples to have scores close to +1, while negative triples scores close to -1.ßdummyTurker
9858ß0.0ßThe second term is the sum of all slack variables introduced in the approximate entailment constraints.ßdummyTurker
9859ß0.0ßThe motivation is, although we allow slackness in those constraints we hope the total slackness to be small, so that the constraints can be better satisfied.ßdummyTurker
9860ß0.0ßThe third term is the L2 regularization on all embeddings to avoid overfitting.ßdummyTurker
9861ß0.0ßThe first two constraints are the approximate entailment constraints on relation representations, and the last one the non-negativity constraint on entity representations.ßdummyTurker
9862ß0.0ßThe rewrite equation (8) can be derived by converting the approximate entailment constraints into penalty terms added to the objective function.ßdummyTurker
9863ß0.0ßWe left the detailed derivation in the appendix.ßdummyTurker
9864ß0.0ßIn fact we have tried both real and imaginary components in the visualization of entity representations, and observed similar phenomena.ßdummyTurker
9865ß0.0ßWe only reported the result of real components in Figure 1 due to space limitation.ßdummyTurker
9866ß0.0ßIn the visualization of relation representations (Figure 3), we showed both real and imaginary components.ßdummyTurker
9867ß0.0ßWe will report the full results of visualization.ßdummyTurker
9868ß0.0ßAbout the appendix section: We are sorry for our negligence.ßdummyTurker
9869ß0.0ßWe followed the instructions provided in the official ACL 2018 style template, which says that code and data should be separate uploads and other kinds of appendices should come after the references.ßdummyTurker
9870ß0.0ßWe didn't notice that the CFP poses a different requirement.ßdummyTurker
9871ß0.0ßBecause the standard deviations are almost the same across different models after rounding up to three digits, say 0.009,ßdummyTurker
9872ß0.0ßwe just present means of the four evaluation indexes to simplify the table.ßdummyTurker
9873ß0.0ßTable 3 shows that our model with unsupervisedßdummyTurker
9874ß0.0ßand RL learning is comparable to other models with supervised learning such as NDM and LIDM which both need a supervised trained dialog state tracker.ßdummyTurker
9875ß0.0ßIn a further version, we would add the standard deviations.ßdummyTurker
9876ß0.0ßFirst, supervised proposed model maximize instantaneous reward at each step in a greedy manner.ßdummyTurker
9877ß0.0ßIn comparison, RL tries to maximize the expected overall reward at the end of the dialog.ßdummyTurker
9878ß0.0ßSecond, we can think of latent variable as intention of a sentence.ßdummyTurker
9879ß0.0ßSupervised learning for other models assumes that each sentence only contains one intention which is too restrictive in reality.ßdummyTurker
9880ß0.0ßIn comparison, unsupervised learning for our model can take all intentions into account since equation 7 summarize all possible intentions.ßdummyTurker
9881ß0.0ßFor writing and figure design, we will continue to make it better.ßdummyTurker
9882ß0.0ßHRED and VHRED are both hierarchical recurrent neural networks.ßdummyTurker
9883ß0.0ßHRED is a determinsic model, while VHRED take uncertainty of dialog into account by introducing guassian variables.ßdummyTurker
9884ß0.0ßCompared to HRED, our model introduces discrete variables to model uncertainty of dialog.ßdummyTurker
9885ß0.0ßOn the other hand, our modelßdummyTurker
9886ß0.0ßhas more physical explanations than VHRED, since we can treat latent variable as different intentions of a dialog.ßdummyTurker
9887ß0.0ßIn contrast, Gaussian latent variables in VHRED don't have such explanation.ßdummyTurker
9888ß0.0ßMoreover, our model can also be trained under semi-supervised and supervised framework when we have some labeled data, while HRED and VHRED can just be trained on unsupervised framework.ßdummyTurker
9889ß0.0ßIn addition, HRED and VHRED are hard to extend to RL training.ßdummyTurker
9890ß0.0ßNDM is the supervised trained model.ßdummyTurker
9891ß0.0ßIt selects a label for each utterance at every dialog turn.ßdummyTurker
9892ß0.0ßIn comparison, our model with unsupervised learning summarizes all intentions by equation 7.ßdummyTurker
9893ß0.0ßIn addition, NDM has a complex pipeline which could propagate errors to down-stream.ßdummyTurker
9894ß0.0ßAt last, supervised proposed model maximize instantaneous reward at each step in a greedy manner.ßdummyTurker
9895ß0.0ßHowever, RL tries to maximize the expected overall reward at the end of the dialog.ßdummyTurker
9896ß0.0ßThanks.ßdummyTurker
9897ß0.0ßWe will try to add comparison experiments with seqGAN and minimum risk training in the revised version.ßdummyTurker
9898ß0.0ßThe confidence interval is 99% and we will report this in the revised paper.ßdummyTurker
9899ß0.0ßThe figure is right.ßdummyTurker
9900ß0.0ßWe will modify our quote to be consistent with the figure.ßdummyTurker
9901ß0.0ßThanks.ßdummyTurker
9902ß0.0ßthis is indeed a problem.ßdummyTurker
9903ß0.0ßWe are doing some research to give a smarter method.ßdummyTurker
9904ß0.0ßWe will try to report more experiment results on the EN-DE data set.ßdummyTurker
9905ß0.0ßThanks, and we will give more details of the methods used in the related work.ßdummyTurker
9906ß0.0ßThanks, and we will refine the organization of our paper according to the comments.ßdummyTurker
9907ß0.0ßThanks.ßdummyTurker
9908ß0.0ßWe will correct the spelling errors and typos according to the comments.ßdummyTurker
9909ß0.0ßThanks.ßdummyTurker
9910ß0.0ßWe will give more curves and results on the training data according to the questions (1)-(4).ßdummyTurker
9911ß0.0ßYou pointed out an important issue that we have ignored that we should give a discussion about the practicability of our method.ßdummyTurker
9912ß0.0ßIn the practical applications, if the training set is very large (more than several 10M sentence pairs), we suggest using distributed training with pertaining, or just using word-level oracle which can also outperform the baseline greatly.ßdummyTurker
9913ß0.0ßIn other cases, it is recommended to use sentence-level oracle.ßdummyTurker
9914ß0.0ßThanks.ßdummyTurker
9915ß0.0ßWe will revise our paper carefully.ßdummyTurker
9916ß0.0ßWe will definitely add the paper (Gu et al.ßdummyTurker
9917ß0.0ß2016) to the related work in the revised version.ßdummyTurker
9918ß0.0ßWe compared the loss on the training data and found in the same epoch, the regular RNNsearch had a lower loss, and this may be taken as an evidence that the RNNsearch overfits to the training data.ßdummyTurker
9919ß0.0ßWe will add the curves in the revised version of the paper.ßdummyTurker
9920ß0.0ßWe sampled some sentences from the training data and tested for FT-NMT and RNNsearch*, and found FT-NMT got a lower BLEU score  (26.44 for FT-NMT and 29.04 for RNNsearch*).ßdummyTurker
9921ß0.0ßThis may infer that FT-NMT is not overcorrected as RNNsearch* .ßdummyTurker
9922ß0.0ßAccording our training log, sampling with decay indeed increased the time to converge, about 5 epochs slower to converge than without sampling.ßdummyTurker
9923ß0.0ßThe reproducibility of our work is not highly dependent on decay rates as long as it is not too high or too low, as we have tried using 8, 10, 12 as the hyper-parameter \mu in Equation(16) and the BLEU scores only changed slightly.ßdummyTurker
9924ß0.0ßBut if the decay rate is too low (near zero), it is difficult for our method to converge to a reasonable model, or too high (near 1), our method will degenerate to the baseline method.ßdummyTurker
9925ß0.0ßUsually the RNNsearch tends to deviate the correct translation as the translation goes long, but our method sometimes can in contrast recover from wrong translation phrases and get back to the correct translation.ßdummyTurker
9926ß0.0ßThese are some examples.ßdummyTurker
9927ß0.0ßReference:       North Korean Delegation: North Korea has no Intention of Making Nuclear WeaponsßdummyTurker
9928ß0.0ßRNNsearch*:      north korea has no intention of producing nuclear weaponsßdummyTurker
9929ß0.0ßFT-NMT:          north korea has no intention to manufacture nuclear weapons: north korean delegationßdummyTurker
9930ß0.0ßReference:       Koizumi said that he would pay special attention this year to the parliament's discussion on the post service sector's privatization.ßdummyTurker
9931ß0.0ßRNNsearch*:      koizumi said that this year he will focus the discussion on privatization of the postal service postal service.ßdummyTurker
9932ß0.0ßFT-NMT:          koizumi said that he would pay attention to the discussion of the congress on postal service privatization this year.ßdummyTurker
9933ß0.0ßWe have not tested the specific time taken to generate the sentence-level oracle, but it indeed leads to a slower convergence.ßdummyTurker
9934ß0.0ßIf one wants to use sentence-level oracle rather than word-level oracle, it needs to do beam search on the training set (we set beam size as 3 in our experiments.)ßdummyTurker
9935ß0.0ßAnother option is to use word-level oracle, the training time of which is similar to the baseline.ßdummyTurker
9936ß0.0ßIf using sentence-level oracle, the training process mainly consists of two phrases: decoding and back propagation.ßdummyTurker
9937ß0.0ßIn the decoding period, using beam search of K will slow the speed by near but less than K times, but in the back propagation period, the time will not change obviously.ßdummyTurker
9938ß0.0ßTherefore, the training time will increase, but far less than K times.ßdummyTurker
9939ß0.0ßWe used the standford Chinese word segmenter (https://nlp.stanford.edu/software/segmenter.html).ßdummyTurker
9940ß0.0ßYes, we will open the source code.ßdummyTurker
9941ß0.0ßThis paper describes two strategies to select the oracle.ßdummyTurker
9942ß0.0ßIndeed sentence-level oracle will take more time to convergence than the baseline, but if using word-level oracle, the time to converge will not change too much.ßdummyTurker
9943ß0.0ß1.ßdummyTurker
9944ß0.0ßWe follow the method and the name of RNNsearch* as in the following paper:ßdummyTurker
9945ß0.0ßMingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu.ßdummyTurker
9946ß0.0ß2016.ßdummyTurker
9947ß0.0ßMemory-enhanced decoder for neural machine translation.ßdummyTurker
9948ß0.0ßIn Proceedings of EMNLP2016.ßdummyTurker
9949ß0.0ß2.ßdummyTurker
9950ß0.0ßFor BPE, we tried three operation numbers (8k, 16k and 32k), and found 16k is the best size for the task (the results are given in Table 2), but we should change a word rather than “according to our knowledge” which is not correct strictly.ßdummyTurker
9951ß0.0ßReply to weakness argument 1(similarity and disparity component experiment):ßdummyTurker
9952ß0.0ßThe similarity and disparity component experiment is essentially an ablation study to determine the contribution made by each component.ßdummyTurker
9953ß0.0ßHowever, in the qualitative study, we have demonstrated how they complement each other in performance.ßdummyTurker
9954ß0.0ßThe similarity component can help to find words that are similar with the question subject, which corresponds to the main ideas expressed in the question subject.ßdummyTurker
9955ß0.0ßThe disparity component can help to find words that are semantically distant with the question subject, which corresponds to the additional information provided in the question body.ßdummyTurker
9956ß0.0ßThus they are indispensable for the question representation.ßdummyTurker
9957ß0.0ßFurthermore, we have implemented three different baselines (line 555, 653 and 654) to exhaustively study the property of the question condensing layer.ßdummyTurker
9958ß0.0ßWe did not include this baseline due to space limitations.ßdummyTurker
9959ß0.0ßNevertheless, we will add this ablation study in the revised version if the space permits.ßdummyTurker
9960ß0.0ßReply to weakness argument 2(component-wise analysis for Question-Answer Alignment):ßdummyTurker
9961ß0.0ßActually, in the question-answer alignment section, we did not use orthogonal decomposition strategy as is used in the question condensing section.ßdummyTurker
9962ß0.0ßInstead, we used multi-dimensional attention mechanism.ßdummyTurker
9963ß0.0ß(see line 330) If you mean analyzing question and answer component separately, we do not think predicting answer quality is possible in the absence of either the question or the answer.ßdummyTurker
9964ß0.0ßTo the best of our knowledge, this is the first work that applies orthogonal decomposition in CQA tasks.ßdummyTurker
9965ß0.0ßOrthogonal decomposition is especially suitable for this task because it can locate similar and additional information in the question body with respect to the question subject.ßdummyTurker
9966ß0.0ßAs a result, the question can be condensed with key information preserved.ßdummyTurker
9967ß0.0ßIn addition, we also proposed question-answer interaction with the multi-dimensional attention mechanism.ßdummyTurker
9968ß0.0ßIt has the advantage of highlighting specific features for a word in the given context, thus is suitable for question-answer matching where context information plays the key role.ßdummyTurker
9969ß0.0ßWe think these contributions are creative as well as conducive for CQA.ßdummyTurker
9970ß0.0ßAs for the qualitative study, we will make it clearer in the revised version.ßdummyTurker
9971ß0.0ßAs is stated in the ablation study section (line 670) and the qualitative study section (line 694), the disparity component contributes to collecting additional information in the question body which is absent in the question subject.ßdummyTurker
9972ß0.0ßIn community questions, the question body usually explains the question subject and adds more related information.ßdummyTurker
9973ß0.0ß"In the example listed in qualitative study, ""arrange packages"" in the question body has little similarity with the question subject, but is indispensable in predicting the answer quality (it is the purpose of the travel)."ßdummyTurker
9974ß0.0ßBy utilizing the disparity part in orthogonal decomposition, we can access the exclusive information in the question body.ßdummyTurker
9975ß0.0ßAs is shown in the heat map in Figure 2, the disparity part of our orthogonal decomposition can effectively focus on this kind of information.ßdummyTurker
9976ß0.0ßTherefore, it can contribute to the performance improvement.ßdummyTurker
9977ß0.0ßThank you for the concern, but we think the proposed model can be easily adapted to a variety of NLP tasks, where a subject summarizes the main points and a body elaborates the subject in details.ßdummyTurker
9978ß0.0ßFor example, in text summarization, the title (or the first paragraph) usually gives a brief summary of the article, and the other parts of the article describe the subject and provide additional details.ßdummyTurker
9979ß0.0ßThanks for the advice.ßdummyTurker
9980ß0.0ß"""Subject only"" and ""body only"" baselines can be viewed as ablation studies to determine the importance of the question subject and the question body."ßdummyTurker
9981ß0.0ßTo study the effectiveness of our question condensing layer, we implemented three different baselines: (a) LSTM-subject-body (Table 4(5)) shows that modeling interaction between subject and body is more effective than processing question and answer separately and then concatenating them together.ßdummyTurker
9982ß0.0ß(b) subject-body alignment (Table 5(3)) shows that orthogonal decomposition is more suitable than multi-dimensional alignment in question condensing.ßdummyTurker
9983ß0.0ß(c) subject-body concatenation (Table 5(4)) shows that treating question subject and body separately is more suitable than simply concatenating them as the question representation.ßdummyTurker
9984ß0.0ßWe did not add the similarity and disparity component experiment due to space limitations, but we will add the baselines in the revised version.ßdummyTurker
9985ß0.0ßAs stated in line 391, considering that the SemEval 2017 task shares the same training set and evaluation metrics with SemEval 2016 (with an updated and improved test set), we did not include SemEval 2016 in our paper due to potential redundancy.ßdummyTurker
9986ß0.0ßWe have experimented on SemEval 2016 and achieved impressive results.ßdummyTurker
9987ß0.0ßWe will add it in the revised version if the space permits.ßdummyTurker
9988ß0.0ßIn Kim's 2014 paper, the author proposed a CNN architecture to combine word embeddings to form sentence representation.ßdummyTurker
9989ß0.0ßWe utilized the same architecture on top of character embeddings to form word representation.ßdummyTurker
9990ß0.0ßThe models are the same while input granularities differ.ßdummyTurker
9991ß0.0ß"We will add more descriptions and additionally cite Kim's 2015 paper ""Character-Aware Neural Language Models"" to facilitate reading."ßdummyTurker
9992ß0.0ßReply to additional comment 2(move the information earlier):ßdummyTurker
9993ß0.0ßGood point!ßdummyTurker
9994ß0.0ßMoving that information earlier can indeed facilitate reading.ßdummyTurker
9995ß0.0ßWe will improve that in the revised version.ßdummyTurker
9996ß0.0ßWe used early stopping in experiments.ßdummyTurker
9997ß0.0ßThe number of training epochs depends on performance on the development set.ßdummyTurker
9998ß0.0ßThank you for the advice.ßdummyTurker
9999ß0.0ßWe intended to visualize the attention probabilities tangibly in Figure 2 to demonstrate (a) orthogonal decomposition is indeed helpful for question condensing and (b) how the question-answer alignment layer can contribute to question answering.ßdummyTurker
10000ß0.0ßThe example is used to illustrate that both the similarity and the disparity component are important for question representation.ßdummyTurker
10001ß0.0ßWe will revise section 6 and add more convincing examples that can convey the full benefit of our model.ßdummyTurker
10002ß0.0ßThanks for pointing out the typos.ßdummyTurker
10003ß0.0ßWe will carefully check the writing in the revision.ßdummyTurker
10004ß0.0ßI am so sorry that my first attempt to write in English did not clearly express my thoughts.ßdummyTurker
10005ß0.0ßBut I keep learning English all the time in order to understand the theses and express my ideas better.ßdummyTurker
10006ß0.0ßDue to the differences between Chinese and English, English datasets cannot be used when calculating the semantic similarity between Chinese words, such as WordSimilarity-353, SimLex-999, MEN and Rare-Word.ßdummyTurker
10007ß0.0ßBesides, we use the Chinese Wikipedia rather than English Wikipedia.ßdummyTurker
10008ß0.0ßI am so sorry that my first attempt to write in English did not clearly express my thoughts.ßdummyTurker
10009ß0.0ßBut I keep learning English all the time in order to understand the theses and express my ideas better.ßdummyTurker
10010ß0.0ßThis is part of a larger task.ßdummyTurker
10011ß0.0ßDue to space constraints, there is not too much introduce on the rest of the work.ßdummyTurker
10012ß0.0ßWhen the LDA event method is used for clustering, if the subject words do not intersect, it is difficult to classify the events correctly.ßdummyTurker
10013ß0.0ßAfter adding the semantic similarity of the words, a better judgment can be made on such texts and the distance between the texts can be calculated.ßdummyTurker
10014ß0.0ßBesides, we are trying to apply this method to English corpus in new tasks.ßdummyTurker
10015ß0.0ßPerhaps ‘beneficial’ can express my ideas more accurately.ßdummyTurker
10016ß0.0ßResponse to Weaknesses:  Thanks for the constructive review.ßdummyTurker
10017ß0.0ßAs common practice, we considered the conversation history by concatenating the current input utterance with previous utterances.ßdummyTurker
10018ß0.0ßAs to the second point, in our corpus, over 97% of the entities in the replies are limited to OSF entities.ßdummyTurker
10019ß0.0ßThis is very reasonable because it is important to be coherent in the discussion.ßdummyTurker
10020ß0.0ßNevertheless, referring to “far” entities does exist in our corpus.ßdummyTurker
10021ß0.0ßFor example, mentioning another concurrently running movie.ßdummyTurker
10022ß0.0ßThis rare situation can be handled by adding a “concurrent” relation using the release date information.ßdummyTurker
10023ß0.0ßWe will add this interesting analysis in the revised version.ßdummyTurker
10024ß0.0ßResponse to Weaknesses: Our work focuses on movie chats, where user has no intent to finish a specific task, nor does the user query for some answers (Figure 1).ßdummyTurker
10025ß0.0ßDifferent from task-oriented dialogues and QA systems, in our scenario, a user may begin a discussion from certain aspects, which are discoverable on the surface and are detected by simple matching techniques (line 463-468).ßdummyTurker
10026ß0.0ßIn contrast, the user intents in task-oriented dialogues are latent and required to be inferred by complicated methods.ßdummyTurker
10027ß0.0ßAlso, the user input utterance in our work is not a question-like query.ßdummyTurker
10028ß0.0ßHence, existing work on question answering is not comparable for our task because the response required in our task is not an existing fact that can be retrieved from the KB.ßdummyTurker
10029ß0.0ßOur work lies in the line of using data-driven approaches to develop chatbots in the field of natural language processing (Ritter et al., 2011; Ji et al., 2014; Serban et al., 2015; Serban et al., 2017; Zhao et al., 2017).ßdummyTurker
10030ß0.0ßSince our task evaluates the discussion skills of the fully-data-driven chatbots, it differs from most work in HCI area.ßdummyTurker
10031ß0.0ßFor example, either (Tarau and Figa, 2004) or (Wanner et al., 2017) relies on extensive rules or extra dialogue act ontology.ßdummyTurker
10032ß0.0ßMoreover, they two are not focusing on discussion as what we propose.ßdummyTurker
10033ß0.0ß(Tarau and Figa, 2004) focuses on storytelling, an interactive way of story narration.ßdummyTurker
10034ß0.0ß(Wanner et al., 2017) develops an agent to have free-chat with users and (expectedly) to answer health-related questions (the method for this part is not presented in the paper).ßdummyTurker
10035ß0.0ßIn addition, both of them are multimodal systems whereas our task is purely text-based.ßdummyTurker
10036ß0.0ßWe really appreciate your suggestion on the comparison and will clarify our task in the revision.ßdummyTurker
10037ß0.0ßAs to Weakness 3, please kindly refer to line 176-191, line 464-468, and the response to Question 1 for Reviewer#1.ßdummyTurker
10038ß0.0ßWe will also make this point clearer in the revision.ßdummyTurker
10039ß0.0ßIn sum, our work is novel in that (1) we propose a systematic approach for developing the discussion skill, a specific skill required for the social chatbots, (2) which is totally distinguished from task-oriented dialogues and QA systems.ßdummyTurker
10040ß0.0ß(3) On the newly developed dataset we empirically validate the effectiveness of our approach by automatic and human evaluation as well as case and ablation studies.ßdummyTurker
10041ß0.0ß[1] Ritter et al., 2011.ßdummyTurker
10042ß0.0ß“Data-driven response generation in social media”.ßdummyTurker
10043ß0.0ßIn EMNLP.ßdummyTurker
10044ß0.0ß[2] Ji et al., 2014.ßdummyTurker
10045ß0.0ß“An information retrieval approach to short text conversation”.ßdummyTurker
10046ß0.0ß[3] Serban et al., 2015.ßdummyTurker
10047ß0.0ß“A Survey of Available Corpora for Building Data-Driven Dialogue Systems”.ßdummyTurker
10048ß0.0ß[4] Serban et al., 2017.ßdummyTurker
10049ß0.0ß“A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues”.ßdummyTurker
10050ß0.0ßIn AAAI.ßdummyTurker
10051ß0.0ß[5] Zhao et al., 2017.ßdummyTurker
10052ß0.0ß“Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders”.ßdummyTurker
10053ß0.0ßIn ACL.ßdummyTurker
10054ß0.0ßWe propose to generate discussion replies by firstly extracting contextual background knowledge and then utilizing them in the encoder-decoder architecture.ßdummyTurker
10055ß0.0ßThis approach can be applied to various domains, though the knowledge base we leverage in this work is domain-specific.ßdummyTurker
10056ß0.0ßBased on the investigation on the movie discussion data, we propose to treat different types of knowledge (relations and entities) differently.ßdummyTurker
10057ß0.0ßThe comparison between the row 4 and row 6 in Table 2 partially validates the necessity of our distinguished strategy.ßdummyTurker
10058ß0.0ßWe believe the differences among open-domain data require certain modifications on our current approach, and we are going to explore more along this line in the future.ßdummyTurker
10059ß0.0ßThere has been considerable work developing advanced approaches for standard relation detection and entity linking.ßdummyTurker
10060ß0.0ßMost of them can be seamlessly applied to our scenario.ßdummyTurker
10061ß0.0ßNevertheless, in line 673, we have examined that our simple detection strategy performed comparably with a rather advanced method.ßdummyTurker
10062ß0.0ßIt is because entity linking in our scenario is within a subset of the entire KB, a simpler scenario than that in the standard task.ßdummyTurker
10063ß0.0ßIt could further boost our performance if more advanced approaches are adopted.ßdummyTurker
10064ß0.0ßThe artists’ movies are ranked based on the movie popularity, i.e., the number of ratings on Douban.com.ßdummyTurker
10065ß0.0ßAnd we consider at most four related movies.ßdummyTurker
10066ß0.0ßWe have tried to larger candidate size, but did not see improvement.ßdummyTurker
10067ß0.0ßIn Table 2, our method outperformed GenDS (Zhu et al., 2017) which kept all of the related entities, indicating that there is not obvious advantage to keep them all.ßdummyTurker
10068ß0.0ßFurthermore, the attention mechanism is not always reliable when the semantics is complex.ßdummyTurker
10069ß0.0ßHence, it is unsafe to find the most beneficial knowledge units solely based on the attention mechanism.ßdummyTurker
10070ß0.0ßThe vector a_{tj} is used to weight the candidate entities in the embedding space, and will influence the attention weights \beta_{tj} in the decoding step because \beta approximates the relevance of the weighted candidate and the context.ßdummyTurker
10071ß0.0ßa_{tj} initially has a large value when it corresponds to those entities occurred in the input utterance because these entities are probable to be discussed in the following turns.ßdummyTurker
10072ß0.0ßAnd the value decreases to almost 0 when the corresponding entity has been generated.ßdummyTurker
10073ß0.0ßWe will include the detailed formulas in the revised paper.ßdummyTurker
10074ß0.0ßResponse to Comments: We did count words rather than characters.ßdummyTurker
10075ß0.0ßDue to limit space, the examples are truncated without changing their meanings.ßdummyTurker
10076ß0.0ßWe thank for your valuable advice and will improve the paper presentation more carefully.ßdummyTurker
10077ß0.0ßWe agree with you that we should describe more examples (with discussion) for readers to grasp our research clearly.ßdummyTurker
10078ß0.0ßWe cut down various information to fit on 4 pages.ßdummyTurker
10079ß0.0ßWe would like to include such information in the revised version.ßdummyTurker
10080ß0.0ß"Our core hypothesis (i.e., ""coherence modeling can be effective for learning discourse relations"") might be straight-forward, but we consider that, in science, it is important that we formulate it and quantitatively verify it."ßdummyTurker
10081ß0.0ßHowever, your comment is to the point.ßdummyTurker
10082ß0.0ßWe are going to continue the further investigation to find more interesting properties of the coherence-based approach and would like to introduce the findings in future.ßdummyTurker
10083ß0.0ßUnfortunately, we do not have detailed prediction results of the previous work on the test set.ßdummyTurker
10084ß0.0ßTherefore, we could not performed significance test.ßdummyTurker
10085ß0.0ßHowever, in this field, the difference of 2% in Macro F1 is not a trivial difference.ßdummyTurker
10086ß0.0ßAlso, it could be possible to improve performances by combining our coherence-based approach with previous connective-based methods, because coherence modeling might learn complementary features to the previous work.ßdummyTurker
10087ß0.0ßWe agree with you that we should add more useful examples (e.g., topic/order-oriented negative samples) for readers to understand more clearly.ßdummyTurker
10088ß0.0ßWe cut down various information to fit on 4 pages.ßdummyTurker
10089ß0.0ßWe would like to include such information in the revised version.ßdummyTurker
10090ß0.0ßAs shown in Table 3, coherence modeling is the most effective in the Temporal relation.ßdummyTurker
10091ß0.0ßUnfortunately, the theoretical analysis has not been done yet.ßdummyTurker
10092ß0.0ß"However, we consider that coherence modeling with topic-oriented negative sampling may learn words and phrases that are likely to appear in Temporal relations (such as ""morning"" and ""evening"")."ßdummyTurker
10093ß0.0ßHowever, this is only a hypothesis and further investigation is necessary.ßdummyTurker
10094ß0.0ßThank you for your insightful comments.ßdummyTurker
10095ß0.0ßThe most different point of our work from previous connective-dependent methods is that, whereas those methods can only take information from the surroundings of detected connectives in an unlabeled corpus, our approach can make full use of the unlabeled corpus.ßdummyTurker
10096ß0.0ßRelying on explicit connectives makes it impossible to utilize sentences that hold natural implicit discourse relations in the unlabeled corpus.ßdummyTurker
10097ß0.0ßOne of our goals is to solve this issue by introducing coherence modeling.ßdummyTurker
10098ß0.0ßUnfortunately, it still remains unclear whether the linguistic knowledge acquired through coherence modeling is complementary to those by the connective-based methods.ßdummyTurker
10099ß0.0ßWe will examine this line of research in future.ßdummyTurker
10100ß0.0ßWe wish to express appreciation for your insightful comments.ßdummyTurker
10101ß0.0ßTable 2 suggests that coherence modeling yields higher Macro F1 scores than the previous methods (i.e., standard methods).ßdummyTurker
10102ß0.0ß"Macro F1 reflects the recognition ability of learning models better than accuracy since the class balance in the PDTB's test set is highly skewed (as can be seen from ""# of training data"" in Table 2)."ßdummyTurker
10103ß0.0ßWe agree with you that we should add more information and details (including hyperparameters) for readers.ßdummyTurker
10104ß0.0ßWe cut down various information to fit on 4 pages.ßdummyTurker
10105ß0.0ßWe would like to include such information in the revised version.ßdummyTurker
10106ß0.0ß"The baseline model (i.e., ""IRel only"") we adopted in this study is a typical architecture in implicit discourse relation recognition (Chen et al., ACL'16; Rutherford et al., EACL'17)."ßdummyTurker
10107ß0.0ßThis paper demonstrates that coherence modeling is effective for the baseline (i.e., typical) model.ßdummyTurker
10108ß0.0ßTherefore, we consider that coherence modeling has a potential to be extended to existing methods.ßdummyTurker
10109ß0.0ßThough it is possible to verify our idea on more complex architectures, this is out of the scope of this study since the effectiveness of incorporating coherence-based linguistic knowledge would be broadly orthogonal to the model complexity.ßdummyTurker
10110ß0.0ßWe adopted a simple neural architecture in this field as the baseline model, and demonstrate that coherence modeling is effective for improving the performance of the baseline model on implicit discourse relation recognition.ßdummyTurker
10111ß0.0ßUnfortunately, it is almost impossible to verify whether an idea (including our approach) works effectively on all existing architectures.ßdummyTurker
10112ß0.0ßWe have not tried to incorporate connective-based features into our approach.ßdummyTurker
10113ß0.0ßHowever, it is interesting to see.ßdummyTurker
10114ß0.0ßWe appreciate your fruitful comments.ßdummyTurker
10115ß0.0ßThe baseline model is similar to the proposed model by Rutherford et al.ßdummyTurker
10116ß0.0ß(EACL'17), though several details are different from them.ßdummyTurker
10117ß0.0ß"Therefore, the row corresponding to ""IRel only"" in Table 3 somewhat reflects the result of standard methods."ßdummyTurker
10118ß0.0ß-----Reply to weakness 1-----ßdummyTurker
10119ß0.0ßOur work differs from their work (Mi et al.ßdummyTurker
10120ß0.0ß2016) in two ways: (1) use of hard attention (2) joint objective during both training and inference.ßdummyTurker
10121ß0.0ßMoreover, Mi et al.ßdummyTurker
10122ß0.0ß(2016) use a different loss function for alignments, and a heuristic to smooth the ground truth alignment distribution.ßdummyTurker
10123ß0.0ßOur implementation of their best method scores 22.2 in BLEU.ßdummyTurker
10124ß0.0ßThis is equal to joint, but still behind joint-aug (23.0) and joint-aug-BSO (23.7).ßdummyTurker
10125ß0.0ß-----Reply to weakness 2-----ßdummyTurker
10126ß0.0ßOur experiments demonstrate that the effectiveness of our approach is more pronounced when we have less data.ßdummyTurker
10127ß0.0ßHowever, at about one-third of the full corpus (50K), the baseline and our method deliver similar performance.ßdummyTurker
10128ß0.0ßWe hypothesize that there’s no need to use hard attention if the model can produce peaked attention weights with a sufficient amount of data.ßdummyTurker
10129ß0.0ß-----Reply to question 1-----ßdummyTurker
10130ß0.0ßWe didn’t use bidirectional alignments in this work, but we believe it’s beneficial to do so.ßdummyTurker
10131ß0.0ßThe most direct way to achieve this is to follow Cheng et al.ßdummyTurker
10132ß0.0ß(2016) to train two models (one for each direction), and encourage agreement of their alignment distributions.ßdummyTurker
10133ß0.0ß-----Reply to weakness 1-----ßdummyTurker
10134ß0.0ß--hard attentionßdummyTurker
10135ß0.0ßWe only propose to use hard attention in low-resource scenarios to combat the diffuseness of attention weights.ßdummyTurker
10136ß0.0ßAt the same time, we acknowledge the advantage of the soft attention (section 3.2, paragraph “justifications” and “augmented attention”) in terms of modeling power.ßdummyTurker
10137ß0.0ßWe demonstrate that it’s beneficial to combine both.ßdummyTurker
10138ß0.0ß--comparison with past work on supervised attentionßdummyTurker
10139ß0.0ßOur work differs from those papers in two ways: (1) use of hard attention (2) joint objective during both training and inference.ßdummyTurker
10140ß0.0ßAt the same time, those papers explored different dimensions of using supervised attention.ßdummyTurker
10141ß0.0ßSpecifically, Mi et al.ßdummyTurker
10142ß0.0ß(2016) investigated the effect of different training schemes and word aligners, and Liu et al.ßdummyTurker
10143ß0.0ß(2016) of different loss functions.ßdummyTurker
10144ß0.0ßChen et al.ßdummyTurker
10145ß0.0ß(2016) use guided alignments to better handle unknown words.ßdummyTurker
10146ß0.0ßOur baseline s2s-al is essentially the same as Chen el.ßdummyTurker
10147ß0.0ßal (2016) and Liu et al.ßdummyTurker
10148ß0.0ß(2016) with cross-entropy loss.ßdummyTurker
10149ß0.0ß--our implementation of s2s-alßdummyTurker
10150ß0.0ßIn response to the concern with our implementation of s2s-al, we ran additional experiments to validate its correctness in two aspects:ßdummyTurker
10151ß0.0ß(1) We reproduced results (Chen et al.ßdummyTurker
10152ß0.0ß2016) for IWSLT.ßdummyTurker
10153ß0.0ßWe report 23.4/23.9 for s2s/s2s-al, matching their results 23.4/23.8.ßdummyTurker
10154ß0.0ß(2) Since the low-resource dataset used by Liu et al.ßdummyTurker
10155ß0.0ß(2016) is licensed, we cannot directly replicate their results.ßdummyTurker
10156ß0.0ßHowever, we ran experiments on parallel Bible corpus, which contains 30K sentences in total, the same magnitude as the their dataset.ßdummyTurker
10157ß0.0ßWith λ=0.3 (Liu et al.ßdummyTurker
10158ß0.0ßused hyperparameter λ to control the weight for alignment loss), we report 30.7/32.3 for German-to-English, and 24.5/25.4 for English-to-German.ßdummyTurker
10159ß0.0ßNote that the gains are bigger than IWSLT, but this discrepancy in gains is not unique to our experiments.ßdummyTurker
10160ß0.0ßIn fact, Chen et al.ßdummyTurker
10161ß0.0ß(2016) reported an increase of only 0.4 in BLEU for IWSLT, compared with 2.3 on an e-commerce dataset.ßdummyTurker
10162ß0.0ß-----Reply to weakness 2-----ßdummyTurker
10163ß0.0ß--the strategy of our paperßdummyTurker
10164ß0.0ßThere are multiple approaches that can benefit MT systems in low-resource scenarios.ßdummyTurker
10165ß0.0ßWhile none of them fully solves the problem, they improve translation quality along different dimensions.ßdummyTurker
10166ß0.0ßTransfer learning methods are orthogonal to the direction we take in the paper, and could be combined with our work.ßdummyTurker
10167ß0.0ßOf course, it remains to be seen what techniques are effective for such a combination.ßdummyTurker
10168ß0.0ß--rich-resource settingßdummyTurker
10169ß0.0ßWhile supervised attention is not limited to low-resource scenarios, the issue of diffuse attention weights, which motivates the adoption of hard attention, is intrinsically a low-resource issue -- there’s no need to use hard attention if the model can produce peaked attention weights with sufficient data.ßdummyTurker
10170ß0.0ßIndeed, our earlier experiments demonstrate the effectiveness of our approach is more pronounced when we have less data.ßdummyTurker
10171ß0.0ßHowever, at about one-third of the full corpus, the baseline and our method deliver similar performance.ßdummyTurker
10172ß0.0ß-----Reply to question 1-----ßdummyTurker
10173ß0.0ßWe observed only a small difference (+0.2) between joint-aug-BSO and joint-aug. We believe it’s a similar issue to what we report about BSO in the notes under Table-3.ßdummyTurker
10174ß0.0ßThank you for your suggestion, we will turn to native speakers of English for help and polish our paper.ßdummyTurker
10175ß0.0ß"Thank you for pointing this out, it is a slip of the pen, we want to say “path1 (from the major-encoder to the aux-decoder)"" in Sec 3.1."ßdummyTurker
10176ß0.0ßWe do so because we view a response that contains frequent bigrams as the most reasonable one.ßdummyTurker
10177ß0.0ßWe design this rule aiming to get exactly one response for one post to minimize noise, there may be other rules that can also select reasonable response.ßdummyTurker
10178ß0.0ßThank you for pointing this out.ßdummyTurker
10179ß0.0ßWe will correct this.ßdummyTurker
10180ß0.0ßThank you for pointing this out.ßdummyTurker
10181ß0.0ßWe have considered exhibiting these five examples in paper, but due to the length limitation, we finally omit them.ßdummyTurker
10182ß0.0ßThe five posts are “What happen tonight?ßdummyTurker
10183ß0.0ßI just can't sleep.”, “Friends in Shanghai and Beijing, how are you?”, “Don't say I haven't taught you anything.”, “I don't believe that I'm the only one!”, “The men's one hundred-meter semifinals will begin immediately.”ßdummyTurker
10184ß0.0ßWe find BLEU scores (BLEU-1 is adopted here) are somewhat incompatible to human annotation.ßdummyTurker
10185ß0.0ßAs prior work (How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation.ßdummyTurker
10186ß0.0ßEMNLP 2016: 2122-2132) have mentioned, BLEU sometimes have minor correlation with human evaluations of response quality.ßdummyTurker
10187ß0.0ßSo, we put more priority on human annotation, and use BLEU scores as reference.ßdummyTurker
10188ß0.0ßThanks for your suggestion.ßdummyTurker
10189ß0.0ßWe consider using more examples or exhibiting latent vector’s distribution in a more convincing manner in later revision.ßdummyTurker
10190ß0.0ßThanks for your suggestion, we consider carrying out more evaluation in future work.ßdummyTurker
10191ß0.0ßThank you for pointing out this weakness, we read the paper you mentioned, and do find the similar explanation of latent vector.ßdummyTurker
10192ß0.0ßIt is our cursoriness.ßdummyTurker
10193ß0.0ßWe draw such conclusion because recently proposed models tend to introduce intermediate layer between encoder and decoder, or borrow the idea of variational auto-encoders (VAE) to introduce stochasticity into generation by first sampling an assignment of the latent variables.ßdummyTurker
10194ß0.0ßThese methods do require more complex structure than classic RNN to model and restrict the middle step.ßdummyTurker
10195ß0.0ßThank you for your constructive suggestion.ßdummyTurker
10196ß0.0ßSince our proposed model needn’t resort to extra data for training, and can be view as adding restriction to conventional object functions, we choose MMI as our baseline for fare comparison.ßdummyTurker
10197ß0.0ßCVAE and MEnc-Dec are seemingly similar, however, they are different in intuition.ßdummyTurker
10198ß0.0ßIn MEnc-Dec the aux-encoder aiming to enhance decoder’s capacity for decoding while in CVEA the reconstruction network can be view to guide the prior network to better capture the distribution of the latent vector.ßdummyTurker
10199ß0.0ßBut they are structurally similar, we consider carrying out experiment to compare their performance in future work.ßdummyTurker
10200ß0.0ßSince we tackle the problem of responses diversity by requiring the latent vectors to be specific to the inputs, we don’t introduce stochasticity into generation by sampling an assignment of the latent variables but introduce an aux-decoder to restrict the latent vector.ßdummyTurker
10201ß0.0ßIn future work we will carry out experiment to compare their performance.ßdummyTurker
10202ß0.0ßWe noticed MEDER’s sensitivity to the initialization, but didn’t find efficient methods to deal with the problem so far.ßdummyTurker
10203ß0.0ßWe will continue to explore the problem in the future work.ßdummyTurker
10204ß0.0ßTrue, a short comparison and a few references to SOA NER work can be included in the camera-ready version.ßdummyTurker
10205ß0.0ßAs suggested in the additional comments, rephrasing some wording will hopefully allow for a few more details mentioned by the reviewers in the camera-ready version.ßdummyTurker
10206ß0.0ßOnly Table 5 includes type aggregation.ßdummyTurker
10207ß0.0ßA second line could be added to Figure 1 to include the aggression results on individual datasets.ßdummyTurker
10208ß0.0ßUnfortunately, type-level aggregation results on individual datasets were omitted due to space reasons, but as mentioned above, could be added to one of the tables of to Figure 1.ßdummyTurker
10209ß0.0ßThe motivation behind this is of psycholinguistic nature:  With each additional feature block more of the information about the eye movements is covered in the same order as this information would be uncovered during the reading process.ßdummyTurker
10210ß0.0ßWhile we agree with the weakness argument, before experimenting with the suggested scenarios, there is a need for the foundational research to show if, how and which eye-tracking features are beneficial for NER.ßdummyTurker
10211ß0.0ßThereafter, more realistic application scenarios can be tackled.ßdummyTurker
10212ß0.0ßFor the type aggregation, the features for all occurrences of a given word type are averaged.ßdummyTurker
10213ß0.0ßFor instance, if the type “dog” appears 10 times in the corpus, the distinct feature values are average over these 10 occurrences.ßdummyTurker
10214ß0.0ßThe asterisk in Table 3 marks significant improvements over the baselineßdummyTurker
10215ß0.0ß(p<0.05, one-sided t-test), analogous to Table 5 - which we mistakenly forgot in the caption.ßdummyTurker
10216ß0.0ßWe realize that a short description of the SOA NER work is missing and hope to include it in the camera-ready version.ßdummyTurker
10217ß0.0ß“Sufficient data” refers to the fact that these eye-tracking corpora are few and limited in size.ßdummyTurker
10218ß0.0ßThus, we decided to work with two datasets to report less biased results and to experiment with cross-corpus approaches towards domain-adaptationßdummyTurker
10219ß0.0ßWhile the augmentation of the model with gaze features could indeed be described in more detailed, we consciously decided not to describe too many details that can be read in Lample et al.ßdummyTurker
10220ß0.0ß(2016) to focus on the eye-tracking part of our work.ßdummyTurker
10221ß0.0ßThis is also the reason why we did not experiment much with the parameters of the LSTM, but used the default values provided by Lample et al.ßdummyTurker
10222ß0.0ß(2016) - the goal was to analyse solely the potential contribution of gaze features without making our results dependent on data-specific hyperparameter tuning, etc.ßdummyTurker
10223ß0.0ßYes, the preprocessing refers to the process of averaging and splitting into quantiles.ßdummyTurker
10224ß0.0ßThe asterisk in Table 3 marks significant improvements over the baselineßdummyTurker
10225ß0.0ß(p<0.05, one-sided t-test) - which we mistakenly forgot in the caption (as in Table 5).ßdummyTurker
10226ß0.0ßTo identify whether the improvement of a proposed method is specific to the cross-domain setting, a typical metric is to show the performance of a non-transfer learning method, which trains a model only on source-domain labeled data and applies it directly to predict on target-domain test data.ßdummyTurker
10227ß0.0ßWe did implement such approaches, denoted by RNCRF and RNGRU in Table 2.ßdummyTurker
10228ß0.0ßMoreover, as requested by reviewer #1, we also additionally implement an upper-bound method, which uses sufficient target-domain labeled data to train a model to make predictions on target-domain test data.ßdummyTurker
10229ß0.0ßYes, the auxiliary task of relation and group learning can be used for semi-supervised setting.ßdummyTurker
10230ß0.0ßHowever, this paper focuses on cross-domain setting to transfer knowledge from labeled source domain to unlabelled target domain, which is much more challenging than single-domain setting.ßdummyTurker
10231ß0.0ßWe'll consider semi-supervised learning in our future work.ßdummyTurker
10232ß0.0ßIn this paper, we only focus on English opinion mining not on Chinese reviews.ßdummyTurker
10233ß0.0ßBut we'll read through related work on Chinese sentiment analysis.ßdummyTurker
10234ß0.0ßWe'll also survey more deep-learning-based approaches.ßdummyTurker
10235ß0.0ßOne reason might be that many crucial relation labels are replaced with wrong ones for this particular data, which produces too much noise to train a good classifier for aspect extraction.ßdummyTurker
10236ß0.0ßThis may not greatly influence opinion extraction because the two domains usually share many common opinion terms.ßdummyTurker
10237ß0.0ßThe significant difference in aspect terms across domains makes learning more dependent on common relations.ßdummyTurker
10238ß0.0ßAs this paper focuses on unsupervised domain adaptation, we mainly compare with domain adaptation approaches.ßdummyTurker
10239ß0.0ßWe'll consider your suggestion to investigate semi-supervised setting in our future work.ßdummyTurker
10240ß0.0ßThe approaches for semi-supervised learning are helpful within the same domain, where some labeled data is needed.ßdummyTurker
10241ß0.0ßSelf-training assumes similar distributions between labeled and unlabelled data.ßdummyTurker
10242ß0.0ßDifferent from semi-supervised learning, unsupervised domain adaptation has no labeled data in target domain and the target domain has different distributions compared to source domain, which is much more challenging than single-domain learning.ßdummyTurker
10243ß0.0ßThe success depends on the ability to exploit shared information across domains to transfer knowledge.ßdummyTurker
10244ß0.0ßIn this case, our model benefits from pivot syntactic relations as a bridge to learn common knowledge.ßdummyTurker
10245ß0.0ßThis paper focuses on unsupervised domain adaptation and conducted comparison experiments with those baselines in the same setting.ßdummyTurker
10246ß0.0ßThanks for your suggestion.ßdummyTurker
10247ß0.0ßWe'll conduct more comparisons in our future work.ßdummyTurker
10248ß0.0ßWe perform RNCRF [Wang et.al.2016] for supervised training on single domain with the same setting.ßdummyTurker
10249ß0.0ßThe results are 74.35/78.20 for restaurant aspect/opinion extraction, 72.28/73.74 for laptop aspect/opinion extraction, 52.71/66.19 for device aspect/opinion extraction.ßdummyTurker
10250ß0.0ßOur paper mainly address unsupervised domain adaptation problem, where labeled data only exists in source domain.ßdummyTurker
10251ß0.0ßAccording to your suggestions, we consider investigating our model's ability in semi-supervised learning, semi-supervised domain adaptation and unsupervised learning in our future work.ßdummyTurker
10252ß0.0ßWe provide some case studies to show which words are clustered to the same group, denoted by {}.ßdummyTurker
10253ß0.0ß{this, the, their, my, here, it, I, our, not}, {quality, jukebox, maitre-d, sauces, portions, volume, friend, calamari, price, menu}, {in, slightly, often, overall, regularly, since, back, much, ago}, {handy, tastier, white, salty, right, vibrant, first, ok}, {get, went, impressed, had, try, said, recommended, call, love}, {is, are, feels, believes, seems, like, will, would}.ßdummyTurker
10254ß0.0ßWe can provide some visualisations if more space is provided.ßdummyTurker
10255ß0.0ßInductive and transductive test data are not the same.ßdummyTurker
10256ß0.0ß"Inductive setting test on the ""out-of-sample"" test data in target domain (not available during training)."ßdummyTurker
10257ß0.0ßTransductive setting test on unlabelled training data in target domain.ßdummyTurker
10258ß0.0ßTheir scores are not directly comparable because the test data are different, but should be similar because they both come from target domain with similar distributions.ßdummyTurker
10259ß0.0ßIn most cases, transductive results are better because the test data is also used for training, while the inductive setting is more practical.ßdummyTurker
10260ß0.0ßThanks for your suggestion.ßdummyTurker
10261ß0.0ßWe'll investigate unsupervised contextual word embedding in our future work.ßdummyTurker
10262ß0.0ßThanks for all your valuable comments.ßdummyTurker
10263ß0.0ßWe will continue improving our work.ßdummyTurker
10264ß0.0ßWe will conduct experiments on the DUC dataset.ßdummyTurker
10265ß0.0ßIn Eq.ßdummyTurker
10266ß0.0ß19, g(S_i) is the ROUGE score gain of sentence i.ßdummyTurker
10267ß0.0ßWe apply the temperature \tau as a smoothing factor to produce a smoothed label distribution, which is easy to learn.ßdummyTurker
10268ß0.0ßWe will explain more about this in the paper.ßdummyTurker
10269ß0.0ßThanks for all your valuable comments.ßdummyTurker
10270ß0.0ßWe will continue improving our work.ßdummyTurker
10271ß0.0ßWe will conduct experiments on the DUC dataset.ßdummyTurker
10272ß0.0ßThe results of LEAD3 and SummaRunner are copied from their papers.ßdummyTurker
10273ß0.0ßThe LEAD3 on the anonymized dataset has 15.7 ROUGE-2 F1 score and its corresponding score on the non-anonymized version is 17.70.ßdummyTurker
10274ß0.0ßOn the anonymized data, SummaRunner achieves 16.2 ROUGE-2 F1 so we include it as a reference following (See et al., 2017) though it is not strictly comparable to our model.ßdummyTurker
10275ß0.0ßWe will try to contact the authors to get the anonymized dataset so we can report our model’s results.ßdummyTurker
10276ß0.0ßWe implemented the baseline (Ren et al., 2017).ßdummyTurker
10277ß0.0ßWe will add more details about the baselines.ßdummyTurker
10278ß0.0ßIn (Cheng & Lapata, 2016), their ILP baseline is lower than NN-SE, so we did not include it as our baseline.ßdummyTurker
10279ß0.0ßWe will add a non-neural ILP model as the baseline.ßdummyTurker
10280ß0.0ß(Cheng & Lapata, 2016) and (Nallapati et al., 2017) share similar scoring and selection methods, that they score all the sentences once and then select sentences by the sorted predicted probabilities (as described in the experimental setting section in Nallapati et al., 2017).ßdummyTurker
10281ß0.0ßTherefore, the selection of a sentence in these methods does not depends on the previously selected sentences.ßdummyTurker
10282ß0.0ßDifferent from them, our method scores all the sentences for each selection by considering both the sentence content and previously selected sentences.ßdummyTurker
10283ß0.0ßThank you for your kindle reminder.ßdummyTurker
10284ß0.0ßWe will describe this part and the related work section more precisely.ßdummyTurker
10285ß0.0ßThe ACL submission guideline requires the submission being anonymized.ßdummyTurker
10286ß0.0ßWe will reveal the URL of our code upon acceptance.ßdummyTurker
10287ß0.0ßThe training set contains about 287k documents.ßdummyTurker
10288ß0.0ßTherefore, we think it is not large enough to update the word embeddings.ßdummyTurker
10289ß0.0ßAccording to our experiments, the performance drops slightly if we update it.ßdummyTurker
10290ß0.0ßWe think this is due two main reasons.ßdummyTurker
10291ß0.0ßFirst, we think that the error propagation leads to worse selection for the third selection.ßdummyTurker
10292ß0.0ßAs shown in Figure 2, the p(@1) and p(@2) are 40.38% and 31.52% so the history is less reliable for the third selection.ßdummyTurker
10293ß0.0ßSecond, intuitively, we think the third selection is harder compared to first two since the most important sentences are already selected.ßdummyTurker
10294ß0.0ßThanks for all your valuable comments.ßdummyTurker
10295ß0.0ßWe will continue improving our work.ßdummyTurker
10296ß0.0ßThanks for your suggestion.ßdummyTurker
10297ß0.0ßWe will do human evaluation on a sample of the test set.ßdummyTurker
10298ß0.0ßThe abstractive baseline PGN network (See et al.ßdummyTurker
10299ß0.0ß2017) is a hybrid model of sequence-to-sequence and CopyNet (Gu et al.ßdummyTurker
10300ß0.0ß2016).ßdummyTurker
10301ß0.0ßWe will better compare our method to the previous work and describe more about the baselines.ßdummyTurker
10302ß0.0ßWe follow the previous works such as (Nallapati et al., 2017; See et al., 2017) to use the 95% confidence interval given by the official ROUGE script.ßdummyTurker
10303ß0.0ßThe lower bound of the confidence interval is greater than the baselines, which shows the significance of the improvement.ßdummyTurker
10304ß0.0ß1.ßdummyTurker
10305ß0.0ßWe will do a thorough proofreading to fix the typo errors, and rewrite the unclear sentences to make them more precise and clearer.ßdummyTurker
10306ß0.0ß2.ßdummyTurker
10307ß0.0ßQ: Which ROUGE version is used during training?ßdummyTurker
10308ß0.0ßA: It is slow to run official ROUGE Perl script on the training set.ßdummyTurker
10309ß0.0ßTherefore, we implemented a ROUGE with Python.ßdummyTurker
10310ß0.0ßDuring evaluation, we use the official ROUGE script (version 1.5.5).ßdummyTurker
10311ß0.0ß3.ßdummyTurker
10312ß0.0ßQ: It is unclear in the main text how the sentences that have already been included in the summary are excluded from the scoring when training the system end-to-end.ßdummyTurker
10313ß0.0ßA: During training, since we know the previously selected sentences so they can be exclude using a mask.ßdummyTurker
10314ß0.0ß4.ßdummyTurker
10315ß0.0ßQ: Lines 421-422 and Eq.ßdummyTurker
10316ß0.0ß12ßdummyTurker
10317ß0.0ßA: For the newswire data, the first several sentences tends to contain important information for summarization.ßdummyTurker
10318ß0.0ßThis is also demonstrated by the oracle sentence position in Figure3.ßdummyTurker
10319ß0.0ßTherefore, we use the last backward state in Eq.ßdummyTurker
10320ß0.0ß12.ßdummyTurker
10321ß0.0ßWe will change “MLP” to “linear layer with tanh activation”.ßdummyTurker
10322ß0.0ß5.ßdummyTurker
10323ß0.0ßSorry for this typo.ßdummyTurker
10324ß0.0ßIt is not z-transformation.ßdummyTurker
10325ß0.0ßIt is r instead of z in Eq.ßdummyTurker
10326ß0.0ß17.ßdummyTurker
10327ß0.0ßIt is the ROUGE-2 F1 score evaluation function r(.)ßdummyTurker
10328ß0.0ßas described in section 3.ßdummyTurker
10329ß0.0ß6.ßdummyTurker
10330ß0.0ßQ: Why do we minimize the KL loss?ßdummyTurker
10331ß0.0ßIsn't this equivalent to minimizing the (simpler) categorical cross-entropy loss?ßdummyTurker
10332ß0.0ßA:  KL divergence and cross-entropy are both widely used.ßdummyTurker
10333ß0.0ßWhen the model prediction P and label data Q are probability distributions, KL loss function is a natural choice.ßdummyTurker
10334ß0.0ßD_KL(P||Q) = H(P, Q) – H(P), where H(P, Q) is the cross-entropy of P and Q, and H(P) is the entropy of P. Minimizing KL will also try to increase the entropy of P, which can possibly help prevent the model from overfitting.ßdummyTurker
10335ß0.0ßTherefore, they are not equivalent.ßdummyTurker
10336ß0.0ß7.ßdummyTurker
10337ß0.0ßQ: How were the hyper-parameters tuned?ßdummyTurker
10338ß0.0ßA: We tune dropout on the dev set.ßdummyTurker
10339ß0.0ßWe found that larger size GloVe does not lead to improvement.ßdummyTurker
10340ß0.0ßTherefore, we use 50-dim for fast training.ßdummyTurker
10341ß0.0ßWe use the default hyper-parameter of Adam optimizer.ßdummyTurker
10342ß0.0ßWe set other hyper-parameters empirically and have not tuned them.ßdummyTurker
10343ß0.0ß8.ßdummyTurker
10344ß0.0ßHinton et al.ßdummyTurker
10345ß0.0ß(2015) use exp(zi / T ) in softmax and we use exp(\tau zi).ßdummyTurker
10346ß0.0ßThey are equivalent (T = 1 / \tau).ßdummyTurker
10347ß0.0ßWe want to make it clear to avoid misunderstanding.ßdummyTurker
10348ß0.0ßThe work of this paper has been greatly inspired by the work of Gulcehre et al.ßdummyTurker
10349ß0.0ß(2015).ßdummyTurker
10350ß0.0ßHowever, we adopted a character-based approach to apply it to spell correction.ßdummyTurker
10351ß0.0ßAnd there are some differences in the fusion method.ßdummyTurker
10352ß0.0ßThe g_t that Gulcehre control the impact of the language model on the final output is only obtained by the translation model’s hidden layer output s_t.ßdummyTurker
10353ß0.0ßThat is, in the work of Gulcehre, there is no hidden layer output of the language model in Equation 10a.ßdummyTurker
10354ß0.0ßAnd in terms of training methods, we also train the integrated model after pre-training unlike Gulcehre only using separate steps to train translation models and language models, This allows the Seq2Seq model to better perceive the knowledge in the language model.ßdummyTurker
10355ß0.0ßThank you for your suggestion.ßdummyTurker
10356ß0.0ßWe will further proofread the paper.ßdummyTurker
10357ß0.0ßYou're right.ßdummyTurker
10358ß0.0ßIn the data set provided by Hagen et al., 2017, The author compares some traditional methods with current mainstream search engines.ßdummyTurker
10359ß0.0ßSo, it's easy to add.ßdummyTurker
10360ß0.0ßOur method is effective compared to the traditional method, but it may be slightly inferior to the effects of some commercial search engines.ßdummyTurker
10361ß0.0ßThis is mainly due to the fact that our approach only considers the text itself for error correction and does not take into account additional features such as click-through rates, user data, etc.ßdummyTurker
10362ß0.0ßOur work can be easily integrated with these features and achieve better results.ßdummyTurker
10363ß0.0ßBecause the deadline was being passed at the time, some experiments may not be sufficient.ßdummyTurker
10364ß0.0ßWe are also continuing to do research in this direction, so follow-up work is still in progress.ßdummyTurker
10365ß0.0ßAnd some other experiments are also in progress.ßdummyTurker
10366ß0.0ßThe work of this paper has been greatly inspired by the work of Gulcehre et al.ßdummyTurker
10367ß0.0ß(2015).ßdummyTurker
10368ß0.0ßHowever, we adopted a character-based approach to apply it to spell correction.ßdummyTurker
10369ß0.0ßAnd there are some differences in the fusion method.ßdummyTurker
10370ß0.0ßThe g_t that Gulcehre control the impact of the language model on the final output is only obtained by the translation model’s hidden layer output s_t.ßdummyTurker
10371ß0.0ßThat is, in the work of Gulcehre, there is no hidden layer output of the language model in Equation 10a.ßdummyTurker
10372ß0.0ßAnd in terms of training methods, we also train the integrated model after pre-training unlike Gulcehre only using separate steps to train translation models and language models, This allows the Seq2Seq model to better perceive the knowledge in the language model.ßdummyTurker
10373ß0.0ßBecause the deadline was being passed at the time, some experiments may not be sufficient.ßdummyTurker
10374ß0.0ßWe are also continuing to do research in this direction, so follow-up work is still in progress.ßdummyTurker
10375ß0.0ßAnd some other experiments are also in progress.ßdummyTurker
10376ß0.0ßWe found that the performance would be poor without data augmentation.ßdummyTurker
10377ß0.0ßSo, as mentioned in the previous one, our next work will also focus on how to use more effective data augmentation methods, rather than just artificially formulated rules.ßdummyTurker
10378ß0.0ßAs you said, this paper is mainly about spelling correction.ßdummyTurker
10379ß0.0ßBut it includes different kinds of mistakes, single word and multiple words.ßdummyTurker
10380ß0.0ßThe difference between the search engine and the general text is that the query string is short, and grammatical errors rarely occur.ßdummyTurker
10381ß0.0ßSo Grammatical errors have less impact on the results of the query.ßdummyTurker
10382ß0.0ß5.ßdummyTurker
10383ß0.0ßThank you for your suggestion.ßdummyTurker
10384ß0.0ßWe will further proofread the paper.ßdummyTurker
10385ß0.0ßThank you for your criticism.ßdummyTurker
10386ß0.0ßThe work of this paper has been greatly inspired by the work of Gulcehre et al.ßdummyTurker
10387ß0.0ß(2015).ßdummyTurker
10388ß0.0ßHowever, we adopted a character-based approach to apply it to spell correction.ßdummyTurker
10389ß0.0ßAnd there are some differences in the fusion method.ßdummyTurker
10390ß0.0ßThe g_t that Gulcehre control the impact of the language model on the final output is only obtained by the translation model’s hidden layer output s_t.ßdummyTurker
10391ß0.0ßThat is, in the work of Gulcehre, there is no hidden layer output of the language model in Equation 10a.ßdummyTurker
10392ß0.0ßAnd in terms of training methods, we also train the integrated model after pre-training unlike Gulcehre only using separate steps to train translation models and language models, This allows the Seq2Seq model to better perceive the knowledge in the language model.ßdummyTurker
10393ß0.0ßOur approach only considers the text itself for error correction and does not take into account additional features such as click-through rates, user data, etc.ßdummyTurker
10394ß0.0ßBut it is also our goal to find an effective model based on the text itself.ßdummyTurker
10395ß0.0ßIn the next work, we can easily use the ensemble model to add other features.ßdummyTurker
10396ß0.0ßWe use a character-based method in order to be able to build a model that can unify different errors.ßdummyTurker
10397ß0.0ßSome errors are difficult to solve with word-based method.ßdummyTurker
10398ß0.0ßAnd our method can be easily applied to languages that require segmentation, such as many Asian languages.ßdummyTurker
10399ß0.0ßWord-based methods are problematic in these languages because word segmentation is only valid on the correct text.ßdummyTurker
10400ß0.0ßBased on the above reasons, the character-based method will be a more general method.ßdummyTurker
10401ß0.0ßSo, more research should be done in this direction.ßdummyTurker
10402ß0.0ßIn the field of query correction, we did not find works based on the Seq2Seq model or other similar methods.We think there are many differences between correction and suggestion.ßdummyTurker
10403ß0.0ßIt is true that tuning alpha and beta requires extra labor and computational resources.ßdummyTurker
10404ß0.0ßBut this actually does not burden us because all hyperparameters can be tuned automatically by running a simple program of grid search on the development set.ßdummyTurker
10405ß0.0ßAlso, in our experiment we found that the setting determined with beam size 10 can be reliably applied to larger beam sizes like 100 and 500.ßdummyTurker
10406ß0.0ßThis observation can significantly reduce computational requirement.ßdummyTurker
10407ß0.0ßFor weights tuning, we employ a simple grid search to choose settings with the best performance.ßdummyTurker
10408ß0.0ßIn our experiments, we put the same effort for tuning these weights by comparing 10 different settings for each of them and we select the best ones.ßdummyTurker
10409ß0.0ßThanks for the comment.ßdummyTurker
10410ß0.0ßWe will add the analysis on those parameters in the next version of the paper (due to the extra page of the short paper publication in ACL).ßdummyTurker
10411ß0.0ßFor CS’s alpha and beta, we tune both on interval [0,1], and step is 0.1.ßdummyTurker
10412ß0.0ßFor LN’s weight, it is tuned on interval [0.5,1.5], and step is also 0.1.ßdummyTurker
10413ß0.0ßCP’s weight uses the same strategy as CS.ßdummyTurker
10414ß0.0ßAll these hyper-parameters are tested on development sets with beam size 10 and we choose the ones with the highest performance, then applied to larger beam sizes.ßdummyTurker
10415ß0.0ßIt is pretty fast to do all experiments because hyper-parameters are selected on beam size 10.ßdummyTurker
10416ß0.0ßThanks for your constructive and thoughtful comments!ßdummyTurker
10417ß0.0ßWe will add more discussions on potential tradeoff.ßdummyTurker
10418ß0.0ßSimilar to prior work in the line (e.g.ßdummyTurker
10419ß0.0ßTransformer; Waswani et al., 2017), our  main concerns include efficiency and accuracy, on both of which S-LSTM shows advantage over BiLSTM.ßdummyTurker
10420ß0.0ßEfficiency is a major concern for industry deployment and one of our main motivations.ßdummyTurker
10421ß0.0ßWe thank all reviewers for your valuable feedback and comments!ßdummyTurker
10422ß0.0ßA1: Thanks for pointing out those evaluation metrics!ßdummyTurker
10423ß0.0ßWe were aware of the evaluation software but did not follow it because:ßdummyTurker
10424ß0.0ß1) The output of our system and previous methods such as Bansal et al., 2014 is a single tree so the number of connected components is always one.ßdummyTurker
10425ß0.0ß"2) The ""ratio of novel edges"", defined as (|Edge_sys| - |Edge_sys & Edge_gold|) / |Edge_gold|, may cause problems if the predicted edges have no overlap with gold edges."ßdummyTurker
10426ß0.0ßIn that case, the ratio of novel edges would be very high, even if the edges are wrong.ßdummyTurker
10427ß0.0ßIn the next competition (TExEval-2), the organizers removed this measure.ßdummyTurker
10428ß0.0ß3) For the Cumulative Fowlkes & Mallows Measure, we tested a similar measure called Fragment-based Similarity (Yang 2012) which also measures the similarities of clusters, and found the scores remained relatively steady during training when the depth of the taxonomy is relatively low.ßdummyTurker
10429ß0.0ßA2: We did intend to extend the system to produce DAG taxonomies.ßdummyTurker
10430ß0.0ßFor example, we may modify the action space of the RL agent by allowing multiple parents.ßdummyTurker
10431ß0.0ßWe leave it as future work.ßdummyTurker
10432ß0.0ßA3: We agree that a more general setting should be to extract terms (and merge synonyms) before building the taxonomy.ßdummyTurker
10433ß0.0ßSimilar to prior works (Bansal et al.ßdummyTurker
10434ß0.0ß"2014, SemEval-2015 Task17, SemEval-2016 Task13, etc) in the same line, we presume a ""clean"" vocabulary is given, meaning all the terms would be selected eventually."ßdummyTurker
10435ß0.0ß"Therefore, there are no ""discarded terms""."ßdummyTurker
10436ß0.0ßAs far as we know, only Subseq (Gupta et al.ßdummyTurker
10437ß0.0ß2017) considers the scenario where a noisy vocabulary is given and they only compare with one variant of itself since there are no strong baselines available.ßdummyTurker
10438ß0.0ßFor harvesting unobserved concepts, one idea is to first perform NP chunking, NER, or TermSuite (Cram 2016) to find terms of interest, and then train the RL agent with an extra action, which is to stop induction.ßdummyTurker
10439ß0.0ßSince we give priority to high-quality edges, the induction will stop when the confidence of the agent is below a threshold.ßdummyTurker
10440ß0.0ßA4: We were aware of and have cited several related works in hypernym classification (e.g., Snow et al., 2005; Fu et al., 2014; Tuan et al., 2016; Shwartz et al., 2016) but we agree that it may be insufficient due to the lack of space.ßdummyTurker
10441ß0.0ßWe will provide more details in the final version.ßdummyTurker
10442ß0.0ßA5: Thanks for pointing that out!ßdummyTurker
10443ß0.0ß"We agree that harvesting new ""concepts"" is equally important and is an interesting task itself."ßdummyTurker
10444ß0.0ßIt would be nice to extend our method to harvest new concepts.ßdummyTurker
10445ß0.0ßQ1: In the tree version, we only keep the first parent of each node if there are multiple parents.ßdummyTurker
10446ß0.0ßC1: We will present the additional materials in a “cleaner” way.ßdummyTurker
10447ß0.0ßThat’s a good point!ßdummyTurker
10448ß0.0ßWe had to shorten the paper to fit the page limit.ßdummyTurker
10449ß0.0ßWe will provide an introduction of the REINFORCE algorithm and add more analysis on how the joint training helps the taxonomy induction process in the final version.ßdummyTurker
10450ß0.0ßThank you for your professional comment.ßdummyTurker
10451ß0.0ßIt’s a pleasure to talk to you.ßdummyTurker
10452ß0.0ßHowever, we argue that the novelty of our paper is not limited.ßdummyTurker
10453ß0.0ß(1)	We focus on the task of dataless text classification with seed words.ßdummyTurker
10454ß0.0ßA major issue in this area is that the supervision provided by seed words is very limited and noisy.ßdummyTurker
10455ß0.0ßMany documents contain no seed words or incorrect seed words, especially when the seed words are scarce.ßdummyTurker
10456ß0.0ßLinking neighboring documents seems a good idea to propagate the supervision and correct the noise.ßdummyTurker
10457ß0.0ßThis motivates us to use the manifold regularization.ßdummyTurker
10458ß0.0ßAnd we show that the manifold regularization method is very suitable for improving dataless text classification.ßdummyTurker
10459ß0.0ßWe argue that not only the model, but the task we focused on is also a highlight.ßdummyTurker
10460ß0.0ßIn this paper, we aim to use manifold regularization to address issues in dataless classification, rather than how to design manifold regularization terms to topic models.ßdummyTurker
10461ß0.0ßBesides, the proposed LapSWTM performs well with very a few seed words, instead of labeled documents.ßdummyTurker
10462ß0.0ßLapSWTM can not only save many human efforts, but also be a practical complementary classifier when labeled documents are extremely difficult to obtain.ßdummyTurker
10463ß0.0ß(2)	Following your suggestion, we compare LapSWTM against a version of DescLDA with manifold regularization.ßdummyTurker
10464ß0.0ßNot surprisingly, LapSWTM still significantly performs better, especially for S^L.ßdummyTurker
10465ß0.0ßBesides, we argue that our empirical results are convincing.ßdummyTurker
10466ß0.0ßFirst, LapSWTM significantly outperforms STM, the best dataless classifier we have seen.ßdummyTurker
10467ß0.0ßSecond, LapSWTM performs well when the seed words are very scarce, i.e., using S^L.ßdummyTurker
10468ß0.0ßThird, LapSWTM is even on a par with supervised algorithms in some settings.ßdummyTurker
10469ß0.0ßThese results indicate that LapSWTM is a practical dataless classifier.ßdummyTurker
10470ß0.0ßBesides, compared to STM, LapSWTM can be directly applied to multi-label classification.ßdummyTurker
10471ß0.0ßWe plan to investigate this in the future.ßdummyTurker
10472ß0.0ßWe were unable to find information about a tool or framework for collaboration called Reddex, and we are not aware of any collaborative work frameworks that automatically analyze users' language, but we are eager to learn more about any projects similar to ours, particularly those that explore the relationship between the linguistic qualities of feedback and the effectiveness of that feedback.ßdummyTurker
10473ß0.0ßThe ultimate goal of our work is to help software developers, and other collaborative workers, to provide feedback that is more likely to be acted upon.ßdummyTurker
10474ß0.0ßAs we mention in the paper, we envision incorporating a system within a tool like Rietveld to automatically flag comments that have the qualities of particularly helpful or unhelpful feedback.ßdummyTurker
10475ß0.0ßAlthough such qualities might seem obvious, our empirical results indicate that the qualities developers themselves believe to be important (e.g., informativeness) are not as important as less obvious style and discourse features (e.g., politeness).ßdummyTurker
10476ß0.0ßIt is true that we have not controlled for non-linguistic factors, such as bug complexity.ßdummyTurker
10477ß0.0ßSince the goal of this particular paper was to help developers improve their feedback, we restricted ourselves to studying the relationship between linguistic cues in a reviewer comment and whether that comment was acted upon.ßdummyTurker
10478ß0.0ßIn our future work, we look forward to exploring other data available in the full code review repository, such as reviewer experience, implementation time, complexity of the bug identified by the reviewer, and potentially social factors (personality traits, etc.)ßdummyTurker
10479ß0.0ßWe compared our work with that of Rahman et al.ßdummyTurker
10480ß0.0ß(2017) in the second paragraph of our Background section.ßdummyTurker
10481ß0.0ßRahman et al.ßdummyTurker
10482ß0.0ß(2017) explored metrics such as reading ease, question ratio, stop word ratio, code element ratio, and conceptual similarity to predict the usefulness of code review comments.ßdummyTurker
10483ß0.0ßIn our work, we explore these and several more subtle features related to linguistic style and discourse.ßdummyTurker
10484ß0.0ßIf our paper is accepted, we will be able to elaborate on the substantial differences between our work and that of Rahman et al.ßdummyTurker
10485ß0.0ß(2017).ßdummyTurker
10486ß0.0ßRietveld, the system that facilitates code review in Chromium is licensed under Apache v2.0.ßdummyTurker
10487ß0.0ßThe linguistic data we have collected from Rietveld and released in our datasets is freely available to anyone with an internet connection.ßdummyTurker
10488ß0.0ßIn the datasets we are releasing, we have de-identified all usernames and email addresses.ßdummyTurker
10489ß0.0ßWe will license our datasets under the Creative Commons ShareAlike license (CC-BY-SA: https://creativecommons.org/licenses/by-sa/4.0/)ßdummyTurker
10490ß0.0ßThe anonymity requirement forced us to be somewhat vague.ßdummyTurker
10491ß0.0ßMost of the missing details are in (Jongejan and Dalianis 2009), which in fact is a description of an early version of Affixtrain that was not publicly available.ßdummyTurker
10492ß0.0ßThe current version on GitHub differs from the early version by having a few additions: the ability to produce multiple lemmas and the automatic computation of training parameters.ßdummyTurker
10493ß0.0ß1)	The first half of the abstract is about training, the second about the experiments.ßdummyTurker
10494ß0.0ß"""The unpruned rules lemmatize all words in the training set correctly"" formulates the stopping criterion for the decision tree building process."ßdummyTurker
10495ß0.0ßI admit it can be (mis)read as a statement about the experiment.ßdummyTurker
10496ß0.0ß2)	An example: If the training set contains the pairs ‘saw’ - ‘see’ and ‘saw’ - ‘saw’, then the unpruned decision tree will produce ‘saw|see’ for the input ‘saw’.ßdummyTurker
10497ß0.0ßI will clarify the text.ßdummyTurker
10498ß0.0ßI had such an example for earlier drafts, which was for a long paper.ßdummyTurker
10499ß0.0ßThen I found the text too little focused and decided to write a short paper.ßdummyTurker
10500ß0.0ßThe example was then deleted.ßdummyTurker
10501ß0.0ßI will include it again.ßdummyTurker
10502ß0.0ßWe did do some limited experiments with true PoS-tags for English and one other language.ßdummyTurker
10503ß0.0ßWe did not study the PoS tags the other training datasets came with, since we were not going to use them.ßdummyTurker
10504ß0.0ßUsing PoS tags would probably have made it harder to discern the relative strengths and weaknesses of the evaluated lemmatizers.ßdummyTurker
10505ß0.0ßThanks for that suggestion.ßdummyTurker
10506ß0.0ßsaw saw /ßdummyTurker
10507ß0.0ßsaw see /ßdummyTurker
10508ß0.0ßseen seeßdummyTurker
10509ß0.0ßThe first two entries are in the chunk because they are homographs, the last two entries are in the chunk because they have the same lemma.ßdummyTurker
10510ß0.0ßAffixtrain is an updated implementation of the software described in Jongejan and Dalianis 2009.ßdummyTurker
10511ß0.0ßWe found it impossible to make this clear in our submission because of the anonymity requirement.ßdummyTurker
10512ß0.0ßThe later additions are mentioned in the text: the ability to produce multiple lemmas and parameter settings that are computed instead of guessed by the experimenter.ßdummyTurker
10513ß0.0ßIt is possible to compare the current version of Affixtrain with the 2009 version, since the original hand crafted parameters can still be used.ßdummyTurker
10514ß0.0ßUsing F1 is a good suggestion.ßdummyTurker
10515ß0.0ßIt penalizes overgeneration of lemma candidates.ßdummyTurker
10516ß0.0ßOn the other hand, F1 is insensitive to false negatives, i.e.ßdummyTurker
10517ß0.0ßinputs for which no output is produced.ßdummyTurker
10518ß0.0ßLexicon based lemmatizers do not produce output for OOV words.ßdummyTurker
10519ß0.0ßMBLEM can produce multiple lemmas, LemmaGen cannot.ßdummyTurker
10520ß0.0ßSee section 2 under 'Baselines'.ßdummyTurker
10521ß0.0ßWithout altering the downloaded code, the PCRF-seq2seq program produces a single answer for each input instance.ßdummyTurker
10522ß0.0ßAlso see Table 1, where Affixtrain and MBLEM have two columns each, and LemmaGen and PCRF-seq2seq have one each.ßdummyTurker
10523ß0.0ßNone of the datasets are created by us.ßdummyTurker
10524ß0.0ßOur plan was to cite the sources of all the data on the extra page that is allowed to accepted short papers.ßdummyTurker
10525ß0.0ßIn the revision, we will evaluate our method on more datasets, such as KBP dataset (Surdeanu et al., 2012).ßdummyTurker
10526ß0.0ßThe hyperparameter settings in Table 1 are determined by some tuning procedures, but they are not presented because of the limitation of pages.ßdummyTurker
10527ß0.0ßWe will add them to the revision.ßdummyTurker
10528ß0.0ß(1) For the threshold \rho, see ‘Q4 in the additional comments’ of Reviewer #1.ßdummyTurker
10529ß0.0ß(2) For the settings of CNNs, the window size refers to the previous DS works (Zeng et al., 2015, Lin et al., 2016); the kernel size is determined base on the performance of binary classification in the pre-training process.ßdummyTurker
10530ß0.0ß(3) The regulator \alpha is tuned on the validation set.ßdummyTurker
10531ß0.0ßWe consider two factors: the convergence rate and the performance of RL agent on the validation set.ßdummyTurker
10532ß0.0ßAs mentioned in line410 and line578, we divided P_k (line 452) into two part: RL training set and RL validation set.ßdummyTurker
10533ß0.0ßIn every epoch, the agent tries to remove the fixed number of sentences from the RL training set, and a new relation classifier is trained base on this cleaned dataset; then, we use the current agent to remove the fixed number of sentences from the RL validation set, and then calculate the F1 score of this cleaned RL validation set.ßdummyTurker
10534ß0.0ßEven though we need to train a relation classifier in every epoch, there are some tricks  to overcome the fluctuations and the expensive:ßdummyTurker
10535ß0.0ß(1) In order to overcome the fluctuations, we use the average F1 score of the last five epochs to calculate the reward (line284).ßdummyTurker
10536ß0.0ß(2) We do not need to train the relation classifier to be convergent.ßdummyTurker
10537ß0.0ßParticularly, we use the F1 scores of the epoch 11~15 (the performance of the relation classifier has exceeded 90%).ßdummyTurker
10538ß0.0ßIf the cleaned dataset is better, more false positive samples are removed; therefore, the better F1 score is obtained in the same training epochs.ßdummyTurker
10539ß0.0ß(3) The network structure of relation classifier is simple, and it is just a binary classifier.ßdummyTurker
10540ß0.0ßSo the training time of the relation classifier in one epoch is affordable(less than 40s with one NVIDIA GeForce GTX 1080 GPU even for the relation type that has the most distant-supervised sentences).ßdummyTurker
10541ß0.0ßThank you.ßdummyTurker
10542ß0.0ßWe will correct the typos in the revision.ßdummyTurker
10543ß0.0ßWe indeed have thought about the strategy that just builds a single agent for all relation types: a binary classifier(TP/FP) or a multi-class classifier(rela1/rela2/.../FP).ßdummyTurker
10544ß0.0ßBut, it has the limitation in the performance.ßdummyTurker
10545ß0.0ßWe found that our one-agent-for-one-relation strategy obtained better performance than the single agent strategy.ßdummyTurker
10546ß0.0ßAlso, for a single agent strategy, with the increasing of relation types, the requirement of discriminative power of the agent is higher.ßdummyTurker
10547ß0.0ßwe will discuss this in the revision.ßdummyTurker
10548ß0.0ßThe pre-training and the retraining process of our agent are merely based on the distant-supervised dataset; thus, ‘Without any supervised information’ means that we do not need any manual annotation dataset to assist the training process of our RL agent.ßdummyTurker
10549ß0.0ßwe will clarify this in the revision.ßdummyTurker
10550ß0.0ßSee ‘Reply to weakness argument 1’.ßdummyTurker
10551ß0.0ßIn order to make the agent more robust, we indeed design and adopt some tricks, including the pre-training strategy(line339), the fixed number of removals(line483), etc.ßdummyTurker
10552ß0.0ßSome are not presented in the submission because of the limitation of pages:ßdummyTurker
10553ß0.0ß(1) In the state(line243), we give relatively bigger weight for the vector of the current sentence, in which way to magnify the dominating influence of the current sentence information for the decision of action.ßdummyTurker
10554ß0.0ß(2) When one relation type has too many distant-supervised positive sentences (for example, ‘/location/location/contains’ has 75768 sentences), we use a reasonable sampling method to sample a subset (8,000 sentences) to train the agent.ßdummyTurker
10555ß0.0ßThis amount is adequate to learn a robust RL agent, and it saves lots of times as well.ßdummyTurker
10556ß0.0ßFor the threshold \rho, we adopt the linear search, where the value is from the set  [0.5, 0.6, 0.7, 0.8, 0.9].ßdummyTurker
10557ß0.0ßThe cross-validation is used to select the optimal value.ßdummyTurker
10558ß0.0ßUnder \rho=0.6, 26.57% of the original positive entity pairs of the test set are redistributed into the negative sample set.ßdummyTurker
10559ß0.0ßFor the average vector of the removed sentences, in the pre-training process and the first state of the retraining process, it is set as all-zero vector.ßdummyTurker
10560ß0.0ß1.ßdummyTurker
10561ß0.0ßIn Section 1, we attempted to show crucial numeric information in the financial domain.ßdummyTurker
10562ß0.0ßThe purpose of different categories is shown in Section 2.ßdummyTurker
10563ß0.0ßFor example, capturing the “support and resistance” price can help us to construct a new technical indicator based on crowd opinion, and capturing “exercise price” and “maturity date” can help us to get the information of the predicted price and the duration of the prediction.ßdummyTurker
10564ß0.0ßThanks for your suggestion, we will add more details showing the usefulness provided by numeral (sub)categorization in the revision.ßdummyTurker
10565ß0.0ß2.ßdummyTurker
10566ß0.0ßThe SemEval dataset consists of the data not only from Twitter, but also from StockTwits, a social trading platform for individual investors discussing their perspectives (Line 295-296).ßdummyTurker
10567ß0.0ßThe selected instances must contain at least one cashtag to confirm that the instance is about certain financial instrument.ßdummyTurker
10568ß0.0ß3.ßdummyTurker
10569ß0.0ßOur task is to resolve the role of numerals in financial social media data.ßdummyTurker
10570ß0.0ßTherefore, the instances which discuss about certain financial instrument in social media platform are proper for this task.ßdummyTurker
10571ß0.0ßSection 2 is an important part of our work showing how the numeric information contributes to the understanding of market sentiment.ßdummyTurker
10572ß0.0ßFor example, capturing the “buying price” and “selling price” in investors’ tweets can be used for evaluating their performances (Line 181-186).ßdummyTurker
10573ß0.0ßThanks for reviewer’s suggestion.ßdummyTurker
10574ß0.0ßWe will refine Section 2 to highlight our motivation.ßdummyTurker
10575ß0.0ßRecently, FinTech has attracts much attentions.ßdummyTurker
10576ß0.0ßRelated workshop and share tasks, including SemEval 2017 Task 5 (collocated with ACL’17), FNP 2018 (collocated with LREC’18), FiQA 2018 (collocated with WWW’18), and ECONLP 2018 (collocated with ACL’18), show a great potential on applying NLP technologies to the financial domain.ßdummyTurker
10577ß0.0ßAs mentioned in Line 124-131, this paper focuses on a novel idea for numeral understanding in the financial social media data.ßdummyTurker
10578ß0.0ßWe introduce a new task with high quality annotated dataset to the NLP community for the interdisciplinary research.ßdummyTurker
10579ß0.0ßWe propose a dedicated model based on CNN for resolving the numeric information and demonstrate the challenging issues of this task.ßdummyTurker
10580ß0.0ßThe sophisticated approaches will be investigated in the future.ßdummyTurker
10581ß0.0ßWe would like to thank the Reviewer for the very informative feedback.ßdummyTurker
10582ß0.0ßThe primary weakness that the Reviewer addresses is that the difference in evaluation scores is already expected.ßdummyTurker
10583ß0.0ßWe agree that it is known that different evaluation metrics are developed to evaluate different features of models.ßdummyTurker
10584ß0.0ßBut the point of the paper is different.ßdummyTurker
10585ß0.0ßIn this paper we tried to find an answer to “could we somehow estimate extrinsic performance of cross-language embeddings given some intrinsic metrics?”.ßdummyTurker
10586ß0.0ßWe found that correlation of models performance on intrinsic and extrinsic tasks is not strong enough.ßdummyTurker
10587ß0.0ßNotably, word similarity and paraphrase detection tasks intrinsically rely on the same language regularities: how well the models estimate semantic distances between words.ßdummyTurker
10588ß0.0ßThus, one could expect that relative scores on word similarity datasets have to be similar to that for paraphrase detection.ßdummyTurker
10589ß0.0ßHowever, our experiments contradict this.ßdummyTurker
10590ß0.0ßWe also may conclude that embedding methods should be compared only in the context of a specific task.ßdummyTurker
10591ß0.0ßThen, the Reviewer says that there is no suggestion for an improved evaluation measure.ßdummyTurker
10592ß0.0ßBut we propose to evaluate word embeddings with cross-language paraphrase detection task, which has not been used for embeddings evaluation previously (and we also created the corresponding dataset).ßdummyTurker
10593ß0.0ßTo sum up, we think we not only need intrinsic datasets of different types if we want to robustly predict the performance of different extrinsic tasks, but we also should overthink the design and by capabilities of existing extrinsic benchmarks by borrowing some methodologies from linguistic typology, e.g.ßdummyTurker
10594ß0.0ßtheory of semantic fields (as we tried to do in our work).ßdummyTurker
10595ß0.0ßFinally, we agree that we should address problems of benchmarks not as limitations, but as potential issues which one could face when evaluating cross-language word embeddings.ßdummyTurker
10596ß0.0ßWe thank the reviewer for the time and effort taken to review our paper.ßdummyTurker
10597ß0.0ßIndeed, the analysis of the classification evaluation task is a bit dense due to space constraints, but given the extra page in the camera-ready version, we will elaborate on that.ßdummyTurker
10598ß0.0ßWe thank the reviewer for the time and effort taken to review our paper.ßdummyTurker
10599ß0.0ßEach training example contains all three components, e.g.ßdummyTurker
10600ß0.0ß(apple, cake, made of, [score]).ßdummyTurker
10601ß0.0ßWe have three subtasks:ßdummyTurker
10602ß0.0ß(1) Input: ‘[w2] made of apple’, predict the missing constituent [w2], gold: cakeßdummyTurker
10603ß0.0ß(2) Input: ‘cake made of [w1]’, predict the missing constituent [w1], gold: appleßdummyTurker
10604ß0.0ß(3) Input: ‘cake [p] apple’, predict the missing paraphrase [p], gold: ‘[w2] made of [w1]’ßdummyTurker
10605ß0.0ßEach subtask has a separate loss computed using the gold item and the prediction, and the losses are summed for each training example.ßdummyTurker
10606ß0.0ßThe subtasks share the pre-trained word embeddings, the special embeddings, and the biLSTM parameters.ßdummyTurker
10607ß0.0ßSubtasks (1) and (2) share the MLP that predicts the index of a word (as opposed to the MLP in (3) that predicts the index of a paraphrase).ßdummyTurker
10608ß0.0ßThanks for pointing this out, we will clarify this in the text and in the figure.ßdummyTurker
10609ß0.0ßWe regard error position detection as a preceding step, which has been studied by Shiue et al.ßdummyTurker
10610ß0.0ß(2017).ßdummyTurker
10611ß0.0ßAs pointed out by them, there are already some difficulties in determining which one in a pair of closely related words needs correction.ßdummyTurker
10612ß0.0ßIt will therefore be even more difficult to handle detection and correction at the same time, so we conduct this research under a relatively restricted setting.ßdummyTurker
10613ß0.0ßHowever, our methods can be applied to multiple potential error positions by generating correction for every position, gathering all candidates and removing those with too low language model ranks.ßdummyTurker
10614ß0.0ßNevertheless, this will induce difficulties in evaluation since there will be much more alternative corrections.ßdummyTurker
10615ß0.0ßWe will add more discussion on this.ßdummyTurker
10616ß0.0ßOur model achieves accuracy 0.81 on preposition error correction (see Table 7), while Huang et al.ßdummyTurker
10617ß0.0ß(2016)’s model achieves 0.77.ßdummyTurker
10618ß0.0ßHowever, the two values are not directly comparable since different datasets are used.ßdummyTurker
10619ß0.0ßTo our knowledge, there is no previous work on correcting Chinese WUEs of content words.ßdummyTurker
10620ß0.0ßWe will elaborate more on this.ßdummyTurker
10621ß0.0ßOur model can handle general WUEs because no matter what sub-type the error belongs to, the two proposed criteria of a suitable correction apply.ßdummyTurker
10622ß0.0ßThe importance of the two criteria might vary across sub-types, e.g.ßdummyTurker
10623ß0.0ßsemantic constraints are more important for content words.ßdummyTurker
10624ß0.0ßHowever, in general the correctness criterion should take higher priority, and our method of combining ranks based on transformed similarities and language model scores ensures this to some extent.ßdummyTurker
10625ß0.0ß"There are some challenging aspects in Chinese WUE correction, which are ""different"" from those of English, so focused investigation is necessary."ßdummyTurker
10626ß0.0ßWe are not making an overall judgment, which is nearly impossible since the two languages are fundamentally different.ßdummyTurker
10627ß0.0ßSince evaluations are based on the rank of the first acceptable correction, alternative corrections ranked after the ground-truth will not change the results.ßdummyTurker
10628ß0.0ßConsidering the high cost of human evaluation, we limit the top candidates to be annotated to those ranked after the ground-truth.ßdummyTurker
10629ß0.0ßWe will resolve the inconsistencies in the reference format.ßdummyTurker
10630ß0.0ßWe actually experimented with machine translation models, which are state-of-the-arts of English GEC.ßdummyTurker
10631ß0.0ßHowever, the performance of our preliminary MT model is lower than the language model baselines.ßdummyTurker
10632ß0.0ßWe omit this result due to the space limit.ßdummyTurker
10633ß0.0ßA possible reason for the poor performance is the lack of data, as we pointed out on line 144-154.ßdummyTurker
10634ß0.0ßMaking improvement on MT models for Chinese is possible, but that is beyond the scope of this research.ßdummyTurker
10635ß0.0ßThe performance of the top output is indicated in Table 6 by accuracy, namely hit@1.ßdummyTurker
10636ß0.0ßSince the annotation is performed only for the top outputs of our model, it will be unfair to compare other models with ours using these annotation labels.ßdummyTurker
10637ß0.0ßTherefore, we do not show such results.ßdummyTurker
10638ß0.0ßFrom the results in Table 4, we believe that our approach is not biased towards using Reduce#n transitions parameterized with a large value of n (which correspond to flat structures).ßdummyTurker
10639ß0.0ßAs can be seen in Table 4, our system has good accuracy not only on constituents with 4 and 5 children, but also on those with 3 or less, outperforming the binary bottom-up algorithm regardless of constituent size.ßdummyTurker
10640ß0.0ßIf the paper is accepted, we will extend Table 2 to include precision and recall.ßdummyTurker
10641ß0.0ßIndeed, as pointed out by the reviewer, the size of the dictionary of non-terminals used by our approach is notably larger than, for instance, that of the binary bottom-up parser.ßdummyTurker
10642ß0.0ßHowever, we can see in the experiments that this does not penalize performance in practice, as modern recurrent neural network models are quite adaptable to a large number of output classes.ßdummyTurker
10643ß0.0ßThis is in line with recent research work in dependency parsing that involves the use of non-local parameterized transitions such as (Qui and Manning, 2017) and (Fernández-González and Gómez-Rodríguez, 2017), where the number of transitions also grows significantly with respect to their baseline algorithms, but the neural network models can still learn them successfully.ßdummyTurker
10644ß0.0ßSorry, we did not notice that our proof is rather similar to the one by Cross and Huang (2016).ßdummyTurker
10645ß0.0ßWe will properly acknowledge this in the final version if accepted.ßdummyTurker
10646ß0.0ßWe will go through the citations to correct the problems with parenthesization.ßdummyTurker
10647ß0.0ßIt is true that larger constituents (in terms of number of children) are less frequent than shorter ones; however, from Table 4, we can see that our system performs well on constituents with, for instance, 4 and 5 children.ßdummyTurker
10648ß0.0ßWe will perform a more thorough study about the frequency of non-binary constituents, to include it in the extra page if accepted.ßdummyTurker
10649ß0.0ßIn Table 2, we only focused on reporting the state of the art in transition-based constituent parsing.ßdummyTurker
10650ß0.0ßIf accepted, we will use the extra page in the final version to include non-shift-reduce parsers as well, as also suggested by Reviewer #3, including Stern et al.ßdummyTurker
10651ß0.0ßWe greatly appreciate the reviewer for all the efforts.ßdummyTurker
10652ß0.0ßThanks for the comments.ßdummyTurker
10653ß0.0ßIt is difficult to improve the performance on SNLI and MultiNLI(which are the most popular NLI datasets now) in recent years, where the state-of-the-art published NLI methods also improved quite modest(e.g.ßdummyTurker
10654ß0.0ß"""Natural Language Inference with External Knowledge""(accuracy 89.1 vs 88.9, 88.6 vs 88.0) in ICLR 2018, ""Natural Language Inference over Interaction Space""(accuracy 88.9 vs 88.8, 88.0 vs 87.5)  in ICLR 2018)."ßdummyTurker
10655ß0.0ßIn this paper we mainly aim to present the helpfulness of using discourse markers for NLI tasks and the experimental results show the effectiveness of our method.ßdummyTurker
10656ß0.0ßIn the sentence encoder model, r1 and r2 have the same dimensionality which is equal to the 4 times dimensionality of the hidden states(we concatenated the results of max pooling and the last hidden states from both directions to represent the whole sentence).ßdummyTurker
10657ß0.0ßThanks for the suggestion, we will elaborate the notations and equations in the extended version of this paper.ßdummyTurker
10658ß0.0ßThe policy gradient upgrades the whole action with the defined reward, which directly optimizes the target, so different loss functions might lead to entirely different results.ßdummyTurker
10659ß0.0ßThe test set of the SNLI has 10000 sentence pairs, and the test set of the MultiNLI has 20000 sentence pairs.ßdummyTurker
10660ß0.0ßWe will add the statistics of the datasets in the extended version of this paper.ßdummyTurker
10661ß0.0ßThe lamda in equation(14) is set as 0.2, which is indicated in the 487th row.ßdummyTurker
10662ß0.0ßWe only present the ablation experiments on SNLI because of the limited pages of the first version of the manuscript, we will add the ablation results on the MultiNLI dataset in the extended version of this paper.ßdummyTurker
10663ß0.0ßWe greatly appreciate the reviewer for all the efforts.ßdummyTurker
10664ß0.0ßThanks for the comments.ßdummyTurker
10665ß0.0ßIt is difficult to improve the performance on SNLI and MultiNLI(which are the most popular NLI datasets now) in recent years, where the state-of-the-art published NLI methods also improved quite modest(e.g.ßdummyTurker
10666ß0.0ß"""Natural Language Inference with External Knowledge""(accuracy 89.1 vs 88.9, 88.6 vs 88.0) in ICLR 2018, ""Natural Language Inference over Interaction Space""(accuracy 88.9 vs 88.8, 88.0 vs 87.5)  in ICLR 2018)."ßdummyTurker
10667ß0.0ßIn this paper we mainly aim to present the helpfulness of using discourse markers for NLI tasks and the experimental results show the effectiveness of our method.ßdummyTurker
10668ß0.0ßThanks for the suggestion, we will polish our writing and correct the typos in the extended version of this paper.ßdummyTurker
10669ß0.0ß"In Table 2, for instance when the label number is 3 in SNLI, ""total=0"" means that no examples have 3 annotators(maybe more or less); ""correct=8748"" means that there are 8748 examples whose number of correct labels is 3(the number of annotators maybe 4 or 5, but some provided wrong labels)."ßdummyTurker
10670ß0.0ßWe greatly appreciate the reviewer for all the efforts.ßdummyTurker
10671ß0.0ßWe noticed the suggestion of ACL that the paper would better not be available online during the anonymity period, and we will publish the extended version of this paper and release our code to the community.ßdummyTurker
10672ß0.0ßThanks for the suggestion, we found that in many cases the two sentences that are separated by sentence-internal discourse markers also have similar patterns to the separate sentences examples, and the size of the dataset would be too small to train a representative DMP model if we only picked out the separate sentences examples.ßdummyTurker
10673ß0.0ßWe will detail our approach about the conditionals in the extended version of this paper.ßdummyTurker
10674ß0.0ßThanks for the suggestion, we will polish our writing in the extended version of this paper.ßdummyTurker
10675ß0.0ßThanks for the suggestion, we will elaborate and update our figures(with their descriptions) and correct the typos in the extended version of this paper.ßdummyTurker
10676ß0.0ßWe are not giving up continuous state representation nor are we claimining that discrete states are more powerful.ßdummyTurker
10677ß0.0ßThe purpose of introducing discrete states is to formulate a model that maintains both discrete and continuous representations so that it provides an elegant way to link a finite-state model (e.g.ßdummyTurker
10678ß0.0ßngram) to an RNN model.ßdummyTurker
10679ß0.0ßAlthough having discrete states can limit performance, it offers several advantages:ßdummyTurker
10680ß0.0ß1.ßdummyTurker
10681ß0.0ßIt can be converted into either an ngram model or an RNN model.ßdummyTurker
10682ß0.0ßThe latter can be used as initialization to train a regular RNN LM (as shown in the paper).ßdummyTurker
10683ß0.0ß2.ßdummyTurker
10684ß0.0ßThe training is decoupled such that it can be trained like a non-recurrent model (no need to worry about padding and bucketing when batching sequences of variable lengths).ßdummyTurker
10685ß0.0ß3.ßdummyTurker
10686ß0.0ßCan be veiwed as student-teacher training where the teacher is a finite-state model (FSM).ßdummyTurker
10687ß0.0ßWe showed, by using ngram model as an example, that we can train RNN LM to behave like an n-gram model.ßdummyTurker
10688ß0.0ßThis can be extended to other FSM (e.g.ßdummyTurker
10689ß0.0ßa grammar created from syntactic rules).ßdummyTurker
10690ß0.0ßWe feel that such a formulation provides more insights and open up the possibility of integrating n-gram models (and other finite-state models) to train a better RNN model.ßdummyTurker
10691ß0.0ßWe hope that the reviewer will reconsider the above as the strengths of the paper and kindly provide more constuctive critisisms based on those aspects.ßdummyTurker
10692ß0.0ßThe primary focus of the paper is to use stimulated RNN to establish a more direct connection between n-gram (or finite-state) and RNN models.ßdummyTurker
10693ß0.0ßWe believe that the specific configurations used to train the baseline models are not critical for us to proof our points that it is indeed possible to train an RNN model using stimulated learning in a decoupled fashion to mimic the behaviour of an n-gram model and that it can serve as an initial model to train a better RNN model.ßdummyTurker
10694ß0.0ßsRNN can be converted to a regular RNN for inference (i.e.ßdummyTurker
10695ß0.0ßthe params associated with the state embeddings are not needed during inference).ßdummyTurker
10696ß0.0ßSo the inference cost and storage will be the same as the underlying regular RNN.ßdummyTurker
10697ß0.0ßScaling up to larger models is a challenge, but not entirely impossible.ßdummyTurker
10698ß0.0ßWe would like to highlight that the stimulated RNN model has the following nice properties/interpretation:ßdummyTurker
10699ß0.0ß1.ßdummyTurker
10700ß0.0ßThe discrete states provide a convenient way to decouple the recurrent connections so that sRNN can be trained like a non-recurrent model.ßdummyTurker
10701ß0.0ß2.ßdummyTurker
10702ß0.0ßIt is a form of student-teacher training that enables transfer learning from FSM to RNN, which is useful for integrating/combining these two types of models.ßdummyTurker
10703ß0.0ßWe hope that the reviewer will weight in the above favorably.ßdummyTurker
10704ß0.0ßWe are using in-house datasets (anonymized to conform to double-blind review guideline).ßdummyTurker
10705ß0.0ßMore details can be included should the paper be accepted.ßdummyTurker
10706ß0.0ßBy reporting results on 3 datasets, we hope that the reviewers are more confident that the results are replicable.ßdummyTurker
10707ß0.0ßNo regularization was used.ßdummyTurker
10708ß0.0ßWe used a held-out development set to pick the best checkpoint based on perplexity.ßdummyTurker
10709ß0.0ßOur goal is to show that stimulated learning allows us to train RNN models that behave similar to an n-gram model and such a model serves as a better initialization for subsequent training.ßdummyTurker
10710ß0.0ßWe do not think that it is critical to aggressively tune the models to proof our points.ßdummyTurker
10711ß0.0ßWe agree with the reviewer that multiple random runs will give more informative comparison.ßdummyTurker
10712ß0.0ßSince we are getting consistent results across three eval sets, we hope that the reviewer will still view the results as useful and encouraging.ßdummyTurker
10713ß0.0ßThank you for pointing out the work on cache models and memory network.ßdummyTurker
10714ß0.0ßWe agree that they are very relevant and will cite them should the paper be accepted.ßdummyTurker
10715ß0.0ßWe will clarify that the paper focuses on character-level language model should the paper be accepted.ßdummyTurker
10716ß0.0ßHowever, we would like to point out that scaling up to larger models is a challenging (but not necessarily impractical) problem to be addressed in future work.ßdummyTurker
10717ß0.0ßOur dataset comes from a recently released dataset, which contains questions, documents and coarse-grain annotations on whether or not an entire assertion is a correct answer.ßdummyTurker
10718ß0.0ßHowever, these coarse-grain annotations could not support explicit reasoning and deep question understanding.ßdummyTurker
10719ß0.0ßTo address these issues, we make a huge effort to do fine-grained annotations, resulting in a relatively large dataset, which we plan to release to the community.ßdummyTurker
10720ß0.0ßWe did not over claim the contribution of our work in terms of methodology.ßdummyTurker
10721ß0.0ßYes, these basic modules are standard neural network models.ßdummyTurker
10722ß0.0ßOur focus is to examine the performances of existing strong KBQA methods and investigate whether incorporating external KB could bring improvement.ßdummyTurker
10723ß0.0ßOur approach could also be applied to other raw documents after we preprocess these documents with openIE toolkit automatically.ßdummyTurker
10724ß0.0ßHowever, existing datasets typically do not require knowledge-driven reasoning, therefore the recall of our approach might be low.ßdummyTurker
10725ß0.0ßNevertheless, we could report results on a dataset of raw text documents.ßdummyTurker
10726ß0.0ßWe'll polish the article to fix them.ßdummyTurker
10727ß0.0ßIn our dataset, questions and passages are collected from query logs of a commercial search engine in order to reflect the real information need of users.ßdummyTurker
10728ß0.0ßFor each question-passage pair, we generate assertions by a powerful open-IE system.ßdummyTurker
10729ß0.0ßTherefore, the collection of questions is independent of the assertions.ßdummyTurker
10730ß0.0ßHuman annotators are asked to label whether or not a subject or an object is the correct answer.ßdummyTurker
10731ß0.0ßWe ignore the subject/object if it occurs in a meaningless or incorrect assertion (subject, predicate, object).ßdummyTurker
10732ß0.0ßThe input of our task includes a question, a document and assertions automatically extracted from the document.ßdummyTurker
10733ß0.0ßThe output is a subject or an object from the assertions which could correctly answer the question.ßdummyTurker
10734ß0.0ßThe task could be viewed as a ranking problem, in which candidate answers come from subjects and objects of the assertions.ßdummyTurker
10735ß0.0ßPlease kindly refer to line 116-124for a formal definition.ßdummyTurker
10736ß0.0ßOur model uses open KBs as external knowledge to enrich the semantics of documents.ßdummyTurker
10737ß0.0ßDespite it is not intuitive to enrich one KB with another KB, we could run our approach on the dataset of Bordes and report a number.ßdummyTurker
10738ß0.0ßThe larger research question we address in our paper is whether eye movement data (underused in NLP tasks with images), in conjunction with linguistic data, can help us bridge the semantic gap in image understanding.ßdummyTurker
10739ß0.0ßOur main contributions are: (1) the creation and release of a unique multimodal dataset for exploring that question, and (2) empirical results on representative experiments we have carried out on that dataset to successfully annotate image regions by combining gaze and linguistic data.ßdummyTurker
10740ß0.0ßWe recognize that our AER is not as low as one might achieve in a unimodal (text-to-text) machine translation scenario.ßdummyTurker
10741ß0.0ßHowever, given the multimodal task, the large number of image regions to be labeled, the large number of spoken words in a given narrative, and the high percentage of words in a typical narrative that do not pertain to regions in the image, our results are competitive and within a reasonable range.ßdummyTurker
10742ß0.0ßWe also demonstrate that our method of aligning spoken words with their associated objects in an image is more accurate than baselines in which gaze and utterance are either simultaneous or separated by a fixed time delay.ßdummyTurker
10743ß0.0ßThe manual corrections of the ASR transcripts for those 5 images were produced in order to verify the quality of the ASR output.ßdummyTurker
10744ß0.0ßWe found the WER to be remarkably low (5%), demonstrating the viability of using ASR to automate the transcription of the narratives.ßdummyTurker
10745ß0.0ßWe agree that our dataset can serve researchers outside NLP, including psycholinguists.ßdummyTurker
10746ß0.0ßSpontaneous speech coupled with eye-tracking data could be useful in answering questions about how humans produce language when engaging with visual tasks.ßdummyTurker
10747ß0.0ßParallel data streams can, for example, help in investigating questions such as the effect of word complexity or frequency on language formation and production.ßdummyTurker
10748ß0.0ßIt might also aid studies of syntactic constructions and argument structure, and how they relate to visual perception.ßdummyTurker
10749ß0.0ßQualitative analysis of our transcripts indicates that they contain some emotional information in the form of holistic comments on the overall effect of the images, which could be helpful in affective visual or linguistic computing tasks.ßdummyTurker
10750ß0.0ßYes, the calibration values are in degrees: 1 degree would translate to approx.ßdummyTurker
10751ß0.0ß40 pixels in our set-up, therefore our mean calibration accuracy was roughly within 38 pixels.ßdummyTurker
10752ß0.0ßThe speech transcriptions are, indeed, timed aligned at the word level with the eye-tracking data.ßdummyTurker
10753ß0.0ßThe cloud ASR API we use, IBM Watson, reports timestamps for each word, and those timestamps are included in the released dataset, which will allow other researchers to investigate the temporal relationship between utterance and gaze.ßdummyTurker
10754ß0.0ßAs in many such studies, the participants in our study were university students.ßdummyTurker
10755ß0.0ßWe sought out participants who were speakers of American English in order to ensure reliable ASR output and a consistent vocabulary across participants.ßdummyTurker
10756ß0.0ßWe will clarify this in the paper.ßdummyTurker
10757ß0.0ßNaturally, we would like to have more data.ßdummyTurker
10758ß0.0ßBecause our data collection procedure involves high-quality eye tracking and audio recording equipment, popular crowdsourcing methods for large-scale human data collection, such as Mechanical Turk, would not be feasible.ßdummyTurker
10759ß0.0ßThat said, eye-tracking equipment and software will in the near future be more widely available.ßdummyTurker
10760ß0.0ßOur dataset is a step toward the collection of larger multimodal datasets using such anticipated eye-tracking technology.ßdummyTurker
10761ß0.0ßThe participants were given a package of Milano cookies along with a choice between entering into a raffle to win one of two $25 gift cards or receiving course credits.ßdummyTurker
10762ß0.0ßThe participants were cooperative and enthusiastic.ßdummyTurker
10763ß0.0ßWe did not observe any significant variation in the quality or amount of data produced.ßdummyTurker
10764ß0.0ßBecause our paper involves an NLP resource and innovatively applies established NLP methods to produce included empirical results, we are confident that it is an appropriate submission for the Resources and Evaluation track at ACL.ßdummyTurker
10765ß0.0ßWe use ASR to generate our transcriptions.ßdummyTurker
10766ß0.0ßIncluded with those transcriptions are timestamps in milliseconds for each word, trivially enabling alignment with the eye-movement data and the audio signal.ßdummyTurker
10767ß0.0ßWe use ASR to generate the transcriptions.ßdummyTurker
10768ß0.0ßWe report in the paper that the word error rate (the standard evaluation metric for ASR) is 5%, which is the current state of the art.ßdummyTurker
10769ß0.0ßThe API we used, IBM Watson, uses special tokens for hesitations and unfilled pauses, which we remove from our transcripts before applying our alignment method since such tokens could not correspond to image regions.ßdummyTurker
10770ß0.0ßAs discussed in the paper, we parse the linguistic input, which allows us to retain only nouns and adjectives and to remove verbs, adverbs, function words, and disfluencies.ßdummyTurker
10771ß0.0ßRepetitions of nouns and adjectives are retained.ßdummyTurker
10772ß0.0ßSince our goal is to label objects in an image, it makes sense to exclude verbs describing actions within the image.ßdummyTurker
10773ß0.0ßThe data release includes the complete spoken descriptions--including verbs and full utterances--and such information can be used by researchers interested in the dataset for other research questions, tasks, and applications.ßdummyTurker
10774ß0.0ßIn fact, this very point is exactly the one we make in the paper by using a simultaneous baseline and a fixed time delay baseline.ßdummyTurker
10775ß0.0ßPeople do not always utter the name of an object at the exact moment they gaze upon it or after a fixed interval.ßdummyTurker
10776ß0.0ßThat is why we have developed an alignment framework that does not explicitly rely on time information.ßdummyTurker
10777ß0.0ßIn the paper, we do provide statistics about the dataset such as average duration of narrative, average fixation duration, and type-token ratio per image, but you are correct that we inadvertently omitted the total number of words in the corpus.ßdummyTurker
10778ß0.0ßWe will include this information, and information about the linguistic variability across participants (of which there was very little), in the final version of the paper.ßdummyTurker
10779ß0.0ßWe are confident that there is sufficient information within the paper to understand the data collection, the methods, and the experiments.ßdummyTurker
10780ß0.0ßWe add, however, that the focus of this paper is the multimodal dataset that we have collected and are releasing with the paper.ßdummyTurker
10781ß0.0ßIn this paper we transformed word embedding of a text document into an artificial image, then we used CNN for text classification.ßdummyTurker
10782ß0.0ßTo the best of our knowledge, this is the first work that transforms word embedding into an image exploiting capabilities of deep CNNs for text classification.ßdummyTurker
10783ß0.0ßOther CNNs for text classification, such as the one proposed by Zhang et al., modified the standard CNN architecture introducing ad-hoc 1-d convolutions, particular kernels and specific embedding layers.ßdummyTurker
10784ß0.0ßResults clearly indicate that the proposed encoding scheme achieves promising results for text classification.ßdummyTurker
10785ß0.0ßFurthermore, our focus was to present a pipeline able to cast a text document into an image rather than improving existing network architectures.ßdummyTurker
10786ß0.0ßWe experimented feeding directly text to an ad-hoc model but results were not as good as the proposed approach, however, we argued that results are similar to ones proposed by Zhang et al., to simplify we did not include them.ßdummyTurker
10787ß0.0ßOur motivation to use a ResNet network is to show readers that achieved results of our scheme can be further improved with state-of-the-art models.ßdummyTurker
10788ß0.0ßWe used a small scale dataset (20news-bydate) to illustrate this observation, as shown in Table 3.ßdummyTurker
10789ß0.0ßTable 3 represents the last part of our experiments, we will reformat its position for better understanding.ßdummyTurker
10790ß0.0ßFinal results do not include ResNet experiment, because the large scale dataset used in this work takes considerable resources.ßdummyTurker
10791ß0.0ßIn our approach we transformed a text document into a single artificial image, preserving relationships between words.ßdummyTurker
10792ß0.0ßA toy example is available in Fig.ßdummyTurker
10793ß0.0ß1.ßdummyTurker
10794ß0.0ßIn case of long text document converted using small image size, we may lose information.ßdummyTurker
10795ß0.0ßWe conducted experiments on 20news dataset, containing long and short text documents and results demonstrate that important information is preserved.ßdummyTurker
10796ß0.0ßThank you so much for pointing strengths and weaknesses of the paper.ßdummyTurker
10797ß0.0ßIn this paper we transformed word embedding of a text document into an artificial image, then we used CNN for text classification.ßdummyTurker
10798ß0.0ßTo the best of our knowledge, this is the first work that transforms word embedding into an image.ßdummyTurker
10799ß0.0ßResults clearly indicate that the proposed encoding scheme achieves promising results.ßdummyTurker
10800ß0.0ßFurthermore, our focus was to present a pipeline able to cast a text document into an image rather than improving existing network architectures.ßdummyTurker
10801ß0.0ßOur motivation to use a ResNet network is to show readers that achieved results of our scheme can be further improved with state-of-the-art models.ßdummyTurker
10802ß0.0ßWe used a small scale dataset (20news-bydate) to illustrate this observation, as shown in Table 3.ßdummyTurker
10803ß0.0ßTable 3 represents the last part of our experiments, we will reformat its position for better understanding.ßdummyTurker
10804ß0.0ßFinal results do not include ResNet experiment, because the large scale dataset used in this work takes considerable resources.ßdummyTurker
10805ß0.0ßThank you so much for pointing strenghts and weaknesses of the paper.ßdummyTurker
10806ß0.0ßThere are some work available in the literature that use directly Word2Vec features as input to a CNN, however, we showed that encoding Word2Vec features into an image a standard CNN produces better results.ßdummyTurker
10807ß0.0ßBecause CNNs are the state-of-the-art approach for image analysis.ßdummyTurker
10808ß0.0ßWe exploit full potential of CNN networks, transforming text into an image with our encoding approach.ßdummyTurker
10809ß0.0ßWe compared our results with Zhang et al., and Conneau et al..ßdummyTurker
10810ß0.0ßBoth papers included percentage error as performance parameter, which led us to use it.ßdummyTurker
10811ß0.0ßThank you for the suggestion of the paper, we will use it in the future work.ßdummyTurker
10812ß0.0ß"We used the configuration described in ""ImageNet Classification with Deep Convolutional"ßdummyTurker
10813ß0.0ß"Neural Networks"", but you are right, we will include parameters and hyper-parameters in the experiments section."ßdummyTurker
10814ß0.0ßThank you so much for pointing strengths and weaknesses of the paper.ßdummyTurker
10815ß0.0ßThe whole process is sensible.ßdummyTurker
10816ß0.0ßIn the revision, we perform the evaluation of the model with POS tags and with time stamps, respectively.ßdummyTurker
10817ß0.0ßThe cosine similarity value is between 0 and 1.ßdummyTurker
10818ß0.0ßThe threshold of median value (0.5) can reflect the performance of the systems more objectively.ßdummyTurker
10819ß0.0ßWe also did the evaluation with different thresholds.ßdummyTurker
10820ß0.0ßHowever, the results with other thresholds were not recorded due to the space limitations.ßdummyTurker
10821ß0.0ßFor example, in terms of F1-score, 0 for PPMI and SVD, respectively, 0.0167 for SGNS, 0.0169 for Ours under the threshold of 0.1; 0.0122 for PPMI, 0.0151 for SVD, 0.0168 for SGNS, 0.0195 for Ours under the threshold of 0.7.ßdummyTurker
10822ß0.0ßWe perform the evaluation with POS tags and with time stamps, respectively in the revision.ßdummyTurker
10823ß0.0ßWe will do the work in the revision.ßdummyTurker
10824ß0.0ßThe model (Fig.ßdummyTurker
10825ß0.0ß1) reads the current word and its POS, producing neighbor words.ßdummyTurker
10826ß0.0ßBy training the model, we learn the embeddings.ßdummyTurker
10827ß0.0ßBasically, more information will help learning word embeddings.ßdummyTurker
10828ß0.0ßThe whole process is sensible.ßdummyTurker
10829ß0.0ßWe will perform an evaluation of the model with POS tags in the revision.ßdummyTurker
10830ß0.0ßThe archaic words are true positives.ßdummyTurker
10831ß0.0ßAnd the other words (excluding true positives) are true negatives.ßdummyTurker
10832ß0.0ßWe tried on different vocabularies we could find.ßdummyTurker
10833ß0.0ßWe’ve found that the set of archaic words is most suitable for our task.ßdummyTurker
10834ß0.0ßIn fact, the set of archaic words is only a small part of the whole words with semantic shift.ßdummyTurker
10835ß0.0ßAs far as we know, there is not any vocabulary comprehensively covering all words with semantic shift.ßdummyTurker
10836ß0.0ßAs far as we know the paper is the first attempt to leverage ground truth for classification in diachronic analysis of words.ßdummyTurker
10837ß0.0ßWe tried on different vocabularies we could find.ßdummyTurker
10838ß0.0ßWe’ve found that the set of archaic words (436 words) is most suitable for our task.ßdummyTurker
10839ß0.0ßIn fact, the set of archaic words is only a small part of the whole words with semantic shift.ßdummyTurker
10840ß0.0ßThat is why precision and recall scores are lower.ßdummyTurker
10841ß0.0ßFor details of 'Archaic Words', please refer to ‘httpsßdummyTurker
10842ß0.0ßEncoding a word’s information into a vector is the way of representing the word in neural networks.ßdummyTurker
10843ß0.0ßMore information (POS) would help improve the representation of the word.ßdummyTurker
10844ß0.0ßAnd the improvement would help learn more accurate word embeddings through training.ßdummyTurker
10845ß0.0ßIn this paper, the word is encoded into a vector (the left block in PROJECTION layer in Fig.ßdummyTurker
10846ß0.0ß1) and its POS is encoded into another vector (the right block), then they two vectors are concatenated as a bigger vector (the block in CONCATENATION layer).ßdummyTurker
10847ß0.0ßThere are 436 words in the 'Archaic Words'.ßdummyTurker
10848ß0.0ßThe size of the full set of target words is more than 250 billion.ßdummyTurker
10849ß0.0ßAnd the size of the vocabulary is 12995.ßdummyTurker
10850ß0.0ßThis set came from the Google Books N-gram dataset where stop words and the words with less than 100 appearances were removed.ßdummyTurker
10851ß0.0ßThe word is considered evolved between any two periods when the cosine similarity of its two time-aware embeddings is less than a predefined threshold, or is unchanged in meaning.ßdummyTurker
10852ß0.0ßThe cosine similarity value is between 0 and 1.ßdummyTurker
10853ß0.0ßThe threshold of median value (0.5) can reflect the performance of the systems more objectively.ßdummyTurker
10854ß0.0ßWe also did the evaluation with different thresholds.ßdummyTurker
10855ß0.0ßHowever, the results with other thresholds were not recorded due to the paper limitations.ßdummyTurker
10856ß0.0ßGrammatical errors and typos were corrected.ßdummyTurker
10857ß0.0ßThe presentation of the revision was improved accordingly.ßdummyTurker
10858ß0.0ßThank you for your comments.ßdummyTurker
10859ß0.0ß"""the best feature selection method is different for each dataset"" is only one of results according to the analysis, and it is not the most important conclusion."ßdummyTurker
10860ß0.0ßThe point of this article is that some feature selection methods tend to be useful all the time.ßdummyTurker
10861ß0.0ßThank you for your comments.ßdummyTurker
10862ß0.0ßwe will revise these expressions in more rigorous way.ßdummyTurker
10863ß0.0ßAbout Figure 1, it is not the purpose to compare the results.ßdummyTurker
10864ß0.0ßFigure 1 has two uses: one, Figure 1 was made by increasing one by one from two features, although arched curves were preferred, the result shows that not all of methods managed to obtain arched curves,perhaps this can become one way to evaluate methods; On the other hand, through Figure 1, we want to explain how the best number was decided.ßdummyTurker
10865ß0.0ßBecause of the limited pages, these figures were shown in such small size.ßdummyTurker
10866ß0.0ßwe will resize them.ßdummyTurker
10867ß0.0ßThere are two reasons why we wrote very few previous papers: (i) a lot of pervious papers exist, however, many previous papers made the same (or similar) conclusions, so some are picked up as representatives and (ii) because ACL is a high-level conference, it's thought that there was no need to explain too much, and also because there are limited pages, space was wanted to be left to explain the analysis as detailed as possible and put focus on the analysis.ßdummyTurker
10868ß0.0ßThank you for your additional comments.ßdummyTurker
10869ß0.0ßIt is very helpful.ßdummyTurker
10870ß0.0ßThere are two reasons why few previous papers were mentioned: (i) a lot of pervious papers exist, however, many previous papers made the same (or similar) conclusions, so some were picked up as representatives and (ii) because ACL is a high-level conference, it's thought that there was no need to explain too much, and also because there are limited pages, space was wanted to be left to explain the analysis as detailed as possible (it seems that I failed to explain Section 4 very well).ßdummyTurker
10871ß0.0ß(i)In abstract, “some methods can always extract useful information to discriminate the classes” explains the general versatility.ßdummyTurker
10872ß0.0ß(ii) Thank you for your comments.ßdummyTurker
10873ß0.0ßwe will add references.ßdummyTurker
10874ß0.0ßI agree with your comments, so twenty-two methods, six types of feature datasets extracted from three types of languages were used, and this should be able to obtain reliable conclusions.ßdummyTurker
10875ß0.0ßBut we agree that a much more thorough evaluation of datasets and features is better.ßdummyTurker
10876ß0.0ß(iii) About datasets, because this article not only focus on binary classification, but also multiclass classification, three authors of Japanese and Chinese were chosen randomly, and all of articles were novels.ßdummyTurker
10877ß0.0ßFor English corpus, because spam dataset has been used in many previous research, the results can be compared with previous research.ßdummyTurker
10878ß0.0ß(iv)ßdummyTurker
10879ß0.0ß(1) About the feature selection methods, all of used methods this time have been mentioned as good or not so good methods in previous papers, this article put them together and compared.ßdummyTurker
10880ß0.0ßIt is true that there are still over one hundred methods and others were not discussed this time.ßdummyTurker
10881ß0.0ß(2) “what Symmetrical Uncertainty from FSelector is”ßdummyTurker
10882ß0.0ßFSelector is the name of R package.ßdummyTurker
10883ß0.0ßThe title of table 4 was named, but if it is not clear enough, we will explain it in article.ßdummyTurker
10884ß0.0ß(3) In page 2, second paragraph, “SVM was proved to be a more reliable feature selection algorithm.ßdummyTurker
10885ß0.0ßFurther, as SVM performs classifications without feature selection, it is used to avoid double selection in this article.”ßdummyTurker
10886ß0.0ß(v) We will revise the English again, thank you for your comments.ßdummyTurker
10887ß0.0ß(vi) About the new weighting of TF-IDF, an example was made to explain why the addition should not be used and the subtractions was optimal in this article.ßdummyTurker
10888ß0.0ßThe correction of Equation (7) can be certificated by (i) the definitions of df, tf…TF-IDF and (ii) the power of features to classify different classes.ßdummyTurker
10889ß0.0ß(vii) About Figure 1, it is not the purpose to compare feature selection methods.ßdummyTurker
10890ß0.0ßFigure 1 has two uses: one, Figure 1 was made by increasing one by one from two features, although arched curves were preferred, the result shows that not all of methods managed to obtain arched curves, however, perhaps this can become one method to evaluate methods; On the other hand, through Figure 1, we want to explain how the best number was decided.ßdummyTurker
10891ß0.0ßThanks for the comment.ßdummyTurker
10892ß0.0ßDue to space limit, we massaged the related work discussion into Section 1&2.ßdummyTurker
10893ß0.0ßWe totally agree that adding a section to explicitly discuss related work would be more helpful and we will certainly do that in the revision.ßdummyTurker
10894ß0.0ßWe actually tried a few popular learning-to-rank/classification algorithms and their performances are very similar.ßdummyTurker
10895ß0.0ßFor efficiency and interpretation reasons, we chose to use boosted tree based algorithms.ßdummyTurker
10896ß0.0ßWe will certainly revise the paper to discuss and justify the choices.ßdummyTurker
10897ß0.0ßWe will revise paper to make this section clear and provide some examples to better illustrate the process.ßdummyTurker
10898ß0.0ßThis is a very good question.ßdummyTurker
10899ß0.0ßIn this paper we utilized a simple rule-based acronym detection module to detect acronyms.ßdummyTurker
10900ß0.0ßBasically this module will try to check if a word matches any of our predefined acronym patterns (e.g., XXX, X.X.X, X-X-X where X is an uppercase letter) and at the same time its uppercase representation is not an English word (e.g., 'OK' will not be treated as an acronym).ßdummyTurker
10901ß0.0ßWe don't have special treatment for stopwords (e.g., we think 'THE' is a valid acronym).ßdummyTurker
10902ß0.0ßFor the 'DIE' example you mentioned, we also treat it as an acronym as long as all three letters are in uppercase.ßdummyTurker
10903ß0.0ßWe will include all these details and discussions in the revision.ßdummyTurker
10904ß0.0ßThe task of acronym detection is also interesting and important, But due to the space limit, the optimization of acronym detection is beyond the scope of this paper.ßdummyTurker
10905ß0.0ß"See ""Reply to weakness argument 2""."ßdummyTurker
10906ß0.0ßThanks for pointing out this.ßdummyTurker
10907ß0.0ßWe tried to vary the window size from 10 to 50 (i.e., 10,20,30,40,50) and found 30 gave the best result.ßdummyTurker
10908ß0.0ßDue to space limit we didn't include this window size study in the paper.ßdummyTurker
10909ß0.0ßWe will certainly include it in the revision to justify the choice.ßdummyTurker
10910ß0.0ßWe will revise paper to include all the missing details.ßdummyTurker
10911ß0.0ßWe will add an example to the Problem Statement section.ßdummyTurker
10912ß0.0ßThanks for all the suggestions on better organizing the Framework section.ßdummyTurker
10913ß0.0ßWe will properly address them.ßdummyTurker
10914ß0.0ß"- The if-condition in line 254 is an ""either-if-or"", in other words, the system will only search public KB if condition(1) in line 254 is not satisfied."ßdummyTurker
10915ß0.0ß- We will revise to include reference to Entity Linking.ßdummyTurker
10916ß0.0ß"- Window size choice, please see ""Reply to Question 3 for Reviewer #1""."ßdummyTurker
10917ß0.0ß- The Wikipedia page will be used for context extraction only if the meaning candidate can be found in Wikipedia.ßdummyTurker
10918ß0.0ßThis is a very good question.ßdummyTurker
10919ß0.0ß"Indeed we cannot guarantee the training data generated via distant supervision is perfect, so we will wrongly treat ""Apple Store"" as a candidate of ""AS""."ßdummyTurker
10920ß0.0ßWe evaluated the quality of the mined acronyms/meanings in Section 6.3, which shows that the candidate generation via distant supervision can achieve reasonably good performance (94.5% precision).ßdummyTurker
10921ß0.0ßThanks for the suggestions on making Experiments section more clear.ßdummyTurker
10922ß0.0ßWe will move Figure6 to the bottom and add more descriptive captions.ßdummyTurker
10923ß0.0ßAs answered to Reviewer#1, this is a first approximation to the problem (work in progress, which is a type of short paper at ACL).ßdummyTurker
10924ß0.0ßWe will investigate further preprocessing techniques as future work.ßdummyTurker
10925ß0.0ßThank you for the suggestion.ßdummyTurker
10926ß0.0ßWe will inspect other models as well, as the one suggested.ßdummyTurker
10927ß0.0ßIn this paper we did not investigate the power of pre-trained word embeddings in these tasks, as this has been already shown in many previous works (Kim, 2014 for example).ßdummyTurker
10928ß0.0ßWe will clarify this in the paper.ßdummyTurker
10929ß0.0ßThank you for the suggestion.ßdummyTurker
10930ß0.0ßGiven that this was a first approximation to the problem, we tried to have diverse preprocessing techniques.ßdummyTurker
10931ß0.0ßStemming would be more similar to lemmatisation and using an entity tagger to multiword grouping.ßdummyTurker
10932ß0.0ßBut of course we will try to explore also these techniques on our future work extending this preliminary work.ßdummyTurker
10933ß0.0ßThe results as displayed in the tables showed no improvements overall by using lemmatisation and lowercasing (taking average of the performance in all datasets for example)ßdummyTurker
10934ß0.0ßThat is true, as answered above, we will try to extend this work to take into account all these other interesting experiments (this was presented as a short paper as it is still work in progress, as a preliminary approach to the problem)ßdummyTurker
10935ß0.0ßThank you for the reference, it would be in fact interesting to perform a deeper analysis to take into account the interactions.ßdummyTurker
10936ß0.0ß> We tried to construct very controlled contrasts such as frequency in training corpus, usages (with or without intransitive usage) and etc.ßdummyTurker
10937ß0.0ßAlso we tried to increase sentences as much as possible.ßdummyTurker
10938ß0.0ßOf course, it is best to create more natural and much more test sets along with keeping such controlled contrasts.ßdummyTurker
10939ß0.0ßWe are thinking maybe we should cooperate with linguists next.ßdummyTurker
10940ß0.0ß"> In terms of the example about PP-attachment (""John on Monday introduces advisors."ßdummyTurker
10941ß0.0ß"""), this kind of structure is actually seen in English treebank of UD and we found w/ attention system succeeded to analyze those sentences in test session, which other systems failed."ßdummyTurker
10942ß0.0ß> If attention avoid the error from incorrect tags, it is very interesting as you said.ßdummyTurker
10943ß0.0ß> Although it is very limited case, the w/ attention system comparatively worked well in predicted tag conditions in the augmented set experiment.ßdummyTurker
10944ß0.0ßSo it would value to explore the possibility in detail.ßdummyTurker
10945ß0.0ß> We just wanted to show the results in the more traditional way (with gold tag) and in the more recent way (with predicted tag) and we omit a portion because the limitation of paper length.ßdummyTurker
10946ß0.0ßMaybe, we can add the result of both gold/predicted processed results for each dev and test sets if accepted.ßdummyTurker
10947ß0.0ß> Yes, they are different, but they are not so helpful to distinguish the attachment unlike the second experiment.ßdummyTurker
10948ß0.0ß> I agree that it's better to use more natural sentences as possible.ßdummyTurker
10949ß0.0ß> We tried to construct very controlled contrasts such as frequency in training corpus, usages(with or without intransitive usage) and etc.ßdummyTurker
10950ß0.0ßTo keep such contrasts,  we unfortunately give up to create the perfect naturality, however, this trade-off is often seen in psycholingustic studies.ßdummyTurker
10951ß0.0ß> Also, we are thinking that we can observe the effectiveness of our method with such sentences which are rare or nothing in training corpus (and in general texts), then it will also work well with natural sentences which parsers can see in training.ßdummyTurker
10952ß0.0ß> In the proceeding of UDPipe v1.2 (http://universaldependencies.org/conll17/proceedings/pdf/K17-3009.pdf, section 4), the changes from v1.1 to v1.2 are seen in some hyperparameters or data split.ßdummyTurker
10953ß0.0ßIf we also do tuning hyperparameters for each treebank, we can expect more performance gains possibly.ßdummyTurker
10954ß0.0ß> The reason to use UDPipe v1.1 is its publicity.ßdummyTurker
10955ß0.0ßAnyone can easily access the resources it used (data split, pre-trained embeddings, models).ßdummyTurker
10956ß0.0ß> Also we consider that the comparison between w/ or w/o attentions is important to assess the capability and UDPipe is just a reference.ßdummyTurker
10957ß0.0ßBecause our system and UDPipe has different architectures (hyperparameters, transition oracle derivation, etc.)ßdummyTurker
10958ß0.0ßeven though both systems base on Chen & Manning (2014).ßdummyTurker
10959ß0.0ß> Please note that our method can be applied any systems using MLP, so UDPipe's performance will also increase by attention.ßdummyTurker
10960ß0.0ß> We realize the lack of references.ßdummyTurker
10961ß0.0ßWe gave up to include them because of the paper length.ßdummyTurker
10962ß0.0ß> If accepted, we add references about attention mechanism and parsing with attention in additional page.ßdummyTurker
10963ß0.0ß> We assume situations where parser cannot see future inputs such as simultaneous translation or dialogue analysis, however, such applications to use dependency parser has not been developed so far unfortunately.ßdummyTurker
10964ß0.0ß> This time, we focus on to confirm that our method works in really incremental situation, so the investigation about the effectiveness in downstream task is what we want to do from now.ßdummyTurker
10965ß0.0ß> I appreciate you to evaluate the error analysis.ßdummyTurker
10966ß0.0ßMaybe we can reconsider the whole construction of this paper.ßdummyTurker
10967ß0.0ß> Briefly, I checked the biases of language types by my eyes, but, I couldn't find specific patterns related to our experiment (w/ or w/o attention).ßdummyTurker
10968ß0.0ß> Our method can be applied to any system using MLP, so probably it also helps to focus on important tokens in joint model as well.ßdummyTurker
10969ß0.0ß> Yes, parsers would achieve correct analysis given such rich information.ßdummyTurker
10970ß0.0ß> And it's important to check whether our attention is still helpful even with rich information or not.ßdummyTurker
10971ß0.0ßThe datasets used in our paper to evaluate word similarity are 2 gold standard datasets (Wordsim-353, RG-65) and 4 widely-used datasets (Rare-Word, SCWS, Men-3k, WS-353-REL), where Men-3k contains 3000 word pairs.ßdummyTurker
10972ß0.0ßFor further evaluation, we will test our models on other larger datasets.ßdummyTurker
10973ß0.0ßWord Similarity is a basic task used in many related approaches to evaluate the ability of trained word embeddings to capture semantic similarities from training corpora, and it is regarded as a basic quality metric of embeddings.ßdummyTurker
10974ß0.0ßSyntactic Analogy task is conducted to evaluate the ability of our models to capture the syntactic property as well as relational similarity from the training corpus.ßdummyTurker
10975ß0.0ßThank you for your helpful suggestion, and we will test our models on some other NLP tasks, like text classification.ßdummyTurker
10976ß0.0ßWe will review and correct the hyphenation of our paper.ßdummyTurker
10977ß0.0ßWe will address your concerns.ßdummyTurker
10978ß0.0ß(1) In general, the LMM-S probably is the best and most robust model, since it pays more attention to more related latent meanings (compared to LMM-A) and it doesn't completely count on the only latent meaning (compared to LMM-M).ßdummyTurker
10979ß0.0ß(2) Different models have different biases towards word embeddings and the performance of our models may be influenced by many factors such as training corpus, test datasets, super-parameters and so on.ßdummyTurker
10980ß0.0ßSimilar phenomenons can also be found in some other work like [1] (listed below).ßdummyTurker
10981ß0.0ßThe lookup table can be derived from the morphological lexicons of each language.ßdummyTurker
10982ß0.0ßTo create the lookup table, we need to extract the morphemes and their meanings from the lexicons, which is a little time-consuming (based on the given resources, it takes several hours to create our lookup).ßdummyTurker
10983ß0.0ßThe good news is we can create it once for all since it represents the common knowledge of a certain language.ßdummyTurker
10984ß0.0ß(1) It's around 38%.ßdummyTurker
10985ß0.0ß(2) Incorrect morpheme segmentations can influence the training process to a certain extent, which might be responsible for giving different results across different datasets.ßdummyTurker
10986ß0.0ßThe weights allocated to latent meanings can usually weaken the influences of incorrect segmentations.ßdummyTurker
10987ß0.0ßUsually, false positives are caused by incorrect segmentations, and they may be incorrectly tuned to the latent meanings of the morphemes that don't belong to them.ßdummyTurker
10988ß0.0ßCompared with the effect of incorrect segmentations, the co-occurrence counts actually dominate the generation of word embeddings.ßdummyTurker
10989ß0.0ßThe proposed models mainly emphasize the thought of incorporating latent meanings of morphemes rather than explicit morphemes to train word embedings, and they can be easily transplanted to a Skip-gram model by modifying the objective function.ßdummyTurker
10990ß0.0ß[1] Qiu Siyu, et al.ßdummyTurker
10991ß0.0ßCo-learning of Word Representations and Morpheme Representations.ßdummyTurker
10992ß0.0ßCOLING2014.ßdummyTurker
10993ß0.0ßI agree that corpus homogeneity is an important issue.ßdummyTurker
10994ß0.0ßHowever, we need some measure to evaluate homogeneity, and this measure will most likely be one of the distance measures we have.ßdummyTurker
10995ß0.0ßSo, in order to break the vicious circle, we must declare at some point that we take homogeneity of some resources for granted (or, to state it in a fancier way, we must resort to expert judgment at some point).ßdummyTurker
10996ß0.0ßI agree that the collection-insensitivity requirement may be lifted; however, I do not agree that it contradicts the natural interpretation of similarity (distance).ßdummyTurker
10997ß0.0ßI think that most people would be surprised to find out that the distance between points A and B must change depending on the presence of points C, D, etc.ßdummyTurker
10998ß0.0ß(it might seem smaller or larger as compared to other distances, but does not need to change).ßdummyTurker
10999ß0.0ßI agree that the choice of measures might seem arbitrary.ßdummyTurker
11000ß0.0ßHowever, it is virtually impossible to evaluate all distance measures in existence, and one has to restrict oneself to some set of measures (for instance, httpßdummyTurker
11001ß0.0ßThis statement is hard to argue against, but it seems to me that it is good if not only new methodology is proposed, but also old methodology is evaluated and extended.ßdummyTurker
11002ß0.0ßIf the paper is accepted, I will provide a more detailed comparison of my results to earlier results on the subset of my data.ßdummyTurker
11003ß0.0ßI agree (and I also state it among other caveats at the end of the paper).ßdummyTurker
11004ß0.0ßTaking smaller chunks yields smaller percentages of correct comparisons, which proves that chunk size is important; I am going to add this to the paper.ßdummyTurker
11005ß0.0ßSee Reply to weakness argument 1.ßdummyTurker
11006ß0.0ßThe list of measures can be extended (and I will definitely do so), however, I will never be able to cover the whole range of existing measures.ßdummyTurker
11007ß0.0ßI don't have an answer at hand, but I guess that this difference must be due to different methods of evaluation (Known-Similarity Corpora vs. anchor texts).ßdummyTurker
11008ß0.0ßI have to test this further.ßdummyTurker
11009ß0.0ßAs for task-orientedness, I am going to make the necessary clarifications; see Response to Review #1.ßdummyTurker
11010ß0.0ßI agree that the problem with the identity of indiscernibles should be discussed in further detail.ßdummyTurker
11011ß0.0ßIn fact, we can state that most of the distance measures discussed in the paper (except for Levenshtein distance) do not satisfy this requirement, which neither makes the requirment invalid nor the measures useless.ßdummyTurker
11012ß0.0ßIf the paper is accepted, I am going to make it more task-oriented and to show in some more detail that the proposed evaluation method is applicable to some tasks and not applicable to others.ßdummyTurker
11013ß0.0ßI am grateful for the suggestions, however, I am not absolutely convinced that this is a _weakness_.ßdummyTurker
11014ß0.0ßI will be glad to implement the ideas proposed by the Reviewer, but they do not contradict the underlying principles of the Known-Similarity-Corpora approach.ßdummyTurker
11015ß0.0ß"Reviewer #1 (discussing my use of the terms ""distance"" and ""similarity""): If I am correct, this means that in this context, « d(A,B)=0 <==> A=B »"ßdummyTurker
11016ß0.0ßI rather meant that d(A,B)>d(A,C) <==> sim(A,B)<sim(A,C).ßdummyTurker
11017ß0.0ßI agree that this must be stated more clearly and will take it into account.ßdummyTurker
11018ß0.0ßThanks for the suggestions -- we address your concerns below:ßdummyTurker
11019ß0.0ßWe will try to clarify some of the design choices w.r.t.ßdummyTurker
11020ß0.0ßthe model components, but hopefully our ablation tests (Section 4) reveal that each component meaningfully contributes to system performance.ßdummyTurker
11021ß0.0ßIndeed our comparison is mainly around Attr2Seq, but note that this is a relatively new problem, and Attr2Seq was published only recently, at which time it was state-of-the-art for this task.ßdummyTurker
11022ß0.0ßL should be changed to M, which represents the number of input phrases.ßdummyTurker
11023ß0.0ßThanks for the suggestions, we address the three main weaknesses here.ßdummyTurker
11024ß0.0ßBesides summaries, our model also takes user and item attributes (e.g.ßdummyTurker
11025ß0.0ßuser ID and item ID) as input, allowing the model to personalize its outputs.ßdummyTurker
11026ß0.0ßEven given the same summary, two users A and B would write different reviews about an item I.ßdummyTurker
11027ß0.0ßThe model leverages user and item IDs to generate personalized reviews by extending the summary into reviews that discuss different aspects.ßdummyTurker
11028ß0.0ßSpecifically, the model learns both general latent representations and aspect-aware latent representations for users and items.ßdummyTurker
11029ß0.0ßThese latent representations model the preferences between users and items, which enables the model to generate personalized reviews.ßdummyTurker
11030ß0.0ßAs mentioned above, even with similar summaries, different users would write different reviews of an item.ßdummyTurker
11031ß0.0ßIn this sense, the ground-truth reviews are personalized.ßdummyTurker
11032ß0.0ßHence comparing the generated results with the ground-truth reviews using BLEU can measure the quality of `personalization'.ßdummyTurker
11033ß0.0ßOn the other hand, we also consider other metrics (e.g.ßdummyTurker
11034ß0.0ßDistinct-1 and Distinct-2) to measure the diversity and quality of the generated results.ßdummyTurker
11035ß0.0ßThe user ID is used to represent a specific user.ßdummyTurker
11036ß0.0ßFor a given user ID, we can obtain the user latent representation and use it to predict the user's preferences over different items.ßdummyTurker
11037ß0.0ßThank you for the informative, and overall positive, comments.ßdummyTurker
11038ß0.0ßWe try to answer your questions as follows:ßdummyTurker
11039ß0.0ß"Though the sentence ""i have not tried the tablet app yet but i do n’t have any problems with it ."""ßdummyTurker
11040ß0.0ß"does not have the exact same words as the ""nice standard apps,"" it generally mirrors the mildly positive sentiment about the app (in a more verbose way), which is the goal of our expansion approach."ßdummyTurker
11041ß0.0ßWe use aspect words obtained from the aspect extraction step to count the number of aspects appearing in each review.ßdummyTurker
11042ß0.0ßAs mentioned in Section 4, we use the method in (He et al., 2017) to extract 15 aspects and consider the top 100 words from each aspect.ßdummyTurker
11043ß0.0ßFor each review, if any of the 100 words for aspect A1 exist, we consider aspect A1 as appearing in that review.ßdummyTurker
11044ß0.0ßBy counting words from those 15 aspects (A1,A2,...,A15), we determine the number of aspects in each review.ßdummyTurker
11045ß0.0ßYour understanding is correct.ßdummyTurker
11046ß0.0ßTable 2 shows the average number of aspects appearing in each review.ßdummyTurker
11047ß0.0ßWe will revise the table.ßdummyTurker
11048ß0.0ß1.278: Sorry for the typo.ßdummyTurker
11049ß0.0ßWe use standard NLTK.ßdummyTurker
11050ß0.0ß"We will remove the word ""Stanford""."ßdummyTurker
11051ß0.0ßl.282: The sparsity is calculated by R/(U*I), where R,U,I is the number of reviews, users and items, respectively.ßdummyTurker
11052ß0.0ßl.287: Typo noted, thanks.ßdummyTurker
11053ß0.0ßl.323: We will change the color of the highlight to make it visible when printed in B&W.ßdummyTurker
11054ß0.0ßl.331: Typo noted.ßdummyTurker
11055ß0.0ßl.335: We will mention that the model obtains better results as more input information is added - except for the ROUGE-L metric.ßdummyTurker
11056ß0.0ßThanks for the valuable comments.ßdummyTurker
11057ß0.0ßWe prefer generating action sequences mainly for the following reasonsßdummyTurker
11058ß0.0ßThanks for pointing out the problem of grammatical errors.ßdummyTurker
11059ß0.0ßWe will revise our paper carefully and invite native English speakers for proofreading.ßdummyTurker
11060ß0.0ßPlease see Reply2Arg1 for detailed response.ßdummyTurker
11061ß0.0ßPlease see Reply2Arg2 for detailed response.ßdummyTurker
11062ß0.0ßThanks for your kind remind of grammar errors.ßdummyTurker
11063ß0.0ßWe will check our paper carefully and invite native English speakers for proofreading.ßdummyTurker
11064ß0.0ßThanks for the valuable reference suggestion.ßdummyTurker
11065ß0.0ßTransition-based approaches are important in both dependency parsing and AMR parsing, we will refer them in our next revision and provide more connection and comparison between these approaches and our approach.ßdummyTurker
11066ß0.0ßIn semantic parsing, our method has a tight-coupling with knowledge base, and constraints can be exploited for more accurate decoding.ßdummyTurker
11067ß0.0ßWe believe this is an enhancement to previous transition-based methods and may also be used in other parsing tasks, e.g., AMR parsing.ßdummyTurker
11068ß0.0ßThanks for reminding the missing reference.ßdummyTurker
11069ß0.0ßRabinovich et al.ßdummyTurker
11070ß0.0ß(2017) is an important approach and we will add and compare it in our next revision.ßdummyTurker
11071ß0.0ßThanks for the valuable comment.ßdummyTurker
11072ß0.0ßThe attention-based Bi-LSTM encoder-decoder model is a general framework for many different NLP tasks.ßdummyTurker
11073ß0.0ßThis paper follows the settings of Jia & Liang (2016), with an additional controller component in decoder for incorporating structure/semantic constraints during decoding.ßdummyTurker
11074ß0.0ßThanks for the helpful comment.ßdummyTurker
11075ß0.0ßSemantic constraints (C2) are stricter than structure constraints (C1).ßdummyTurker
11076ß0.0ßTherefore we argue that C1 should be first met for C2 to be met.ßdummyTurker
11077ß0.0ßSo in our experiments we add constraints incrementally.ßdummyTurker
11078ß0.0ßThanks for the helpful suggestion.ßdummyTurker
11079ß0.0ßThe semantic representation for “first-class” is (class_type, x, first_cl), and this representation exists in the gold parse (we mark it with red color) but missed in the system output.ßdummyTurker
11080ß0.0ßThis problem is common in NN model, e.g., the coverage problem in neural machine translation.ßdummyTurker
11081ß0.0ßThe adopted dataset was constructed by SemEval-2017 task 5 organizers (Cortis et al., 2017).ßdummyTurker
11082ß0.0ßThe details of train/turning/test set are provided in supplementary material, and mentioned in Line 227.ßdummyTurker
11083ß0.0ßAs described in Section 3.2, the ancestors and the decedents of the target cashtag form the key snippet.ßdummyTurker
11084ß0.0ßThe output of DB method is a snippet.ßdummyTurker
11085ß0.0ßWe will provide an example for the output in revised version.ßdummyTurker
11086ß0.0ßAs described in Line 271-277, DB method is better in single-cashtag instance, and CNN model is better in multi-cashtag instance.ßdummyTurker
11087ß0.0ßThanks for reviewer’s suggestion.ßdummyTurker
11088ß0.0ßWe will add some error analysis in revised version.ßdummyTurker
11089ß0.0ßWe acknowledge that the accuracy improvements are limited, but the additional computation required is far from unmanageable.ßdummyTurker
11090ß0.0ßIn a nutshell, the training time for the hinge-CoNLL loss is approximately double that for the cross entropy (8 hours vs 4 hours).ßdummyTurker
11091ß0.0ßThese times can be easily computed from the values reported in the paper (computational time per iteration, Section 5, and average number of iterations, Figure 2).ßdummyTurker
11092ß0.0ßThe proposed dynamic programming algorithm is indeed an extension of Zhang et al.ßdummyTurker
11093ß0.0ß2017.ßdummyTurker
11094ß0.0ßHowever, the extension to multi-token, multi-class entities is non-trivial, not only in complexity, but conceptually.ßdummyTurker
11095ß0.0ßZhang et al.ßdummyTurker
11096ß0.0ß2017 posits that TP and FP can be decided by the comparison of a single ground-truth label and a single predicted label.ßdummyTurker
11097ß0.0ßThis assumption is very restrictive and inadequate for most NLP losses (e.g., CoNLL, ROUGE, BLEU).ßdummyTurker
11098ß0.0ßFor instance, in the case of ROUGE or BLEU the prediction has to be compared with the entire reference sentence(s).ßdummyTurker
11099ß0.0ßThe proposed algorithm paves the way for this generalization.ßdummyTurker
11100ß0.0ßThe formal computational complexity per sentence of the proposed loss-augmented inference is O(L*P*T^2), where L is the number of distinct labels, P is the number of entities in the sentence (maximum value for TP), and T is the number of tokens in the sentence (both the sentence length and the maximum value for FP).ßdummyTurker
11101ß0.0ßThe quadratic complexity in T is the heavy factor; however, the actual computational time is manageable for typical sentence lengths (150-250 tokens).ßdummyTurker
11102ß0.0ßThe baseline performance differs from Lample's for both principled and practical reasons.ßdummyTurker
11103ß0.0ßFor the former, to improve the reproducibility of our results we have decided to 1) avoid random shuffling of the training data, and 2) report the average of three runs rather than a single (possibly, fortunate) run.ßdummyTurker
11104ß0.0ßYet, the main reason is practical: Lample's original Theano's implementation does not allow loading a constant at run time.ßdummyTurker
11105ß0.0ßTo implement the proposed approach, we need to be able to load the loss-augmented labeling in the computational graph at each iteration, and to have it treated as a constant in the loss function.ßdummyTurker
11106ß0.0ßThese features are permitted by TensorFlow, so we resorted to the TensorFlow implementation of the BiLSTM-CRF of NeuroNER (Dernoncourt et al., 2017).ßdummyTurker
11107ß0.0ßEven with the original embeddings of Lample's, the baseline results differ significantly (better for one language, worse for two).ßdummyTurker
11108ß0.0ßWe acknowledge that the accuracy improvements are limited, but much care has been taken to compare the approaches at a parity of conditions and ensure the reproducibility of the results (avoiding random shuffling of the training data and reporting the average of three runs).ßdummyTurker
11109ß0.0ßWe will add the standard deviations in the final version.ßdummyTurker
11110ß0.0ßWe are working around the clock to follow the reviewer's suggestion and include results for POS tagging and chunking in the extra page allowed by the cameraready.ßdummyTurker
11111ß0.0ßWe agree with the reviewer that the double hinge's good results require more probing.ßdummyTurker
11112ß0.0ßAn alternative reason for its better performance is that it teaches the model to stay away from two bad predictions, not just one.ßdummyTurker
11113ß0.0ß"To further test, we have run a comparison over only the sentences in the English test set with more than 4 entities each (""many"" per sentence)."ßdummyTurker
11114ß0.0ßIn this case, the performance of the double hinge becomes the highest (vs hinge-Hamming: 0.21 pp; hinge-CoNLL: 0.46 pp; cross entropy: 0.31 pp).ßdummyTurker
11115ß0.0ßThe fact that the accuracy is higher than both hinge-Hamming and hinge-CoNLL confirms the usefulness of training the model to stay away from two undesirable labelings.ßdummyTurker
11116ß0.0ßThe fact that it is higher than the cross entropy shows that training with an F score-like objective is especially useful when the entities are dense (i.e., many per sentence).ßdummyTurker
11117ß0.0ßWe will also compute and include results for the other languages.ßdummyTurker
11118ß0.0ßPlease see our reply to Reviewer 1, question 2.ßdummyTurker
11119ß0.0ßWe agree with the reviewer entirely and we are grateful for the gracious suggestion.ßdummyTurker
11120ß0.0ßLosses useful for NLP evaluation (e.g., ROUGE for summarization) can have a sentence/paragraph/document scope.ßdummyTurker
11121ß0.0ßThis paper shows how such losses can be used also at training time to steer the classifier toward a more desirable behavior.ßdummyTurker
11122ß0.0ß- following the reviewer's advice, we have subdivided section Experiments and Results;ßdummyTurker
11123ß0.0ß- for correct record, the key function of our code (the loss-augmented inference under the CoNLL loss) was already included in our Supplementary Material.ßdummyTurker
11124ß0.0ßThanks.ßdummyTurker
11125ß0.0ßWe will revise the method part to make it more clear and accessible.ßdummyTurker
11126ß0.0ßYu et al.ßdummyTurker
11127ß0.0ßdidn’t release their code, but our AREL results outperform theirs on all metrics (BLEU-3 and CIDEr scores are improved by ~2 points).ßdummyTurker
11128ß0.0ßWe are very willing to perform the human evaluation with their results once their code is released.ßdummyTurker
11129ß0.0ßWang et al.ßdummyTurker
11130ß0.0ßwas released on February 2nd, 2018, while the ACL deadline is February 14th.ßdummyTurker
11131ß0.0ßAccording to ACL’s review and citation policy, Wang et al.ßdummyTurker
11132ß0.0ßis considered contemporaneous, so we are not obliged to compare with theirs (see http://acl2018.org/call-for-papers/).ßdummyTurker
11133ß0.0ßHowever, we are glad to re-implement their model for comparison in the revision if necessary.ßdummyTurker
11134ß0.0ßWe will refer to some related work and add more discussions in the revision.ßdummyTurker
11135ß0.0ßThanks.ßdummyTurker
11136ß0.0ßWe will rigorously proofread the document for any errors.ßdummyTurker
11137ß0.0ßGood point.ßdummyTurker
11138ß0.0ßWe discussed the complicated characteristics of visual stories in the Introduction.ßdummyTurker
11139ß0.0ßAnd we also mentioned that “the references to the same image sequence are quite different to each other ...” (Line 649).ßdummyTurker
11140ß0.0ßBut we do agree to add more analysis about the VIST dataset.ßdummyTurker
11141ß0.0ßThe invalid images are the broken images that cannot even be read.ßdummyTurker
11142ß0.0ßThere are only 3 (out of 21075) invalid images, which basically has no influence on the final results.ßdummyTurker
11143ß0.0ßMoreover, previous methods (e.g.ßdummyTurker
11144ß0.0ßYu et al.)ßdummyTurker
11145ß0.0ßalso removed the 3 pictures, so it is still a fair comparison.ßdummyTurker
11146ß0.0ßThe term “Human demonstration” is commonly used in reinforcement learning, and here it refers to human-annotated stories (training data pairs).ßdummyTurker
11147ß0.0ßWe will clarify.ßdummyTurker
11148ß0.0ßBy concatenation, not summation.ßdummyTurker
11149ß0.0ßSorry about the typo.ßdummyTurker
11150ß0.0ßWe have corrected it.ßdummyTurker
11151ß0.0ßThanks for your valuable comments.ßdummyTurker
11152ß0.0ßIn this paper, we aim to balance the complementary pros and cons in topic models and neural networks.ßdummyTurker
11153ß0.0ßWe agree that our SLTM is a topic modeling based structure, which may restrict the predict learning.ßdummyTurker
11154ß0.0ßHowever, hidden variables in topic models are much easier to interpret than neural networks.ßdummyTurker
11155ß0.0ßFor example, real-world events that evoke users’ particular emotions can be discovered (Table 8) and used for psychotherapy.ßdummyTurker
11156ß0.0ßAs one of our most important baselines, sNTM is in essence a neural network by following the document-topic distribution in topic models (Cao et al., 2015).ßdummyTurker
11157ß0.0ßAccording to our experimental results, we can observe that the proposed SLTM outperformed sNTM in the predict learning by a large margin (Tables 2 and 3).ßdummyTurker
11158ß0.0ßThis indicates that our hybrid approach yields significant improvements compared to the hybrid one based on neural networks.ßdummyTurker
11159ß0.0ßFurthermore, although neural network-based methods can achieve competitive results (Chen et al., 2015), they need an extra effort to enhance network interpretability.ßdummyTurker
11160ß0.0ßTo our best knowledge, the proposed SLTM is the first successful hybrid method of exhibiting strengths of topic models and neural networks.ßdummyTurker
11161ß0.0ß(i) Indeed, the results in Figure 3 are not exact Kuznets curves, because the performance of predicting different emotions is unbalanced.ßdummyTurker
11162ß0.0ßWe have examined the accuracy of each emotion and found that the most unpredictable label on the ISEAR dataset is “guilt”, which is consistent to (Agrawal & An, 2012ßdummyTurker
11163ß0.0ßAs presented in the equation of calculating the coherence score of topics (Mimno et al., 2011), this metric is inconsistent to bag-of-words based models (Tables 6 and 7) and n-gram models (e.g., sNTM).ßdummyTurker
11164ß0.0ßThis is because the co-document frequency of words is naturally larger than that of bigrams or trigrams.ßdummyTurker
11165ß0.0ßTherefore, we only conduct qualitative analysis on topics discovered by sNTM (lines 677-691).ßdummyTurker
11166ß0.0ßThanks for your valuable comments!ßdummyTurker
11167ß0.0ßThe primary motivation of this research is to provide a reasonable explanation of the hidden variables in supervised learning.ßdummyTurker
11168ß0.0ßTake emotion prediction as an example, it is quite valuable to associate user perceptions with a topic beyond the predictive learning.ßdummyTurker
11169ß0.0ßSuch topics with label (e.g., emotion and sentiment) focus can be used for supporting personality psychology and aspect-level sentiment analysis.ßdummyTurker
11170ß0.0ßHowever, it is difficult for existing supervised topic models to generate coherent label-specific topics (Tables 6-8).ßdummyTurker
11171ß0.0ßThis is because unconstrained mapping of topics to labels will produce many topics outside labels.ßdummyTurker
11172ß0.0ßIn our work, labels are pre-defined by users rather than post-hoc generated by models in (Mei et al., 2007; Lau et al., 2011).ßdummyTurker
11173ß0.0ßThanks for these good points, and we will improve the definition and motivation accordingly.ßdummyTurker
11174ß0.0ßUnder the definition detailed by the reviewer, we observe that one of our baselines (sNTM) is a multi-task learning method.ßdummyTurker
11175ß0.0ßIn sNTM, topic distributions learned by an unsupervised NTM are first used to train a document labeller.ßdummyTurker
11176ß0.0ßThen, the output of the document labeller is used for training NTM iteratively.ßdummyTurker
11177ß0.0ßWe will add the above statement to the last paragraph of Section 5.1.ßdummyTurker
11178ß0.0ß(i) Accordingly, we have added some standard supervised approaches (CNN, SV{M|R}, and fastText) as baselines on label prediction.ßdummyTurker
11179ß0.0ßAlthough CNN is observed to achieve competitive performance, it needs an extra effort to enhance network interpretability.ßdummyTurker
11180ß0.0ßFor the evaluation of word embeddings, we have introduced labels to tune CBOW/Skip-Gram/SSPMI by fastText.ßdummyTurker
11181ß0.0ßHowever, their unsatisfied results indicate that the label information is counterproductive.ßdummyTurker
11182ß0.0ßAs suggested, we will include the NPMI-based topic coherence in Tables 6 and 7.ßdummyTurker
11183ß0.0ß(ii) To tune topic numbers, a small proportion of training data can be selected for validation.ßdummyTurker
11184ß0.0ßHowever, it may be unsuitable to our datasets with only 8.53 and 8.56 words for each document in average.ßdummyTurker
11185ß0.0ßTherefore, we adopt 10-fold cross-validation and report mean results.ßdummyTurker
11186ß0.0ßBesides, the number of topics is fixed for all models according to (Yan et al., 2013ßdummyTurker
11187ß0.0ßThanks for reviewer’s suggestion.ßdummyTurker
11188ß0.0ßWe followed ACL submission guidelines to write this short paper to share our work in progress, and focused on our novel idea in order to attract interdisciplinary audience.ßdummyTurker
11189ß0.0ßFurthermore, in guidelines, a short paper is not a shortened long paper, a small and focused contribution is also required.ßdummyTurker
11190ß0.0ßAs we mentioned in conclusions, we are enlarging our dataset.ßdummyTurker
11191ß0.0ß(line 394) The experimental results in this paper could be seen as the performance of each models conditioned on limited dataset.ßdummyTurker
11192ß0.0ßBecause the duration of the data from StockTwits is the last 6 months of 2017, we only show the results till December in Figure 1.ßdummyTurker
11193ß0.0ß"""reached that of analysts in January 2018"" was not shown in Figure 1."ßdummyTurker
11194ß0.0ßWe will describe it in revised version.ßdummyTurker
11195ß0.0ßFigure 1 shows the experimental results of $GOOGL, and Table 5 shows the experimental results of 30 constituent stocks of Dow Jones Index ($GOOGL is not in these 30 stocks).ßdummyTurker
11196ß0.0ßWe attempted to use Figure 1 to show the Bloomberg-like graph, and use Table 5 to show the experimental results of application scenario.ßdummyTurker
11197ß0.0ßTherefore, Table 5 shows the statistical results of the opinion of individual investors and professional analysts.ßdummyTurker
11198ß0.0ßNo, we think aspect, which defined as “a particular way of thinking about something” in dictionary, is more proper for describing the viewpoint of investors.ßdummyTurker
11199ß0.0ßThanks for reviewer’s suggestion.ßdummyTurker
11200ß0.0ß(Liu et al., 2015) used RNN, which is also adopted in our experiments, to solve fine-grained opinion mining task, and (Cheng et al., 2017) used CNN, which is also adopted in our experiments, to solve fine-grained opinion mining of microblogs.ßdummyTurker
11201ß0.0ßAs we mentioned in Line 394, we are constructing the specific model for the proposed task.ßdummyTurker
11202ß0.0ßThe comparison of the models of previous work will be added in revised version.ßdummyTurker
11203ß0.0ß(Liu et al., 2015): https://pdfs.semanticscholar.org/857b/d3f2b691bf53ac6ae571f23c093e5c0b62d0.pdfßdummyTurker
11204ß0.0ß(Cheng et al., 2017)ßdummyTurker
11205ß0.0ßhttps://link.springer.com/chapter/10.1007%2F978-3-319-67274-8_9ßdummyTurker
11206ß0.0ßThanks for reviewer’s suggestion.ßdummyTurker
11207ß0.0ßWe will add the details of NN models in revised version.ßdummyTurker
11208ß0.0ßFigure 1 shows the experimental results of $GOOGL, and Table 5 shows the experimental results of 30 constituent stocks of Dow Jones Index ($GOOGL is not in these 30 stocks).ßdummyTurker
11209ß0.0ßWe attempted to use Figure 1 to show the Bloomberg-like graph, and use Table 5 to show the experimental results of application scenario.ßdummyTurker
11210ß0.0ßTherefore, Table 5 shows the statistical results of the opinion of individual investors and professional analysts.ßdummyTurker
11211ß0.0ßWe will show the parameters in revised version.ßdummyTurker
11212ß0.0ßFigure 1 shows the experimental results of $GOOGL, and Table 5 shows the experimental results of 30 constituent stocks of Dow Jones Index ($GOOGL is not in these 30 stocks).ßdummyTurker
11213ß0.0ßWe attempted to use Figure 1 to show the Bloomberg-like graph, and use Table 5 to show the experimental results of application scenario.ßdummyTurker
11214ß0.0ßTherefore, Table 5 shows the statistical results of the opinion of individual investors and professional analysts.ßdummyTurker
11215ß0.0ßThanks for the comment.ßdummyTurker
11216ß0.0ßWe will carefully revise the readability of Section 4 according to your suggestions, e.g., not abusing r_t, and more formal writing of REINFORCE.ßdummyTurker
11217ß0.0ßThanks for the insightful comment.ßdummyTurker
11218ß0.0ß"We pointed out the drawbacks of micro-averaging that ""The scoring function ...... score estimation of low-frequency words"" between Line 426 and Line 431."ßdummyTurker
11219ß0.0ßWe omitted the experimental results due to space limit.ßdummyTurker
11220ß0.0ßHere, we show some quantitative results when using micro averaging: 84.2 on SNLI, 88.7 on SST-2, 51.3 on SST-5 and 80.20 on age prediction.ßdummyTurker
11221ß0.0ßBesides, micro averaging will lead to low-quality tree structures, which focus on some high-frequency words (e.g., the).ßdummyTurker
11222ß0.0ßThanks for the comment.ßdummyTurker
11223ß0.0ßOur methods statistically significantly outperform other latent structured sentence embedding methods at the 95% confidence level.ßdummyTurker
11224ß0.0ßWe will provide significance test information for all experiments in revision.ßdummyTurker
11225ß0.0ßSince Choi’s paper does not provide the detailed description of data preprocessing, for fair comparison, we ran their public codes with exactly identical settings as ours and reported the results that we obtained.ßdummyTurker
11226ß0.0ßFor more comparisons, we also conducted one more experiment, age prediction, with the same settings between Choi's model and ours, to demonstrate that our model performs better.ßdummyTurker
11227ß0.0ßWe appreciate the valuable suggestion.ßdummyTurker
11228ß0.0ßWe will follow the suggestion in our revision, such as comparing between Table 5 and IDF ranking and showing the trees from different methods.ßdummyTurker
11229ß0.0ßHowever, error analysis is hard for two reasons: 1) There is no ground truth about our tree structures, so the evaluation of structural quality is very subjective.ßdummyTurker
11230ß0.0ßAccording to our observation, most learned structures for the test set are reasonable and interpretable.ßdummyTurker
11231ß0.0ß2) A wrong predicted label doesn't mean a low-quality structure.ßdummyTurker
11232ß0.0ßIt depends on the weight learning of the LSTM and TreeLSTM units.ßdummyTurker
11233ß0.0ßIn future we will explore a reasonable solution for evaluation of structural quality.ßdummyTurker
11234ß0.0ßWe use “self” to highlight the attention-guided structure is learned without any supervision.ßdummyTurker
11235ß0.0ßIn other literature, “self” refers to “self-supervised” and “unsupervised”, which fits our scenario.ßdummyTurker
11236ß0.0ßWe will modify the name to avoid the misleading.ßdummyTurker
11237ß0.0ßWe sincerely thank for all of your constructive and insightful comments.ßdummyTurker
11238ß0.0ßThere are some questions of the original crawled data which cannot be answered by utilized the semi-structured product facts.ßdummyTurker
11239ß0.0ßFor example, the subjective question “Is this mobile phone good?”, the purchasing date/time question “Can this cell phone be snapped up at 10 a.m.?” and so on.ßdummyTurker
11240ß0.0ßTo focus the models on resolving factual questions and reference to previous methods such as GenQA and COREQA, we filter the question-answer pairs that cannot link with the semi-structured facts.ßdummyTurker
11241ß0.0ßWe will perform careful proofreading and check grammatical errors in our paper.ßdummyTurker
11242ß0.0ßWe sincerely thank for all of your constructive and insightful comments.ßdummyTurker
11243ß0.0ßPrevious NAG methods such as GenQA (Yin,et al.,2016) and COREQA (He,et al.,2017) was completely impossible to utilize semi-structured KBs, that is, they cannot answering any questions on semi-structured KBs.ßdummyTurker
11244ß0.0ßIf we see the semi-structured KB as the structured one and implement GenQA and COREQA on them, the P, R, and F1 will same as the basic Seq2Seq models.ßdummyTurker
11245ß0.0ßThe reason is that most values of semi-structured facts will be converted to the word “UNK” because we much adopt a fixed vocabulary (consisted of high-frequency words).ßdummyTurker
11246ß0.0ßIn specific, the natural answer is word sequence that needs to be generated word-by-word.ßdummyTurker
11247ß0.0ßEach word predicted by a softmax classifier, which should exact the word in the fixed vocabulary.ßdummyTurker
11248ß0.0ßWithin the copy mode, the copy word should exactly match the value of KB facts, for example, to copy the word “4.7” in natural answer “the size is 4.7”, previous NAG methods need the structured fact {“screen_size”:”4.7”}.ßdummyTurker
11249ß0.0ßHowever, they cannot match with any semi-structure fact because the value of “screen_size” is the phrase “4.7_in”.ßdummyTurker
11250ß0.0ßThe proposed model is similar to COREQA when dealing with structured KB.ßdummyTurker
11251ß0.0ßHowever, previous methods such as GenQA and COREQA cannot deal with semi-structured KB.ßdummyTurker
11252ß0.0ßIn fact, it is very difficult to convert semi-structured data into structured KB.ßdummyTurker
11253ß0.0ßWe will try to do such transformation by some direct methods (e.g.ßdummyTurker
11254ß0.0ßutilizing manual patterns) and show the comparison results in the next version.ßdummyTurker
11255ß0.0ßConsidering that the property, sub-property, and value indicate different types of information, we utilize different encoders to transform them into numeric representations.ßdummyTurker
11256ß0.0ßIn fact, the min/average/max word numbers of R_1, R_2, and V are 1/1.3/4, 1/1.7/7, and 1/2.5/15, respectively.ßdummyTurker
11257ß0.0ßNo, different questions utilize their owning semi-structured facts, for example, the questions about “iPhone 6S 16GB” own its corresponding product information.ßdummyTurker
11258ß0.0ßWe sincerely thank for all of your constructive and insightful comments.ßdummyTurker
11259ß0.0ßThe semi-structured KBs are more popular in the real-world application such as the product information of e-commerce website.ßdummyTurker
11260ß0.0ßTherefore, the proposed model is more practical.ßdummyTurker
11261ß0.0ßIt should be noted that previous NAG methods such as GenQA (Yin,et al.,2016) and COREQA (He,et al.,2017) were completely impossible to utilize semi-structured KBs, that is, they cannot answering any questions on semi-structured KBs.ßdummyTurker
11262ß0.0ßIf we see the semi-structured KB as the structured one and implement GenQA and COREQA on them, the P, R, and F1 will same as the basic Seq2Seq models.ßdummyTurker
11263ß0.0ßThe reason is that most values of semi-structured facts will be converted to the word “UNK” because we much adopt a fixed vocabulary (consisted of high-frequency words).ßdummyTurker
11264ß0.0ßIn specific, the natural answer is word sequence that needs to be generated word-by-word.ßdummyTurker
11265ß0.0ßEach word predicted by a softmax classifier, which should exact the word in the fixed vocabulary.ßdummyTurker
11266ß0.0ßWithin the copy mode, the copy word should exactly match the value of KB facts, for example, to copy the word “4.7” in natural answer “the size is 4.7”, previous NAG methods need the structured fact {“screen_size”:”4.7”}.ßdummyTurker
11267ß0.0ßHowever, they cannot match with any semi-structure fact because the value of “screen_size” is the phrase “4.7_in”.ßdummyTurker
11268ß0.0ßThe results of the removing copy-mode are shown in the third row (corresponding with the model “Hier-Seq2Seq”) of Table2,Table3,Table4 and Table5.ßdummyTurker
11269ß0.0ßIn general, the metrics at the char-level are better than word-level, which mainly influenced by the results of Chinese word segmentation.ßdummyTurker
11270ß0.0ßFor example, the word “no” (the Chinese word is “没有”, similarly hereinafter) cannot match with the word “yes”(有)，but they own parts of matching at the char-level (the char“有” of “没有” can match the word “有”).ßdummyTurker
11271ß0.0ßIn addition, there are many overlapping words in the question (e.g, ‘Is this an invert air-conditioner or constant frequency air-conditioner?’ (是变频还是定频？)) and the answer (e.g, ‘This one is a constant frequency air-conditioner’ (这款是定频的)) in consultations of customer service.ßdummyTurker
11272ß0.0ßTherefore, the copy-mode from question may promote the overall performance to some extent.ßdummyTurker
11273ß0.0ßFor the above question, through NAGSS generate a wrong answer ‘This one is an invert air-conditioner’ (该款是变频的), the metrics will be better at the char-level (“变频” and “定频” share the word “频”).ßdummyTurker
11274ß0.0ßThe vocabulary seq2seq+KB is much larger than NAGSS that incorporates the copy-mode.ßdummyTurker
11275ß0.0ßFor example, there are 3 properties in the restricted domain and the largest word number of properties’ values is 6.ßdummyTurker
11276ß0.0ßThus, there are only 18 candidate positions should be predicted.ßdummyTurker
11277ß0.0ßHowever, the seq2seq+KB still need to predict the concrete values of facts, therefore, it has not changed significantly from known to unknown entity.ßdummyTurker
11278ß0.0ßIn summary, it is much easier to memory the expression patterns than concrete facts.ßdummyTurker
11279ß0.0ßFor the space reason, we did not show examples.ßdummyTurker
11280ß0.0ßAnd some examples could be seen in the file “error_analysis.xlsx” of released data (https://goo.gl/UzLP9H).ßdummyTurker
11281ß0.0ßWe will append some examples of gold and generated answers in the revised version.ßdummyTurker
11282ß0.0ßWe will perform careful proofreading and check grammatical errors in our paper.ßdummyTurker
11283ß0.0ßWe are grateful for your insightful comments and thoughtful suggestions.ßdummyTurker
11284ß0.0ßIn the ACE corpus, a token can be annotated as the triggers of more than one type of event.ßdummyTurker
11285ß0.0ßThis is because such a token has different meanings in different sentences.ßdummyTurker
11286ß0.0ßBut in a particular sentence, a token is annotated with a single definite event type.ßdummyTurker
11287ß0.0ßThus, event detection can be boiled down to a multi-class classification problem, classifying a token into either one of the predefined ACE event types or “non-trigger”.ßdummyTurker
11288ß0.0ßSELF is a two-channel model.ßdummyTurker
11289ß0.0ßThe structurally consistent Bi-RNNs in the two channels enable a conjugated framework to come into being.ßdummyTurker
11290ß0.0ßUnder such a framework, although the Bi-RNNs generate different latent features for every token, the features can be compared in the same feature space.ßdummyTurker
11291ß0.0ßPlease consider the projection areas of two kinds of features of a token.ßdummyTurker
11292ß0.0ßIf the features are projected onto some areas in the same feature space, it should be easier to differentiate between them.ßdummyTurker
11293ß0.0ßBy contrast, if a RNN and a CNN are taken, we need to overcome the bottleneck that it is difficult to compare the features directly.ßdummyTurker
11294ß0.0ßLambda is set as 0.001.ßdummyTurker
11295ß0.0ßWe follow Glorot and Bengio (2010) to set the value range of the parameters.ßdummyTurker
11296ß0.0ßBy taking into account the numbers of nodes, input and output layers in a network, Glorot and Bengio predict that the proper range should be set as [-0.1, 0.1].ßdummyTurker
11297ß0.0ßSELF and Hybrid show similar performance (SELFßdummyTurker
11298ß0.0ßWe will follow the suggestions to revise the tables and the article titles.ßdummyTurker
11299ß0.0ßWe are grateful for your thoughtful suggestions.ßdummyTurker
11300ß0.0ßWe totally agree with your point that a case study enables the systemic analysis of experimental results.ßdummyTurker
11301ß0.0ßWe promise to carry out the analysis by evaluating and comparing the detection performance in two different situations.ßdummyTurker
11302ß0.0ßIn one situation, the unbroken sentences are used as the input of SELF, while in the other situation, we will discard the tokens in the sentences which may cause the emergence of spurious features.ßdummyTurker
11303ß0.0ßAlthough an event detection system (such as ours), in general, takes a sentence as the basic input, every token in the sentence is regarded as a sample need to be treated.ßdummyTurker
11304ß0.0ßTherefore, it is a word-level problem.ßdummyTurker
11305ß0.0ßIn the ACE corpus, there are in total 126,696 tokens (in 599 documents) have been annotated as a trigger or “non-trigger”.ßdummyTurker
11306ß0.0ßFrom the perspective of structure, SELF has a conjugated framework, employing two Bi-RNNs to constitute a two-channel model.ßdummyTurker
11307ß0.0ßAlthough the Bi-RNNs generate different features for a token, they can be directly compared in the same feature space.ßdummyTurker
11308ß0.0ßBut we need to train the GAN beforehand, in order to ensure that GAN is capable of generating some spurious features before the cooperative network learns to filter such features.ßdummyTurker
11309ß0.0ßWe are grateful for your insightful comments on the weakness of this paper.ßdummyTurker
11310ß0.0ßThe attributes of an event, such as the trigger, participants and roles, have inherent relationships with each other.ßdummyTurker
11311ß0.0ßSELF outperforms the competitors for trigger identification due to a much higher recall.ßdummyTurker
11312ß0.0ßTherefore, SELF is able to provide more examples of triggers.ßdummyTurker
11313ß0.0ßThis may contribute to the discovery of new provenances (sentences) that contain event mentions, and therefore provide additional contexts for the recognition of participants and roles.ßdummyTurker
11314ß0.0ßThe present study of coreference resolution has achieved a performance of 68.8% F-score (Lee et al (2017)’s neural coreference resolution), appearing to be promising to cooperate with the existing event detection models.ßdummyTurker
11315ß0.0ßAs mentioned in the conclusion section, we will combine coreference resolution and SELF to form a more sophisticated system in the future.ßdummyTurker
11316ß0.0ßBut due to the lack of examples of event coreference (59 in total) in the ACE corpus, we will run the model over other corpora, such as the TAC-KBP event coreference corpus.ßdummyTurker
11317ß0.0ßThe source codes to reproduce all experiments in the paper will be made publicly available.ßdummyTurker
11318ß0.0ßWe welcome any thought or comment on the idea, model, source codes and instruction document, with the purpose to make the system easier to use and reconstruct.ßdummyTurker
11319ß0.0ßWe totally agree with your point that the advantages should be summarized in an assertive way.ßdummyTurker
11320ß0.0ßWe will enumerate the advantages as belowßdummyTurker
11321ß0.0ßWe treated the NEs as given.ßdummyTurker
11322ß0.0ßBIO2 is used to initialize the vector representations of NEs.ßdummyTurker
11323ß0.0ßFor example, the tag “PERSON-B” indicates the first name of a person entity.ßdummyTurker
11324ß0.0ßAll the first name will be initialized with the same real-valued vector.ßdummyTurker
11325ß0.0ßReply to weakness argument 6 (Overfitting and Domain adaptation): We promise to enrich the discussion on the domain adaptation.ßdummyTurker
11326ß0.0ßPlease let us clarify the overfitting problem first in this reply.ßdummyTurker
11327ß0.0ßWe assume that if a system suffers from the overfitting problem, it may perform worse on the out-of-domain corpus and fail to be robust in a resource-poor situation.ßdummyTurker
11328ß0.0ßHybrid exhibits the greater performance losses in such conditions.ßdummyTurker
11329ß0.0ßTherefore, we think Hybrid may suffer from the overfitting problem more severely.ßdummyTurker
11330ß0.0ßWe will go over the whole paper and improve the presentation by avoiding speaking in an anthropomorphic way.ßdummyTurker
11331ß0.0ßWe will also follow the suggestions to reorganize the paper.ßdummyTurker
11332ß0.0ßThank for your constructive remarks!ßdummyTurker
11333ß0.0ßWe have reported question-word baseline in Table 3 as 54.9%.ßdummyTurker
11334ß0.0ßIt is hard to use precision-oriented grammars like English Resource Grammar because they cannot parse majority of queries from search engines.ßdummyTurker
11335ß0.0ß"For example, for ""What is the birthplace of obama?"ßdummyTurker
11336ß0.0ß""", the demo here says: ""Cannot find ""obama"" in the lexicon."""ßdummyTurker
11337ß0.0ßhttp://www.delph-in.net/erg/ßdummyTurker
11338ß0.0ßThis is why we have resorted to a scalable system that depends on the knowledge of the crowd.ßdummyTurker
11339ß0.0ßWe will provide stastistical significance numbers in the final version of the paper.ßdummyTurker
11340ß0.0ßWe tried using dependency features but did not obtain a gain, hence omitted them from here.ßdummyTurker
11341ß0.0ßWe will include that analysis in the final version.ßdummyTurker
11342ß0.0ßThank you for your constructive comments and encouragement.ßdummyTurker
11343ß0.0ßSorry for the confusion.ßdummyTurker
11344ß0.0ßWe meant to say queries where 4 or more annotators agree.ßdummyTurker
11345ß0.0ßThis would be q_wf <=0.2 and q_wf >= 0.8 (this sums up to 19000 in the histogram).ßdummyTurker
11346ß0.0ßWe will clarify this further.ßdummyTurker
11347ß0.0ßDefinitely, with the extra space, in the final version, we will report kappa numbers.ßdummyTurker
11348ß0.0ßThank you, we will remove the upper-bound of human performance from the paper.ßdummyTurker
11349ß0.0ßThe 88% figure was one way of verifying how a trained NLP researcher agrees with the annotation of the crowd.ßdummyTurker
11350ß0.0ßWe wanted to make the connection between grammar correction and well-formedness identification, as a well-formed question should also be ideally grammatically correct.ßdummyTurker
11351ß0.0ßBut we agree that it is hard to define either of those properties and hence we will remove the connection.ßdummyTurker
11352ß0.0ßYes, we collect n-gram of POS sequences and use them as features.ßdummyTurker
11353ß0.0ß"For example, the POS 2-gram for ""red house"" will be ""JJ_NN""."ßdummyTurker
11354ß0.0ßWe represent every POS-n-gram feature also as a vector of real numbers and then let it train with the network, resulting in trained POS-embeddings.ßdummyTurker
11355ß0.0ßWe personally request you to have a look at this paper from an application perspective.ßdummyTurker
11356ß0.0ßWe have addressed your concerns below.ßdummyTurker
11357ß0.0ßLack of a demonstration of the usefulness or interestingness of the task:ßdummyTurker
11358ß0.0ßWe have clearly shown that the well-formedness score of the classifier can be used to improve the performance of a SOTA seq-2-seq question generation model.ßdummyTurker
11359ß0.0ß"Increasing the performance of a core text generation task shows how a loose notion of ""well-formedness"" can actually help a downstream task."ßdummyTurker
11360ß0.0ßML models can handle noisy text but not to the most optimal extent, that is why there are workshops on user-generated noisy text like WNUT http://noisy-text.github.io/2017/ .ßdummyTurker
11361ß0.0ßOur attempt at classifying a query is a similar way of identifying noisy text.ßdummyTurker
11362ß0.0ß"As you clearly mentioned how difficult it is to define ""grammaticality"", we rely on the wisdom of the crowd to decide the well-formedness of a query based on certain factors including a loose notion of grammaticality that a novice can understand."ßdummyTurker
11363ß0.0ß"Defining ""grammaticality"" is an arduous task and an NLP conference provides platform to approach this from an application-oriented perspective as done traditionally in NLP, for example in textual inference (Dagan et al 2006, ""The Second PASCAL Recognising Textual Entailment Challenge"") or formality identification (Pavlick & Tetreault 2016, ""An Empirical Analysis of Formality in Online Communication"")."ßdummyTurker
11364ß0.0ßThe performance of a trained model is low because it is a hard classification task.ßdummyTurker
11365ß0.0ßA model to perform only 5 points above the baseline is not a valid criticism.ßdummyTurker
11366ß0.0ßA word bi-LSTM baseline is SOTA baseline in several NLP tasks (Liu et al 2015, Zhou et al 2016)ßdummyTurker
11367ß0.0ßThe presence of paraphrases present absolutely no inflation in performance of the classifier.ßdummyTurker
11368ß0.0ßGiven the size of paralex corpus (18,000,000 query pairs), the probability of the paraphrase of a query to be in the annotated corpus is 0.001.ßdummyTurker
11369ß0.0ß(We will include computation of this probability in the final version).ßdummyTurker
11370ß0.0ßIf anything, they make the model more robust, because one of the paraphrases can be a well-formed question and another can be a non-wellformed question.ßdummyTurker
11371ß0.0ßFor example, here one query is well-formed whereas another in non-wellformed:ßdummyTurker
11372ß0.0ßIndian satellites are in space?ßdummyTurker
11373ß0.0ß||| How many artificial Indian satellites are in space ?ßdummyTurker
11374ß0.0ßThere is no easier metric to understand than accuracy for a binary classification task.ßdummyTurker
11375ß0.0ßAnything that is not a well-formed question (according to the annotation guidelines) is a true negative.ßdummyTurker
11376ß0.0ßThank you!ßdummyTurker
11377ß0.0ßWe will correct the limits to [0, 6500]ßdummyTurker
11378ß0.0ßThank you for your helpful comments.ßdummyTurker
11379ß0.0ßWe could not directly show the enhancement of the encoder and attention, due to the end-to-end architecture.ßdummyTurker
11380ß0.0ßHowever, we think that our comparison is fair to evaluate it, because all the systems are trained on the same set of unique sentences at the target side.ßdummyTurker
11381ß0.0ßWe consider that we observed the effect of the over-sampling strategy by our learning rate adjustment.ßdummyTurker
11382ß0.0ßMore specifically, we used the learning rate $\eta$ for the manual back-translation and $\eta/N$ for the N synthetic sentences.ßdummyTurker
11383ß0.0ßBecause it was difficult to quantify the diversity, we showed an example in Table 1.ßdummyTurker
11384ß0.0ßIt includes various sentences including the ones with low likelihood.ßdummyTurker
11385ß0.0ßObviously, two different sentences have a more diversity than one single sentence, but it does not necessarily hold in cases N>2.ßdummyTurker
11386ß0.0ßWe plan to quantify the diversity with edit-distance, for instance, among the N synthetic source sentences.ßdummyTurker
11387ß0.0ßThank you for your comments.ßdummyTurker
11388ß0.0ßWe will adequately refer to them.ßdummyTurker
11389ß0.0ßWe will make this experimental comparison in our future work.ßdummyTurker
11390ß0.0ßWe are aware of that Zhang and Zong (2016) enhanced the encoder using pre-training of an autoencoder.ßdummyTurker
11391ß0.0ßWhile they enhanced only the encoder, our proposed method basically enhances the decoder, and further enhances the encoder and attention.ßdummyTurker
11392ß0.0ßThat is, the coverage of the enhancement is wider.ßdummyTurker
11393ß0.0ßDue to space limitations, we presented only the experiments for the major contribution.ßdummyTurker
11394ß0.0ßThe results of the statistical testing will be added.ßdummyTurker
11395ß0.0ßThank you very much for the arguments.ßdummyTurker
11396ß0.0ßThe main technical contribution lies in our view in the paper being the first survey paper combining distance matrix generation and stemma generation.ßdummyTurker
11397ß0.0ßMSTs are most successful on this data, presumably since the copyists in general tried to be as accurate as possible, while some were more accurate than others in copying, which reflects what MST tries to detect: the tree with a minimal edge weight sum (collective effort/attitude towards minimizing errors).ßdummyTurker
11398ß0.0ßThis argument needs to be underpinned by givens from the data and to be elaborated which space limitations prevented.ßdummyTurker
11399ß0.0ßUsing RHM or SemStem [1] its successor could be included in the final version.ßdummyTurker
11400ß0.0ßHowever, let me give a thought on why this was not done so far.ßdummyTurker
11401ß0.0ßApart from no implementation being freely available making comparability more straightforward, the current paper surveys bio-informatic methods and focusses on an interface between the disciplines, a further elaboration on RHM and SemStem, which have not been used and are not intended to be used in bio-informatics was thought to be potentially confusing to the reader.ßdummyTurker
11402ß0.0ßA comparison with the results of your [1] and our [1] is possible through the use of the same quite small datasets, which enables a rough the comparison with the results mentioned in their papers already.ßdummyTurker
11403ß0.0ß[1] T. Roos and Y. Zou, (2011).ßdummyTurker
11404ß0.0ßAnalysis of textual variation by latent tree structures, in Proc.ßdummyTurker
11405ß0.0ßIEEE International Conference on Data Mining (ICDM-2011), IEEE Press, pp.ßdummyTurker
11406ß0.0ß567–576.ßdummyTurker
11407ß0.0ßThank you very much for the arguments.ßdummyTurker
11408ß0.0ßThe NPMI innovation is a negative result, but negative results deserve to be mentioned.ßdummyTurker
11409ß0.0ßEspecially in this case, it was not a priori clear that a method like NPMI, which takes into account the whole distribution of variation would perform worse than other methods.ßdummyTurker
11410ß0.0ßOur MST approach tests all possible MSTs and proposes post-processing methods based on multiple MSTs, which has not been done (neither all MSTs nor postprocessing of them) in other stemmatological studies using MSTs.ßdummyTurker
11411ß0.0ßThus, we are showing that MSTs are extremely successful in stemmatology, that all MSTs matter in stemmatology and propose methods how to deal with them.ßdummyTurker
11412ß0.0ßAdditionally we extend their application for the biological case.ßdummyTurker
11413ß0.0ßWe feel that few papers use MSTs at all and very few use all MSTs if at all.ßdummyTurker
11414ß0.0ßBy this contribution, we show MSTs usefulness in full awareness of its shortcomings which we try to overcome by framing the use of MSTs into two disciplines which is the main contribution apart from being the first systematic survey on distance matrix generation x stemma generation.ßdummyTurker
11415ß0.0ßMSTs are most successful on this data, presumably since the copyists in general tried to be as accurate as possible, while some were more accurate than others in copying, which reflects what MST tries to detect: the tree with a minimal edge weight sum (collective effort/attitude towards minimizing errors).ßdummyTurker
11416ß0.0ßOf course, this argument needs to be underpinned by givens from the data and to be elaborated which space limitations prevented.ßdummyTurker
11417ß0.0ßThank you, we will add this for clarity in Table 3!ßdummyTurker
11418ß0.0ßThe metric used is ASD, apart from column 2, where GED (TED) is alternatively given, since on the bifurcating NJ, UPGMA and FM trees, GED is not applicable without further intervention (or not comparable with column 2 if used).ßdummyTurker
11419ß0.0ßLiterature on stemmatology uses mostly ASD.ßdummyTurker
11420ß0.0ßThe full paper should include experimental results on disambiguation and a more coarse description.ßdummyTurker
11421ß0.0ßThanks for pointing out the AAAI18 paper.ßdummyTurker
11422ß0.0ßWe will cite it and discuss the difference.ßdummyTurker
11423ß0.0ßAs shown in Line 115-116, by “single model” we mean one encoder and one decoder.ßdummyTurker
11424ß0.0ßNote that the AAAI18 paper uses one encoder, but two different decoders.ßdummyTurker
11425ß0.0ßWe will explicitly define “single model” and narrow its scope to avoid confusion.ßdummyTurker
11426ß0.0ßSorry, we are not sure what you mean by “unified decoding”, and we didn’t claim our approach is “unified decoding”.ßdummyTurker
11427ß0.0ßOur inference method is different from conventional reranking.ßdummyTurker
11428ß0.0ßReranking usually uses the score/loss of an additional model (different from the generation model) to rerank the candidates from the generation model and the score/loss used is usually inconsistent with the training loss.ßdummyTurker
11429ß0.0ßThe loss/score to perform the argmax operation in our method is exactly the loss we used in the training, and is outputted by our generation model.ßdummyTurker
11430ß0.0ßSo our method uses a unified loss for both training and inference, and the training and inference are consistent in terms of loss.ßdummyTurker
11431ß0.0ßWe respectively disagree with your several comments on experiments.ßdummyTurker
11432ß0.0ß(1) We focus on single model for NMT and thus we didn’t compare with methods with 2 models or decoders, since they are of larger model complexity than our method and the comparison would be unfair.ßdummyTurker
11433ß0.0ßAnyway, we will add such comparison in the new version of the paper.ßdummyTurker
11434ß0.0ß(2) We indeed reimplemented the baseline for WMT14 En-De, and got 28.36 BLEU score.ßdummyTurker
11435ß0.0ßTherefore, we simply list the score 28.4 reported in the original paper.ßdummyTurker
11436ß0.0ß(3) We don’t agree that “It is meaningless to compare the proposed system to other published results”.ßdummyTurker
11437ß0.0ßActually, in many recent papers (e.g., Gehring et al., 2017, Vaswani et al., 2017), a method has been compared to other published results, even with different model structures.ßdummyTurker
11438ß0.0ß(4) In abstractive summarization, our method is complementary and not competitive with SOTA methods and can combine them to achieve better results.ßdummyTurker
11439ß0.0ßWe will add them in the new version.ßdummyTurker
11440ß0.0ßThanks for you comments.ßdummyTurker
11441ß0.0ßWe will correct the typos in the new version.ßdummyTurker
11442ß0.0ßPlease refer to the reply to weakness argument 3.ßdummyTurker
11443ß0.0ßTake WMT17 Zh->En translation as an example.ßdummyTurker
11444ß0.0ßIt takes 672s to decode 2001 sentences with beam size $16$ using the left-to-right model.ßdummyTurker
11445ß0.0ßMeanwhile, the left-to-right probabilities have already been obtained and recorded.ßdummyTurker
11446ß0.0ßIt takes additional 71s to calculate the right-to-left probabilities of the $2001*16=32016$ candidates, which is 10.6% of generation time.ßdummyTurker
11447ß0.0ßThere is just 10.6% additional cost, as calculating the probability of an already generated sentence takes much less time than generating the sentence through beam search.ßdummyTurker
11448ß0.0ßSimilar situation can be found in right-to-left decoding.ßdummyTurker
11449ß0.0ßAs shown in Line 112-113 and 130-131, the main advantage of our method over other bidirectional methods is the training and memory efficiencyßdummyTurker
11450ß0.0ßAccording to our statistics on WMT14 En-De test set, for the final generation results, 66.2% sentences are from left-to-right decoding while 33.8% are from right-to-left decoding.ßdummyTurker
11451ß0.0ßIf we only use sentences generated in left-to-right as candidates, the final BLEU score drops from 29.30 to 28.73 (the unidirectional baseline is 28.40).ßdummyTurker
11452ß0.0ßThis result demonstrates that our improvements are largely due to sentence candidates generated in both directions, but not self-ensembling.ßdummyTurker
11453ß0.0ßPlease refer to the reply to weakness argument 1.ßdummyTurker
11454ß0.0ßThe beam sizes for text summarization and image captioning tasks are both set to 10, as described in Line 631-632 and Line 724-725.ßdummyTurker
11455ß0.0ßWe calculate the BLEU score of each n-best output on WMT17 Zh-En test set, and then calculate the mean and standard variance of the BLEU scores among the n-best candidates, averaged over all sentences.ßdummyTurker
11456ß0.0ßThe mean and standard variance are 21.99 and 3.64, which shows a considerable diversity in the n-best output.ßdummyTurker
11457ß0.0ßThe left-to-right and right-to-left generated sentences are not that similar (just with 4.5% overlap).ßdummyTurker
11458ß0.0ßWe will add case studies in the new version.ßdummyTurker
11459ß0.0ßThere is no obvious degradation.ßdummyTurker
11460ß0.0ßOn WMT14 En-De, the two BLEU scores are 28.25 (our method + uni-decoding) and 28.40 (baseline); 24.31 (our method + uni-decoding) and 24.29 (baseline) on WMT17 Zh-En; 32.14 (our method + uni-decoding) and 32.17 (baseline) on IWSLT14 De-En.ßdummyTurker
11461ß0.0ßLength penalty is widely used (e.g., Gehring et al., 2017, Vaswani et al., 2017) in sequence generation, otherwise it will prefer shorter sentence as you mentioned.ßdummyTurker
11462ß0.0ßWe use the same length penalty as used in left-to-right and right-to-left beam search for Equation (4).ßdummyTurker
11463ß0.0ßTherefore, the sentence length is similar with the uni-model.ßdummyTurker
11464ß0.0ßWe regard length penalty as a common practice and do not put details in the paper, but we will add them into the new version.ßdummyTurker
11465ß0.0ßWe will add them as related work and discuss the difference.ßdummyTurker
11466ß0.0ßIt is cited in Table 3, Line 464.ßdummyTurker
11467ß0.0ßThanks.ßdummyTurker
11468ß0.0ßWe will copy-edit in the new version.ßdummyTurker
11469ß0.0ß"W1:[relation]...""to_other_existing_approaches...on_the_FIGER_dataset?"""ßdummyTurker
11470ß0.0ßThe FIGER dataset task (mentionßdummyTurker
11471ß0.0ßtyping) is different from our task.ßdummyTurker
11472ß0.0ßWe agree with the reviewer that adding additionalßdummyTurker
11473ß0.0ßbaselines is a good idea: we haveßdummyTurker
11474ß0.0ßdescribed what we will do in this respect in response to W1 of reviewer-1.ßdummyTurker
11475ß0.0ß"W2:""performance_of_CTXT+NAME10+DESC_only_using_English_views"""ßdummyTurker
11476ß0.0ßWe had done this setting as well, and using only English is worse by more than 1% micro F1.ßdummyTurker
11477ß0.0ßSo, we agree with you, but, apart from 1% improvements which is significant consideringßdummyTurker
11478ß0.0ßthe large size of our test set, our multilingual modeling is more robustßdummyTurker
11479ß0.0ßand works for entities without English data.ßdummyTurker
11480ß0.0ß"Q1:""setting_of_crossview_experiment"""ßdummyTurker
11481ß0.0ßThe test set is the same for all experiments, but for each comparison of singleview and crossview,ßdummyTurker
11482ß0.0ßwe use only one of the 12 views and turn off all other views for each example.ßdummyTurker
11483ß0.0ßSo, the setup is fair for comparing singleview and crossview.ßdummyTurker
11484ß0.0ß"Q2:""dataset..include_the_raw_format..."""ßdummyTurker
11485ß0.0ßWe will include the raw format as well.ßdummyTurker
11486ß0.0ß"Q3:""cross-lingual_entity_linking_literature"""ßdummyTurker
11487ß0.0ßThanks for pointing this out, we will add this and other related papers.ßdummyTurker
11488ß0.0ßWe had a limited space and so we just added few key related works.ßdummyTurker
11489ß0.0ßIn comparison to N16-1072, we represent entities in different representation views,ßdummyTurker
11490ß0.0ßuse attention to use their complementary information, and evaluate them for entity typing.ßdummyTurker
11491ß0.0ß"W1:""comparisons_with_state-of-the-art_methods."""ßdummyTurker
11492ß0.0ßNote that the FIGER model is a system that predicts types of each *individual mention*ßdummyTurker
11493ß0.0ßof entities.ßdummyTurker
11494ß0.0ßIt has been used in (Yaghoobzadeh&Schütze 2017) as a baseline,ßdummyTurker
11495ß0.0ßbut it was substantially worse than models using entity representations.ßdummyTurker
11496ß0.0ßOur different entity representations are basically equivalent to ones in (Yaghoobzadeh&Schütze 2017),ßdummyTurker
11497ß0.0ßbut we also add multiple languages as another view.ßdummyTurker
11498ß0.0ßAlso, we improved the aggregation of views by attention.ßdummyTurker
11499ß0.0ßWe will add the result of CTX+NAME+DESC (only English) to the possible final version,ßdummyTurker
11500ß0.0ßwhich corresponds to the requested Yaghoobzadeh-et-al.ßdummyTurker
11501ß0.0ßbaseline.ßdummyTurker
11502ß0.0ßThe reason we didn't use the dataset in (Yaghoobzadeh&Schütze 2017) isßdummyTurker
11503ß0.0ßthat their corpus is commercial and because it's based onßdummyTurker
11504ß0.0ßFreebase, a resource that is no longer officially supported.ßdummyTurker
11505ß0.0ßWhen using Wikipedia, many Freebase entities are OOVs andßdummyTurker
11506ß0.0ßtherefore, we built our own dataset.ßdummyTurker
11507ß0.0ß"W2:""statistics_of_the_dataset...in_supplementary..."""ßdummyTurker
11508ß0.0ßWe agree that the dataset description in supplementary is not so clear,ßdummyTurker
11509ß0.0ßand in fact there is an error in the numbers.ßdummyTurker
11510ß0.0ßWe will correct it.ßdummyTurker
11511ß0.0ßNote:  this description is for the dataset we used in our experiments.ßdummyTurker
11512ß0.0ßThe full dataset contains >100 languages and >2 millionßdummyTurker
11513ß0.0ßentities.ßdummyTurker
11514ß0.0ßFor experimental setup, we needed to make a constraint on the number of languagesßdummyTurker
11515ß0.0ßto do our extensive evaluations.ßdummyTurker
11516ß0.0ßWe used a set of high-resource languages as well as some languages thatßdummyTurker
11517ß0.0ßthe authors have familiarity with (to makeßdummyTurker
11518ß0.0ßanalysis of results possible).ßdummyTurker
11519ß0.0ßFor the other 6 languages, we used only entity NAMEs because of the limitationsßdummyTurker
11520ß0.0ßin computation as well as paper limit.ßdummyTurker
11521ß0.0ßThe reason to build the dataset was a need for a public multilingual benchmarkßdummyTurker
11522ß0.0ß(Wikipedia) for entity typing.ßdummyTurker
11523ß0.0ß"Q1:""name-view_could_easily_be_expanded_with_a_character-based_representation."""ßdummyTurker
11524ß0.0ßWe used FASTTEXT embeddings to get word vectors, which is a subword basedßdummyTurker
11525ß0.0ßmodel.ßdummyTurker
11526ß0.0ßWe observed that adding task-specific character-based representation learnedßdummyTurker
11527ß0.0ßusing CNNs could make minor improvements for English, but it makes our evaluation studiesßdummyTurker
11528ß0.0ßmuch more expensive and also not practical for low-resource languages.ßdummyTurker
11529ß0.0ßWe can add results of adding character-level for our four mainßdummyTurker
11530ß0.0ßlanguages if the reviewer considers this to be important forßdummyTurker
11531ß0.0ßthe paper.ßdummyTurker
11532ß0.0ß"Q2:[Why]""...difference_between_performance_on_head_and_tail_entities...""?"ßdummyTurker
11533ß0.0ß1- tail entities usually don't have too much granularity in types/labels,ßdummyTurker
11534ß0.0ßcompared to the head entities which usually have multiple types with differentßdummyTurker
11535ß0.0ßdistribution.ßdummyTurker
11536ß0.0ß2- the frequency used to divide the entities comes from the entity mentions in the corpus,ßdummyTurker
11537ß0.0ßand this is not related to the quality of their description and name representations.ßdummyTurker
11538ß0.0ßAdditional-Comments-(Optional)ßdummyTurker
11539ß0.0ß"""...analysis_section.."""ßdummyTurker
11540ß0.0ßThat is a great suggestion.ßdummyTurker
11541ß0.0ßWe had done a related analysis on the performanceßdummyTurker
11542ß0.0ßwith respect to number of missing views.ßdummyTurker
11543ß0.0ßAnalyzing the attention weights is also interesting.ßdummyTurker
11544ß0.0ßWe will add this to the possible final version.ßdummyTurker
11545ß0.0ßThanks for your helpful comments.ßdummyTurker
11546ß0.0ßWe agree that the task of extracting SEU can be difficult.ßdummyTurker
11547ß0.0ßIn this paper, we propose a simple but effective extraction strategy.ßdummyTurker
11548ß0.0ßThe strategy focuses on two syntactic patterns that are curated for modeling sentiments in opinionated texts.ßdummyTurker
11549ß0.0ßBy comparing with models using words (JST, AT), sentences (ASUM), and segments which are extracted without using syntactic information (AATS), we demonstrate the effectiveness of using SEUs.ßdummyTurker
11550ß0.0ßTo further evaluate the impact of SEUs, we will compare the performance of Limbic using SEUs with the one using sentences.ßdummyTurker
11551ß0.0ßWe will also add an error analysis section to illustrate the role of SEUs.ßdummyTurker
11552ß0.0ßWe have considered both orders and chose this one because it performed better in some experiments in a related framework.ßdummyTurker
11553ß0.0ßWe will evaluate the order for the present framework as well, and summarize the finding in the revised paper.ßdummyTurker
11554ß0.0ßThanks for your helpful comments.ßdummyTurker
11555ß0.0ßAuthor, SEU, word embeddings, and discourse relations are major contributions in this paper.ßdummyTurker
11556ß0.0ßWe will add baselines using individual contributions to analyze the improvement of each contribution.ßdummyTurker
11557ß0.0ßWord semantic similarity helps on discovering less frequently occurred sentiment words.ßdummyTurker
11558ß0.0ßFor example, Limbic discovers 'contemporary' and 'tasteful' in aspect 'Decoration' as shown in Table 1.ßdummyTurker
11559ß0.0ßBy discovering sentiments of these words, Limbic is able to accurately reveal the sentiments of SEUs containing these words.ßdummyTurker
11560ß0.0ßAs you suggest, we will add an error analysis in Section 3.4 Sentiment Classification to demonstrate cases where discourse relations, aspects and sentiments of SEUs help on supporting predicting document level sentiment.ßdummyTurker
11561ß0.0ßWe will use the one additional page in the final version to enhance the description of the baseline models, and insert the majority baseline.ßdummyTurker
11562ß0.0ßWe use current structure in order to fit within space limit.ßdummyTurker
11563ß0.0ßGiven the extra one page space in the final version, we will move related work with baseline description to Section 2 after Introduction.ßdummyTurker
11564ß0.0ß"Subscript ""c"" in line 205 is a typo and should be ""r""."ßdummyTurker
11565ß0.0ß"Line 205 defines a binary potential for a general relation ""r""."ßdummyTurker
11566ß0.0ß"Subscript ""c"" in line 234 indicates comparison relation."ßdummyTurker
11567ß0.0ßSection 2.4 describes the overall conditional probability in Gibbs sampling.ßdummyTurker
11568ß0.0ßTo incorporate word embeddings, we use a Generalized Polya Urn process during the sampling process.ßdummyTurker
11569ß0.0ß"We will make ""Incorporating Word Embeddings"" as a separate section with two subsections: ""A Generalized Polya Urn Process"" and ""Sentiment Alignment""."ßdummyTurker
11570ß0.0ßWe created TripUser and YelpUser.ßdummyTurker
11571ß0.0ßWe run all models include AATS (Poddar et al.ßdummyTurker
11572ß0.0ß2017) with these datasets.ßdummyTurker
11573ß0.0ßWe will extend Table 6 and include it in Section 3.ßdummyTurker
11574ß0.0ß"We chose ""Limbic"" just as a reference to the limbic system in human brain which is the central part for supporting emotions."ßdummyTurker
11575ß0.0ßThanks for the insightful comments and suggestions about this paper.ßdummyTurker
11576ß0.0ßIn addition to the ROUGE evaluation, we also conducted human evaluation on 20 random samples from CNN/DailyMail test set and compared the summaries generated by our method with the outputs of Lead-3, Seq2seq-baseline and Coverage (See et al., 2017).ßdummyTurker
11577ß0.0ßThe output summaries of Coverage are provided by the authors.ßdummyTurker
11578ß0.0ßThree participants were asked to compare the generated summaries with the human summaries, and assess each summary from four independent perspectives:ßdummyTurker
11579ß0.0ß(1) Informative: How informative the summary is?ßdummyTurker
11580ß0.0ßWhether the summary describes the main points of the document?ßdummyTurker
11581ß0.0ß(2) Concise: How concise the summary is?ßdummyTurker
11582ß0.0ßIs there any redundant or non-salient information in the summary?ßdummyTurker
11583ß0.0ß(3) Coherent: How coherent (between sentences) the summary is?ßdummyTurker
11584ß0.0ß(4) Fluent: How fluent, grammatical the sentences of a summary are?ßdummyTurker
11585ß0.0ßEach property is assessed with a score from 1(worst) to 5(best).ßdummyTurker
11586ß0.0ßThe average results are presented as following:ßdummyTurker
11587ß0.0ßMethod                      Informative          Concise           Coherent           FluentßdummyTurker
11588ß0.0ßLead-3                            3.48                 3.20                  3.83                  4.01ßdummyTurker
11589ß0.0ßSeq2seq-baseline          3.08                 3.01                   2.96                  3.69ßdummyTurker
11590ß0.0ßCoverage                        3.26                 3.22                   3.19                  3.74ßdummyTurker
11591ß0.0ßOur Model                      3.73                  3.49                  3.51                  3.79ßdummyTurker
11592ß0.0ßThe results show that our model consistently outperforms the Seq2seq-baseline model and the previous state-of-the-art method Coverage.ßdummyTurker
11593ß0.0ßThe Seq2seq-baseline model usually generates repeated sentences, which causes it to achieve low informativeness score, conciseness score and coherence score.ßdummyTurker
11594ß0.0ßThe Coverage method reduces repetitions in summary, however, it usually loses salient information and includes non-salient information.ßdummyTurker
11595ß0.0ßBy contrast, our method is more effective to select salient information from the original document and generate more informative summaries.ßdummyTurker
11596ß0.0ßMoreover, the sentence-level modeling of document and summary makes the generated summaries achieve better inter-sentence coherence.ßdummyTurker
11597ß0.0ßCompared with the strong extractive baseline Lead-3, our model is able to generate more informative and concise summaries, which shows the advantage of abstractive methods.ßdummyTurker
11598ß0.0ßThe fluency scores also show the good ability of our model to generate fluent and grammatical sentences.ßdummyTurker
11599ß0.0ßThe human evaluation results demonstrate that the quality of summaries generated by our model is significantly better than the Coverage baseline.ßdummyTurker
11600ß0.0ßIn order to be directly comparable with the reported results in recent state-of-the-art methods, we use the same settings with NN-SE (Cheng and Lapata, 2016), SummaRuNNer (Nallapati et al., 2017) and Tan et al.ßdummyTurker
11601ß0.0ß(2017) which use Rouge recall metrics on the DailyMail test set while using full-length F1 metrics on other datasets.ßdummyTurker
11602ß0.0ßWith the same experimental settings, the results show that our model achieves significantly better performance.ßdummyTurker
11603ß0.0ßThe above human evaluation results also demonstrate that our model is able to generate summaries with significantly higher quality in terms of the informative, concise and coherent properties.ßdummyTurker
11604ß0.0ßThe Coverage method is effective in reducing repetitions in summary, but it usually loses salient information and contain some non-salient information.ßdummyTurker
11605ß0.0ßThe information selection layer in our method makes our system more effective to select salient information from the original document and generate more informative summaries.ßdummyTurker
11606ß0.0ßMoreover, our method models both the documents and summaries sentence by sentence, which causes the generated summaries to achieve better inter-sentence coherence.ßdummyTurker
11607ß0.0ßThe human evaluation results demonstrate that our model is able to generate more informative, concise and coherent summaries compared with the Coverage baseline.ßdummyTurker
11608ß0.0ßThe sentence selection module selects salient and relevant sentences for each summary sentence with a soft selection vector.ßdummyTurker
11609ß0.0ßBut the vector has sparsity property, which can be intuitively observed in Figure 2(a).ßdummyTurker
11610ß0.0ßOn average, the top 3 document sentences occupy over 90% weights of each summary sentence.ßdummyTurker
11611ß0.0ßFurthermore, the sentence selection vectors of different summary sentences are usually diversified.ßdummyTurker
11612ß0.0ßIt demonstrates that our model learns to select different sentences to generate different summary sentences to avoid repetitions.ßdummyTurker
11613ß0.0ßWe are grateful to receive the constructive and valuable comments!ßdummyTurker
11614ß0.0ßPlease allow us to do some responses:ßdummyTurker
11615ß0.0ßOur focus is a new framework for CWS and we implemented this idea with deep convolutional networks.ßdummyTurker
11616ß0.0ßAs you pointed, the framework is much simpler and reduces the CWS task into end-to-end binary classification, without further post-processing module.ßdummyTurker
11617ß0.0ßWe did not attempt to introduce a new neural module in our paper.ßdummyTurker
11618ß0.0ßAccording to ACL Author Guidelines, “if you have posted a non-anonymized version of your paper online before the start of the anonymity period, you may submit an anonymized version to the conference.” The anonymity period runs from 1 month before the submission deadline.ßdummyTurker
11619ß0.0ßWe have previously consulted the area chair Hai Zhao (zhaohai@cs.sjtu.edu.cn) on this point.ßdummyTurker
11620ß0.0ßYes, character-based CRF produces better recall as you noticed.ßdummyTurker
11621ß0.0ßHowever, precision and recall are just two aspects in the evaluation.ßdummyTurker
11622ß0.0ßF score is more commonly used for the evaluation of the performance.ßdummyTurker
11623ß0.0ßThank you for your suggestion.ßdummyTurker
11624ß0.0ßWe will add some analysis on it in the later version.ßdummyTurker
11625ß0.0ßWe are sorry to make you confused.ßdummyTurker
11626ß0.0ßHere “input depth” and “output depth” are the number of feature channels, while the former refer to the input tensor and the latter refer to the output tensor (line 300-324).ßdummyTurker
11627ß0.0ßWe will clarify this point more clearly in the later version.ßdummyTurker
11628ß0.0ßWe are grateful to receive the constructive and valuable comments!ßdummyTurker
11629ß0.0ßPlease allow us to do some responses:ßdummyTurker
11630ß0.0ßAccording to ACL Author Guidelines, “if you have posted a non-anonymized version of your paper online before the start of the anonymity period, you may submit an anonymized version to the conference.” We have previously consulted the area chair Hai Zhao (zhaohai@cs.sjtu.edu.cn) on this point.ßdummyTurker
11631ß0.0ßWhy neural net parameters are successful is still an open research problem.ßdummyTurker
11632ß0.0ßAs for the design of our net architecture, you can find our intuition in line 243 and line 357 that a dense block can explicitly concatenate multi-level character combination features.ßdummyTurker
11633ß0.0ß“Gap-based” is in comparison with “character-based” and “word-based”, and focus on gaps rather than characters or words (Line 013 – 026).ßdummyTurker
11634ß0.0ßDue to the page limit, we did not describe the latter two frameworks in detail.ßdummyTurker
11635ß0.0ßThank you for your suggestion.ßdummyTurker
11636ß0.0ßWe will add a brief description of these two in the later version.ßdummyTurker
11637ß0.0ßWord segmentation is a fundamental and preliminary task in Chinese.ßdummyTurker
11638ß0.0ßFor example, Facebook’s fastText “used the Stanford word segmenter for Chinese” in order to get Chinese word embedding.ßdummyTurker
11639ß0.0ß(https://fasttext.cc/docs/en/crawl-vectors.html)ßdummyTurker
11640ß0.0ßYes, we believe an end-to-end framework can reduce handcrafted design and help the model fit better.ßdummyTurker
11641ß0.0ßMoreover, deep learning provides us the opportunity to train an end-to-end model.ßdummyTurker
11642ß0.0ßWe are sorry but maybe you are confused with “Dense Block, 3, 128 × 28”.ßdummyTurker
11643ß0.0ßActually, you can find the detailed definition of “Dense Block” in Figure 3 (line 301).ßdummyTurker
11644ß0.0ßThank you for your suggestion.ßdummyTurker
11645ß0.0ßWe will add references for benchmark datasets into Introduction in the later version.ßdummyTurker
11646ß0.0ß(It can be found at line 418 and line 424 currently)ßdummyTurker
11647ß0.0ßThank you for your suggestion.ßdummyTurker
11648ß0.0ßWe choose the network architectures and hyper-parameters by the performance on the validation dataset (line 448).ßdummyTurker
11649ß0.0ßWe will clarify this point in the later version.ßdummyTurker
11650ß0.0ßWe are grateful to receive the constructive and valuable comments!ßdummyTurker
11651ß0.0ßPlease allow us to do some responses:ßdummyTurker
11652ß0.0ßIn fact, not all of the models that we compared reported their performance on a single model.ßdummyTurker
11653ß0.0ßFor example, in our results comparison Table 6, Sun (2010) used “Bootstrap aggregating (Bagging)” to ensemble their models (line 609) and Zhang et al.ßdummyTurker
11654ß0.0ß(2016) used a hybrid model (line 621).ßdummyTurker
11655ß0.0ßWe believe ensemble is a reasonable way for our baselines to achieve better performance.ßdummyTurker
11656ß0.0ßThank you for your consideration.ßdummyTurker
11657ß0.0ßWe will report our single model performance in the later version.ßdummyTurker
11658ß0.0ßActually, we have also noticed this point.ßdummyTurker
11659ß0.0ßWe mentioned in line 781 that “the use of LSTM-RNN and its variants in the gap-based framework is also interesting to be investigated.” However, our focus is a new framework for CWS and we implemented this idea with deep convolutional networks.ßdummyTurker
11660ß0.0ßThank you for your suggestion.ßdummyTurker
11661ß0.0ßWe will investigate how LSTMs works with gap-based framework in our future work.ßdummyTurker
11662ß0.0ßWe did not claim that “oracle” is the upper bound of the combined model.ßdummyTurker
11663ß0.0ßWe just tried to show that “Therefore, a more practical way to combine the results of pure uni-character and pure bi-character models is left for future investigation” (line 543-549).ßdummyTurker
11664ß0.0ßIn addition, we provide the examples to more clearly illustrate the performance of our model.ßdummyTurker
11665ß0.0ßYou can find our intuition of the gap-based framework in line 121-141 and of deep CNN in line 234-249.ßdummyTurker
11666ß0.0ßOur gap-based framework overcomes the character-based’s drawback of redundant labelling and word-based’s drawback of being nonparallel.ßdummyTurker
11667ß0.0ßThe training for a single model can finish in around 1 hour on a NVIDIA GeForce GTX 1080 Ti GPU.ßdummyTurker
11668ß0.0ßActually, the dense blocks can help reduce parameters, which Huang et al.ßdummyTurker
11669ß0.0ß(2017) have shown.ßdummyTurker
11670ß0.0ßTherefore, we did not regard the speed as a problem.ßdummyTurker
11671ß0.0ßWe did not report our OOV recall because we found that most of the models we make comparison with did not report their OOV recall.ßdummyTurker
11672ß0.0ßThank you for your suggestion.ßdummyTurker
11673ß0.0ßWe will add OOV recall to our baseline performance table in the later version.ßdummyTurker
11674ß0.0ßYes, we have tried to fine-tune the character bigram embedding and found it was easy to overfit for the bigram embedding.ßdummyTurker
11675ß0.0ßThank you for your suggestion.ßdummyTurker
11676ß0.0ßWe will clarify this point in the later version.ßdummyTurker
11677ß0.0ßThanks for your comments.ßdummyTurker
11678ß0.0ß(1) There were some considerations concerning the comparison.ßdummyTurker
11679ß0.0ßA fact on the automatic generation is different approaches generate different items.ßdummyTurker
11680ß0.0ßFor one thing, the items generated by other approaches are difficult to get, nor can we replicate their experiments to get the data.ßdummyTurker
11681ß0.0ßFor another, the experimental subjects are key to the experiment and directly affect the experimental results.ßdummyTurker
11682ß0.0ßTherefore, it is unfair to compare with other approaches as there lacks a uniform standard of comparison.ßdummyTurker
11683ß0.0ßIn line 695-709, we have compared the plausibility of the distractors generated by our approach with that of the distractor chosen randomly.ßdummyTurker
11684ß0.0ßHowever, we are now comparing with the baseline that all the distractors are selected randomly.ßdummyTurker
11685ß0.0ß(2) The advantages of word2vec-based approach areßdummyTurker
11686ß0.0ßIt is true that the difficulty of a question also depends on the stem target word and distractors if the question centers on a content words such as nouns, adjectives, etc.ßdummyTurker
11687ß0.0ßHowever, our work focus on the automatic generation of multiple-choice items for prepositions.ßdummyTurker
11688ß0.0ßPreposition class in English is a closed class (with about 150 prepositions).ßdummyTurker
11689ß0.0ßFrom this perspective, the target word and distractors don’t affect the question difficulty so much as they do on questions for content words.ßdummyTurker
11690ß0.0ßThat’s why we examine the whole question difficulty considering the stem difficulty.ßdummyTurker
11691ß0.0ßWe generated 400 preposition questions based on the BNC sub-corpus, and 4 preposition tests designed for evaluation, each including 25 items.ßdummyTurker
11692ß0.0ßNote that this sub-corpus contains only 1765 sentences.ßdummyTurker
11693ß0.0ßThe number of questions that can be generated depends on the size of the corpus.ßdummyTurker
11694ß0.0ßTherefore, our approach is quite efficient.ßdummyTurker
11695ß0.0ßThanks for your valuable comments.ßdummyTurker
11696ß0.0ßWe feel rather disappointed at your comments, since we did considerable work and also many participants were involved in the evaluation.ßdummyTurker
11697ß0.0ßHere we restate the main contributions.ßdummyTurker
11698ß0.0ß(1) We described an approach to automatic generation of multiple-choice items for prepositions by addressing stem and distractor generation with valid and feasible methods.ßdummyTurker
11699ß0.0ß(2) The multiple-choice items generated are of decent quality.ßdummyTurker
11700ß0.0ßGiven the existing tools and resources, a teaching and learning platform for the preposition use can be built, which will have an impact in the field of education.ßdummyTurker
11701ß0.0ß(3) We proposed a method to identify sentences that are of approximate similar difficulty, so that generating the multiple-choices items for students at a certain level became possible.ßdummyTurker
11702ß0.0ßIn the field of SLA, a principle is that the material to which learners are exposed must be comprehensible to the learners in order for leaning to proceed optically (Krashen,1981).ßdummyTurker
11703ß0.0ßTherefore, the stem difficulty of the multiple-choice items is crucial.ßdummyTurker
11704ß0.0ßOur consideration on stem difficulty during the generation process is undoubtedly significant and contributive to the ESL teaching and learning.ßdummyTurker
11705ß0.0ßEssentially our work is a great attempt on the application of CL techniques to language learning.ßdummyTurker
11706ß0.0ßYour proposal on the baseline based on the language model score is very creative, worth trying.ßdummyTurker
11707ß0.0ßLM probability has been proved to be a strong sentence complexity feature in the work of Specia et al (2013).ßdummyTurker
11708ß0.0ßPlease refer to Specia, L., Shah, K., Camargo, J. G., & Cohn, T. (2013).ßdummyTurker
11709ß0.0ßQUEST - A TRANSLATION QUALITY ESTIMATION FRAMEWORK.ßdummyTurker
11710ß0.0ßACL.ßdummyTurker
11711ß0.0ßWe have normalized every feature by z-scores when calculating the Euclidean distance.ßdummyTurker
11712ß0.0ßWe will clarify in the revised paper.ßdummyTurker
11713ß0.0ßTextbook compilation follows the rules of cognitive development and consider the teaching syllabus.ßdummyTurker
11714ß0.0ßGenerally, the textbook covers most learners’ need and can reflect their average language level.ßdummyTurker
11715ß0.0ßThus, we didn’t consider the average score of the students from each level on the parallel sentences from the textbook.ßdummyTurker
11716ß0.0ßBesides, we have evaluated the quality through human annotation and students’ performance in the tests.ßdummyTurker
11717ß0.0ßThanks for the good question.ßdummyTurker
11718ß0.0ßWe are now considering giving each feature a weight by calculating their correlation to the sentence difficulty.ßdummyTurker
11719ß0.0ßWe thank the reviewer for the helpful comments!ßdummyTurker
11720ß0.0ßAs pointed out by R1, recent work in deep learning has created a debate about the utility of features, or more generally of traditional linguistic knowledge, in end tasks with ample data.ßdummyTurker
11721ß0.0ßIn particular, previous work in relation extraction [2] has shown that word sequence-based models such as augmented LSTMs can outperform existing dependency-based models on a large-scale dataset.ßdummyTurker
11722ß0.0ßWe argue that this is likely because existing dependency models either fail to exploit the dependency structure, or prune the trees too aggressively and therefore lose critical information, where we provide real-world examples for the latter case.ßdummyTurker
11723ß0.0ßMotivated by these examples, we proposed the path-centric pruning technique.ßdummyTurker
11724ß0.0ßWhile as pointed out by R3 this technique does not guarantee optimal pruning on each input tree, we find it to function as an inductive bias that is particularly suitable for relation extraction.ßdummyTurker
11725ß0.0ßWe further show this on three different dependency-based models (Section 5.5).ßdummyTurker
11726ß0.0ßThis was not previously found and we believe it is useful contribution.ßdummyTurker
11727ß0.0ßMoreover, although GCN has been used in other tasks like semantic role labeling, the proposed architecture which combines entity encoding and sentence encoding for relation extraction was not obvious and the result of considerable experimentation.ßdummyTurker
11728ß0.0ßTherefore, we do believe this model architecture and the proposed pruning technique to be significant contributions.ßdummyTurker
11729ß0.0ßAdditionally, as also pointed out by R1 and R2, we believe our analysis on different evaluation settings on the SemEval dataset provides insights for future evaluation and use of relation extraction systems, and is also important contribution.ßdummyTurker
11730ß0.0ßThe introduction of BiLSTM components only increases the runtime marginally.ßdummyTurker
11731ß0.0ßOn a Titan Xp GPU and with a minibatch size of 25, the best Tree-LSTM model used in the paper took 6.54 seconds per minibatch, while the best C-GCN model took 0.08 seconds per minibatch and the best GCN model took 0.07 seconds per minibatch.ßdummyTurker
11732ß0.0ßWe implemented all models with PyTorch.ßdummyTurker
11733ß0.0ßWe will add detailed efficiency comparisons into the paper.ßdummyTurker
11734ß0.0ßOur intention in including the C-GCN + PA-LSTM result was to show the complementary strengths of these two different model architectures and that this can be exploited using a simple interpolation method, which we further analyze in Section 6.2.ßdummyTurker
11735ß0.0ßOn the other hand, the best PA-LSTM ensemble result as reported in [2] is an ensemble of 5 different model runs via majority vote, and to some extent a direct comparison between these two is not fair.ßdummyTurker
11736ß0.0ßAdditionally, when we ensembled 2 PA-LSTM model runs using the authors’ open-source implementation, F1 drops from 67.2 to 65.8.ßdummyTurker
11737ß0.0ßUsing the paired bootstrap test as described in [1], we find the difference in F1 score between our reported C-GCN model and the best PA-LSTM model (detailed results created with open-source implementation) is statistically significant, with p = 0.029.ßdummyTurker
11738ß0.0ßWe will include this result in the paper.ßdummyTurker
11739ß0.0ßOur intention in including the interpolation results on TACRED was not to establish new benchmarks that future work should compare with, but to show the complementary strengths between our dependency-based models and the sequence-based model, where an established ensemble benchmark (of PA-LSTM) is also available for reference.ßdummyTurker
11740ß0.0ß[1] Berg-Kirkpatrick, Taylor, David Burkett, and Dan Klein.ßdummyTurker
11741ß0.0ß"""An empirical investigation of statistical significance in NLP."""ßdummyTurker
11742ß0.0ßEMNLP 2012.ßdummyTurker
11743ß0.0ß[2] Zhang, Yuhao, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning.ßdummyTurker
11744ß0.0ß"""Position-aware Attention and Supervised Data Improve Slot Filling."""ßdummyTurker
11745ß0.0ßEMNLP 2017.ßdummyTurker
11746ß0.0ßWe thank the reviewer for the helpful comments!ßdummyTurker
11747ß0.0ßWe will move the hyperparameter settings into the main body using the additional page.ßdummyTurker
11748ß0.0ßWe experimented with adding dependency type information by adding input gates parameterized by the dependency type at each layer of the network.ßdummyTurker
11749ß0.0ßHowever, this augmentation did not yield improvement on the dev set.ßdummyTurker
11750ß0.0ßWe will add discussion of this in the model section.ßdummyTurker
11751ß0.0ßThe example in Figure 2 represents a simple sentence with a shallow dependency structure.ßdummyTurker
11752ß0.0ßK=0 corresponds to keeping only the shortest path, therefore in this example only “relative” and the entity tokens will be kept; K=1 corresponds to keeping all tokens in this example, since all tokens are at most distance 1 away from the path.ßdummyTurker
11753ß0.0ßIn a sentence with more complex structure, more tokens will be pruned away by setting K=1.ßdummyTurker
11754ß0.0ßWe will add more examples and make the explanation of our pruning technique clearer.ßdummyTurker
11755ß0.0ßOn the TACRED dataset, using the paired bootstrap test as described in [1], we find the difference in F1 score between our reported C-GCN model and the best PA-LSTM (detailed results created with the authors’ open-source implementation) is statistically significant, with p = 0.029.ßdummyTurker
11756ß0.0ßOn the SemEval dataset, the difference between the C-GCN model and PA-LSTM model is also significant with p < 0.001 under the with-entity evaluation and p = 0.031 under the mask-entity evaluation.ßdummyTurker
11757ß0.0ßWe will add this into the paper.ßdummyTurker
11758ß0.0ß[1] Berg-Kirkpatrick, Taylor, David Burkett, and Dan Klein.ßdummyTurker
11759ß0.0ß"""An empirical investigation of statistical significance in NLP."""ßdummyTurker
11760ß0.0ßEMNLP 2012.ßdummyTurker
11761ß0.0ßThank you for your useful comments and letting us know the relevant papers.ßdummyTurker
11762ß0.0ßDue to the page limit, we could not include related work section describing the works relevant to our work.ßdummyTurker
11763ß0.0ßIf this paper is accepted, we will include the section and refer these papers from the section.ßdummyTurker
11764ß0.0ßWe also intend to conduct experiments using more recent entity linking methods such as the ones proposed in these papers.ßdummyTurker
11765ß0.0ßThank you for your thoughtful comments.ßdummyTurker
11766ß0.0ßNABoE-entity is the model without using the embeddings described in Section 2.3.ßdummyTurker
11767ß0.0ßIn this model, all parameters are initialized randomly.ßdummyTurker
11768ß0.0ßWe intend to test the non-neural bag-of-entities scenario using entity linking methods.ßdummyTurker
11769ß0.0ßThank you for letting us know the interesting paper.ßdummyTurker
11770ß0.0ßWe would like to cite this paper in our camera-ready version.ßdummyTurker
11771ß0.0ßHowever, our paper is quite different to this paper.ßdummyTurker
11772ß0.0ßAlthough the motivation of this paper is somewhat similar to ours, the paper focuses on document ranking task in information retrieval domain.ßdummyTurker
11773ß0.0ßFurther, the ranking model is based on a simple combination of three sub-models each of which consists of a linear model with conventional hand-engineered features.ßdummyTurker
11774ß0.0ßWe appreciate your insightful comments!ßdummyTurker
11775ß0.0ßIt is possible to use attention mechanism for the TAGME baseline and we would like to test the scenario.ßdummyTurker
11776ß0.0ßHowever, because the performance of the TAGME baseline was lower than that of the NABoE-entity (no att.)ßdummyTurker
11777ß0.0ßmodel  (the model without using attention mechanism), it is likely that the NABoE entity model (with the attention mechanism) will outperform the TAGME baseline with the attention mechanism.ßdummyTurker
11778ß0.0ßAs shown in Table 3, most of the entities with strong attention scores are closely related to the corresponding topics, thus they are likely referred from the documents.ßdummyTurker
11779ß0.0ßWe would like to conduct an additional experiments to confirm that such entities are actually the correct referent entities of mentions in the documents.ßdummyTurker
11780ß0.0ßWord averaging is used in state-of-the-art entity linking methods (e.g., Yamada et al., 2016, Yamada et al., 2017) and also text classification methods (e.g., Iyyer et al., 2015, Joulin et al., 2016, Jin et al., 2016).ßdummyTurker
11781ß0.0ßTherefore, we think word averaging model can successfully capture the useful semantics in the documents.ßdummyTurker
11782ß0.0ßApplying attention mechanism over words is an interesting idea, but it is out of the scope of this work.ßdummyTurker
11783ß0.0ßAlso, please note that, the BoE baseline achieved the better accuracy than various state-of-the-art document embeddings including the model based on LDA and Doc2Vec (Liu et al., 2015, Jin et al., 2016).ßdummyTurker
11784ß0.0ßWe will describe this clearly in the camera-ready paper.ßdummyTurker
11785ß0.0ßWe could not include many baseline methods due to the limitation of the page space.ßdummyTurker
11786ß0.0ßIf this paper is accepted, we will include more models such as LDA and Doc2Vec in the additional page of the camera-ready paper.ßdummyTurker
11787ß0.0ßFurther, the reason why some scores of the LM-LSTM baseline is missing is that the original paper (Dai and Le, 2015) conducted experiments only using accuracy on the 20NG dataset.ßdummyTurker
11788ß0.0ßFurther, because the source code is not publicly available, it is not easy to reproduce the results.ßdummyTurker
11789ß0.0ßWe did not use any particular regularizations in our model; We simply used stochastic gradient descent and the learning rate was controlled by Adam.ßdummyTurker
11790ß0.0ßNote that, because we will publicize the source code, readers can easily reproduce our results.ßdummyTurker
11791ß0.0ßFurther, we will describe the model size (number of total parameters) in our camera-ready version.ßdummyTurker
11792ß0.0ßDue to the space limit, we could not provide detailed error analysis.ßdummyTurker
11793ß0.0ßWe also intend to include additional error analysis in the camera-ready version.ßdummyTurker
11794ß0.0ßState-of-the-art entity linking methods are computationally intensive because they typically model the global entity coherence in the target document, which is known to be NP-hard (Kulkarni et al,, 2009, Ratinov et al., 2011).ßdummyTurker
11795ß0.0ßIf an entity linking method only depends on simple local contexts, the theoretical computational cost is similar to that of our model.ßdummyTurker
11796ß0.0ßHowever, addressing the both problem (entity linking and text classification) in a single neural network model is generally advantageous when we implement the model with the linear algebra implementations such as BLAS and recent GPU-based ones.ßdummyTurker
11797ß0.0ßDal and Le, 2015: Semi-supervised Sequence LearningßdummyTurker
11798ß0.0ßIyyer et al., 2015: Deep Unordered Composition Rivals Syntactic Methods for Text ClassificationßdummyTurker
11799ß0.0ßKulkarni et al., 2009: Collective Annotation of Wikipedia Entities in Web TextßdummyTurker
11800ß0.0ßJin et al., 2016: Bag-of-Embeddings for Text ClassificationßdummyTurker
11801ß0.0ßJoulin et al., 2016: Bag of Tricks for Efficient Text ClassificationßdummyTurker
11802ß0.0ßLiu et al., 2015: Topical Word EmbeddingsßdummyTurker
11803ß0.0ßRatinov et al., 2011: Local and Global Algorithms for Disambiguation to WikipediaßdummyTurker
11804ß0.0ßYamada et al., 2016: Joint Learning of the Embedding of Words and Entities for Named Entity DisambiguationßdummyTurker
11805ß0.0ßYamada et al., 2017: Learning Distributed Representations of Texts and Entities from Knowledge BaseßdummyTurker
11806ß0.0ßThe main goal of the paper is neither similar word selection nor new (novel) word modelling which are old, but hot problems.ßdummyTurker
11807ß0.0ßOur main focus is to improve the performance of NNLM by expanding its vocabulary coverage.ßdummyTurker
11808ß0.0ßIn other words, adding words to the vocabulary set of pretrained NNLM.ßdummyTurker
11809ß0.0ßThus, we believe that our paper contains a thorough explanation why it works.ßdummyTurker
11810ß0.0ßAgain, selecting similar words is not the main focus of the paper as measuring word similarity has been thoroughly studied and very good results are obtained.ßdummyTurker
11811ß0.0ße.g.ßdummyTurker
11812ß0.0ßskip-gram,CBOW,Glove,LSA.ßdummyTurker
11813ß0.0ßAny existing techniques can be used, you can also select them manually.ßdummyTurker
11814ß0.0ßOur experiments are enough to show the effectiveness of proposed method.ßdummyTurker
11815ß0.0ßNo_commentßdummyTurker
11816ß0.0ß"-""The word embedding matrix S and U are fine-tuned in the training of language model."ßdummyTurker
11817ß0.0ß"Right?"""ßdummyTurker
11818ß0.0ß+Right, and in our case, they contain 10k unique words.ßdummyTurker
11819ß0.0ß"-""The embeddings of OOS words are not."ßdummyTurker
11820ß0.0ß"How do you assure that the embeddings of OOS and IS words are still in the same semantic space as you use similarity of embeddings to find candidate words?"""ßdummyTurker
11821ß0.0ß+To find candidate words we used skip-gram model trained over 1 million unique words on the same OBWB dataset.ßdummyTurker
11822ß0.0ßIf OOS word is not covered by skip-gram, candidates can be selected manually.ßdummyTurker
11823ß0.0ß"-""If they are in same space, you can directly use the embedding of OOS word to expand matrix, rather than approximate it with candidate IS words."ßdummyTurker
11824ß0.0ß"There is a contradiction."""ßdummyTurker
11825ß0.0ßNote that word embedding matrices learned by NNLM and skip-gram are different.ßdummyTurker
11826ß0.0ßThus, OOS words are not in the embedding space learned by baseline LSTM.ßdummyTurker
11827ß0.0ßHence, there is no contradiction.ßdummyTurker
11828ß0.0ß"-""In this paper, the author use skip-gram, a classical neural language model, to find similar words to the new words."ßdummyTurker
11829ß0.0ß"I don't think it appropriate, since the purpose of the paper is to generate word representation for new words."""ßdummyTurker
11830ß0.0ß+Dear reviewer, this paper addresses the limitations in vocabulary coverage of pretrained neural network based language models (NNLM).ßdummyTurker
11831ß0.0ßTherefore, the purpose of the paper is to efficiently expand the vocabulary coverage of pretrained NNLM without incurring expensive retraining and using additional training data.ßdummyTurker
11832ß0.0ß+To achieve this, we observed that input and output projection layers of NNLM typically learn the input and output word embedding matrices.ßdummyTurker
11833ß0.0ßHence, we used these learned word embedding vectors to model words which are not in the initial vocabulary coverage of NNLM, we call them out-of-shortlist (OOS) words.ßdummyTurker
11834ß0.0ßThe OOS words can be either existing word or new (novel) word.ßdummyTurker
11835ß0.0ßOur method can model both of them in the word embedding space learned by NNLM.ßdummyTurker
11836ß0.0ßIn other words, our method efficiently adds OOS words into the vocabulary set of existing NNLM.ßdummyTurker
11837ß0.0ß"-""The main challenge is to select appropriate candidate words. """ßdummyTurker
11838ß0.0ß+To model an OOS word, we need to select appropriate candidate words from the initial vocabulary set of NNLM.ßdummyTurker
11839ß0.0ßIt is challenging, but it is not a new problem as measuring word similarity has been thoroughly studied and very good results are obtained.ßdummyTurker
11840ß0.0ßThe candidates can be selected manually or automatically.ßdummyTurker
11841ß0.0ßFor example, if a target OOS word is a novel word, candidate words can be selected manually by a human expert.ßdummyTurker
11842ß0.0ßLikewise, if a target OOS word is an existing word, they can be selected automatically using popular models such as skip-gram, CBOW and GLOVE.ßdummyTurker
11843ß0.0ßTherefore, in our paper, we just employed the skip-gram model, which can be trained over very large vocabulary within several hours, and we didn't pay much attention to this challenge.ßdummyTurker
11844ß0.0ßOur main interest is to efficiently expand the vocabulary coverage of pretrained NNLM rather than selecting candidate words.ßdummyTurker
11845ß0.0ß"-""if we can find similar words to the new words using the skip-gram word vectors of them, why not use the skip-gram word vectors directly?"ßdummyTurker
11846ß0.0ß"It kind of 'circular dependency'."""ßdummyTurker
11847ß0.0ß+As was mentioned above, the skip-gram model was used to find similar words for existing words.ßdummyTurker
11848ß0.0ßFor new (novel) words, the similar words can be selected manually.ßdummyTurker
11849ß0.0ßIn our case, our test data didn't have many new words, thus, we just ignored them.ßdummyTurker
11850ß0.0ß"-""Although the proposed model outperforms KN5 and LSTM in the experiment, the comparison is not fair."ßdummyTurker
11851ß0.0ßFor KN5 and LSTM, only the training data of OBWB is used.ßdummyTurker
11852ß0.0ß"However, for the propose VA-LSTM, the publicized skip-gram vector use much large training data."""ßdummyTurker
11853ß0.0ß+All language models and skip-gram model were trained on exactly the same training data OBWB.ßdummyTurker
11854ß0.0ß"-""In addition, the baseline neural language model is quite simple."""ßdummyTurker
11855ß0.0ß+I assure you that baseline recurrent LSTM language model is the state-of-the-art which achieves the lowest perplexity and word error rates (in speech recognition), please check the recent papers in speech recognition.ßdummyTurker
11856ß0.0ß"-""The authors are also expected to compare VA-LSTM to strong baselines, such as Glove."""ßdummyTurker
11857ß0.0ß+The VA-LSTM is obtained by expanding the vocabulary of baseline recurrent LSTM language model.ßdummyTurker
11858ß0.0ßThus, it is still the same language model, but with the larger vocabulary.ßdummyTurker
11859ß0.0ß"As you know, the language models are used to quantify the ""correctness"" of a given word sequence."ßdummyTurker
11860ß0.0ßOn the other hand, GLOVE is a special model used to produce word embeddings.ßdummyTurker
11861ß0.0ßHence, I didn't understand your statement.ßdummyTurker
11862ß0.0ßThis letter itself means a rebuttal, but your suggestion is actually correct.ßdummyTurker
11863ß0.0ßWe are not native speakers, and will do the language sharpening accordingly.ßdummyTurker
11864ß0.0ßIn addition, we obtain quite many nice suggestions from you and other two reviewers, they are helpful.ßdummyTurker
11865ß0.0ßMerci.ßdummyTurker
11866ß0.0ßThese are good refs.ßdummyTurker
11867ß0.0ßWe do need to address more of the dataset selection.ßdummyTurker
11868ß0.0ßMetric will be used in our revised version.ßdummyTurker
11869ß0.0ßNice suggestions.ßdummyTurker
11870ß0.0ßThans.ßdummyTurker
11871ß0.0ßN/AßdummyTurker
11872ß0.0ßN/AßdummyTurker
11873ß0.0ßN/AßdummyTurker
11874ß0.0ßN/AßdummyTurker
11875ß0.0ßYes, better design of syntactic distance will be helpful.ßdummyTurker
11876ß0.0ßIn addition, we obtained quite many helpful suggestions from you and other two reviewers.ßdummyTurker
11877ß0.0ßThese are nice.ßdummyTurker
11878ß0.0ßThanks a lot.ßdummyTurker
11879ß0.0ßWe are considering to use some other database to do the evaluation.ßdummyTurker
11880ß0.0ßWe are non-native English speakers.ßdummyTurker
11881ß0.0ßWill sharpen the writing in language.ßdummyTurker
11882ß0.0ßN/AßdummyTurker
11883ß0.0ßN/AßdummyTurker
11884ß0.0ßWe didn't treat distance(A,B) if they appear in separate sentences.ßdummyTurker
11885ß0.0ßKeep the shortest one might be an option.ßdummyTurker
11886ß0.0ßHowever, word2vec helped to calculate distance(synonym_A, synonym_B), in this circumstance.ßdummyTurker
11887ß0.0ßGood suggestion.ßdummyTurker
11888ß0.0ßYes, we can give statistics of found words.ßdummyTurker
11889ß0.0ßCorrect.ßdummyTurker
11890ß0.0ßThe distance here is the dependency relationships.ßdummyTurker
11891ß0.0ßActually this is an improved work of Xiaolei&al 2017, who is also member of our team.ßdummyTurker
11892ß0.0ßYes, you are right, we need do more enhancement in this version.ßdummyTurker
11893ß0.0ßThanks for many suggestions given by you and other two reviewers.ßdummyTurker
11894ß0.0ßHit the point!ßdummyTurker
11895ß0.0ßFrom our view, the smaller the text size, the greater the difficulty.ßdummyTurker
11896ß0.0ßAnd we need explain more in the size setting of the dataset.ßdummyTurker
11897ß0.0ßYes.ßdummyTurker
11898ß0.0ßWe need more metrics.ßdummyTurker
11899ß0.0ßPrecision, recall, MAP, ROUGE...ßdummyTurker
11900ß0.0ßThat's an typos.ßdummyTurker
11901ß0.0ßShould be changed into keyword extraction.ßdummyTurker
11902ß0.0ßN/AßdummyTurker
11903ß0.0ßSuggestion of Hulth2003, zkrapivin209, SemEval2010 is nice!ßdummyTurker
11904ß0.0ßThanks.ßdummyTurker
11905ß0.0ßThe importance of context before and after entities is widely known.ßdummyTurker
11906ß0.0ßAccording to the best of our knowledge, we present the first approach for applying split sentence embedding from a sentence depending on each entity that can have a relationship without considering together in a pair.ßdummyTurker
11907ß0.0ßEven if a single sentence does not contain a pair of entities, we have considered the extensibility that can be used to obtain the important context needed for training for relation extraction.ßdummyTurker
11908ß0.0ßIn the null-subject language (e.g.ßdummyTurker
11909ß0.0ßKorean, Polish, Japanese), a large amount of free text is presented with only one entity at the single sentence boundary which is completely excluded from the distant supervision method.ßdummyTurker
11910ß0.0ßThis paper does not consider the various features that were used in other relational classification studies.ßdummyTurker
11911ß0.0ßWe used only textual features (i.e.ßdummyTurker
11912ß0.0ßsentence itself) without any external resources such as WordNet, or the NLP tools such as a parser, POS tagger and so on.ßdummyTurker
11913ß0.0ß(We wanted to discuss improvements to the ground truth of contextual features  that works in any language environment).ßdummyTurker
11914ß0.0ßTherefore, it is hard to directly compare with other state-of-the-art systems at the moment, but we will build the same feature environment and fix it for direct comparison.ßdummyTurker
11915ß0.0ßSince the method proposed in the paper is not affected by the data specification, we will supplement the experiment in the near future.ßdummyTurker
11916ß0.0ßThank you for your comment.ßdummyTurker
11917ß0.0ßIn this paper, we propose a method for efficiently generating sentence embedding (strictly chunk embedding) in the learning stage of relation extraction without pre-trained word embedding.ßdummyTurker
11918ß0.0ßWe will use the proposed embedding technique in the end-to-end relation extraction.ßdummyTurker
11919ß0.0ßCurrently, the architecture of relation extraction is being expanded by using the proposed chunk embeddings with neural network architectures such as LSTM and RNN.ßdummyTurker
11920ß0.0ßAccording to the best of our knowledge, we present the first approach for applying split sentence embedding from a sentence depending on each entity that can have a relationship without considering together in a pair.ßdummyTurker
11921ß0.0ßEven if a single sentence does not contain a pair of entities, we have considered the extensibility that can be used to obtain the important context needed for training for relation extraction.ßdummyTurker
11922ß0.0ßIn the null-subject language (e.g.ßdummyTurker
11923ß0.0ßKorean, Polish, Japanese), a large amount of free text is presented with only one entity at the single sentence boundary which is completely excluded from the distant supervision method.ßdummyTurker
11924ß0.0ßIn this paper, we propose a simple but effective method for performing without NLP tools such as parser.ßdummyTurker
11925ß0.0ßFor many non-English language data, it is difficult to acquire a highly efficient parser; thus, the practical application cannot extract relations in various language environments.ßdummyTurker
11926ß0.0ßDBpedia is a multilingual KB with the ontology written in English, with various language resources attached.ßdummyTurker
11927ß0.0ßFor example, the following triple exists in Korean versions of DBpediaßdummyTurker
11928ß0.0ß<http://ko.dbpedia.org/resource/스타워즈> <http://dbpedia.org/ontology/writer> <http://ko.dbpedia.org/resource/조지_루카스>.ßdummyTurker
11929ß0.0ß"In this ""스타워즈"" represents “Star Wars” and “George_Lucas”."ßdummyTurker
11930ß0.0ßOur goal is to extend Korean structured data with the common ontology of knowledge base using DBpedia.ßdummyTurker
11931ß0.0ßWe obtained the precision, recall, and F1 scores for each of the 55 relationship types in the experiment.ßdummyTurker
11932ß0.0ßAnd the reported average is a prevalence-weighted macro-average across classes.ßdummyTurker
11933ß0.0ßThe table shows the sum of the weighted average P / R / F1 scores from each class, rather than simply the harmonic mean for P / R. The explanation was omitted and caused confusion.ßdummyTurker
11934ß0.0ßThere are 230,000 sentences that collect all the sentences that correspond to tuples of DBpedia using the distant supervision method.ßdummyTurker
11935ß0.0ßThe whole labeled data has an imbalance in the number of labeled sentences by relation type.ßdummyTurker
11936ß0.0ßIn our experiments, we conducted a relation classification for the top-55 (>1,000 labeled sentences) relations except for the types of relations that have very little labeled data.ßdummyTurker
11937ß0.0ßIn total, approximately 142,000 labeled sentences were collected based on 215 DBpedia relations.ßdummyTurker
11938ß0.0ßWe obtained the precision, recall, and F1 scores for each of the 55 relationship types in the experiment.ßdummyTurker
11939ß0.0ßAnd the reported average is a prevalence-weighted macro-average across classes.ßdummyTurker
11940ß0.0ßThe table shows the sum of the weighted average P / R / F1 scores from each class, rather than simply the harmonic mean for P / R. The explanation was omitted and caused confusion.ßdummyTurker
11941ß0.0ßThanks for reviewing our paper.ßdummyTurker
11942ß0.0ßIn the next revision, we would add the requested.ßdummyTurker
11943ß0.0ßUsing the exact same word would work for intrinsic evaluation, i.e.ßdummyTurker
11944ß0.0ßall extracted minimum spans contain the annotated ones.ßdummyTurker
11945ß0.0ßHowever, it wouldn't affect the extrinsic evaluation at all, i.e.ßdummyTurker
11946ß0.0ß"we get two percent performance improvements by using MINA minimum spans while there would be a zero effect on the overall performance by ""exactly the same word"" baseline."ßdummyTurker
11947ß0.0ßLines 297-299 indicate the choice of a PCFG parser vs. a neural one.ßdummyTurker
11948ß0.0ßWe would add this information in the next revision.ßdummyTurker
11949ß0.0ßWe only use a neural parser for intrinsic evaluations, footnote4 but we didn't use that for coreference evaluations.ßdummyTurker
11950ß0.0ßSince the use of gold vs. PCFG parser only results in about 0.2 difference in coreference evaluations, the effect of using neural parser couldn't be any larger than that.ßdummyTurker
11951ß0.0ßNo, it wouldn't change any of the coreference results.ßdummyTurker
11952ß0.0ßThanks for reviewing our paper.ßdummyTurker
11953ß0.0ßWe use the comparison of our automatically extracted minimum spans to those that are manually annotated in MUC and ARRAU, as the justification of our algorithm.ßdummyTurker
11954ß0.0ßAgain, we use manual minimum span evaluations of MUC and ARRAU as the intrinsic evaluation of MINA, i.e.ßdummyTurker
11955ß0.0ßlines 246-271.ßdummyTurker
11956ß0.0ßMUC, ACE and ARRAU corpora that include manually annotated minimum spans are all only for English.ßdummyTurker
11957ß0.0ßWe could have applied MINA to other languages, but we wouldn't have a ground truth to evaluate the generated minimum spans.ßdummyTurker
11958ß0.0ßThanks for reviewing our paper.ßdummyTurker
11959ß0.0ßThe extracted minimum span of gold and system mentions should be exactly the same for them to be considered various spans of the same mention.ßdummyTurker
11960ß0.0ßTherefore, if one extracted minimum span is a subset of the other, they are minimum spans of two different mentions.ßdummyTurker
11961ß0.0ß"For instance, for the text span ""Mary and John"", there are three mentions, namely ""Mary"", ""John"", and ""Mary and John""."ßdummyTurker
11962ß0.0ß"The minimum spans of the above mentions are ""Mary"", ""John"", and {""Mary"", ""John""}."ßdummyTurker
11963ß0.0ßThe first two minimum spans are a subset of the third, but they belong to different mentions.ßdummyTurker
11964ß0.0ßSince we use the same algorithm to extract the minimum spans for both gold and system mentions, doing the exact right thing is not of critical importance, as long as the algorithm extracts different minimum spans for different mentions.ßdummyTurker
11965ß0.0ßa.ßdummyTurker
11966ß0.0ßThe majority of annotated minimum spans in MUC and ARRAU only include one word.ßdummyTurker
11967ß0.0ßTherefore, for the majority of mentions with more than one word, the maximum span is not equal to the minimum span.ßdummyTurker
11968ß0.0ßWe can also compute the exact ratios.ßdummyTurker
11969ß0.0ßb & c: we would incorporate those in the next revision of the paper.ßdummyTurker
11970ß0.0ßd. We believe that it isn't beneficial to try to get in the system mentions that are shorter than the detected minimum spans.ßdummyTurker
11971ß0.0ßDo you have an example in mind that there is some good in incorporating system mentions that are shorter than a detected minimum span?ßdummyTurker
11972ß0.0ßa.ßdummyTurker
11973ß0.0ßThe improvements in the performance come from finding gold matches for some unmatched system mentions, e.g.ßdummyTurker
11974ß0.0ß“an extensive presence, of course in this country” in Example1.ßdummyTurker
11975ß0.0ßIt is not about moving around the span of unmatched system mentions, it is about finding a gold match for some of those unmatched ones.ßdummyTurker
11976ß0.0ßBy using minimum spans performance improves for different systems and on different datasets, it isn't a by chance improvements.In the next revision,  we add examples about the system-gold mention alignments that causes this improvements.ßdummyTurker
11977ß0.0ßb.ßdummyTurker
11978ß0.0ßLines 161-174 describe the results of Peng et al.ßdummyTurker
11979ß0.0ß2015 head finding rules in coreference evaluation.ßdummyTurker
11980ß0.0ßCai and Strube, 2010 is not related to our work.ßdummyTurker
11981ß0.0ßThe issue that is addressed in our work is unmatched system mentions that actually have a corresponding gold mention, but their allignment is missed because of parsing errors and incorrect boundaries.ßdummyTurker
11982ß0.0ßHowever, Cai and Strube's work is about how to incorporate unmatched system mentions in standard evaluation metrics.ßdummyTurker
11983ß0.0ß"For instance, for the gold mention ""a good meal that we have yesterday in the X restarurant"", the manual annotation only considers ""meal"" as the minimum span while we also incorporate the determiner and the adjective, i.e."ßdummyTurker
11984ß0.0ß"""a good meal""."ßdummyTurker
11985ß0.0ßThe reason is that they are all (a good meal) annotated in a single NP node and we operate at the syntax node level and we do not dig into the details of each node.ßdummyTurker
11986ß0.0ßThats (the inclusion of determiners and adjectives) the reason for automatically extracted minimum spans to be larder than manually annotated ones.ßdummyTurker
11987ß0.0ß"As long as we use a reasonable parser, ""a good meal"" would be put into a single NP node."ßdummyTurker
11988ß0.0ß"Based on our minimum span extraction, there is no difference between a system that detects ""a good meal"" as a mention or a system that detects ""a good meal that we have yesterday"" or ""a good meal that we have yesterday in the X restarurant""."ßdummyTurker
11989ß0.0ßAs a result, there is no preference for longer mentions.ßdummyTurker
11990ß0.0ßWe prefer to stay at the syntaxtic node levels and do not go into using heuristics for further parsing syntactic nodes, i.e.ßdummyTurker
11991ß0.0ßto remove determiners or adjectives included in a single node.ßdummyTurker
11992ß0.0ßL197: The minimum span is implemented as a set of texts.ßdummyTurker
11993ß0.0ßL198: Given a parse tree from which the minimum span is extracted and the resulting minimum span, you can deterministically determin the maximum span.ßdummyTurker
11994ß0.0ßIn the given example the gold mention is “the new Chief of Naval Operations”.ßdummyTurker
11995ß0.0ßFor our coreference evaluations, we discard manually annotated minimum spans, because they do not exist for the all and the main coreference corpora.ßdummyTurker
11996ß0.0ßWe automatically extract the minimum spans for both gold and system mentions, whether the gold data includes the manually annotated minimum spans or not.ßdummyTurker
11997ß0.0ßThanks for the comment.ßdummyTurker
11998ß0.0ßWe cannot agree that our work is “an incremental work based on the two mentioned previous work, i.e., Xing et al., 2017 and Tian et al.” The main difference of our proposed conversational response generation model and the above two state-of-the-art models is the two attention mechanisms for obtaining the contextual representation of a conversation.ßdummyTurker
11999ß0.0ßRather than use a hierarchical attention neural network (Xing et al., 2017) to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is simpler in structure and has less number of parameters than the hierarchical attention approach.ßdummyTurker
12000ß0.0ßMeanwhile, rather than use a heuristic approach to weigh the importance of each utterance in the context (Tian et al., 2017), in our proposed approach, the weights of utterance in the context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach.ßdummyTurker
12001ß0.0ßThanks for the comment.ßdummyTurker
12002ß0.0ßThe reason of “why the “static attention” performs better?” is stated in line 525-532.ßdummyTurker
12003ß0.0ßThe reason of “why the one direction model performs better than the bi-direction model?” is stated in line 533-539.ßdummyTurker
12004ß0.0ß“why including 3 and 7 previous utterances give the best performance but not 4, 5, 6?”—ßdummyTurker
12005ß0.0ßThe reason is that the number of previous utterances is a parameter in experiment.ßdummyTurker
12006ß0.0ßFigure 3 shows the results of parameter tuning on the number of utterances.ßdummyTurker
12007ß0.0ßThanks for the comment.ßdummyTurker
12008ß0.0ßWe will revise the presentation and proofread the paper carefully.ßdummyTurker
12009ß0.0ßWe consider as axiological statements the evaluations of behavior, objet or fact.ßdummyTurker
12010ß0.0ßWe grasp, into this category, the appreciations and the judgments described in the Martin and White model.ßdummyTurker
12011ß0.0ßThe affective statements are related to the affect class in the same model.ßdummyTurker
12012ß0.0ßThe volition is not included in the Appraisal model of Martin and White, because, they can be considered as related to the user’s likes and dislikes only in the specific negotiation context.ßdummyTurker
12013ß0.0ßThe proposed model calls, so far, a reduced number of dialog acts referring to the Searle's classification : directive, including interrogative and suggestive, and representative acts.ßdummyTurker
12014ß0.0ßA previous corpus analysis showed that these two dialog acts are the most correlated to the axiological and affective statements.ßdummyTurker
12015ß0.0ßAnaphor resolution is indeed a relevant track for our system improvement, as some errors are due to incorrect target identification.ßdummyTurker
12016ß0.0ßWe should indicate that we tried to apply different existant anaphor resolution systems.ßdummyTurker
12017ß0.0ßHowever, as their effectiveness has been tested on non conversational and non oral utterances, they caused many errors and noise and did not improve the results.ßdummyTurker
12018ß0.0ßThe contribution of the paper toward ABSA is the following : (i) the first detection of opinion-target pairs in an interaction context (collaborative ULD) (ii) the linguistic modeling of opinion expressions in a negotiation context (which are very different to the expressions found in product reviews).ßdummyTurker
12019ß0.0ßRelying on the existing polarity rules (Neviarouskaya, et al 2010), the proposed system tackles the following new linguistic issues : (i) syntactic patterns that model the collaborative aspect human-agent interactions (ii) modeling the volitive statements for the negotiation context.ßdummyTurker
12020ß0.0ßAs a pioneering system of likes and dislikes detection in a human-agent negotiation, we provide a first evaluation protocol (including annotation campaign) that could be worthy for both human-agent and opinion mining communities.ßdummyTurker
12021ß0.0ßWe provide also a discussion of the relevance of system outputs.ßdummyTurker
12022ß0.0ßWe think that before integrating and evaluating the system in an end-to-end experiment, it is necessary to evaluate the NLP performances of the model and a NLP-centered evaluation.ßdummyTurker
12023ß0.0ßFor the non-anonymous version of the paper, if accepted, we will include the link toward the annotation platform.ßdummyTurker
12024ß0.0ßUnfortunately, the human-agent interaction (developed by another team) is not yet publicly available.ßdummyTurker
12025ß0.0ßBut as soon as it is available, we will give access to the corresponding annotations.ßdummyTurker
12026ß0.0ßMore statistics can be found in (Gratch, et al., 2015).ßdummyTurker
12027ß0.0ß77 sections have been used for the development of rules, and another set of 77 sections was annotated and used for the evaluation.ßdummyTurker
12028ß0.0ßIf the paper is accepted, we will make more clear the contribution of the negotiation model vs. small talk model.ßdummyTurker
12029ß0.0ßHere are the main differences: (i) the addition of the volitive patterns ; (ii) the adaptation of the axiological and affective patterns (previously designed for the small talk model) to the negotiation context.ßdummyTurker
12030ß0.0ßFor example, we ignored patterns related to emotions or inner sentiments such as :ßdummyTurker
12031ß0.0ß<Source><linkVb><affectiveAdj> —> I am/feel sadßdummyTurker
12032ß0.0ß<source><have><NPaffective> —> I had very good deal with youßdummyTurker
12033ß0.0ßThe last sentence can be employed in a negotiation context, however such a structure is never used for expressing a like/dislike targeting negotiation object.ßdummyTurker
12034ß0.0ßAmong the remaining patterns, there is <source><vblike><target>ßdummyTurker
12035ß0.0ßThe study of two different applicative contexts allows the identification of generic and specific patterns and we will provide this distinction in the final paper.ßdummyTurker
12036ß0.0ßFor example, at the phrase level (Section 4.1.2), the patterns used for detecting noun, adjective and verb phrases are generic.ßdummyTurker
12037ß0.0ßOnly the kinds of lexical units that are used in the patterns have been adjusted.ßdummyTurker
12038ß0.0ßAt the adjacency pair level, the proposed polarity rules are specific to the human-agent interaction context but general for both applicative contexts.ßdummyTurker
12039ß0.0ßThis should indeed be clarified : the CRF-based approach was not developed for the negotiation corpus and the present work is the first one carried out in this context.ßdummyTurker
12040ß0.0ßReply to question 1 and question 2ßdummyTurker
12041ß0.0ßWe are really grateful for the very detailed examples you provided.ßdummyTurker
12042ß0.0ßWe agree with all the values you give, except ObjNego(I like Lamp = True), we should insisted on the fact that this test is used only at the phrase level and not at the sentence level.ßdummyTurker
12043ß0.0ßThe number of APs varies from one session to another.ßdummyTurker
12044ß0.0ßIndeed, some annotators have longer session.ßdummyTurker
12045ß0.0ßThe average time of annotation has been estimated at 20 minutes.ßdummyTurker
12046ß0.0ßThe instructions given to the annotators specify that when an object is not mentioned in the current APs, the corresponding check box has to remain empty.ßdummyTurker
12047ß0.0ß"We will replace the sentence ""gives access to the preceding annotated AP"" by ""the dialog history is available at each annotation step within the annotation interface."ßdummyTurker
12048ß0.0ßIn (Anonymous, 2017) screen captures are available.ßdummyTurker
12049ß0.0ßThe work presented by Rillof and Wiebe, (2003) lays the foundations for the subjectivity analysis.ßdummyTurker
12050ß0.0ßHowever, the patterns they developed are made for detecting sentences with ''non compositional meaning''.ßdummyTurker
12051ß0.0ßOur ULD patterns allow us to go beyond the surface structure by dealing with the compositional principle.ßdummyTurker
12052ß0.0ßThey rely on deeper sentence structure and formalized the dependency relations inside the predicate.ßdummyTurker
12053ß0.0ßBesides, at the sentence level, they are able to identify the source and the target.ßdummyTurker
12054ß0.0ßWe choose not to provide separate model for the implicite statements but the ULD patterns coverage such expressions.ßdummyTurker
12055ß0.0ßFor the negotiation context, some implicite statements can be easily detected by the volitive patterns.ßdummyTurker
12056ß0.0ßThat's a good point, thanks.ßdummyTurker
12057ß0.0ßWe did find terminology to be a bit tricky here - we'll have to think more about the best ways to refer to different methods of incorporating sub-word information.ßdummyTurker
12058ß0.0ßWe are using the widely-use OpenNMT as our baseline, which has been used to obtain state-of-the-art results on other existing datasets.ßdummyTurker
12059ß0.0ßWe understand our datasplit makes it difficult to compare with previously published work, but because we are building on OpenNMT, we feel relatively confident that our conclusions regarding the pre-trained embeddings are valid.ßdummyTurker
12060ß0.0ßWe agree this would be interesting.ßdummyTurker
12061ß0.0ßWe focused on lower-resource settings initially because we thought they would benefit more from pre-trained embeddings.ßdummyTurker
12062ß0.0ßWe used the parameters given by the fastText paper.ßdummyTurker
12063ß0.0ßWe did not experiment with more hyper-parameters due to computational constraints.ßdummyTurker
12064ß0.0ßWe did perform some experiments on filtering out rarer morphological variants but cut them from the submission due to length constraints.ßdummyTurker
12065ß0.0ßThe point about monolingual text is good - ideally we would compare to other methods of incorporating monolingual corpora as well.ßdummyTurker
12066ß0.0ßThis is an interesting line of work, thank you for bringing it to our attention.ßdummyTurker
12067ß0.0ßIt would be difficult to make a direct comparison, though with more space this would be a good paper to put in related work.ßdummyTurker
12068ß0.0ßWe recognize the methods are not all novel.ßdummyTurker
12069ß0.0ßOur main contribution is empirical results that compares sub-word/morphological embeddings for Arabic-English translation.ßdummyTurker
12070ß0.0ßThanks, these are all good suggestions for future work.ßdummyTurker
12071ß0.0ßWe do use the TED talks dataset from the WIT3 website.ßdummyTurker
12072ß0.0ßWe just use a non-standard split to focus on settings with less available parallel data, because we thought this was a case where pre-trained embeddings would generally be more beneficial.ßdummyTurker
12073ß0.0ßWe will release this split for reproducibility.ßdummyTurker
12074ß0.0ßSee above.ßdummyTurker
12075ß0.0ßThis would be interesting, and would go along well with your suggestion of trying En-Ar translation as well - you might expect morphological word embeddings to matter more for the more morphologically rich language in both cases.ßdummyTurker
12076ß0.0ßUnfortunately we have not yet explored this.ßdummyTurker
12077ß0.0ßWe did consider this, but have not yet tried it.ßdummyTurker
12078ß0.0ßSorry about that.ßdummyTurker
12079ß0.0ßThere were 2 layers and BPE was on both source and target sides with 35,000 merge operations.ßdummyTurker
12080ß0.0ßWe didn't experiment with how this impacted the results.ßdummyTurker
12081ß0.0ßWe trained word embeddings on both the Wikipedia data and the TED-train, and the OOV rates with respect to this combination in TED-test were: 3% of types, 1% of tokens.ßdummyTurker
12082ß0.0ßIf you're interested additionally in OOV rates of just Wikipedia data to TED-test and TED-train, they were: 4% of types, 1% of tokens for TED-test and 18% of types, 1% of tokens for TED-train.ßdummyTurker
12083ß0.0ßWe agree, this would be a good additional evaluation.ßdummyTurker
12084ß0.0ßIt would be a bit tricky because we'd have to write some heuristics for detecting which plurals are broken (it's not directly given from the analyzer), but it should be doable.ßdummyTurker
12085ß0.0ßThe sentence length for the model is set as the max sentence length of each training dataset.ßdummyTurker
12086ß0.0ßThose sentences that have fewer words than the max sentence length are filled with a special character, which is initialized with a random vector as its word vector.ßdummyTurker
12087ß0.0ßSince the SemEval datasets only have annotations of explicit aspects, our paper is also about extraction of explicit aspects.ßdummyTurker
12088ß0.0ßAspect term bounds are set by B (start tokens) and O (end tokens (exclude)) of the BIO scheme.ßdummyTurker
12089ß0.0ßSemEval has an evaluation program, which considers only full matches as correct as that is more suitable for applications.ßdummyTurker
12090ß0.0ßThe evaluation program is used by us and other papers.ßdummyTurker
12091ß0.0ßThus, the results are comparable.ßdummyTurker
12092ß0.0ßWord embedding is used to initialize word vectors in our experiments.ßdummyTurker
12093ß0.0ßRandom initialization (or One-hot vector) for word vectors needs much more training time and is hard to achieve stable results due to the small training datasets in our experiments.ßdummyTurker
12094ß0.0ßSorry for missing this paper.ßdummyTurker
12095ß0.0ßWe will cite and discuss it in the related work section of our paper.ßdummyTurker
12096ß0.0ß1) We use the Stanford Parser Package to generate dependency-based parse tree which is described in the Experiments Section (line 500, Page 6).ßdummyTurker
12097ß0.0ß2) Figure 2 has three parts from bottom to top.ßdummyTurker
12098ß0.0ßBiDTree is harder to understand due to its complexity.ßdummyTurker
12099ß0.0ßWe will make it clearer for readers in the revised version of the paper.ßdummyTurker
12100ß0.0ß3) The fact that $r_{pc_i}$ and $r^\uparrow(k)$ both use the lowercase $r$ is because they both refer to syntactic relations.ßdummyTurker
12101ß0.0ß$r_{pc_i}$ is a variable and $r^\uparrow(k)$ is a mapping function.ßdummyTurker
12102ß0.0ßThey are explained in the paper (lines 295-301 for $r_{pc_i}$, line 357 for $r^\uparrow(k)$).ßdummyTurker
12103ß0.0ßWe will improve the explanation in the revised paper.ßdummyTurker
12104ß0.0ß$h^\downarrow$ is part of the output of the bidirectional dependency tree network (lines 217-320), which is used as the input of BiLSTM (lines 405).ßdummyTurker
12105ß0.0ß$h^\leftarrow$ and $h^\rightarrow$ are concatenated as the output of BiLSTM (lines 423-427), which is used as the input of CRF (line 434).ßdummyTurker
12106ß0.0ß"""5"" means early-stop training after 5 epochs."ßdummyTurker
12107ß0.0ßFine-tuning the initial embedding means to modify the word embedding during gradient updates of the neural network using back-propagating gradients.ßdummyTurker
12108ß0.0ßDue to the space limit, we did not detail it in the paper.ßdummyTurker
12109ß0.0ßWe will elaborate it in the revised paper.ßdummyTurker
12110ß0.0ßFor the baseline method, LSTM-CNNs-CRF, we set the number of filters to 300 because the word embedding size is 300 in our experiments.ßdummyTurker
12111ß0.0ßWe have conducted another experiment with the same hyper-parameters and 30 filters.ßdummyTurker
12112ß0.0ßThe F1 scores are 80.61%, 83.97%, 69.80%, and 73.63% for L-14, R-14, R-15, and R-16, respectively.ßdummyTurker
12113ß0.0ßThe first three values are smaller than those of 300 filters, which gives the scores of 80.72%, 84.44%, and 69.82%.ßdummyTurker
12114ß0.0ßSo, 300’s performance is not poorer.ßdummyTurker
12115ß0.0ßIn the last paragraph of Section 2, we described the characteristics of our framework, which is inspired by the two models together.ßdummyTurker
12116ß0.0ßIt may cause some misunderstanding.ßdummyTurker
12117ß0.0ßWe will correct it in the new version.ßdummyTurker
12118ß0.0ßWe will carefully proofread the paper to correct any language errors.ßdummyTurker
12119ß0.0ßThanks for your instructive comments.ßdummyTurker
12120ß0.0ßDetailed analysis on experiment results was not presented due to the page limit.ßdummyTurker
12121ß0.0ßWe will give more specific analysis, and make the notations clear enough.ßdummyTurker
12122ß0.0ßThis is a good point.ßdummyTurker
12123ß0.0ßWe are also curious about performance of word embeddings with different hyper-parameter settings, including 'n' for ngrams.ßdummyTurker
12124ß0.0ßIn this short paper, we focus on building the dataset and exploring the performance of word embedding on analogical reasoning.ßdummyTurker
12125ß0.0ßThus for context features, we test the major factors: with/without the features (characters and ngrams).ßdummyTurker
12126ß0.0ßThe experiment shows a distinct improvement by introducing bigrams and characters.ßdummyTurker
12127ß0.0ßWe will report more results with different settings of context features and discuss their influences in the future.ßdummyTurker
12128ß0.0ßCA8 contains words from different domains.ßdummyTurker
12129ß0.0ßFor example, some reduplication words appear frequently in spoken language, while the words from history and geography questions have greater chance to appear in formal written language.ßdummyTurker
12130ß0.0ßExperiment shows that different parts of this dataset are sensitive to different features, which further proves that CA8 is good at sketching specialties of different embeddings.ßdummyTurker
12131ß0.0ßWe call CA8 a balanced dataset because it (1) introduces diverse relation types, and (2) make the number of questions balanced across different types.ßdummyTurker
12132ß0.0ßFootnote--- Gladkova(2016) points out that existing English analogy datasets face problems of data balance: (1) some of them tend to include only a certain type of relations (semantic-only: SAT and SemEval2012-Task2, morphology-only: MSR).ßdummyTurker
12133ß0.0ß(2) The influential Google analogy test contains 9 morphological and 5 semantic categories, but the capital:country relation constitutes 56.72% of all the 8869 semantic questions.ßdummyTurker
12134ß0.0ßThanks for your valuable comments.ßdummyTurker
12135ß0.0ßYes, we should rephrase to use a more suitable word here.ßdummyTurker
12136ß0.0ßYes, we should.ßdummyTurker
12137ß0.0ßThanks a lot for your insightful comments.ßdummyTurker
12138ß0.0ß(1) About affixßdummyTurker
12139ß0.0ßFirstly, the root-affix phenomenon in Chinese is not as obvious as in inflectional languages such as English and German.ßdummyTurker
12140ß0.0ßEven in Sinica Affix Database, there are only high-frequency initial and final morphemes, not affixes.ßdummyTurker
12141ß0.0ßAfter investigating many researches on Chinese morphology, we found it is very difficult to define and sketch Chinese affixes, which has been discussed for several decades.ßdummyTurker
12142ß0.0ßThus, we follow Liu Yuehua et al.ßdummyTurker
12143ß0.0ß"(2001)’s work to use ""semi-affixation"" to describe both the typical affixes and some high-frequency initial and final morphemes behaving like affixes."ßdummyTurker
12144ß0.0ßTo avoid the ambiguous linguistic discussions, we will elaborate this concept in the next version of paper.ßdummyTurker
12145ß0.0ß(2) About word segmentationßdummyTurker
12146ß0.0ßSince there are no explicit word boundaries in Chinese, we take dictionaries, word segmentation specifications (GB/T-13715-92) and common tokenization tools as references to confirm the inclusion of each word pair.ßdummyTurker
12147ß0.0ßAlso, the ngram2vec and fasttext toolkits are able to generate any word or ngram embeddings, which could greatly alleviate the problems in Chinese word segmentation.ßdummyTurker
12148ß0.0ßRules can handle some morphological questions well.ßdummyTurker
12149ß0.0ßThis condition would be similar in English -- One can also easily solve most morphological questions such as “amazing : amazingly :: apparent : apparently” with rules.ßdummyTurker
12150ß0.0ßHowever, the objective of this paper is not to solve the questions, but to explore to what extent word embedding can capture Chinese linguistic regularities.ßdummyTurker
12151ß0.0ßThe dataset is also built for evaluation of word embeddings.ßdummyTurker
12152ß0.0ßRegarding the A:AA::XY:XXYY questions, it might not be a suitable type, because analogical reasoning requires two pairs of words share the same relation (Mikolov et al.ßdummyTurker
12153ß0.0ß2013).ßdummyTurker
12154ß0.0ßIn Appendix, we describe all relations in each sub-category with definitions and examples.ßdummyTurker
12155ß0.0ßAs shown in Appendix Table 1, AA (for verbs) signals doing something a little bit or things happen briefly, while AABB (for verbs) indicates the action is repeated or lasts for a while, e.g.ßdummyTurker
12156ß0.0ß说说(shuo1shuo1) means say a little, and 说说笑笑(shuo1shuo1xiao4xiao4) means talk and laugh for a while.ßdummyTurker
12157ß0.0ßIt would be hard to find two pairs in A:AA and XY:XXYY (where A and XY are verbs) sharing the same relation.ßdummyTurker
12158ß0.0ßThe resource building process is important.ßdummyTurker
12159ß0.0ßWe might not make it very clear due to the space limitation.ßdummyTurker
12160ß0.0ßStep 1.ßdummyTurker
12161ß0.0ßCollect word pairs for each relation.ßdummyTurker
12162ß0.0ß4 graduate students major in linguistics participate in this work.ßdummyTurker
12163ß0.0ßTwo of them collect morphological word pairs, and the other two students deal with semantic word pairs.ßdummyTurker
12164ß0.0ß"Reduplication words are automatically retrieved from the ""combination"" corpus (mentioned in the paper) for references."ßdummyTurker
12165ß0.0ßA word pair will be accepted only when both of the students agree.ßdummyTurker
12166ß0.0ßStep 2.ßdummyTurker
12167ß0.0ßSift through the candidate word pairs.ßdummyTurker
12168ß0.0ßTo avoid the imbalance problem addressed in English benchmarks, we set a limit of 50 word pairs at most for each relation.ßdummyTurker
12169ß0.0ßA linguistic expert selects the final word pairs based on dictionaries, word segmentation specifications, and tools.ßdummyTurker
12170ß0.0ßIn this step, 1852 unique Chinese word pairs are retrieved.ßdummyTurker
12171ß0.0ßStep 3.ßdummyTurker
12172ß0.0ßYield the analogy questions automatically with word pairs retrieved above.ßdummyTurker
12173ß0.0ßWe will rephrase to use a more suitable word here.ßdummyTurker
12174ß0.0ß+char -> word+char,ßdummyTurker
12175ß0.0ß+ngram -> word+ngram.ßdummyTurker
12176ß0.0ßThis paper introduced a new method for RL with parameterized reward function trained using a discriminator, while traditional RL approaches require fixed reward functions.ßdummyTurker
12177ß0.0ßIn the MRT setting, using 25 samples outperforms using the top-best translation (37.74 vs. 37.44, see rows 2 and 5 of Table 1), which is consistent with the finding in (Shen et al.ßdummyTurker
12178ß0.0ß2016).ßdummyTurker
12179ß0.0ßWe have not investigated such factors in our framework.ßdummyTurker
12180ß0.0ßWe will try to include more ablation study in the revised version.ßdummyTurker
12181ß0.0ßYes, our method indeed improves translation adequacy as expected.ßdummyTurker
12182ß0.0ßWe will include some examples in the revised version.ßdummyTurker
12183ß0.0ßCDR is designed mainly for alleviating under-translation, which we believe is the key problem of current NMT models based on human evaluations from published papers (Tu et al.ßdummyTurker
12184ß0.0ß2016; Zheng et al.ßdummyTurker
12185ß0.0ß2018).ßdummyTurker
12186ß0.0ßWe will include detailed human evaluation scores for under- and over-translation in the revised version.ßdummyTurker
12187ß0.0ßWe will do a careful proof-reading and correct the typos in the revised version.ßdummyTurker
12188ß0.0ßWe expect the discriminator to differentiate bad generated translations from good ones.ßdummyTurker
12189ß0.0ßTherefore, the discriminator is learned to assign a precise score for each generated translation, which is consistent with its calculated CDR score, and thus can better measure its adequacy.ßdummyTurker
12190ß0.0ßWe will clarify the description in the revised version.ßdummyTurker
12191ß0.0ßTable 1 lists the training speed (words/second) of different variants.ßdummyTurker
12192ß0.0ßAs expected, introducing an RNN-based discriminator decreases the speed from 1.4K to 1.0K, which is mainly due to the generation of samples during training.ßdummyTurker
12193ß0.0ßIntroducing an adequacy-oriented metric further decreases the speed to 0.8K because of generating the required attention matrix by force-decoding the reference sentence.ßdummyTurker
12194ß0.0ßFortunately, these approaches only affect the training procedure, and we employ a standard NMT decoder after model parameters are trained.ßdummyTurker
12195ß0.0ßThe difference in the metric values reflects the amount of increase in average frequency the summary words in the Subtlex corpus as defined in equation 12.ßdummyTurker
12196ß0.0ßSince we use the frequency of words in Subtlex corpus as a measure of simplicity, this difference in the average frequency indicates an improved simplicity of the generated summary.ßdummyTurker
12197ß0.0ßWeakness 4 (Flesh Reading Ease is not a good measure of readability): Our approach is agnostic to the scoring function.ßdummyTurker
12198ß0.0ßGiven any function defined on a word, our approach tunes the summaries towards maximizing the score.ßdummyTurker
12199ß0.0ßIn Sec 4.3, we use Flesh Reading Ease as such a tuning function.ßdummyTurker
12200ß0.0ßSince our approach is generic, it can extended to any appropriate word-level readability metric.ßdummyTurker
12201ß0.0ßThe Flesh Reading Ease is used to illustrate the ability of the approach to generate tuned summaries.ßdummyTurker
12202ß0.0ßWeakness 1 (CWE and VoTing have same scores on all analysis): The value of scores (simplicity/readability according to the task) achieved by CWE/VoTing can be varied according to a strength parameter (Eq 7,10).ßdummyTurker
12203ß0.0ßTo compare the different methods, we have tuned the parameters so that we achieve equivalent levels of tuning (refer to Table 1,4), and then compare the qualities of different summaries in terms of rouge and PMI, showing that our method achieves higher values on those dimensions.ßdummyTurker
12204ß0.0ßOur method aims to maximize the vocabulary score of the summary by using contextually appropriate new words which preserve the meaning.ßdummyTurker
12205ß0.0ßSince we have the scores predefined for each word, we have used them to measure tuning.ßdummyTurker
12206ß0.0ßWe have used human evaluation for evaluating meaning preservation in the summaries – by asking the annotators to rate change (meaning preservation) in the tuned summaries.ßdummyTurker
12207ß0.0ßSince, we use the simplicity (fluency) and readability as pre-defined scores on the vocabulary words, evaluating this with human annotators would amount to annotating the respective scoring function than the proposed tuning approach.ßdummyTurker
12208ß0.0ßHence, we did not evaluate this with human annotators.ßdummyTurker
12209ß0.0ßThe synonyms were taken from www.thesaurus.com.ßdummyTurker
12210ß0.0ßWe will update the references in the camera-ready version (if accepted)ßdummyTurker
12211ß0.0ßQuestion 1 (prior art using word frequencies to measure simiplicity): We have cited Paetzold and Specia (2016) (please refer to section 5 of their paper) who have used word frequencies to measure simplicity of words.ßdummyTurker
12212ß0.0ßAs mentioned above, the synonyms were taken from www.thesaurus.com.ßdummyTurker
12213ß0.0ßWe will update the references in the camera-ready version (if accepted)ßdummyTurker
12214ß0.0ßIn Section 4.2 (4th paragraph) in our paper, we have provided the details of the number of replacements by each of the methods.ßdummyTurker
12215ß0.0ßExperimental setup for human evaluation: We carried out the human evaluation on Amazon Mechanical Turk.ßdummyTurker
12216ß0.0ßWe have described the task in detail in section 4.4 of the paper.ßdummyTurker
12217ß0.0ßMathematical form of the temperatured softmax: Equation 7 has the expression exp( [ln p(w)]/r ).ßdummyTurker
12218ß0.0ßIt is not exp( ln [p(w)/r] ).ßdummyTurker
12219ß0.0ßBecause the r is outside the log, it would not cancel out.ßdummyTurker
12220ß0.0ßInterpretation of simplicity score: Since we use the frequency of words in Subtlex corpus as a measure of simplicity, higher simplicity score indicates the usage of more words that frequent the Subtlex.ßdummyTurker
12221ß0.0ßSince Subtlex contains simpler text, higher simplicity score signifies more simplicity in text and hence is better.ßdummyTurker
12222ß0.0ßHow is Syn(w) for section 4: Syn(w) is the list of synonyms taken from thesaurus.com.ßdummyTurker
12223ß0.0ßIt is a property of the model and remains the same irrespective of the task (simplification/readability enhancement etc.)ßdummyTurker
12224ß0.0ßNecessity of taking context into account in prior work on text adaptation: We believe that any algorithm aimed at modifying text to make it suit a particular style/vocabulary has an implicit goal to preserve the meaning of the text being edited, hence we have to account for the context while adaptation.ßdummyTurker
12225ß0.0ßWhere do human judges come from?ßdummyTurker
12226ß0.0ß: We carried out the human evaluation using workers from Amazon Mechanical Turk.ßdummyTurker
12227ß0.0ßThanks for your comments.ßdummyTurker
12228ß0.0ßWe did not conduct experiments to evaluate the reconstruction performance of our model.ßdummyTurker
12229ß0.0ßThe reconstruction could be performed as a language modeling task.ßdummyTurker
12230ß0.0ßWe can use Equation (6) to compute a lower bound of the log likelihood log p(x).ßdummyTurker
12231ß0.0ßThe previous work (Cheng et al, 2017) has done this while we mainly focus on dependency grammar induction.ßdummyTurker
12232ß0.0ßYes, we have tried “baby-step” training.ßdummyTurker
12233ß0.0ßUnfortunately, it did not work in our model.ßdummyTurker
12234ß0.0ßWe suppose that short sentences usually have different patterns as longer sentences, and neural networks have a forgetting property.ßdummyTurker
12235ß0.0ßThey tend to forget what has been learnt in the previous baby step and fit on the new training set from scratch.ßdummyTurker
12236ß0.0ßWe will cite this paper in our final draft.ßdummyTurker
12237ß0.0ßThanks for your comments.ßdummyTurker
12238ß0.0ßIt is true that our work is based on the RNNG + neural variational inference framework proposed by Cheng et al (2017).ßdummyTurker
12239ß0.0ßHowever, Cheng et al.ßdummyTurker
12240ß0.0ßuse the model for supervised parsing.ßdummyTurker
12241ß0.0ßTheir paper doesn't contain any experiments with unsupervised parsing (there is merely a footnote in the paper claiming that the model can be used for unsupervised parsing).ßdummyTurker
12242ß0.0ßWe, however, show that the Cheng model does not work for unsupervised parsing as is, which is a novel contribution.ßdummyTurker
12243ß0.0ßWe then go on to show how the model can be augmented using posterior regularization, which makes the model work for unsupervised parsing.ßdummyTurker
12244ß0.0ßThis is a major contribution that is not based on Cheng et al.ßdummyTurker
12245ß0.0ßand one they couldn't have made as they only reported supervised results (more detailed discussion on RP in our reply to reviewer #3).ßdummyTurker
12246ß0.0ßIt is not straightforward to apply PR to our framework.ßdummyTurker
12247ß0.0ßWe need to approximate the normalization term in PR by MC sampling, use projected gradient descent for optimization and make it function well within the revised EM algorithm.ßdummyTurker
12248ß0.0ßTo the best of our knowledge, there is no prior work on this topic even in machine learning conferences.ßdummyTurker
12249ß0.0ßWe agree that the factorisation of various probability terms follows the original work of RNNG and Cheng et al.ßdummyTurker
12250ß0.0ßSo we follow the same mathematical foundations here, and we think it’s necessary to explain them briefly in the model section of our paper.ßdummyTurker
12251ß0.0ßIn any case, we will definitely follow your advice to change the descriptions to re-emphasise our difference and relation to previous work of Cheng et al.ßdummyTurker
12252ß0.0ßand also Dyer et al.ßdummyTurker
12253ß0.0ßIn the lexicalized version, we use both the gold POS tags and word representation.ßdummyTurker
12254ß0.0ßTypically, it takes several hours for the lexicalized model to train on WSJ corpus on a 2.40GHz CPU with DyNet implementation.ßdummyTurker
12255ß0.0ßWe are very grateful to you for your suggestion.ßdummyTurker
12256ß0.0ßIt is necessary to analyze model parameters, and some experiments of parameter analysis will be conducted in the future work.ßdummyTurker
12257ß0.0ßThis question is responded in [Reply to weakness argument 1]ßdummyTurker
12258ß0.0ßThank you very much for your meaningful question.ßdummyTurker
12259ß0.0ßIn this paper, we present a novel THAM model, which first integrates topic model and HIN for aspect mining with rating bias.ßdummyTurker
12260ß0.0ßTHAM respectively designs a topic model with bias and a topic propagation network to make full use of textual information and structural information.ßdummyTurker
12261ß0.0ßBy setting reviews as the sharing factor, we integrate the two parts into a uniform optimization framework.ßdummyTurker
12262ß0.0ßExperiments validate effectiveness of these delicate designs.ßdummyTurker
12263ß0.0ßAs a LDA-based model, our THAM is more complex model with more free parameters, and it performs better than a simple PLSA-based RABI model.ßdummyTurker
12264ß0.0ßIn addition, we also compared THAM with other LDA-based models (SATM and AIR, containing similar numbers of free parameters).ßdummyTurker
12265ß0.0ßExperiments also validate that THAM consistently outperforms these baselines.ßdummyTurker
12266ß0.0ßAs a consequence, we believe that the performance improvement of our THAM contributes to its delicate model designs, rather than more free parameters.ßdummyTurker
12267ß0.0ßThank you very much for your question.ßdummyTurker
12268ß0.0ßHere we will give more explanation about our model.ßdummyTurker
12269ß0.0ß(1)	There are two reasons for the use of the log of f1.ßdummyTurker
12270ß0.0ß(I) It is easier to compute log of f1 than f1.ßdummyTurker
12271ß0.0ß(II) The log of f1 can amplify the subtle changes of f1, since f1 is usually very small.ßdummyTurker
12272ß0.0ß(2)	We make use of the linear sum of squares for f2 and f3 (i.e., the L-2 norm of the two parts) so as to put more emphasis on difference between two objects.ßdummyTurker
12273ß0.0ßIn addition, L-2 norm is easier to optimize.ßdummyTurker
12274ß0.0ß(3)	In this paper, we consider that each review depends on both its shop and its user, so we assume that the user and shop are of equal importance to the review.ßdummyTurker
12275ß0.0ßSome personalized weights can be taken into consideration in the future work.ßdummyTurker
12276ß0.0ß(4)	In Eq.ßdummyTurker
12277ß0.0ß(8), we simply assign the equal weight on f2 and f3.ßdummyTurker
12278ß0.0ßIt may be better to assign the personalized weights on f2 and f3.ßdummyTurker
12279ß0.0ßHowever, it is difficult to do so in real applications, because of lacking supervision information.ßdummyTurker
12280ß0.0ßWe will carefully clarify them in the final version.ßdummyTurker
12281ß0.0ßThank you very much for your suggestion.ßdummyTurker
12282ß0.0ßIn this paper, aspects (or topics) are pre-defined.ßdummyTurker
12283ß0.0ßFor example, in the Dianping dataset, there are three aspects: taste, service, and environment (see Table 2); in the TripAdvisor, there are three aspects: value, service, and food.ßdummyTurker
12284ß0.0ßWe will carefully proofread this paper in the final version.ßdummyTurker
12285ß0.0ßThis question is responded in [Reply to weakness argument 2]ßdummyTurker
12286ß0.0ßThank you for your valuable comments!ßdummyTurker
12287ß0.0ßWe propose a semi-supervised method to improve relation extraction performance with a relabeled dataset.ßdummyTurker
12288ß0.0ßGenerating valid labels for unlabeled instances is the first attempt on distant supervised relation extraction either with the type-based or GAN based generator.ßdummyTurker
12289ß0.0ßMoreover, our improved GAN module is easy to train.ßdummyTurker
12290ß0.0ßBetter than former relation extractors, our GAN based approach can learn to extract relations from a number of high-quality instances and large-scale uncertain ones.ßdummyTurker
12291ß0.0ß1)	GAN does well in generating valid data from the distribution of limited true data, which is highly fit for our label generation task.ßdummyTurker
12292ß0.0ß2)	In terms of actual results, GAN based approach outperforms other state-of-the-art relation extractors.ßdummyTurker
12293ß0.0ßPCNN+ATT_NYT in Figure 4(a) and PCNN+ATT+U in Figure 4(b) are in two experiments with different training and testing data.ßdummyTurker
12294ß0.0ßTesting data for PCNN+ATT+U does not have wrong negative instances, while that of PCNN+ATT_NYT does.ßdummyTurker
12295ß0.0ßWe will fix grammatical errors and add a line from BGRU to labels generator in figure 2.ßdummyTurker
12296ß0.0ßGAN is indeed an image processing method and it is hard to be trained and applied in relation generation.ßdummyTurker
12297ß0.0ßHowever, we propose a new RGAN by upgrading the two-player min-max game to a three-player min-max game, which is fit for relation generation.ßdummyTurker
12298ß0.0ß1)	RGAN generates the distribution of data pair [label, instance] presented as p(y_gen, x_uncertain) [=p(y_gen | x_uncertain)p(x_uncertain)] by three models.ßdummyTurker
12299ß0.0ßThe representative model extracts relation features of x.ßdummyTurker
12300ß0.0ßThe generative model generates labels based on the features.ßdummyTurker
12301ß0.0ßThe discriminative model tries to distinguish p(y_gen, x_uncertain) from p(y_label, x_true).ßdummyTurker
12302ß0.0ßThe three models use different parameters (θ_g, θ_d, θ_n), which improves the training stability and prevents the model converging to a local minimum.ßdummyTurker
12303ß0.0ß2)	To compare with a direct classifier, we implement a self-training method that labels uncertain instances with the relation extractor trained by true data.ßdummyTurker
12304ß0.0ßShown in the following table, directly self-labeling method does not work well with neural relation extractor because self-generative labels will mislead the model to converge to a local optimum.ßdummyTurker
12305ß0.0ßHowever, our RGAN model alleviates the risk by splitting feature extraction and labels generation.ßdummyTurker
12306ß0.0ßMethod--------------------		PR-Curve/P@100/P@200/P@300/MeanßdummyTurker
12307ß0.0ßPCNN+ATT-----------------		0.51/88.0/92.0/87.7/89.2ßdummyTurker
12308ß0.0ßPCNN+ATT+Self-training		0.48/90.0/85.5/83.3/86.3ßdummyTurker
12309ß0.0ß**Our model**------------------		**0.55/95.0/93.0/90.3/92.8**ßdummyTurker
12310ß0.0ßSince the NYT dataset is widely used in distant supervised relation extraction, we conduct experiments on the same dataset with our baselines such as Lin et al.ßdummyTurker
12311ß0.0ß(2016).ßdummyTurker
12312ß0.0ßWe will explore other datasets in the future.ßdummyTurker
12313ß0.0ßThank you for your valuable comments!ßdummyTurker
12314ß0.0ßData relabeling is to improve the confidences of negative instances, which is always ignored by former works.ßdummyTurker
12315ß0.0ßOur filtered negative instances are much more accurate than the former ones by using entity descriptions.ßdummyTurker
12316ß0.0ßThis has been proved in section 4.2.ßdummyTurker
12317ß0.0ßa)	We use string matching and collect all the matched Wikipedia pages to obtain the description of the entity without entity linking algorithms.ßdummyTurker
12318ß0.0ßb)	It is enough for A to occur in B's OR B to occur in A'sßdummyTurker
12319ß0.0ßc)	Positive instances are sentences that mention two entities in a relation triple.ßdummyTurker
12320ß0.0ß(line 68-70).ßdummyTurker
12321ß0.0ßAll the relations and instances are well-defined in the NYT dataset.ßdummyTurker
12322ß0.0ßThere are 53 relation types including NA.ßdummyTurker
12323ß0.0ßAll the numbers are counted automatically and the rate 96.3% is from 157031/163108.ßdummyTurker
12324ß0.0ßd)	The assumption for distant supervised relation extraction is too strong to guarantee that all the instances are correct.ßdummyTurker
12325ß0.0ßOur goal is to obtain high confident negatives.ßdummyTurker
12326ß0.0ße)	Uncertain instances are sentences which cannot be labeled as positive or negative.ßdummyTurker
12327ß0.0ß(line 134-135)ßdummyTurker
12328ß0.0ßThe relabeled dataset NYT-New keeps 53 relations, 403561 accurate instances (163108 positives and 240453 negatives) and 338975 uncertain instances.ßdummyTurker
12329ß0.0ßIn Riedel et al.ßdummyTurker
12330ß0.0ß(2010) paper, 470 is the total relation types in Freebase rather than that in the NYT dataset.ßdummyTurker
12331ß0.0ßThe NYT dataset only contains 53 relations, which has been used in many related works such as Zeng et al., (2015) and Lin et al.ßdummyTurker
12332ß0.0ß(2016).ßdummyTurker
12333ß0.0ßWe will release our code and data in the future.ßdummyTurker
12334ß0.0ßWe propose a new RGAN model which is different from the GAN described by Goodfellow et al.ßdummyTurker
12335ß0.0ßThe GAN is a two-player min-max game to generate the valid data p(x_gen) in the same distribution of p(x_true).ßdummyTurker
12336ß0.0ßHowever, it cannot be used in relation extraction task straightforwardly.ßdummyTurker
12337ß0.0ßIn our situation, we need to generate valid labels for uncertain instances, which means p(y_gen, x_uncertain) instead of p(x_gen).ßdummyTurker
12338ß0.0ßTo make the data distribution p(y_gen, x_uncertain) equals p(y_label, x_true), p(y|x)p(x) should be well computed.ßdummyTurker
12339ß0.0ßTherefore, we propose a three-player min-max game named RGAN including representative model, generative model, and discriminative model.ßdummyTurker
12340ß0.0ßThe representative model extracts relation features from x_true and x_uncertain.ßdummyTurker
12341ß0.0ßThe generative model generates relation labels for uncertain instances.ßdummyTurker
12342ß0.0ßThe discriminative model distinguishes if the label from true data or not.ßdummyTurker
12343ß0.0ßThe three models guarantee that all the data distributions including p(x), p(y_gen | x_uncertain) and p (y_label | x_true) have been well computed.ßdummyTurker
12344ß0.0ßFinally, RGAN works stably and generates valid relation labels.ßdummyTurker
12345ß0.0ßDescriptions are auxiliary external data to recognize the co-reference of two entities, which can be replaced with other data sources such as encyclopedia, books and so on.ßdummyTurker
12346ß0.0ßRGAN can be applied to other domains with their labeled and unlabeled data.ßdummyTurker
12347ß0.0ßWe would like to discuss more detail on the additional page.ßdummyTurker
12348ß0.0ßThank you for reminding.ßdummyTurker
12349ß0.0ßWe will cite it later.ßdummyTurker
12350ß0.0ßThe supertagging-based parser does parse the original sentences.ßdummyTurker
12351ß0.0ßThe parser first computes the top-k supertags for each token of the input sentence, based on the original words in the sentence.ßdummyTurker
12352ß0.0ß"It then creates the string ""1 2 ... n"" and assigns the supertags for each original token w_i to the artificial token i."ßdummyTurker
12353ß0.0ßThus the parser uses precisely the supertags for the original sentence.ßdummyTurker
12354ß0.0ßOne technical challenge in supertagging-based parsing is that if the same word occurs twice in a sentence, the supertagger may assign different top-k supertags to these two occurrences.ßdummyTurker
12355ß0.0ßBy replacing each word by its position in the string, we avoid mixing the top-k supertag sets for different tokens.ßdummyTurker
12356ß0.0ßThis allows us to use an existing TAG parser without modifications.ßdummyTurker
12357ß0.0ßWe will clarify this in the final version if the paper is accepted.ßdummyTurker
12358ß0.0ßParsing strings of POS tags made sense to us because our PCFG is not sensitive to lexical information and POS tags can be automatically computed with high accuracy.ßdummyTurker
12359ß0.0ßThus we found this the most straightforward realistic experimental setup.ßdummyTurker
12360ß0.0ßWe agree that the improvements in parsing accuracy should not be overinterpreted, and will clarify this if the paper is accepted; our main point is speed.ßdummyTurker
12361ß0.0ßFor the TAG experiments, the deletion of extra adjunctions is needed to make the corpus of Chen & Vijay-Shanker conform to standard TAG.ßdummyTurker
12362ß0.0ßConverting the Chen derivations into standard TAG without shortening the sentences is nontrivial.ßdummyTurker
12363ß0.0ßThe reviewer raises the valid question whether shortening the sentences in the TAG experiment skews the experiment in favor of the pruning methods we evaluate.ßdummyTurker
12364ß0.0ßHere is a plot of parsing times as a function of sentence length (k=10, theta=0.95): https://imagebin.ca/v/3wRbXTFlNkAZ As the sentence length increases, both CC curves grow asymptotically more slowly than their respective baselines (unpruned and supertag-only); note that the vertical axis is logarithmic.ßdummyTurker
12365ß0.0ßThus we'd expect _higher_ speedups for the original-length sentences.ßdummyTurker
12366ß0.0ßWe thank you for carefully pointing out places that needed more detailed explanations.ßdummyTurker
12367ß0.0ßWe used most of our limited space budget on novel components of our system, leaving out details of paraphrasing and parsing components, which are not novel.ßdummyTurker
12368ß0.0ßWe will use the additional page in the camera-ready version to add more details of our approach.ßdummyTurker
12369ß0.0ßWe do not make any assumptions on the types of the queries and the KBs.ßdummyTurker
12370ß0.0ßHowever, since we first retrieve relevant assertions for a query, filter valid candidate assertions and then derive answers from these assertions, it might be possible for the query to find several assertions that are relevant but invalid.ßdummyTurker
12371ß0.0ßIn such a case, standard IR techniques to improve the retrieval can be applied, such as query expansion and document re-ranking.ßdummyTurker
12372ß0.0ßWe wanted to compare schema-agnostic querying with pattern-based querying for complex questions.ßdummyTurker
12373ß0.0ßWe chose OQA as it uses pattern-based querying primarily for triple assertionsßdummyTurker
12374ß0.0ßOn simple questions, our system achieves performance comparable to (or better than) other Open-QA systems (e.g., OQA, TAQA) operating over noisy, automatically constructed KBs.ßdummyTurker
12375ß0.0ßParasempre does better since it operates over highly-precise information in Freebase.ßdummyTurker
12376ß0.0ßFurthermore, the simple questions in WEBQ are known to be answerable with the information from Freebase, allowing Parasempre to learn simple SPARQL queries that operate over Freebase.ßdummyTurker
12377ß0.0ß"E.g., WEBQ has questions, such as ""What to do in Hong Kong?"ßdummyTurker
12378ß0.0ß""", ""What does Tim Gunn do for a living?"ßdummyTurker
12379ß0.0ß""", that can be easily mapped to Freebase predicates (tourist_attractions and job_title, respectively)."ßdummyTurker
12380ß0.0ßAutomatically extracted open KBs do not have good coverage for such questions, as is reflected in the performances of OQA, TAQA and Nestique.ßdummyTurker
12381ß0.0ßOn the other hand, Parasempre cannot handle complex questions that are not known to be answerable from Freebase, as shown in its performance on the COMPQ-T dataset.ßdummyTurker
12382ß0.0ßFor each question, we generate its paraphrased equivalent questions.ßdummyTurker
12383ß0.0ßEach paraphrased question is translated to a query graph, which is then executed independently.ßdummyTurker
12384ß0.0ßWe combine the answers from various executions and rank the aggregated answer set.ßdummyTurker
12385ß0.0ßWe will explain this more clearly in the final version.ßdummyTurker
12386ß0.0ßThe index is generated using the assertions from all the KBs.ßdummyTurker
12387ß0.0ßIt is an inverted index of terms/tokens in the assertions.ßdummyTurker
12388ß0.0ßThe tool used returns a ranked list of assertions based on the terms/tokens in the assertions and those in the query graph.ßdummyTurker
12389ß0.0ßKindly see our response to Weakness 1.ßdummyTurker
12390ß0.0ßWe thank the reviewer for the insightful comments.ßdummyTurker
12391ß0.0ß1.ßdummyTurker
12392ß0.0ßWe will include the inter-annotator agreement and the annotation guideline in the revised version.ßdummyTurker
12393ß0.0ßTo reduce the subjectiveness during labelling, we have defined explicit standards for each rating level, which we will release along with the data.ßdummyTurker
12394ß0.0ß2.ßdummyTurker
12395ß0.0ßThe main difference is that the stock KB is numeric, which embed temporal elements among the relations between numbers in KB.ßdummyTurker
12396ß0.0ßFor example, MovAvg5 denotes the averaged price of the past 5 days.ßdummyTurker
12397ß0.0ßBy comparing the open price and MovAvg5, we can know whether this stock is in a declining trend in the past 5 days.ßdummyTurker
12398ß0.0ß3.ßdummyTurker
12399ß0.0ßThe current state of the art prediction algorithm is a variation of our model by removing the number processing component.ßdummyTurker
12400ß0.0ßIn our preliminary experiments it cannot produce reasonable answers due to the serious OOV problem.ßdummyTurker
12401ß0.0ßWe will include the results in the revised version.ßdummyTurker
12402ß0.0ß4.ßdummyTurker
12403ß0.0ßWe have employed multiple strategies to clean the data, which will be included in the revised version.ßdummyTurker
12404ß0.0ß5.ßdummyTurker
12405ß0.0ßSince the release webpage reveals the author information, we will include the link once this paper is publicly available.ßdummyTurker
12406ß0.0ßWe thank the reviewer for the insightful comments.ßdummyTurker
12407ß0.0ßAll the crawled data are publicly available, and we are allowed to use and share the QA corpus for research purpose.ßdummyTurker
12408ß0.0ß1.ßdummyTurker
12409ß0.0ßWe try to propose a general Grounded NLG task, which attempts to generating meaningful sentences based on numerical ground-knowledge and/or textual queries.ßdummyTurker
12410ß0.0ßWe choose the stock domain since we can crawl enough data in this domain.ßdummyTurker
12411ß0.0ßHowever, we expect that the proposed models can be transferred to other similar domains, such as weather-related and medical domains.ßdummyTurker
12412ß0.0ßTherefore, “generating actionable investing advice itself and (and its reasoning)” is not the main focus of this work, although it is indeed very important for the StockQA task.ßdummyTurker
12413ß0.0ßWe will clarify this in the revised version.ßdummyTurker
12414ß0.0ß2.ßdummyTurker
12415ß0.0ßDue to space limitation, we didn’t present annotation guideline and examples, which are planned to be listed on the data release webpage.ßdummyTurker
12416ß0.0ßWe will also release our generated results and their labelled scores, which we expect to help others to reproduce the evaluation.ßdummyTurker
12417ß0.0ßAll the rating metrics are independent from each other when annotators label the responses.ßdummyTurker
12418ß0.0ßFor example, a response can be labelled as informative yet not relevant.ßdummyTurker
12419ß0.0ßThe rating metrics have different priorities when we map them to a ranking score ranging from 0 to 1.ßdummyTurker
12420ß0.0ßFor instance, when a response is labelled as “poor in relevance”, then the score is 0 regardless of its ratings on other metrics such as informativeness.ßdummyTurker
12421ß0.0ßConcerning Table 8, it is mainly due to that a large portion of the generated responses are generic answers, which are poor in informativeness and do not refer to factual descriptions (thus do not have factual error).ßdummyTurker
12422ß0.0ß3.ßdummyTurker
12423ß0.0ßActually, in our preliminary experiments we have tried such alternative, which unfortunately cannot produce reasonable answers.ßdummyTurker
12424ß0.0ßThe main reason is that in this task numbers in both question-answer pair and stock knowledge base are low-frequent or even unseen in the training corpus.ßdummyTurker
12425ß0.0ßTherefore, this variation suffers from serious OOV problem, which affects the generation of not only the numbers but also other natural words in the answers.ßdummyTurker
12426ß0.0ßWe will include the results in the revised version.ßdummyTurker
12427ß0.0ß4.ßdummyTurker
12428ß0.0ßThe percentage in Table 6 denotes the portion of generated responses contain numbers.ßdummyTurker
12429ß0.0ßFor example, 10.4% of the responses generated by the “Word” baseline contain numbers.ßdummyTurker
12430ß0.0ßWe thank the reviewer for the insightful comments.ßdummyTurker
12431ß0.0ß1.ßdummyTurker
12432ß0.0ßAll the data we crawled refer to the stock markets in mainland China.ßdummyTurker
12433ß0.0ßAccordingly, the language used in stock markets is Chinese.ßdummyTurker
12434ß0.0ß2.ßdummyTurker
12435ß0.0ßIt is hard to find a state-of-the-art baseline for generating natural sentences in the KB-based QA task, since the majority of the previous work focus on selecting entities from the knowledge base.ßdummyTurker
12436ß0.0ßA variation of the proposed approach by removing the number processing architecture can be viewed as a baseline, which is widely employed in previous work (Yin et al.ßdummyTurker
12437ß0.0ß2016).ßdummyTurker
12438ß0.0ßIn our preliminary experiments it cannot produce reasonable answers due to the serious OOV problem.ßdummyTurker
12439ß0.0ßWe will include the results in the revised version.ßdummyTurker
12440ß0.0ß3.ßdummyTurker
12441ß0.0ßWe will mention the language investigated is Chinese in the abstract.ßdummyTurker
12442ß0.0ßWe believe the results can be transferred to other languages (e.g.ßdummyTurker
12443ß0.0ßEnglish), since we don’t exploit any language-specific knowledge.ßdummyTurker
12444ß0.0ßPrevious research has proven that a language-dependent Seq2Seq model can consistently perform well across languages in many NLP tasks, such as machine translation and dialogues.ßdummyTurker
12445ß0.0ß4.ßdummyTurker
12446ß0.0ßWeb site owners generally use the /robots.txt file to give instructions about their site to web robots.ßdummyTurker
12447ß0.0ßWe followed such instruction to crawl the sites as of July 21, 2017.ßdummyTurker
12448ß0.0ßWe crawled 91K instances from cf8.com.cn, 72K instances from live.9666.cn, and 21K instances from licaishi.sina.com.cn.ßdummyTurker
12449ß0.0ßWe will include the details in Section 2.3.ßdummyTurker
12450ß0.0ß1.ßdummyTurker
12451ß0.0ßSince all the crawled data are publicly available, we have permission to share the QA corpus.ßdummyTurker
12452ß0.0ß2.ßdummyTurker
12453ß0.0ßThe examples in Table 1 are translated from real examples in Chinese, except that we replace the stock names and prices with English ones (in order that more readers can understand them well).ßdummyTurker
12454ß0.0ßWe will clarify the origination of the examples.ßdummyTurker
12455ß0.0ß3.ßdummyTurker
12456ß0.0ßIn the crawled data, each QA pair is associated with a distinct stock.ßdummyTurker
12457ß0.0ßFor example, on some websites, users need to fill in the stock name in a structured way.ßdummyTurker
12458ß0.0ßFor these sites, we directly use the stock name input by the users as the stock index.ßdummyTurker
12459ß0.0ß4.ßdummyTurker
12460ß0.0ßWe think our results are independent from the Chinese language, since our approaches don’t use any language-specific knowledge.ßdummyTurker
12461ß0.0ß5.ßdummyTurker
12462ß0.0ßYes, we believe our results can be transferred to other domains (e.g.ßdummyTurker
12463ß0.0ßweather-related or medical domains), which belong to the same research thread of generating answers based on questions and numeric knowledge bases.ßdummyTurker
12464ß0.0ßThis is exactly one of the research direction we will explore in the future.ßdummyTurker
12465ß0.0ßThanks for your insightful comments.ßdummyTurker
12466ß0.0ßW1.ßdummyTurker
12467ß0.0ßThe improvements of our structured alignment model over the Decomposable Attention and Simple Span Alignment baseline are statistically significant under one-tailed paired t-test (p=0.05), we will clarify this in the future versions.ßdummyTurker
12468ß0.0ßQ1.ßdummyTurker
12469ß0.0ßWe will report more experimental results which were neglected in this version due to space limitations.ßdummyTurker
12470ß0.0ßStructure Alignment (Without Tags) on TREC-QA, MAP: 0.766 MRR: 0.838.ßdummyTurker
12471ß0.0ßAs we stated in the paper, the Structure Alignment model used for SNLI is based on Shared Parameters and the ACC of Separate Parameters version is 86.2.ßdummyTurker
12472ß0.0ßQ2.ßdummyTurker
12473ß0.0ßFirst, our model is not designed for unsupervised grammar induction but for modeling the latent structures for semantic-level tasks, so it may only learn the structures that are useful for the task.ßdummyTurker
12474ß0.0ßSecond, as explained in [1], RBRANCH exploits a systematic right-branching tendency of English, and an unsupervised system has no a priori reason to prefer right chains to left chains.ßdummyTurker
12475ß0.0ß[1] Dan Klein and Christopher D Manning.ßdummyTurker
12476ß0.0ß2002.ßdummyTurker
12477ß0.0ßNatural language grammar induction using a constituent context model.ßdummyTurker
12478ß0.0ßIn Proceeding of the NIPS Conference.ßdummyTurker
12479ß0.0ßThanks for your insightful comments.ßdummyTurker
12480ß0.0ßW1.ßdummyTurker
12481ß0.0ßWe agree the computational cost is now a barrier for our proposed model.ßdummyTurker
12482ß0.0ßW2: It is not standard practice to balance computational cost; instead the standard practice in this kind of work is to balance the size of the model (i.e., number of parameters).ßdummyTurker
12483ß0.0ßMaking an ensemble out of the decomposable attention model would give that model more parameters than our model.ßdummyTurker
12484ß0.0ßW3.ßdummyTurker
12485ß0.0ßThough we did not reach SOTA results, our model can be considered as a plug-in component for other more complex models, and may boost their performance by modeling the latent structures.ßdummyTurker
12486ß0.0ßAlso, the structured alignment model can provide better interpretability for sentence matching tasks, which is a problem with most neural models.ßdummyTurker
12487ß0.0ßThanks for your insightful comments and suggestions.ßdummyTurker
12488ß0.0ßW1: e_{ij,kl} is the score for span_ij and span_kl being aligned, and ln(\rho_{kl}) can be considered as a score for span_kl selected as a constituent, so we can sum them as the score for span_ij being matched to a constituent, and we use a normalized exponential function to get a probabilistic distribution.ßdummyTurker
12489ß0.0ßIf we had not included the rhos in the normalization, this would be an expectation over two independent latent variables: the alignment between the (given) source span and all possible target spans, and the probability that the target span is actually a span in the parse tree.ßdummyTurker
12490ß0.0ßBecause we included the rhos in the normalization, this is a categorical distribution over alignments between the (given) source span and all possible target spans, parameterized by a combination of encoding similarity and whether the target span is likely to be a span in the parse tree.ßdummyTurker
12491ß0.0ß"One of these treats the alignment and the target ""span-ness"" as independent in the expectation, and the other (which we actually did) models the alignment between spans as a function of the ""span-ness"" of the target span."ßdummyTurker
12492ß0.0ßThe latter is more expressive.ßdummyTurker
12493ß0.0ßWe can give a better explanation of this in the final version of the paper.ßdummyTurker
12494ß0.0ßW2: We took experiments on unsupervised parsing to show that though our model is not designed for grammar induction, it still can achieve some consistencies with PCFG trees.ßdummyTurker
12495ß0.0ßAnd for the RBRANCH baseline, as explained in [1], RBRANCH exploits a systematic right-branching tendency of English, and an unsupervised system has no a priori reason to prefer right chains to left chains, and does not need to beat RBRANCH to claim partial success at grammar induction.ßdummyTurker
12496ß0.0ßActually some models that are specifically designed for PCFG grammar induction can only beat RBRANCH baseline with a relatively small margin [2].ßdummyTurker
12497ß0.0ßAlso, our model is not designed for unsupervised grammar induction but for modeling the latent structures for semantic-level tasks, so it may only learn the structures that are useful for the task.ßdummyTurker
12498ß0.0ßW3: We will make our code publicly available.ßdummyTurker
12499ß0.0ßThe outside algorithm is not described in the paper due to space limitations and the fact that given our descriptions of the modified inside algorithm, it can be inferred naturally from the original version of the inside-outside algorithm.ßdummyTurker
12500ß0.0ßWe will add the descriptions if allowed an appendix in a revised version.ßdummyTurker
12501ß0.0ßWe do cite the Manning and Schutze textbook when we first mention the inside-outside algorithm, in the introduction - we could give another pointer closer to the algorithm description, if that's helpful.ßdummyTurker
12502ß0.0ßAs stated in the paper, all hyperparameters are selected based on performance on the devset, and we will give details of the range in the future version and give clear instructions when releasing the code.ßdummyTurker
12503ß0.0ßQ1: See our answer for W1.ßdummyTurker
12504ß0.0ßQ2: The marginal posterior probabilities only include T_1 tags, as T_0 is used to provide a fall-back for not being selected as a constituent.ßdummyTurker
12505ß0.0ßSo the model can learn to assign a higher score for T_0 (whichßdummyTurker
12506ß0.0ßwill lead to a lower score of  T_1 since they are together normalized by the inside outside algorithm) when it decides to de-select a span.ßdummyTurker
12507ß0.0ßWe can give a better explanation of this in the final version of the paper.ßdummyTurker
12508ß0.0ßQ4: 'Boeing is' seems a more proper alignment, we will fix this in the next version.ßdummyTurker
12509ß0.0ßWe will correct the typos and carefully check our paper.ßdummyTurker
12510ß0.0ßWe do notice there some recent papers on latent structure learning.ßdummyTurker
12511ß0.0ßWe had to remove some content in related work due to space limitation and we will give more detailed references.ßdummyTurker
12512ß0.0ß[1] Dan Klein and Christopher D Manning.ßdummyTurker
12513ß0.0ß2002.ßdummyTurker
12514ß0.0ßNatural language grammar induction using a constituent context model.ßdummyTurker
12515ß0.0ßIn Proceeding of the NIPS Conference.ßdummyTurker
12516ß0.0ß[2]Klein D, Manning C D. A generative constituent-context model for improved grammar induction.ßdummyTurker
12517ß0.0ßProceedings of the 40th Annual Meeting on Association for Computational Linguistics.ßdummyTurker
12518ß0.0ßMany thanks for your valuable comments!ßdummyTurker
12519ß0.0ßIn this study, we simply use fully connected layer as functions g and \phi in experiments, which is also used in Non-local Neural Network (Want et, al.ßdummyTurker
12520ß0.0ß2017) (Equation [1]).ßdummyTurker
12521ß0.0ßWe plan to reorganize our paper to make it more readable in revised version.ßdummyTurker
12522ß0.0ßHigher order functions (like quartic, quintic and other higher-order functions) normally involve more parameters, which might cause the overfiting problem.ßdummyTurker
12523ß0.0ßBesides, it requires more human intervention to design an appropriate higher order function.ßdummyTurker
12524ß0.0ßWe would like to eliminate such human intervention in feature engineering and make the model able to learn how to capture these features automatically.ßdummyTurker
12525ß0.0ßWe found that Shi et al.ßdummyTurker
12526ß0.0ß(2017) is mainly about decoding and training strategies for dependency parsing, which could be orthogonal to our study.ßdummyTurker
12527ß0.0ßOur emphasis in this work was on comparing the spectral word clusters to the Brown ones.ßdummyTurker
12528ß0.0ßWe tested them as features, analyzed the results and performed the oracle analysis on them.ßdummyTurker
12529ß0.0ßWe feel that - in the scope of this work - conducting this process for two fundamental NLP tasks (SRL and dependency parsing) gave us some very interesting insights as to the very similar performance exhibited by the two cluster sets as well as to their complementary nature.ßdummyTurker
12530ß0.0ßSince this work was not about solving SRL or dependency parsing, we used off-the-shelf yet powerful feature-based software packages for that purpose (CRF++ and MSTParser), referencing both in the paper.ßdummyTurker
12531ß0.0ßHowever, following your comment, we will add more details regarding the experiments in the final version of the paper.ßdummyTurker
12532ß0.0ßWe focused mainly on the Propbank dataset to test the cluster sets and analyze the results, while the dependency parsing experiment was mainly meant for reinforcing and validating our hypothesis by testing it in on an additional NLP task with a different nature (syntax vs. semantics).ßdummyTurker
12533ß0.0ß(This is also a reply to question 4).ßdummyTurker
12534ß0.0ßWe wanted to pragmatically compare the spectral word clusters to the Brown ones.ßdummyTurker
12535ß0.0ßFor this purpose, we chose an extrinsic approach - using them as features for common NLP tasks, compare the performance, and perform an analysis to quantify the amount of overlap in the information the cluster sets provide for these tasks.ßdummyTurker
12536ß0.0ßWe felt that SRL and dependency parsing are two fundamental, interesting yet different tasks which are exactly suitable for our purposes.ßdummyTurker
12537ß0.0ßWe will endeavor to make that more clear in the final version.ßdummyTurker
12538ß0.0ßPreliminary experiments showed that the choice of K=250 was consistently best over all the approaches we tried in this work.ßdummyTurker
12539ß0.0ßWe will make that clear in the final version.ßdummyTurker
12540ß0.0ßWe used the CRF++ package (cited in the paper), which implements a Conditional Random Fields model for NLP tasks.ßdummyTurker
12541ß0.0ßIt utilizes a variant of the Limited memory   Broyden–Fletcher–Goldfarb–Shanno algorithm for training.ßdummyTurker
12542ß0.0ßWe will note this in the final version.ßdummyTurker
12543ß0.0ßPlease see the reply to argument 2.ßdummyTurker
12544ß0.0ßa) We agree and thank you for the citation, which will be added in the final version.ßdummyTurker
12545ß0.0ßb) We used the word2vec embeddings available on the google website (They were trained on a much larger corpus than the one we used).ßdummyTurker
12546ß0.0ßIn this work we set out to test the spectral clustering method, which is fairly new to the NLP field, for the task of word clustering, and compare it to the widely-used Brown method.ßdummyTurker
12547ß0.0ßWe felt that testing the clusters from the two methods on a fundamental NLP task, comparing their performance and conducting the oracle analysis on the results, would achieve this goal.ßdummyTurker
12548ß0.0ßWe did want to perform this comparison over more than one NLP task, in order to achieve a sense of consistency.ßdummyTurker
12549ß0.0ß1) We thank you for the typos detection, and will make the proper corrections.ßdummyTurker
12550ß0.0ß2) We felt that this work was more suitable for submission as a short paper, according to the guidelines under the ‘Short Papers’ section in ACL 2018’s Call for Papers.ßdummyTurker
12551ß0.0ßHowever, following your comments, we will use the extra page allocation to add more details concerning the experiments.ßdummyTurker
12552ß0.0ßOur main goal was not to try and compete with existing systems for SRL and dependency parsing, but rather to focus on the word clusters we computed and the information they contain with relation to these tasks.ßdummyTurker
12553ß0.0ßFor this purpose, we used them as the features for performing the two tasks, comparing the results and performing the oracle analysis on them.ßdummyTurker
12554ß0.0ßWe will strive to make that more clear in the final version of the paper.ßdummyTurker
12555ß0.0ßWe did cite state-of-the-art systems, for completeness purposes.ßdummyTurker
12556ß0.0ßThanks for your useful comments!ßdummyTurker
12557ß0.0ßIt has been shown that better estimation of word importance leads to better extractive summaries [1].ßdummyTurker
12558ß0.0ßAutomatic keyword extraction has been studied extensively [2].ßdummyTurker
12559ß0.0ßWe use a popular and well tested method, RAKE.ßdummyTurker
12560ß0.0ßA disadvantage with such methods is that they do not guarantee representation, via extracted keywords, of all the topics in the text [2].ßdummyTurker
12561ß0.0ßSo, if RAKE key words are directly applied to the input test document (without using word decoder trained on RAKE words, obtained from gold summary), then there is a possibility of missing sentences from the missed topics.ßdummyTurker
12562ß0.0ßSo, we use the key words in training and also model the interactions between keywords and important sentences.ßdummyTurker
12563ß0.0ßThe importance of all 3 interactions: sentence-sentence, word-word and sentence-word, for summarization, have been studied in [3] using graph-based approaches.ßdummyTurker
12564ß0.0ßThe importance of all 3 interactions: sentence-sentence, word-word and sentence-word, for summarization, have been studied using graph-based approaches [3].ßdummyTurker
12565ß0.0ßIn particular they show that methods that consider both factors:ßdummyTurker
12566ß0.0ß1.ßdummyTurker
12567ß0.0ßSaliency of a sentence (word) if it is strongly linked with other salient sentences (words)ßdummyTurker
12568ß0.0ß2.ßdummyTurker
12569ß0.0ßSaliency of a sentence containing salient words.ßdummyTurker
12570ß0.0ßSaliency of a word contained in salient sentences.ßdummyTurker
12571ß0.0ßoutperform methods that consider either one.ßdummyTurker
12572ß0.0ßIn our baseline NN [4] the sentence decoder is parameterized by only sentence encoding.ßdummyTurker
12573ß0.0ßSWAP-NET outperforms NN, showing the importance of parameterizing sentences by additional word encodings.ßdummyTurker
12574ß0.0ßOur code and all parameter settings are on github (anonymized) as stated in line 732.ßdummyTurker
12575ß0.0ß[1] Hong, Nenkova, Improving the estimation of word importance for news multi-document summarization, EACL 2014ßdummyTurker
12576ß0.0ß[2] Hasan, Ng, Automatic Keyphrase Extraction: A Survey of the State of the Art, ACL 2014ßdummyTurker
12577ß0.0ß[3] Wan et-al, Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction, ACL 2007ßdummyTurker
12578ß0.0ß[4] Cheng, Lapata, Neural summarization by extracting sentences and words, ACL 2016ßdummyTurker
12579ß0.0ßThanks for your suggestions!ßdummyTurker
12580ß0.0ßOur code, in Tensorflow, is on github (anonymized) as stated in line 732.ßdummyTurker
12581ß0.0ßDetails of parameter settings, run time and hardware are also given there.ßdummyTurker
12582ß0.0ßWe ran the pair-Wilcox test on 500 daily-mail document summaries generated by Lead-3 and SWAP-NET by calculating full length F-scores for rouge-1 and rouge-2.ßdummyTurker
12583ß0.0ßP-values show that the improvement is significant and can be added to the paper.ßdummyTurker
12584ß0.0ßP-value (Rouge-1 F-score): 5.8e-09ßdummyTurker
12585ß0.0ßP-value (Rouge-2 F-score): 1.2e-16ßdummyTurker
12586ß0.0ßWe manually checked the generated summaries.ßdummyTurker
12587ß0.0ßSince the summarization is extractive, and not abstractive, we found that ROUGE scores combined with a measure of semantic distance provides a good and objective quality measure.ßdummyTurker
12588ß0.0ßAdditional human evaluation tests can also be conducted.ßdummyTurker
12589ß0.0ßThanks for your suggestions!ßdummyTurker
12590ß0.0ßTo train doc2vec we use genism (with cosine similarity measure) with embedding size 200 and initial learning rate 0.025.ßdummyTurker
12591ß0.0ßThe model is trained on 500 documents from Daily-Mail dataset for 10 epochs and learning rate is decreased by 0.002 at each epoch.ßdummyTurker
12592ß0.0ßParagraph vectors have been found to be effective semantic representations of sentences [1] and experiments in [2] also show that paragraph vectors can be effectively used to measure semantic similarity using cosine distance.ßdummyTurker
12593ß0.0ßWe manually checked the generated summary samples.ßdummyTurker
12594ß0.0ßSince the summarization is extractive, and not abstractive, we found that ROUGE scores combined with a measure of semantic distance provides a good and objective quality measure.ßdummyTurker
12595ß0.0ßAdditional human evaluation can be conducted.ßdummyTurker
12596ß0.0ßRAKE uses co-occurrence to determine importance of words.ßdummyTurker
12597ß0.0ßSuch methods do not guarantee representation, via extracted keywords, of all the topics in the text [3].ßdummyTurker
12598ß0.0ßSo, if RAKE key words are directly applied to the input test document (without using word decoder trained on RAKE words from gold summary), then there is a possibility of missing sentences from the missed topics.ßdummyTurker
12599ß0.0ßSo, we use the key words in training and also model the interactions between keywords and important sentences.ßdummyTurker
12600ß0.0ßThe importance of all 3 interactions: sentence-sentence, word-word and sentence-word, for summarization, have been studied in [4] using graph-based approaches.ßdummyTurker
12601ß0.0ßIn our baseline model [5], their sentence decoder is parameterized by only the sentence encoding, and SWAP-NET that models all the interactions outperforms their method.ßdummyTurker
12602ß0.0ßFrom line 293, p^s_{kj}=alpha^s_{kj} * p(Q_j = q^s_j) and p^w_{ij}=alpha^w_{kj} * p p(Q_j = q^s_j) And from equation 3, p(Q_j = q^w_j) =1- p(Q_j = q^s_j)ßdummyTurker
12603ß0.0ßSo when sentence k is important at the jth time step i.e.ßdummyTurker
12604ß0.0ßq^w_j=0, q^s_j=1, then loss -log(p^s_{kj}) will favour p(Q_j = q^s_j) for sentence selection by switch and  also alpha^s_{kj} for sentence k. Similarly, when word i is important at the jth time step i.e.ßdummyTurker
12605ß0.0ßq^w_j=1, q^s_j=0, then loss -log(p^w_{ij}) will favour p(Q_j = q^w_j) for word selection by switch and  also alpha^w_{ij} for word i.ßdummyTurker
12606ß0.0ßThere are more keywords than important sentences, and higher chances of incorrect word prediction, as stated in lines 501-505.ßdummyTurker
12607ß0.0ßWe add the term -log p(Q_j) to the loss, to help in dealing with imbalance in number of words and sentences.ßdummyTurker
12608ß0.0ßIt is possible to get the same sentence or word in (usually consecutive) multiple decoding steps.ßdummyTurker
12609ß0.0ßIn that case, in Eq.ßdummyTurker
12610ß0.0ß4 we consider the maximum value of alpha obtained across these steps.ßdummyTurker
12611ß0.0ßWe consider maximum scores of distinct sentences and words.ßdummyTurker
12612ß0.0ßThis is also similar to our baselines NN [5] and SummaRuNNer [6]:  top sentences with highest prediction probability are used in the summary.ßdummyTurker
12613ß0.0ßThere are 3.11 sentences on average in the gold highlights of the training set and so we select 3 sentences.ßdummyTurker
12614ß0.0ßThe same argument can be found in other works [7].ßdummyTurker
12615ß0.0ß[1] Le, Mikolov, Distributed representations of sentences and documents, ICML 2014ßdummyTurker
12616ß0.0ß[2] Dai et-al, Document embedding with paragraph vectors, arxiv.org/abs/1507.07998ßdummyTurker
12617ß0.0ß[3] Hasan, Ng, Automatic Keyphrase Extraction: A Survey of the State of the Art, ACL 2014ßdummyTurker
12618ß0.0ß[4] Wan et-al Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction, ACL 2007ßdummyTurker
12619ß0.0ß[5] Cheng, Lapata, Neural summarization by extracting sentences and words, ACL 2016ßdummyTurker
12620ß0.0ß[6] Nallapati et al, SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents, AAAI 2017.ßdummyTurker
12621ß0.0ß[7] Narayan et-al, Neural extractive summarization with side information.ßdummyTurker
12622ß0.0ßAAAI, 2018ßdummyTurker
12623ß0.0ßIn the introduction, we discuss the obstacles of existing QA systems and introduce challenges by observing the problems of existing approaches in QA benchmarks.ßdummyTurker
12624ß0.0ß"For each challenge we propose examples to explain the ''new terms""."ßdummyTurker
12625ß0.0ßFor instance, we shows the word “cosmonauts” and “they” in the question of Figure 1 refer to the same thing, which is the co-reference challenge.ßdummyTurker
12626ß0.0ßSince some of these challenges cannot be handled by existing QA systems, we propose four operations to solve them.ßdummyTurker
12627ß0.0ßFor each operation we give a definition and explain why they are necessary.ßdummyTurker
12628ß0.0ßFor instance, the Merge operation is used to handle co-reference challenge.ßdummyTurker
12629ß0.0ßThe main contribution is that we propose a state transition method based on the four operations.ßdummyTurker
12630ß0.0ßComparing with existing solutions, our framework can tackle with more complex questions.ßdummyTurker
12631ß0.0ßThe first difference between our solution (Figure 1) and STAGG (Figure 2) is that STAGG cannot generate a query graph with circle, since it cannot introduce the two edges between v1 and v2 in Figure 1.ßdummyTurker
12632ß0.0ßThe second difference is STAGG only recognizes one variable (question focus), which means it cannot find v4 (?place) in Figure 1.ßdummyTurker
12633ß0.0ßThe third difference is STAGG generates limited structures (i.e., a chain) while we can generate more complex structures such as a star or a circle.ßdummyTurker
12634ß0.0ßWe will further clarify these differences in our revision.ßdummyTurker
12635ß0.0ßFor each operation, we define them formally (see Section 2.2) and give the motivation of these operations.ßdummyTurker
12636ß0.0ßFor instance, the fold operation is appropriate to those nodes that are useless or mis-recognized.ßdummyTurker
12637ß0.0ßAnd then we give an example (Figure 3(b)) to illustrate it.ßdummyTurker
12638ß0.0ßWe discuss the learning process of SQG generation with four operations in Section 3.2.ßdummyTurker
12639ß0.0ßWe propose the four conditions to speed up and reduce the search space for SQG generation.ßdummyTurker
12640ß0.0ßThe experiment (Table 1 in supplementary file) shows that the introduced four conditions can improve both efficiency and effectiveness of our method.ßdummyTurker
12641ß0.0ßThank you for your advice and we will add experiments to analyze the precision of each stage of the whole pipeline in our revision.ßdummyTurker
12642ß0.0ßNode Recognition is the basement of our system as the four operations are all based on nodes.ßdummyTurker
12643ß0.0ßOur NR module combined by an entity linking system and sequence labeling model has 89% accuracy on QALD-6 100 test questions.ßdummyTurker
12644ß0.0ßIn which 6 questions failed by recognizing useless nodes and 5 by choosing false entity.ßdummyTurker
12645ß0.0ßThen we build 91 correct SQG with four operations (without conditions).ßdummyTurker
12646ß0.0ß"Notice the fold operation revise 2 of the 6 ""useless nodes"" questions."ßdummyTurker
12647ß0.0ßFinally, we get 77% F1 by executing SQG to get answers.ßdummyTurker
12648ß0.0ßIn this stage the errors are mainly caused by the relation extraction model (implemented by MCCNN).ßdummyTurker
12649ß0.0ßThe four conditions are designed to reduce the search space and speed up the system.ßdummyTurker
12650ß0.0ßThe comparison results (824ms to 700ms and 77% to 80% F1) are not very significant because most of questions (90%) have only two or three nodes.ßdummyTurker
12651ß0.0ßHowever, the conditions can save most search space when four or more nodes.ßdummyTurker
12652ß0.0ßOn the other hand, they reduce the requirement of the ranking model (because the candidates are decreased on a large scale).ßdummyTurker
12653ß0.0ß"For instance, we can revise 5 of the 6 ""useless nodes"" questions with fold operation and condition (revise 2 without the condition)."ßdummyTurker
12654ß0.0ßThanks for your fruitful and helpful comments and suggestions, which are very important to refine the paper.ßdummyTurker
12655ß0.0ßWe agree that comparison with other related works are needed to make this paper more convincing.ßdummyTurker
12656ß0.0ßThe related works, such as Bengio et al.ßdummyTurker
12657ß0.0ß2015 and MRT methods, will be discussed and compared in the refined version, including the differences you mentioned (all the points are correct).ßdummyTurker
12658ß0.0ßBesides, we think our method is effective and simple to implement, and it can be directly applied to other sequence-to-sequence tasks.ßdummyTurker
12659ß0.0ßFor joint training procedure, we found 3 iterations are enough to reach convergence, and this detail is shown in the section of Implementation Details.ßdummyTurker
12660ß0.0ßMore analysis about joint training procedure will be added in the paper, such as the BLEU changes with the increase of iterations.ßdummyTurker
12661ß0.0ßWe show the effectiveness of our method by comparing “Transformer-Big+BT” and “Transformer-Big+BT+RT”.ßdummyTurker
12662ß0.0ßWe will add comparison results with “Transformer-big+BT+JS” and “Transformer-big+RT” to make it more sufficient in the camera-ready version.ßdummyTurker
12663ß0.0ßMany thanks for your helpful comments and constructive criticism.ßdummyTurker
12664ß0.0ßThe functions might not look so different, but the changes are key for more expressiveness (also see our answer to weakness argument 2).ßdummyTurker
12665ß0.0ßWe would argue the improvement is moderate; but note that existing datasets or encoders might not fully leverage our compositions.ßdummyTurker
12666ß0.0ßA recent article by Gururangan et al.ßdummyTurker
12667ß0.0ß(accepted) shows that a 67% accuracy can be achieved on SNLI with no composition, and without even looking at the first sentence (s_1), which is problematic for a NLI task.ßdummyTurker
12668ß0.0ßSuch artifacts in training and evaluation dataset might be the reason why the difference is moderate.ßdummyTurker
12669ß0.0ßOur work allows for the investigation of this question with larger datasets.ßdummyTurker
12670ß0.0ßDasgupta et al.ßdummyTurker
12671ß0.0ß(2018) backs this hypothesis too.ßdummyTurker
12672ß0.0ßGururangan et al.ßdummyTurker
12673ß0.0ß(accepted).ßdummyTurker
12674ß0.0ßAnnotation Artifacts in Natural Language Inference Data.ßdummyTurker
12675ß0.0ßAccepted at NAACL 2018. https://arxiv.org/abs/1803.02324v1ßdummyTurker
12676ß0.0ßMany thanks for your helpful comments and constructive criticism.ßdummyTurker
12677ß0.0ßPlease note that three of the compositions we propose are new and tailored to textual relational learning.ßdummyTurker
12678ß0.0ßDistmult and ComplEx are indeed used in their standard form but component weighted Unstructured, component weighted TransE with fixed translations and ComplEx \alpha variation are new.ßdummyTurker
12679ß0.0ßWe would argue that our novel compositions obtain a moderate but significant improvement, which is consistent across tasks.ßdummyTurker
12680ß0.0ßWe would be glad to incorporate the improvements proposed by the three reviewers, and add appropriate clarifications.ßdummyTurker
12681ß0.0ßOur understanding is that the ConvE model is not better than ComplEx on all metrics, and it is not obvious whether the reported improvements are due to the composition, or rather due to optimizer and hyperparameters.ßdummyTurker
12682ß0.0ßFor instance, Kadlec et al.ßdummyTurker
12683ß0.0ß(2017) show that a well-tuned Distmult model is able to outperform ConvE; properly tuned ComplEx might have even better results.ßdummyTurker
12684ß0.0ßBesides, a motivation for ConvE is expressivity with small dimensional embeddings, while sentence representation learning typically benefits from high dimensional embeddings (4096 in our work).ßdummyTurker
12685ß0.0ßIt is not clear either how to reframe it as a composition function between two embeddings, but it would be interesting to compare them.ßdummyTurker
12686ß0.0ßPlease note that our contribution is not limited to the evaluation of relational representations; we also show that our novel compositions are useful for sentence representation learning.ßdummyTurker
12687ß0.0ßChanging the evaluation functions on Senteval is only a detail of our work, and it allows us to see how sentence encoder embeddings trained with a given composition behave when evaluated with identical or different compositions.ßdummyTurker
12688ß0.0ßSentence representation learning is clearly linked to composition functions in our framework (and in the Infersent framework as well).ßdummyTurker
12689ß0.0ßWe’d like to emphasize that it is impossible for a symmetric composition to leverage all relational information from NLI data - so it is impossible for most previously used compositions as well (except for tensor product, which is impractical).ßdummyTurker
12690ß0.0ßOur compositions do not suffer from symmetry or linearity, and we believe they will be useful for future research for learning sentence representations.ßdummyTurker
12691ß0.0ßKadlec et al.ßdummyTurker
12692ß0.0ß2017.ßdummyTurker
12693ß0.0ßKnowledge Base Completion: Baselines Strike Back.ßdummyTurker
12694ß0.0ßProceedings of the 2nd Workshop on Representation Learning for NLP, pages 69-74. https://arxiv.org/pdf/1705.10744.pdfßdummyTurker
12695ß0.0ßR2-W1: As we mentioned in the paper, “In the future, ESE performance can be improved by enhancing the performance of candidate entities extraction”.ßdummyTurker
12696ß0.0ßThis component can be easily replaced with a better performing method, and we leave that to the future.ßdummyTurker
12697ß0.0ßR2-W2: Thank you, we will handle all of them and carefully revise the paper.ßdummyTurker
12698ß0.0ßR2-W3: We will provide percentages in the revised paper.ßdummyTurker
12699ß0.0ßKholghi et al.’s technique “investigate how machine-assisted pre-annotations provided by active learning models can further reduce the manual annotation time compared to when annotating from scratch”.ßdummyTurker
12700ß0.0ßThat is, their method pre-annotates texts and requires annotators to review and accept/modify those annotations for all sentences while ours reduce the number of sentences that an annotator is going to scan and annotate (similar to Tsuruoka et al., 2008), so we are unable to compare with it fairly.ßdummyTurker
12701ß0.0ßWe thank the reviewer for pointing this out; we will mention the differences in more details in the related work section in the revised version.ßdummyTurker
12702ß0.0ßR2-Q1: we do not disambiguate senses, we use all the senses of a given noun phrase to draw as many edges as possible with other noun phrases to have more coverage.ßdummyTurker
12703ß0.0ßWe thank all the reviewers for their invaluable feedback that has led to important clarifications and improvements in the revised paper.ßdummyTurker
12704ß0.0ßBelow we give specific details and responses to reviewer identified weaknesses that we hope will ally their concerns.ßdummyTurker
12705ß0.0ßDue to the space constraint, we did omit some details.ßdummyTurker
12706ß0.0ßR1-W1: As pointed out by [1], “annotation behavior can vary substantially from one annotator to the next”, and that the cost calculation can show “high degree of variability”.ßdummyTurker
12707ß0.0ßEven though we have not conducted careful user studies, our work can result in savings in annotator time and effort for the same quality of annotation.ßdummyTurker
12708ß0.0ßInstead of picking random sentences to annotate, our method shows significant improvement in the nature of sentence to be annotated and a reduction in the number of sentences to be scanned and annotated (similar to Tsuruoka et al., 2008).ßdummyTurker
12709ß0.0ßEffectively, the value of the technique becomes clear in case annotators are to be hired and paid per sentence.ßdummyTurker
12710ß0.0ßWe thank the reviewer for requesting to clarify “annotation cost reduction”.ßdummyTurker
12711ß0.0ß[1] Settles et.ßdummyTurker
12712ß0.0ß"al., ""Active learning with real annotation costs."""ßdummyTurker
12713ß0.0ßNIPS.ßdummyTurker
12714ß0.0ß2008.ßdummyTurker
12715ß0.0ßR1-W2: We agree with the review, we will fix the interchangeable usage of model and data.ßdummyTurker
12716ß0.0ßLabeling Starvation Problem: Scarcity of entities of the desired class for annotation in sentences.ßdummyTurker
12717ß0.0ßThis results in a manual overhead of looking at sentences unlikely to contain relevant entities, resulting in a wasteful effort without boosting the learning rate of a model.ßdummyTurker
12718ß0.0ßR1-W3: Our approach was able to achieve higher accuracies with less amount of data, provide more practical auto-annotation modes, and better stopping criteria.ßdummyTurker
12719ß0.0ßR1-W4: In baseline model building and data annotation, annotators label sentences by scanning all of them sequentially or by using a random sampler.ßdummyTurker
12720ß0.0ßEffectively, the annotators are presented |S| number of sentences in one shot.ßdummyTurker
12721ß0.0ßIn contrast, our technique reduces the number of sentences that a user will ever read to |S| - n, where n is the number of sentences that were auto-annotated and/or not seen by the user due to reaching the desired F-Score.ßdummyTurker
12722ß0.0ßTherefore, x sentences is a small subset of the |S| - n that the user is supposed to annotate and then learn a model to sample the next batch and so on.ßdummyTurker
12723ß0.0ßR1-W5: The data we used in our experiments is already freely available online, and our code will be open sourced and freely available for other researchers and commercial use.ßdummyTurker
12724ß0.0ßR1-Q1: Yes.ßdummyTurker
12725ß0.0ßR1-Q3: The model learning frequency can be calculated if we know the sentence pool size, the batch size, the auto-annotation margin, and the desired F1 (stopping).ßdummyTurker
12726ß0.0ßPlease also see R1-W3.ßdummyTurker
12727ß0.0ßR1-AC-1: The intuition behind this heuristic to auto-annotate only the sentences that show a “significant” difference in probability between the first and second most probable sequence labels which can be clarified using the following example: If the probability of the first sequence is 0.9 and that of the second sequence is 0.05, then the model is very confident on the first being the most probable set of labels.ßdummyTurker
12728ß0.0ßIn contrast, if the first sequence probability is 0.4 and that of the second sequence is 0.35, then there is some ambiguity in the labeling and the model is not as confident in the first sequence like before.ßdummyTurker
12729ß0.0ßR1-AC-2: We tested many settings and we observed that 0.05 was very tight and won’t contribute to any auto-annotations, while 0.3 was huge contributing to a tremendous reduction in the accuracy of extractions.ßdummyTurker
12730ß0.0ßWe, therefore, propose to include these three best performing modes.ßdummyTurker
12731ß0.0ßR1-AC-4: Similar to the use in IR, we used the two equations to assign the weights on edges between noun phrases and features.ßdummyTurker
12732ß0.0ßThese TFIDF transformations determine how important a feature (f) is for a given noun phrase (n).ßdummyTurker
12733ß0.0ßFor example, the word shape features of domain-specific entities should be valued more than their syntactic features because the latter may be associated with many more noun phrases and hence less specific.ßdummyTurker
12734ß0.0ßThanks for pointing this out, we will include in the revised version.ßdummyTurker
12735ß0.0ßR1-AC-6: Our work extends the work of Shen et al.ßdummyTurker
12736ß0.0ßby using a wider range of features to introduce more semantics of the desired class to be learned.ßdummyTurker
12737ß0.0ßUsing only context features, as in Shen et al., won’t achieve the same performance especially on small datasets and sparse entities.ßdummyTurker
12738ß0.0ßWe will include that in the related work section.ßdummyTurker
12739ß0.0ßWe thank the reviewer for their detailed reading of the paper and supportive comments.ßdummyTurker
12740ß0.0ßOur aim is to learn a continuous representation of the relationship between two words.ßdummyTurker
12741ß0.0ßOften this relationship will be more fine-grained than what can be expressed using discrete relation types such as hypernymy.ßdummyTurker
12742ß0.0ßConsider, for instance, the relationship between “chair” and “table”.ßdummyTurker
12743ß0.0ßWhile we can express e.g.ßdummyTurker
12744ß0.0ßthat they are co-hyponyms of furniture, or that they are frequently found next to each other, their precise relationship is more subtle.ßdummyTurker
12745ß0.0ßIn this sense, the reviewer is correct that we are modelling examples (i.e.ßdummyTurker
12746ß0.0ßrelation instances) rather than discrete relation types, although we see this as a strength rather than a weakness (i.e.ßdummyTurker
12747ß0.0ßour aim is to model more fine-grained information than what can be expressed using discrete types).ßdummyTurker
12748ß0.0ßRelation types intuitively correspond to regions in the relation embedding space, and these regions can be learned from training examples (which we do using SVM classifiers in the paper).ßdummyTurker
12749ß0.0ßReply to weakness argument 2aßdummyTurker
12750ß0.0ßWe tuned the parameters for each test fold separately.ßdummyTurker
12751ß0.0ßFor each test fold, we used 25% of the 9 training folds as tuning data.ßdummyTurker
12752ß0.0ßAfter the optimal parameters have been determined, we retrain the model on the full training data, and apply it on the test fold.ßdummyTurker
12753ß0.0ßWe used this approach (rather than e.g.ßdummyTurker
12754ß0.0ßfixing a train/tune/test split) because the total number of examples for some of the relations is very small.ßdummyTurker
12755ß0.0ßReply to weakness argument 2bßdummyTurker
12756ß0.0ßIn the multi-class setting considered in (Vylomova et al., 2016), we already know that the word pair belongs to one of a predefined number of classes and we only have to determine which is the most likely class.ßdummyTurker
12757ß0.0ßIn the binary setting, the negative examples are chosen in a more “adversarial” fashion, e.g.ßdummyTurker
12758ß0.0ßtrying to confuse the classifier by swapping one tail word for another tail word.ßdummyTurker
12759ß0.0ßIn principle, we could of course also make the multi-class setting more challenging by introducing an “other” category containing such adversarial examples, but we wanted to stay close to the (one of the) settings from (Vylomova et al., 2016).ßdummyTurker
12760ß0.0ßThank you for your constructive comments and suggestions.ßdummyTurker
12761ß0.0ßWe really appreciate your valuable suggestion!ßdummyTurker
12762ß0.0ßDue to the wide usage of neural networks in NLP these years, we do omit some details about the networks for simplicity.ßdummyTurker
12763ß0.0ßWe will present more background details about our work and release our code to provide more information.ßdummyTurker
12764ß0.0ßWe suppose you mention “non-trivial KG” as those KGs with hierarchical relations.ßdummyTurker
12765ß0.0ßIn fact, the hierarchical relations are ubiquitous in many KGs, such as Freebase, Wikidata, and YAGO.ßdummyTurker
12766ß0.0ßMoreover, it is straightforward and efficient to maintain a coarse-to-fine grained relation system in large-scale KGs.ßdummyTurker
12767ß0.0ßHence, we believe that our proposed method can generalize well to these scenarios for relation extraction.ßdummyTurker
12768ß0.0ßThank you for your constructive comments and suggestions.ßdummyTurker
12769ß0.0ßThe NYT dataset is the most classic and widely used dataset adopted by the previous works [1,2,3,4].ßdummyTurker
12770ß0.0ßThe dataset is also quite noisy, and suitable to evaluate models' denoising abilities (which is the main issue we wish to tackle in our work).ßdummyTurker
12771ß0.0ßIn fact, most existing datasets are not as large-scale and noisy as the NYT dataset.ßdummyTurker
12772ß0.0ßHowever, we are very willing to build a new dataset to evaluate our models in the future.ßdummyTurker
12773ß0.0ßThanks for the insightful suggestion!ßdummyTurker
12774ß0.0ßIn fact, we consider the statistical significance of our experiments.ßdummyTurker
12775ß0.0ßAs following the previous works [1,2], we do not report the statistical significance in the paper.ßdummyTurker
12776ß0.0ßWe will then report this value in the appendix to make our results more solid.ßdummyTurker
12777ß0.0ßOur code will be released in the future.ßdummyTurker
12778ß0.0ß[1] Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.ßdummyTurker
12779ß0.0ßDistant supervision for relation extraction via piecewise convolutional neural networks.ßdummyTurker
12780ß0.0ßEMNLP2015.ßdummyTurker
12781ß0.0ß[2] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun.ßdummyTurker
12782ß0.0ßNeural relation extraction with selective attention over instances.ßdummyTurker
12783ß0.0ßACL2016.ßdummyTurker
12784ß0.0ß[3] Tianyu Liu, Baobao Chang, and Zhifang Sui.ßdummyTurker
12785ß0.0ßA soft-label method for noise-tolerant distantly supervised relation extraction.ßdummyTurker
12786ß0.0ßEMNLP2017.ßdummyTurker
12787ß0.0ß[4] Yi Wu, David Bamman, and Stuart Russell.ßdummyTurker
12788ß0.0ßAdversarial training for relation extraction.ßdummyTurker
12789ß0.0ßEMNLP2017.ßdummyTurker
12790ß0.0ßWe thank the reviewers for their invaluable comments and constructive suggestions.ßdummyTurker
12791ß0.0ßWe address the issues raised in the reviews below.ßdummyTurker
12792ß0.0ßWe checked with the terms of use of the web pages from where we collected data and found no copyright violation issues.ßdummyTurker
12793ß0.0ßWe plan to add a web link to download the dataset in the camera ready version, once the paper is accepted to the ACL-18 conference.ßdummyTurker
12794ß0.0ßThe shared link will also contain the source code for the models used in experiments, which has been described as a supplementary file on the submission stage.ßdummyTurker
12795ß0.0ßThe hyper-parameters used for generating our results are stated in the supplementary file, Appendix B (Implementation Details).ßdummyTurker
12796ß0.0ßTo check the validity of our data generation, we hired human annotators to manually read 1,000 randomly sampled synthetic articles and check whether they have inconsistent contents.ßdummyTurker
12797ß0.0ßResponses of human coders showed that our method successfully generate news articles with inconsistent contents, and hence partially confirmed that our method successfully mimics clickbait articles in the wild.ßdummyTurker
12798ß0.0ßIn order to cover a larger number of articles, we plan to employ alternative methods like crowdsourcing.ßdummyTurker
12799ß0.0ßOn a separate note, this dataset has been tested rigorously via a nationwide challenge for detecting news clickbaits (total research prize: 1 million USD).ßdummyTurker
12800ß0.0ßIn this competition, all participants had to propose and train detection models by preparing their own training dataset.ßdummyTurker
12801ß0.0ßThe performance was evaluated by a separate test set, which was manually created by the challenge organizers.ßdummyTurker
12802ß0.0ßOur clickbait model, trained over the dataset generated as described in the paper, has won the first place among the college teams and we believe this achievement demonstrates the reliability and efficacy of our data generation method.ßdummyTurker
12803ß0.0ßThe related researches of using IS in RNN is really not much, we did miss the paper you mentioned, however we can found no more.ßdummyTurker
12804ß0.0ßAbout https://arxiv.org/pdf/1706.00043.pdf, they naively use the loss of the mini-batch as the proportion of the probability.ßdummyTurker
12805ß0.0ßThey assume the loss of a specific mini-batch can be PREDICTED while the training is still ongoing.ßdummyTurker
12806ß0.0ßWe think it is really impractical.ßdummyTurker
12807ß0.0ßAs show in our source code, we use the standard initialization procedure as proposed in Keras for each layer.ßdummyTurker
12808ß0.0ßIn fact, for the final stage where the accuracy improves very little after each epoch, the improvement of our approach does not seems to be visually significant.ßdummyTurker
12809ß0.0ßHowever, we think on the estimation of the algorithm, one most intuitive way is to compare the epoch needed to achieve the same accuracy.ßdummyTurker
12810ß0.0ßWe think in all cases, IS-RNN shows sufficient progress.ßdummyTurker
12811ß0.0ßFor NMT cases, it actually saves more than 50 epochs to reach the same-level accuracy.ßdummyTurker
12812ß0.0ßSince page number is strictly limited, we had to balance the content of proof and evaluation, maybe we give more detailed explanations.ßdummyTurker
12813ß0.0ßWe will try hard to add all necessary citations.ßdummyTurker
12814ß0.0ßWe did missed this paper, no execuse.ßdummyTurker
12815ß0.0ßYet this paper is based on the accurate prediction of the training loss of mini-batches in the future.ßdummyTurker
12816ß0.0ßWe really don't think predicting the future loss (of a specific mini-batch) while the training is still ongoing is rational.ßdummyTurker
12817ß0.0ßWe still want pointed out that our approach is by far the most practical one to the best of our knowledge.ßdummyTurker
12818ß0.0ßWe will add a paragraph for these related literatures (however in considering IS for RNN, only this one).ßdummyTurker
12819ß0.0ßWe have run the evaluations sufficient times with different seeds, yet we have not explicitly mentioned this in paper since we really think it's a 'must do'.ßdummyTurker
12820ß0.0ßWe will explicitly point out that the curve is the average of many tries in next version.ßdummyTurker
12821ß0.0ßThe update of sampling probability distribution is performed at the end of each epoch.ßdummyTurker
12822ß0.0ßIn fact, when run practical trainings, many metrics are summarized at that time e.g., true cost, true gradient, etc, for the purpose of monitoring.ßdummyTurker
12823ß0.0ßComputing the base-gradient here naturally takes the advantage of these information, i.e.,'piggybacking'.ßdummyTurker
12824ß0.0ßThe exact absolute acceleration varies depending on the workload, by far, it achieves average reduction of total time at 17% for the NMT case and 32% total time of RNN-RBM.ßdummyTurker
12825ß0.0ßWe will add the numerical result in the next version.ßdummyTurker
12826ß0.0ßOn contrary, we found that after many iterations, there are some samples with extremely low probability, cumulative approach will make sure they still have chance to be sampled when the training is at the very end stage.ßdummyTurker
12827ß0.0ßIt's quite important for the final accuracy.ßdummyTurker
12828ß0.0ßCurrently T is fixed as one epoch.ßdummyTurker
12829ß0.0ßAdaptive T seems to be a good try.ßdummyTurker
12830ß0.0ßWe haven't tested it yet, but the RNN-RBM case is a hybrid network, we expect IS still works in deeper net.ßdummyTurker
12831ß0.0ßWe thank the reviewer2 for the constructive comments.ßdummyTurker
12832ß0.0ßIn our supplementary material, lines from 027 to 033 say that we have to clearly define how to measure the robustness of VQA models before we build a robust VQA model.ßdummyTurker
12833ß0.0ßIf we do not have valid and sophisticated robustness measure, we cannot build robust VQA models.ßdummyTurker
12834ß0.0ßThe purpose of this work is to propose the first framework and robustness measure for the robustness analysis of VQA models.ßdummyTurker
12835ß0.0ßBuilding a robust VQA model is one of our future works.ßdummyTurker
12836ß0.0ßThe Table 3 of our supplementary material shows some ranking results of semantically relevant basic questions based on our proposed LASSO ranking methods.ßdummyTurker
12837ß0.0ßIf basic questions are very similar, the answers to these basic questions should be the same or very similar to the answer to the main question.ßdummyTurker
12838ß0.0ßThe purpose of providing Figure 4(a)-2 and 4(b)-2 is to help clearly understand what is the difference of accuracy,i.e.ßdummyTurker
12839ß0.0ßAccdiff, in Equation-(2).ßdummyTurker
12840ß0.0ßBecause the results of the YNBQD and GBQD are very similar and space is limited, we mainly provide the GBQD ranking results and examples in the paper and supplementary material.ßdummyTurker
12841ß0.0ßHowever, we will provide more concrete examples for YNBQD in our final version.ßdummyTurker
12842ß0.0ßThe proposed Rscore is based on Equation-(3) with (a, b) = (0, 1).ßdummyTurker
12843ß0.0ßParameters t and m are given by users.ßdummyTurker
12844ß0.0ßAccdiff is based on lines from 526 to 534.ßdummyTurker
12845ß0.0ßLines from 539 to 549 explain how and why we design Rscore in that way.ßdummyTurker
12846ß0.0ßIn the final version, we will add a plot of the proposed Rscore function to demonstrate how Rscore value changes when we play with the parameters t and m.ßdummyTurker
12847ß0.0ßWe take the VQA dataset to do question sentences preprocessing (referring to line 378), and then we get our 186027 basic question candidates.ßdummyTurker
12848ß0.0ßWe thank the reviewer1 for the constructive comments.ßdummyTurker
12849ß0.0ßSome papers in our references list, from lines 876 to 882, have a plentiful discussion about why attention-based methods can work but without sophisticated experimental proof.ßdummyTurker
12850ß0.0ßHowever, our current experimental results validate the discussion in those papers.ßdummyTurker
12851ß0.0ßOur purpose of this work is to propose a framework to serve as a benchmark to help the VQA community in building more accurate and robust VQA models.ßdummyTurker
12852ß0.0ßAlso, because of the space limitation issue, that’s why we focus more on the framework part.ßdummyTurker
12853ß0.0ßFurthermore, lines from 390 to 400 explain that why we use skip-thought to encode our sentences and the Equation-(1) in our paper shows that we use the L_1 norm to enforce the sparsity.ßdummyTurker
12854ß0.0ßBased on the above, we know why LASSO with a good sentence encoder works well and it also implies that our LASSO with the skip-thought encoder method will find the most similar basic questions.ßdummyTurker
12855ß0.0ßLines from 755 to 770 also discuss why our method works and how to work better.ßdummyTurker
12856ß0.0ßThe authors of those n-gram based evaluation metrics claim that their metrics can capture the semantic information in some sense.ßdummyTurker
12857ß0.0ßThat is the most important reason why we take our LASSO with the skip-thought encoder method to compare to their metrics.ßdummyTurker
12858ß0.0ßOur experimental results show that the other metrics, except for our LASSO with the skip-thought encoder method, cannot capture the semantic meaning of sentences well.ßdummyTurker
12859ß0.0ßBecause other metrics cannot capture sentences information very well, that’s why our proposed method is valuable and useful.ßdummyTurker
12860ß0.0ßHowever, comparing our proposed method to the cosine distance plus the skip-thought encoder could be interesting.ßdummyTurker
12861ß0.0ßWe will try to do that based on our proposed framework in the final version.ßdummyTurker
12862ß0.0ßAccording to lines from 538 to 549 in our paper, we explain how we design this Rscore function to fit what we need.ßdummyTurker
12863ß0.0ßIf we remove the square root, then the generated Rscore function cannot fit our experimental results and practical experiences very well.ßdummyTurker
12864ß0.0ßThat’s why we need to design this Rscore function in that way.ßdummyTurker
12865ß0.0ßTraditional training method optimizes the model with one-hot labels, where the label indicates the start and end positions for the ground-truth answer.ßdummyTurker
12866ß0.0ßHowever, one-hot labels fail to represent the relative similarity and difference between multiple answer candidates.ßdummyTurker
12867ß0.0ßFor example, in Table 1, although “Santa Clara, California” is the ground truth, “Santa Clara” is also an acceptable answer.ßdummyTurker
12868ß0.0ßThe key intuition behind knowledge distillation is to provide the student with soft targets instead of one-hot labels, since the soft label contains much more information than the hard one.ßdummyTurker
12869ß0.0ßLearning soft distributions can thus prevent models from fitting too tightly to one-hot labels and contributing to a better generalization point.ßdummyTurker
12870ß0.0ßThe result in Table 2 supports our claim, showing that our approach has a better generalization ability, which in turn improves the model robustness.ßdummyTurker
12871ß0.0ßIn addition, we present the F1 score of our best single models on each question type to give a clear explanation (results on SQuAD dev set)ßdummyTurker
12872ß0.0ßwhat / how / who / when / which / where / why / be / otherßdummyTurker
12873ß0.0ßRMR: 85.32 / 85.16 / 91.09 / 93.05 / 85.71 / 83.25 / 75.65 / 84.61 / 78.54ßdummyTurker
12874ß0.0ßRMR+A2D: 86.15 / 85.94 / 90.94 / 94.48 / 87.40 / 85.55 / 77.71 / 83.88 / 82.19ßdummyTurker
12875ß0.0ßWe can see that some question types such as “which”, “where, “why” and “other” benefit more from our approach.ßdummyTurker
12876ß0.0ßThe common trait is that their original F1 scores are relatively lower than other types, meaning that our approach performs better on these more difficult questions.ßdummyTurker
12877ß0.0ßWe will add the above analysis into our paper.ßdummyTurker
12878ß0.0ßWe think that comparing the distilled model with the baseline trained without A2D is a reasonable way to demonstrate the effectiveness of our approach.ßdummyTurker
12879ß0.0ßWe also agree with your argument that the single student should be compared with the ensembled teacher.ßdummyTurker
12880ß0.0ßIn our experiment (please refer to Reply to question 2 for more details), the distilled student runs 12 times faster than the ensembled teacher with a slight decrease of 1 point on F1.ßdummyTurker
12881ß0.0ßHowever, we still insist our second claim based on the experimental result that the ensembled student outperforms its ensemble teacher with the same inference speed, achieving the best EM score among all models (EMßdummyTurker
12882ß0.0ßMonte-Carlo sampling strategy is the first approximation method that we tried to use.ßdummyTurker
12883ß0.0ßHowever, it results in little performance improvement (EMßdummyTurker
12884ß0.0ßWe will truncate the x/y axis to focus on the later training stage, emphasizing the performance gaps between different models.ßdummyTurker
12885ß0.0ßWe agree that including a speed experiment can help the community better understand our work.ßdummyTurker
12886ß0.0ßWe conduct the experiment on a NVIDIA Tesla P100 GPU and report the averaged time of three runs, measured as minutes required to process one epoch.ßdummyTurker
12887ß0.0ßHere is the result for RMR (trainingßdummyTurker
12888ß0.0ßHowever, the concatenation of word category to produce the relation is not a good way.ßdummyTurker
12889ß0.0ßWe have 17 word categories.ßdummyTurker
12890ß0.0ßAccordingly, the word category sub-task will produce 17*17 kinds of possible relations if we connect two predicted categories of two entities.ßdummyTurker
12891ß0.0ßFurther, please, consider the following aspects:ßdummyTurker
12892ß0.0ß(i)  For example, if entity_1 is predicted as Component and entity_2 is predicted as ‘Destination’, there does not exist a relation named ‘Component-Destination’ in the predefined relation classes.ßdummyTurker
12893ß0.0ß(ii) Concatenation of word category will not tell us the  direction of their relationship.ßdummyTurker
12894ß0.0ßAlthough two entities are labeled as ‘e1’ and ‘e2’, these labels only mean that they are entities, and it does not indicate e1 is the head and e2 is the tail, so we still do not know their order.ßdummyTurker
12895ß0.0ßSo either of the relation and direction cannot be predicted only based on the sub-task of word category classification.ßdummyTurker
12896ß0.0ßWe will modify this in the final version of our paper.ßdummyTurker
12897ß0.0ßWe have also implement a system without attention mechanisms and it only follows the the MTL architecture and achieved 77.90% on the test set.ßdummyTurker
12898ß0.0ßWe will include the results in the final version of the paper.ßdummyTurker
12899ß0.0ßResponse to the 2nd ReviewerßdummyTurker
12900ß0.0ßTuning neural nets are tricky, it depends on machines, operating systems,ßdummyTurker
12901ß0.0ßversions of libraries.ßdummyTurker
12902ß0.0ßMoreover, we investigated different factors to fine tune the system, such as hidden neuron size, dropout, system performance is very sensitive to the word embeddings, embedding dimension etc.ßdummyTurker
12903ß0.0ßThe way to process the unknown words and entities in terms of the embedding significantly affects the results as well.ßdummyTurker
12904ß0.0ßWe follow the common way to connect the two-word entities as one word using the underscore symbol.ßdummyTurker
12905ß0.0ßThis preprocessing introducesßdummyTurker
12906ß0.0ßmany unknown entities and we can not have meaningful word embeddings for theseßdummyTurker
12907ß0.0ßwords from the pre-trained word embedding model and we plan to change it in future.ßdummyTurker
12908ß0.0ßIt is worth to mention that the difference between our best performing system and the top-performing system in SemEval-2010 task 8 is only 1.57% in terms of macro-F1.ßdummyTurker
12909ß0.0ßWe are still working on tuning the systems now, and we believe  that we will reach and outperform the published results.ßdummyTurker
12910ß0.0ßCurrently we are working on improving system performance and will report the significance test in the final version of the paper.ßdummyTurker
12911ß0.0ßWe will present our results on another data set.ßdummyTurker
12912ß0.0ßAccording to our hypothesis an individual attention to a specific task is designed to focus only on that task and due to the nature of MTL architecture (the previous task outputs are getting concatenated before computing the attention weights) the last task relation-direction prediction is presented with more contextual information.ßdummyTurker
12913ß0.0ßOn the other hand shared attention is created based on the outputs of recurrent layer and it is being appended to the different output layers.ßdummyTurker
12914ß0.0ßIt is possible that for the first task the shared attention is doing good but for the other tasks the information from the shared attention is insufficient.ßdummyTurker
12915ß0.0ßDue to the page limitation we did not presented a detail analysis of each system, which we plan to present in the final version of the paper.ßdummyTurker
12916ß0.0ßThe goal of our method is not to emphasise on relation extraction architecture or MTL framework.ßdummyTurker
12917ß0.0ßThe main motivation behind this work is to use the existing methods to define the relation extraction problem in a slightly different way - which can be looked as - ‘Learning Relation By Construction of a Triple through MTL’.ßdummyTurker
12918ß0.0ßThe MTL framework is designed to break the task into three categories (noun-category-prediction, relation prediction and direction prediction).ßdummyTurker
12919ß0.0ßOne cannot tell the relation category unless word-categories are known, similarly the word-category and the relation information will make it easier to predict the direction.ßdummyTurker
12920ß0.0ßThe system learns to identify a relation by performing these three task one by one.ßdummyTurker
12921ß0.0ßFurther the predictions from each task flows to the next level of tasks.ßdummyTurker
12922ß0.0ßWe tried to replicate the human understanding of relation into our framework.ßdummyTurker
12923ß0.0ßFor example - `John is going to USA`, in this sentence the relationship between ‘John’ and ‘USA’ can be told if one can understand ‘John’ = ‘Entity’ and ‘USA’ = ‘Destination’ i.e.ßdummyTurker
12924ß0.0ßthe word-categories are known.ßdummyTurker
12925ß0.0ßSo identifying word-category is the first task.ßdummyTurker
12926ß0.0ßOnce the word-categories are known the relationship can be identified as ‘Entity-Destination’, further if one tries to tell direction of this relation, he/she needs to know the word categories (Entity/Destination), the relationship (Entity-Destination) and the sentence/context.ßdummyTurker
12927ß0.0ßOnly comparing the head entity with first half of relation will not guarantee that it is a forward relationship, one has to understand that ‘Entity_1  is moving towards Entity_2’, that’s why presenting the context along with  the previous task’s prediction is necessary for predicting the direction.ßdummyTurker
12928ß0.0ßProblems where name entity and their relation needs to be predicted at the same time, we believe this framework can be useful.ßdummyTurker
12929ß0.0ßWe also plan to present our results on such data sets in the final version of the paper.ßdummyTurker
12930ß0.0ßWe have also implement a system without attention mechanisms and it only follows the the MTL architecture and achieved 77.90% on the test set.ßdummyTurker
12931ß0.0ßIt is worth to mention that the difference between our best performing system and the top-performing system in SemEval-2010 task 8 is only 1.57% in terms of macro-F1.ßdummyTurker
12932ß0.0ßTuning neural nets are tricky, it depends on machines, operating systems,ßdummyTurker
12933ß0.0ßversions of libraries, hidden neuron size, dropout.ßdummyTurker
12934ß0.0ßSystem performance is very sensitive to the word embeddings, embedding dimension etc.ßdummyTurker
12935ß0.0ßThe way to process theßdummyTurker
12936ß0.0ßunknown words and entities in terms of the embedding significantly affects the results as well.ßdummyTurker
12937ß0.0ßWe are still working on tuning the systems now, and we believe  that we will reach and outperform the published results.ßdummyTurker
12938ß0.0ßThanks for your valuable comments and suggestions.ßdummyTurker
12939ß0.0ßFollowings are response to your specific arguments:ßdummyTurker
12940ß0.0ßThanks for your bad case analysis suggestion.ßdummyTurker
12941ß0.0ßWe will introduce one extra section for this in the next version.ßdummyTurker
12942ß0.0ßThanks for your suggestion of illustrating more training details.ßdummyTurker
12943ß0.0ßDue to the page limitation, we reported them in the supplementary material.ßdummyTurker
12944ß0.0ßWe will try to include some vital training details in next version of our paper.ßdummyTurker
12945ß0.0ßAlso we will make our source code openly available.ßdummyTurker
12946ß0.0ßThanks for your valuable comments and arguments.ßdummyTurker
12947ß0.0ßFollowings are response to your specific arguments and questions:ßdummyTurker
12948ß0.0ßOur method can also be applied to English event detection.ßdummyTurker
12949ß0.0ßHowever, English is a language with natural word delimiters, and more than 95% event triggers in English is exactly one token, i.e., word-trigger mismatch is not severe in English.ßdummyTurker
12950ß0.0ßThanks for interpretation suggestion.ßdummyTurker
12951ß0.0ßDue to the page limitation, we only annotated Chinese word’s interpretation when it first appears in the paper.ßdummyTurker
12952ß0.0ßWe will annotate all appearances of Chinese word with its English interpretation in the next version.ßdummyTurker
12953ß0.0ßIn Figure 2, the labels for Nugget Generator and Type Classifiers are different.ßdummyTurker
12954ß0.0ßLabels for Nugget Generator are potential event nugget spans and labels for Type Classifier are specific event types.ßdummyTurker
12955ß0.0ßThanks for pointing out this and we will make it more clearly in the revision.ßdummyTurker
12956ß0.0ßThe hybrid representations in our model are jointly trained with nugget generator and type classifiers.ßdummyTurker
12957ß0.0ßAnd the reasons we use a two-step classification rather than jointly generating trigger nugget and classifying its type mainly are: 1) Making jointly prediction will significantly increase the label size of nugget generator(from 7 to (6*type_size) + 1), which may increase the overlapped conflict mentioned in Section 3.3 and thus may undermine the system performance; 2) separating the nugget generation part and type classification part can make the type classifier more focus on classifying between different event types, which reduce the label imbalance problem and result in a better type classifier.ßdummyTurker
12958ß0.0ßFor the question about why we use 7 possible candidate nuggets, as mentioned in line 388, more than 98.5% triggers in ACE and RichERE contain no more than three characters.ßdummyTurker
12959ß0.0ßAnd there are 6 possible nuggets that contain one specific character whose length does not exceeds 3.ßdummyTurker
12960ß0.0ßSo the label size of trigger nugget generator is 7 including one NIL label.ßdummyTurker
12961ß0.0ßHowever, our method can use any different length for trigger nuggets, too.ßdummyTurker
12962ß0.0ßFor the question about why redundant prediction works, the reason behind this is that the main challenge in event detection is making correct prediction between trigger/non-trigger (i.e., NIL/trigger conflict in Section 3.3).ßdummyTurker
12963ß0.0ßThe event type classification errors and nugget shape classification errors (i.e., overlapped conflict in Section 3.3) are rare.ßdummyTurker
12964ß0.0ßThis characteristic makes that classifying any one character in trigger nugget correctly (and classifying other characters into NIL) can result in correct detection result.ßdummyTurker
12965ß0.0ßAs mentioned in Section 3.3 and 4.4, making use of this characteristic makes NPNs significantly outperform traditional character-based methods.ßdummyTurker
12966ß0.0ßargument 1: We can see where there was confusion here: it is true that the labels are based solely on images, though the image position is clearly observable to the Turkers.ßdummyTurker
12967ß0.0ßAnnotating data based on *actual* tendon activations is challenging because tendons are not observable to anyone except the one experiencing the tendon activations (hence the need for mirror neurons).ßdummyTurker
12968ß0.0ßWe are only simulating this effect.ßdummyTurker
12969ß0.0ßWill clarify.ßdummyTurker
12970ß0.0ßargument 2: That our data is small is an obvious weakness, though arguably not fatal.ßdummyTurker
12971ß0.0ßThe beauty of our model is that it is able to work well with a small dataset, even if the data we collected is very complex.ßdummyTurker
12972ß0.0ßWith smaller data sets, more problems become accessible since the amount of data stops being a gatekeeper to worthwhile research.ßdummyTurker
12973ß0.0ßWe will clarify this, and provide references to other recent work that uses smaller datasets as preliminary work.ßdummyTurker
12974ß0.0ßargument 3: The purpose of using WAC for this paper was *because* of simplicity and interpretability.ßdummyTurker
12975ß0.0ßThough the WAC model has been used in generative tasks before, we opt for an established understanding task, as many prior and current research continues to do, similar common reference resolution tasks.ßdummyTurker
12976ß0.0ßargument 4: As stated by another reviewer, being “[One of the] first to do tactile/proprioceptic grounding” needs some, if not expected exploratory results.ßdummyTurker
12977ß0.0ßEven if the results are “mostly as expected”, they still need to be shown, not just assumed, for establishing scientific bases.ßdummyTurker
12978ß0.0ßThough the results are clearly not earth-shattering, they are nevertheless meaningful.ßdummyTurker
12979ß0.0ßOur research shows the human intuition of “how something feels” can be represented in a machine learning model and used to gain a deeper semantic understanding of hand poses.ßdummyTurker
12980ß0.0ßargument 1:  Research facilities, such as ours, which have limited resources could still make scientific contributions using less expensive amounts of data.ßdummyTurker
12981ß0.0ßWe agree that more data would be preferred.ßdummyTurker
12982ß0.0ßThe Turker instructions allowed for maximum freedom to write what they wanted; the 1-100 character constraint was to keep the from gaming the system by typing meaningless things, or nothing at all.ßdummyTurker
12983ß0.0ßWill clarify.ßdummyTurker
12984ß0.0ßargument 2: Really, really good point.ßdummyTurker
12985ß0.0ßWe’re learning more about the WAC model ourselves, and we will use the extra space in the final version to explain it further.ßdummyTurker
12986ß0.0ßWe can see why not just calling WAC logistic regression would be quite annoying, but it’s the application of the model that makes it not just a classifier in that it is way of modeling formal intensional semantics where the classifier determines class membership.ßdummyTurker
12987ß0.0ßThis is very different from embeddings; high-dimensional vectors don’t determine class membership in a formal semantic intensional (intension with an ‘s’) sense, which is what WAC does.ßdummyTurker
12988ß0.0ßRe mirror neurons: there’s a subtle, yet crucial difference in that the tendon features are derived from visual features, not just directly inputted.ßdummyTurker
12989ß0.0ßThe concept here is simulating something that a mirror neuron would do, we just happened to use known stats techniques (which is common in NLP research).ßdummyTurker
12990ß0.0ßWe will tone the claims down (see Reviewer 3, weakness 4 below).ßdummyTurker
12991ß0.0ßargument 3:In the Analysis section we use the term “probability” loosely, and we will instead use the term to “fit” as it is a better description of what we do.ßdummyTurker
12992ß0.0ßThough unconventional, we found the top layer to provide higher accuracies than the pre-softmax layer in our held-out set.ßdummyTurker
12993ß0.0ßWe will follow conventions of other work more closely.ßdummyTurker
12994ß0.0ßWe used regularization, and we will be more careful wrt the references.ßdummyTurker
12995ß0.0ßThank you for pointing that out very clearly.ßdummyTurker
12996ß0.0ßOur approach to setting up and explaining the experiments doesn’t follow current neural network research conventions, partly because our model is equivalent to a maximum entropy (i.e., direct input-to-output mapping) classifier.ßdummyTurker
12997ß0.0ßGiven the extra page, we will also clarify these concerns.ßdummyTurker
12998ß0.0ßargument 4:  In response to your general remark: we can see how our discussion and conclusions would lead a reader to conclude that; rather, this paper is an exploration in an area that is, as you say, worth exploring.ßdummyTurker
12999ß0.0ßWe will clarify this and tone down our claims.ßdummyTurker
13000ß0.0ßOne application example is soft robotics where tendon activations are actually used, or multimodal virtual agents.ßdummyTurker
13001ß0.0ßargument 5: We included several references to prior work in other modalities in the second paragraph, but we will work on that.ßdummyTurker
13002ß0.0ßAs for vision, we will cite survey papers.ßdummyTurker
13003ß0.0ßargument 6: That’s fair.ßdummyTurker
13004ß0.0ßWill fix.ßdummyTurker
13005ß0.0ßWe agree that VisionSpeechCNN has the same structure as that of Kamper et al.ßdummyTurker
13006ß0.0ß(the main difference is that our visual tagger is trained on significantly more data).ßdummyTurker
13007ß0.0ßHowever, as the reviewer mentioned in their comments on the paper’s contributions, our main contribution is showing that this type of model can be used for semantic analysis, and to give a complete analysis of the model.ßdummyTurker
13008ß0.0ßWe also stated explicitly that a new model is not our goal, but rather applying an existing model to a new task and to perform a new analysis.ßdummyTurker
13009ß0.0ßGiven transcribed data with which to train a supervised keyword spotter, it is true that these can be extended to semantic retrieval (one of our baselines does this).ßdummyTurker
13010ß0.0ßHowever, our focus is mainly on settings where transcribed data is not available; rather we only have images as a supervisory signal.ßdummyTurker
13011ß0.0ßWe also show that our model actually outperforms the speech recognition+TextParagram fully supervised approach on some metrics (Model 7 in Table 3 is a best-case scenario of a keyword spotter + semantic expansion).ßdummyTurker
13012ß0.0ßWe agree with the reviewer that applying our approach to a truly low-resource language is a very interesting area of future work.ßdummyTurker
13013ß0.0ßRegarding unwritten languages, clearly we would not be able to use textual tags in those languages; one option would be to use tags in a written language (e.g.ßdummyTurker
13014ß0.0ßthe one used by the image tagger).ßdummyTurker
13015ß0.0ßWe agree that the new set of annotations is a core contribution, and we commit to releasing the data upon acceptance.ßdummyTurker
13016ß0.0ßIt is difficult to argue that our model would do better than fully supervised models in all settings; indeed we show that some of the supervised models perform better than our approach on some of the metrics.ßdummyTurker
13017ß0.0ßHowever, we believe we do make a strong case that our model gives good performance in settings where only images are available with untranscribed speech.ßdummyTurker
13018ß0.0ßRegarding the task, we have argued that it is a natural one for searching for content in spoken content.ßdummyTurker
13019ß0.0ßOf course any new task might result in different performance depending on details of the data collection and evaluation.ßdummyTurker
13020ß0.0ßWe have made no special effort to tailor the keywords/task to be advantageous to any given model, but we are open to suggestions if you feel there could be a more natural data collection and/or evaluation setup.ßdummyTurker
13021ß0.0ßWe also suspected that filterbank features would do well on this task, and used these in preliminary experiments.ßdummyTurker
13022ß0.0ßWe found a small benefit of using MFCCs instead, presumably due to the limited size of the training set.ßdummyTurker
13023ß0.0ßIf accepted, we could include the filterbank results to show the difference.ßdummyTurker
13024ß0.0ßTo our knowledge, there are no similar visually grounded models that can make labelled (semantic) keyword predictions.ßdummyTurker
13025ß0.0ßWe thank the reviewer for pointing out these related studies, and we will incorporated these comments into the updated submission (if the paper is accepted).ßdummyTurker
13026ß0.0ßRegarding CNNs applied to MFCCsßdummyTurker
13027ß0.0ßYes, the conclusion is rather than obvious.ßdummyTurker
13028ß0.0ßHowever, referring to line 74-83 in our article, people have been taking the hypothesis for granted without proving it, thus our work fills in this gap.ßdummyTurker
13029ß0.0ßThe methodology of our experiment is explained in subsection 3.1, which is to shuffle the subsets of translation dataset to simulate the distribution of dialogue dataset.ßdummyTurker
13030ß0.0ßI agree with your opinion that our article only gives the reason with the absence of potential solutions.ßdummyTurker
13031ß0.0ßHowever, we think our experiment can be helpful when explaining why more context and training objective replacement can promote the dialogue system’s performance.ßdummyTurker
13032ß0.0ßAs you said in review opinions, a translation of a sentence is under much stricter semantic control than a response in a dialogue.ßdummyTurker
13033ß0.0ßIn our article, this is shown by the characteristic of word alignment, which we believe is the most obvious expression of the difference you mentioned.ßdummyTurker
13034ß0.0ßIn other words, we have shown that words in a dialogue are less aligned.ßdummyTurker
13035ß0.0ßReferring to line 74-83 in the article, people have been taking the reason of this difference for granted without proving it, thus our work fills in this gap.ßdummyTurker
13036ß0.0ßBy less semantic control we did not mean there is zero semantic control in a dialogue.ßdummyTurker
13037ß0.0ßWe shuffled subsets of the translation dataset to simulate the distribution of dialogues’ dataset.ßdummyTurker
13038ß0.0ßTherefore, when the subsets are not 100% shuffled, there is still some semantic control over the dataset which makes it resembles dialogue dataset distribution.ßdummyTurker
13039ß0.0ßIn a word, our motivation was to show it is the non-alignment phenomenon that causes the seq2seq model to perform badly.ßdummyTurker
13040ß0.0ßHence, with less semantic control and word alignment, dialogue systems also make seq2seq model perform poorly.ßdummyTurker
13041ß0.0ßI agree.ßdummyTurker
13042ß0.0ßHowever, we just tried to explain the poor performance of seq2seq model from dataset distribution’s point of view, and a model’s output is also a measurement of the model’s performance.ßdummyTurker
13043ß0.0ßThe primary objective of our paper was to present a significant work in progress along with an application nugget.ßdummyTurker
13044ß0.0ßConsidering the value of the dataset and the novelty of the problem, we feel that this would benefit the research community to explore a novel direction that has not been considered previously.ßdummyTurker
13045ß0.0ßAlthough the models used are general to all seq2seq generation problems, the heuristics we used to select the source sequence are specific to generating the news-blog title for a research paper (take for example, the heuristic based on readability - it was motivated by the fact that the blog titles have a higher readability, hence the network has to focus towards better readable information in the research article in order to generate the title for the news-blog post).ßdummyTurker
13046ß0.0ßSince the abstract generally consists of 5-6 sentences, a simple method like TF-IDF was sufficient to select the most representative sentence from this small set.ßdummyTurker
13047ß0.0ßGiven that we have to select from a limited number of sentences, any complex model will most likely select similar sentences to the one selected by using the TF-IDF measure.ßdummyTurker
13048ß0.0ßPlease, note that our primary contribution here is towards the principles of heuristics than the actual heuristics themselves (for example, one can replace Flesch reading ease with some other metric for assessing readability.ßdummyTurker
13049ß0.0ßSimilarly, TF-IDF can also be replaced with a different metric for assessing representativeness).ßdummyTurker
13050ß0.0ßThe increase in readability corresponds to a transfer in style from research article to the blog.ßdummyTurker
13051ß0.0ßThe blog articles had Flesch reading ease around 30-35, while the research article's Flesch reading ease was between 15-20.ßdummyTurker
13052ß0.0ßOur generated samples had a reading ease above 30 along the lines of the reading ease of the blog.ßdummyTurker
13053ß0.0ßIf accepted, we’ll explicitly add this comparison in the final draft.ßdummyTurker
13054ß0.0ßFrom the model’s perspective, it is intuitive that style is implicitly learned as we are training on the source and target sequence pairs, where the source and target are in different styles.ßdummyTurker
13055ß0.0ßAnother observation here is that it is possible to produce reasonable titles even though the style is not explicitly modeled in the architecture.ßdummyTurker
13056ß0.0ßModeling style transfer explicitly may improve generation, but that is a subject of future work.ßdummyTurker
13057ß0.0ßThis argument is similar to the first argument.ßdummyTurker
13058ß0.0ßTo the best of our knowledge, there is no existing similar work.ßdummyTurker
13059ß0.0ßAs mentioned in the paper, existing work in style transfer is done in non-parallel data setting and a key assumption across all these non-parallel style transfer works is a significant overlap between the vocabulary of the source and target style.ßdummyTurker
13060ß0.0ßHowever, in the context of science journalism, the overlap in vocabulary is not significant and hence these methods cannot be directly extended to our problem.ßdummyTurker
13061ß0.0ßWe welcome feedback about how to make these results less technical/more accessible.ßdummyTurker
13062ß0.0ßWe have attempted to include intuitive explanations in addition to technical ones where space permits, and if accepted we will use the extra space to add more of these where possible.ßdummyTurker
13063ß0.0ßThank you for suggesting these clarifications!ßdummyTurker
13064ß0.0ßSome of our previous experiments have investigated how various properties affect the grammar size increase.ßdummyTurker
13065ß0.0ßWe find that the worst case size increase occurs when 50% of the original grammar is already in GNF; larger or smaller proportions of GNF productions lead to smaller size increases.ßdummyTurker
13066ß0.0ßThis is because the size increase depends on both the number of prefix-lexicalized trees in the intermediate grammars (which grows as the proportion of GNF rules increases) and the number of productions which need to be lexicalized (which shrinks as the proportion of GNF rules increases).ßdummyTurker
13067ß0.0ßAt 50%, both of these factors contribute appreciably to the grammar size, in much the same way that f(x) = x(1-x) has a maximum at 0.5.ßdummyTurker
13068ß0.0ßThe grammar you mention has a proportion of GNF rules close to 50%, which is why it grows so dramatically.ßdummyTurker
13069ß0.0ßIf space permits, we can include these results in the camera-ready version, so that they can be used to better understand how various kinds of grammar will behave under our transformation.ßdummyTurker
13070ß0.0ßIt is true that TAG parsing makes these grammars more complicated to use in practice, but this does not necessarily imply greater asymptotic complexity.ßdummyTurker
13071ß0.0ßFor example, our supplementary material outlines how Siahbani et al.ßdummyTurker
13072ß0.0ß's decoding algorithm can be adapted to use STAG without increasing the overall time complexity from O(n^2 b).ßdummyTurker
13073ß0.0ßThat said, we appreciate the importance of empirical evaluation in addition to asymptotic bounds, and we have ongoing experiments using our grammars for a word-alignment task.ßdummyTurker
13074ß0.0ßSome of our results have been obtained since the submission deadline, however, so it would be inappropriate to include them in this work.ßdummyTurker
13075ß0.0ßFurthermore, as you noted the focus of this work is primarily theoretical, and we decided not to include even preliminary empirical results under the belief that they would distract from the focus of the paper and hamper our ability to present the theoretical contributions completely and correctly.ßdummyTurker
13076ß0.0ßIt is true.ßdummyTurker
13077ß0.0ßWe agree that another evaluation (intrinsic or extrinsic) would strengthen the paper, but due to the page limit we left this for the future work.ßdummyTurker
13078ß0.0ßAs you said, we extend RT to a bidirectional setting.ßdummyTurker
13079ß0.0ßWe also add the orthogonality constraint.ßdummyTurker
13080ß0.0ßBoth the bidirectionality and the orthogonality constraints are achieved by a simple trick – to force matrix T to satisfy TT^\top = 1 (identity matrix) during the gradient descent.ßdummyTurker
13081ß0.0ßMoreover, the bidirectional assumption is novel.ßdummyTurker
13082ß0.0ßThere is a strong correlation between monolingual performance of semantic spaces and their bilingual counterparts.ßdummyTurker
13083ß0.0ßThe methods presented in our paper are designed to be independent of dimension or the algorithms for building semantic spaces.ßdummyTurker
13084ß0.0ßTo provide additional comparisons will lead to breaking the page limit, so we left such experiments for future work.ßdummyTurker
13085ß0.0ßWe already tried some preliminary experiments and there is a strong correlation between the results from STS and the document classification.ßdummyTurker
13086ß0.0ßHowever, only the linear combination of word vectors can be used for document classification, because other two methods (OM and PA) need a pair of documents as an input.ßdummyTurker
13087ß0.0ßWe have not tested such methods yet.ßdummyTurker
13088ß0.0ßWe left additional experiment for the future work.ßdummyTurker
13089ß0.0ßIn any case, thank you very much for your review.ßdummyTurker
13090ß0.0ßThe Pearson correlation was the main and only evaluation metric for SemEval since 2012.ßdummyTurker
13091ß0.0ßIn the current version of the paper, we compare our methods only with the best performing systems.ßdummyTurker
13092ß0.0ßIn camera ready version we can add another numbers (top three systems, mean over all systems, etc.ßdummyTurker
13093ß0.0ß).ßdummyTurker
13094ß0.0ßYes, you are right, by using our approach the supervised systems can be probably further improved.ßdummyTurker
13095ß0.0ßBut we do not see this as a weakness of our approach.ßdummyTurker
13096ß0.0ßYes, we agree.ßdummyTurker
13097ß0.0ßWe will try to put some additional experiments on an extra page of camera ready version.ßdummyTurker
13098ß0.0ßTo the best of our knowledge, there is only one STS system which is not based directly on translating full sentences, but uses Least Squares Transformation (Glavas et al.ßdummyTurker
13099ß0.0ß2017).ßdummyTurker
13100ß0.0ßWe compare our results with this work in Table 3.ßdummyTurker
13101ß0.0ßIn any case, thank you very much for your review.ßdummyTurker
13102ß0.0ßYes, you are right, but to translate words is not the same as to translate full sentences.ßdummyTurker
13103ß0.0ßThis can be done very easily from reasonably large parallel corpus.ßdummyTurker
13104ß0.0ßMoreover, there are many offline bilingual dictionaries for many languages.ßdummyTurker
13105ß0.0ßReply to question 1 (How exactly we build bilingual dictionaries):ßdummyTurker
13106ß0.0ßWe take the most frequent words from Wikipedia corpus for the particular language and translate them via Google Translate.ßdummyTurker
13107ß0.0ßWe consider only those bilingual pairs consisting of unigrams.ßdummyTurker
13108ß0.0ßWe use only the best translation given by Google Translate API.ßdummyTurker
13109ß0.0ßWe do not care about polysemy etc.ßdummyTurker
13110ß0.0ßThere are some related works explaining how exactly to select the bilingual pairs for the transformation, but since we wanted to put the lowest possible supervision, we just used the raw output from the translator.ßdummyTurker
13111ß0.0ßWe will add more detailed description in the camera ready version of the paper.ßdummyTurker
13112ß0.0ßReply to question 2 (Will the code be available?ßdummyTurker
13113ß0.0ß):ßdummyTurker
13114ß0.0ßYes, sure.ßdummyTurker
13115ß0.0ßWe have Java implementation of all presented algorithms.ßdummyTurker
13116ß0.0ßIn any case, thank you very much for your review.ßdummyTurker
13117ß0.0ßReply to weakness argument 1 (improvements by pinyin and wubi separately & why wubi works):ßdummyTurker
13118ß0.0ß1) Yes, thanks for pointing it out.ßdummyTurker
13119ß0.0ßWe conducted these important experiments which are helpful for understanding the effectiveness of pinyin and wubi.ßdummyTurker
13120ß0.0ßTake CityU for example, feeding extra pinyin and wubi embeddings separately lead to 0.7% and 0.9% improvement respectively.ßdummyTurker
13121ß0.0ßThe results and related analysis will be reported in the final version.ßdummyTurker
13122ß0.0ß2) No, wubi helps over 90% of the characters (seeing reply 2).ßdummyTurker
13123ß0.0ßDepending on the semantic meanings, different Chinese characters could have some common components in the structure of the drawing.ßdummyTurker
13124ß0.0ßMore specifically, firstly, radical (偏旁) effect [Sun et al.ßdummyTurker
13125ß0.0ß2014] is completed captured by wubi codes.ßdummyTurker
13126ß0.0ß(e.g.ßdummyTurker
13127ß0.0ßFig.3 give some examples.ßdummyTurker
13128ß0.0ß'提'(Carry), '打'(Hit) and '抬'(Lift) are both verbs whose meaning is using hand to do something (The wubi code ‘R’ has successfully captured it).ßdummyTurker
13129ß0.0ßSecondly, more implicit meaning relationship is depicted by wubi codes.ßdummyTurker
13130ß0.0ß(e.g.ßdummyTurker
13131ß0.0ßAs shown in Fig.1 Input Layer, code 'I' implies water(水), '江'(river), '满'(full of water), '藻'(algae) are all related to water.ßdummyTurker
13132ß0.0ßWe will illustrate it more clearly in the final version.ßdummyTurker
13133ß0.0ßReply to weakness argument 3 (1% lower than Wang and Xu (2017) on MSR):ßdummyTurker
13134ß0.0ßIt has been agreed by reviewer 1 and 3 that we have reached the competitive results without leveraging any extra dictionary!ßdummyTurker
13135ß0.0ßWe could not agree with your opinion on our performance.ßdummyTurker
13136ß0.0ß1) We achieved the best performance on two corpora (AS and CityU).ßdummyTurker
13137ß0.0ß2) We do not use any extra dictionary at all but Wang and Xu does.ßdummyTurker
13138ß0.0ßIt is hard to reproduce the result without using their dictionary (like Chen 2015.b).ßdummyTurker
13139ß0.0ßThanks, we will address them carefully in the final version.ßdummyTurker
13140ß0.0ßThanks for your suggestion.ßdummyTurker
13141ß0.0ßThe relationship you mentioned will be reported in the final version for a better understanding of the systems.ßdummyTurker
13142ß0.0ßAt first, thank you for having interests in our idea.ßdummyTurker
13143ß0.0ßActually, two extra embeddings (pinyin and wubi) are the main contributions in this paper.ßdummyTurker
13144ß0.0ßCompared with this novel, elegant and original idea, even the novel models, solid experiments, comprehensive analysis and the state-of-the-art performance (1st on CityU and AS) are not so dominant!ßdummyTurker
13145ß0.0ßPrevious papers neglected the exploration on the representations of languages.ßdummyTurker
13146ß0.0ßHowever, the representations are of great significance (e.g.ßdummyTurker
13147ß0.0ßpoint clouds in 3D vision and most of languages).ßdummyTurker
13148ß0.0ßOur paper opens a new door to leverage characters.ßdummyTurker
13149ß0.0ßMoreover, this idea could be applied to other NLP tasks and transplanted into other languages.ßdummyTurker
13150ß0.0ßIt is potentially a very influential idea.ßdummyTurker
13151ß0.0ßWe do believe it is a big contribution to the community.ßdummyTurker
13152ß0.0ß(The models and performance are also inspiring.)ßdummyTurker
13153ß0.0ßYes, three embeddings are fed one by one (Fig.5 and line 474).ßdummyTurker
13154ß0.0ßThanks, we will show it more explicitly in the final version.ßdummyTurker
13155ß0.0ßThanks for the suggestion.ßdummyTurker
13156ß0.0ßOur experiments showed that larger embeddings and deeper bilstm result in worse performance.ßdummyTurker
13157ß0.0ßThe results and analysis will be reported in the final version.ßdummyTurker
13158ß0.0ß80 characters.ßdummyTurker
13159ß0.0ßThanks, we will address them carefully in the final version.ßdummyTurker
13160ß0.0ßApologies for not elaborating enough on the construction of the dataset.ßdummyTurker
13161ß0.0ß"The intention was to refer to a paper that reports the methodology in detail ""(Aut)""."ßdummyTurker
13162ß0.0ßUnfortunately, this reference got corrupted.ßdummyTurker
13163ß0.0ßTable 2 shows the number of errors per sentence.ßdummyTurker
13164ß0.0ßSentences in the artificial corpus are longer and contain more collocations (at least, our collocation identification metric identifies more collocations per sentence).ßdummyTurker
13165ß0.0ßStill, the collocation error - collocation ratio and error distribution are maintained.ßdummyTurker
13166ß0.0ßThe synthetic dataset is constructed as follows: First, collocations are identified via an association measure.ßdummyTurker
13167ß0.0ßThen, they are classified according to the error types they may contain, so that a list of collocations is created for each error type.ßdummyTurker
13168ß0.0ßThe insertion is done in iterations.ßdummyTurker
13169ß0.0ßIn each iteration, an error type is probabilistically chosen; then, a collocation from the list is selected and an error of the type in question is produced and inserted into the sentence.ßdummyTurker
13170ß0.0ßIn order to preserve the CEDEL2 error distribution, the creation of the corpus ends when the number of candidate collocations is equal to zero for any error type.ßdummyTurker
13171ß0.0ßThe first LSTM encodes the words that have been processed and marked, the second one encodes the words that are to be processed, and the third one encodes the list of actions performed.ßdummyTurker
13172ß0.0ßThis follows the approach of Ballesteros and Wanner (2016), which is based on the Stack-LSTM framework of Dyer et al.ßdummyTurker
13173ß0.0ß(2015).ßdummyTurker
13174ß0.0ßThank you for the encouragement.ßdummyTurker
13175ß0.0ßIndeed, our plan is to continue to work on this topic.ßdummyTurker
13176ß0.0ßWe found pretrained vectors to benefit the results all over the board.ßdummyTurker
13177ß0.0ßThey helped a lot with OOV words given the limited size of our training set.ßdummyTurker
13178ß0.0ßWe will add more details about this.ßdummyTurker
13179ß0.0ßWe could not discuss the results in more detail due to space constraints.ßdummyTurker
13180ß0.0ßIt is true that a baseline would be desirable and helpful.ßdummyTurker
13181ß0.0ßWe will provide a baseline and further discussion of the results in the final version of the paper.ßdummyTurker
13182ß0.0ßSorry about not being clear on this aspect.ßdummyTurker
13183ß0.0ß"""Non-existing base/collocate"" means that the base/collocate lexeme does not exist in L2."ßdummyTurker
13184ß0.0ßSubstitution determiner errors are not included because according to Alonso Ramos et al.ßdummyTurker
13185ß0.0ß(2010) indefinite/definite determiner confusion is not a collocation but a grammatical determination error.ßdummyTurker
13186ß0.0ßThe annotation scheme was applied to annotate the whole CEDEL2.ßdummyTurker
13187ß0.0ßThis is reported in the paper, whose reference (Aut) got corrupted.ßdummyTurker
13188ß0.0ßTo generate lexical errors, we use different subcategories from the explanatory dimension of the typology by Alonso Ramos et al.ßdummyTurker
13189ß0.0ß(2010), including L1-transfer and semantic similarity, precisely as suggested by the Reviewer.ßdummyTurker
13190ß0.0ßThese issues are discussed in the paper (Aut).ßdummyTurker
13191ß0.0ßWe are sorry that the reference got corrupted.ßdummyTurker
13192ß0.0ßIn the final version, we will make the paper self-explaining.ßdummyTurker
13193ß0.0ßTable 2 shows that the number of the errors in the artificial corpus per sentence is higher than in CEDEL2.ßdummyTurker
13194ß0.0ßThis is because the sentences in the artificial corpus are longer and contain more collocations (at least, our collocation identification metric identifies more collocations per sentence).ßdummyTurker
13195ß0.0ßThe collocation error - collocation ratio and error distribution are maintained.ßdummyTurker
13196ß0.0ßThe suggested datasets contain English material.ßdummyTurker
13197ß0.0ßOur goal was to focus on Spanish, for which much less research has been done.ßdummyTurker
13198ß0.0ßIt is a  good idea to run our models also on English material.ßdummyTurker
13199ß0.0ßThis requires annotation (and thus some time) but is not an obstacle.ßdummyTurker
13200ß0.0ßWe should have mentioned the size of CEDEL2.ßdummyTurker
13201ß0.0ßIt was an oversight not to do it.ßdummyTurker
13202ß0.0ßThe L1 group we tested our model with are American English native speakers learning Spanish (CEDEL2 is a Spanish learner corpus).ßdummyTurker
13203ß0.0ßIn principle, the typology we work with is language-independent, such that the model should be applicable to other languages.ßdummyTurker
13204ß0.0ßWe will show this in the final version of the paper on English learner material.ßdummyTurker
13205ß0.0ßThe algorithm was trained and developed on artificial data and tested on artificial *and* real data.ßdummyTurker
13206ß0.0ßWe think that Table 3 shows that the algorithm performs worse on learner data, but still reasonably well.ßdummyTurker
13207ß0.0ßAs observed in other works, a performance decrease on learner data is to be expected.ßdummyTurker
13208ß0.0ßAnswers to questions 1, 2 and 3 are provided in the replies to weakness arguments.ßdummyTurker
13209ß0.0ßThe baseline models for classifying offensive tweets into racist, sexist and non-offensive were discussed in the paper “Deep Learning for Hate Speech Detection in Tweets” [https://dl.acm.org/citation.cfm?id=3054223] where deep learning based models (CNN, FastText and LSTM) outperformed Bag of Words, TF_IDF and Char-ngrams models.ßdummyTurker
13210ß0.0ßThe CNN model implemented using Glove embedding stood as the third best framework for the classification task outperforming the baseline off the shelf models by a large margin.ßdummyTurker
13211ß0.0ßThus the exploration of the problem of Hinglish tweet classification was started from the perspective of application of CNN model.ßdummyTurker
13212ß0.0ßAlthough the LSTM based classifier was claimed to be the best neural network architecture for the task, we have clearly mentioned in the paper that experimentation on LSTM networks would be strongly pursued in future.ßdummyTurker
13213ß0.0ßThe use of Trans-CNN model is not just an off the shelf implementation of transfer learning but encompasses additional pre-processing of Hinglish tweets along with hyper parameter tuning.ßdummyTurker
13214ß0.0ßThe weights trained in this step are also used in the subsequent proposed framework and hence the model requires special mention.ßdummyTurker
13215ß0.0ßThe experiments proved that the binary classification of tweets into offensive and non-offensive performed way better that tertiary classification of tweets into non-offensive, abusive and hate inducing.ßdummyTurker
13216ß0.0ßThe main cause of this observation was that the abusive and hate inducing classes are more similar to each other as compared to non-offensive tweets.ßdummyTurker
13217ß0.0ßThus, a two way classification was thought off to first segregate the offensive tweets from the rest and further bifurcate them into abusive and hate inducing.ßdummyTurker
13218ß0.0ßThe last leg of experiments required a framework that can learn the similarity and differences between the abusive and hate inducing text.ßdummyTurker
13219ß0.0ßHence the use of Siamese networks came into picture.ßdummyTurker
13220ß0.0ßThe Siamese networks require a pair of objects as input and outputs a boolean value indicating the similarity between both.ßdummyTurker
13221ß0.0ßIn the starting of the experiment, 200 tweets were categorically set aside and did not take part in the experiment.ßdummyTurker
13222ß0.0ßThese were called as Siamese confidence tweets.ßdummyTurker
13223ß0.0ßWhen the test data was run, each tweet in the test dataset was paired with each tweet in Siamese confidence tweets (SCT).ßdummyTurker
13224ß0.0ßAs a result, now the test tweets were no longer individual tweets, but a pair of unknown and pre determined tweet.ßdummyTurker
13225ß0.0ßOut of these 200 pairs for each test tweet, the actual decision of the class allocation is just the majority of class label indicated by the 200 outputs for a particular pair.ßdummyTurker
13226ß0.0ßFor eg.ßdummyTurker
13227ß0.0ßTweet A , which falls in abusive class in test set, is coupled with 200 Siamese confidence tweets to form 200 pairs ranging from 1A to 200A.ßdummyTurker
13228ß0.0ßTWEET-1 TWEET-2 OUTPUT FREQUENCYßdummyTurker
13229ß0.0ßUnknown Abusive  1      82ßdummyTurker
13230ß0.0ßUnknown Abusive  0      28ßdummyTurker
13231ß0.0ßUnknown Hate     0      76ßdummyTurker
13232ß0.0ßUnknown Hate     1      14ßdummyTurker
13233ß0.0ßThe table cumulatively means that (82+76=158) pairs indicate abusive while (28+14=42) pairs indicate hate inducing.ßdummyTurker
13234ß0.0ßThis means that 79% Siamese confidence tweets vote the test tweet A as abusive.ßdummyTurker
13235ß0.0ßHence in the classifier performance testing hypothesis, this tweet will be reported to be detected as ‘Abusive’.ßdummyTurker
13236ß0.0ßIn this manner, the consensus based voting enabled on Siamese framework was used to determine the secondary class labels of offensive tweets.The overall accuracy of this methodology was calculated to be 98.5%.ßdummyTurker
13237ß0.0ßWe gracefully accept and thank the reviewer for it's informative reviews.ßdummyTurker
13238ß0.0ßWe would like to add that as far as issues of readability and analysis of the critical methods discussed in this paper are concerned, the authors would try their best to modify particular sections of the paper suitably before the camera-ready submission.ßdummyTurker
13239ß0.0ßWe are open to adding some explicit details and explanations of the proposed framework so as to make this paper adapt to the standards of the esteemed conference.ßdummyTurker
13240ß0.0ßthere is an ambiguity to fix in our text about the notion of repelling term.ßdummyTurker
13241ß0.0ßIn Counter-fitting, the repelling term refers to the exploitation of antonymy relations, provided as an external resource.ßdummyTurker
13242ß0.0ßIn Paragram, the provided relations are only similarity relations and the repelling term refers to counter-examples generated automatically by relying on the similarity between word vectors.ßdummyTurker
13243ß0.0ß"It should be noted that a variant of Paragram, Attract-Repel, includes a ""true"" repelling term."ßdummyTurker
13244ß0.0ßBut we do not use it.ßdummyTurker
13245ß0.0ßMoreover, the major interest of Pseudofit is that it does not depend on any external resource.ßdummyTurker
13246ß0.0ßsee answer to question 2 of the third reviewer.ßdummyTurker
13247ß0.0ßThe evaluation of Table 4 is actually an evaluation about sentence similarity and not only word similarity.ßdummyTurker
13248ß0.0ßWe will add the most significant results of the SemEval evaluation, which was not possible in the submission format.ßdummyTurker
13249ß0.0ß"the principles underlying Pseudofit are clearly ""based on an assumption of monosemy"" and it is why Pseudofit is less effective for high frequency words, which are supposed to be more polysemous, than for low frequency words."ßdummyTurker
13250ß0.0ßNevertheless, we can observe a significant improvement even for high frequency words (see answer to question 3 of reviewer 3).ßdummyTurker
13251ß0.0ßMoreover, there are some means to improve the current state by avoiding to build too heterogeneous pseudo-senses (see answer to question 3).ßdummyTurker
13252ß0.0ßFinally, Pseudofit can quite well be applied to a word sense disambiguated corpus whether or not such disambiguation is performed in a supervised or unsupervised way.ßdummyTurker
13253ß0.0ßin the initial version of Pseudofit, pseudo-senses are distinguished only for the target words while in Pseudofit context, pseudo-senses are distinguished both for the target words and the words of the context of these target words.ßdummyTurker
13254ß0.0ßIn that case, these pseudo-senses are distinguished for nouns, verbs and adjectives while our target words are only nouns (but there is no particular difficulty for having verbs and adjectives as target words).ßdummyTurker
13255ß0.0ßWe didn't achieve a careful analysis about this fact but we can first note that the lower half of vocabulary in terms of frequency doesn't cover only low frequency words since the median is equal to 439.ßdummyTurker
13256ß0.0ß"More importantly, we assume that words in the upper frequency range tends to be more polysemous, which is not taken into account by the approach since we split the occurrences of a word in a document into two different pseudo-senses (one occurrence is linked to one pseudo-sense, the following to the other one and so on) whereas, according to the ""one sense per discourse"" hypothesis, they are likely to correspond to only one sense."ßdummyTurker
13257ß0.0ßThis can be disturbing and taking into account this phenomenon is clearly a direct extension to investigate.ßdummyTurker
13258ß0.0ß- pseudo-word vs pseudo-sense: this is something to fix by replacing pseudo-word by pseudo-senseßdummyTurker
13259ß0.0ß- synonymy extraction: the status of this task, intrinsic or extrinsic, tends to vary from one publication to another but considering it as an intrinsic evaluation would be rather logicalßdummyTurker
13260ß0.0ß### We globally thank our three reviewers who pointed out some issues suggesting new research directions.ßdummyTurker
13261ß0.0ß###ßdummyTurker
13262ß0.0ßwe have already results concerning frequency (see answer to question 3 of the third reviewer) that we could develop with more space and we could easily add an analysis in terms of ambiguity, in relation with the answer to the question 1 of the second reviewer.ßdummyTurker
13263ß0.0ßRetrofitting and Counter-fitting were used as Paragram with the parameters recommended by their authors.ßdummyTurker
13264ß0.0ßA summary of these parameters will be done.ßdummyTurker
13265ß0.0ßPseudo-senses were defined as for Pseudofit.ßdummyTurker
13266ß0.0ß- a global package with the code used for the experiments will be releasedßdummyTurker
13267ß0.0ß- a less confusing example will be givenßdummyTurker
13268ß0.0ßThe present research is on the assessing the difficulty of concepts/words, which will be useful for any text generation research, i.e.ßdummyTurker
13269ß0.0ßit can be used to generate text appropriate to the particular target in term of its text difficulty.ßdummyTurker
13270ß0.0ßWe used the students’ grade of acquisition as the “gold standard” of the difficulty of the concept itself.ßdummyTurker
13271ß0.0ßWe used this because we argue that concept difficulty is relatively different for each person (some concepts that are difficult for person-A might be considered easy by person-B).ßdummyTurker
13272ß0.0ßThus, actually we don’t discuss what type of curriculum should be followed in K-12, or anything related to that.ßdummyTurker
13273ß0.0ßN/AßdummyTurker
13274ß0.0ßFor the NLP community, in my opinion, there are many research that could benefit for the concept/word difficulty prediction.ßdummyTurker
13275ß0.0ßOur target was on the text readability research.ßdummyTurker
13276ß0.0ßThe current research on the text readability still relies on the factors that represent two broad features of text difficulty: lexical sophistication and syntactic complexity.ßdummyTurker
13277ß0.0ßThis research is directed toward our long-term goal: content-based text readability, which is predicting the difficulty of text from the difficulty of the content of the text instead of the lexical and syntactic complexity of the text itself.ßdummyTurker
13278ß0.0ßOther research that would benefit is in the any text generation research (NLG), e.g.ßdummyTurker
13279ß0.0ßa summarisation research could used the result of this research to output/generate summary appropriate to particular target, or tailored according to our needs in term of difficulty (easy, medium, difficult.. etc).ßdummyTurker
13280ß0.0ßIn education, this work could be used for writing textbooks and teaching materials appropriate for particular a target since students’ comprehension is often affected by the difficulty of the word/concepts appear in the materials/textbook.ßdummyTurker
13281ß0.0ßAs we mentioned in the future work, we used more than one document resources for the concept mapping process due to limitation of the document resources, i.e.ßdummyTurker
13282ß0.0ßthere was no resource that provides textbooks from grade 1 to 12.ßdummyTurker
13283ß0.0ßWe thought that we could take balance of both native and non-native, thus we used both type of resources.ßdummyTurker
13284ß0.0ßAgree.ßdummyTurker
13285ß0.0ßWe plan to compute the frequency on the more general corpus such as Wikipedia.ßdummyTurker
13286ß0.0ßWe really appreciate your valuable comments.ßdummyTurker
13287ß0.0ßThe reviews help us a lot to improve our paper, and gave us insightful analyses and many advice useful to our ongoing research.ßdummyTurker
13288ß0.0ßWe weren’t familiar with the complex word identification field before, which is indeed very related to our paper.ßdummyTurker
13289ß0.0ßThank you for pointing it out.ßdummyTurker
13290ß0.0ßN/AßdummyTurker
13291ß0.0ßN/AßdummyTurker
13292ß0.0ßLike you’ve figured it out, for the evaluation, we indeed divide the data (491,503 pairs), half for training (246,503 pairs) dan half for test set (245,000 pairs).ßdummyTurker
13293ß0.0ßWe did not use development set.ßdummyTurker
13294ß0.0ßWe did mention in the paper, but very briefly.ßdummyTurker
13295ß0.0ßThe reported evaluation scores in Table 4 were computed only on the subset of the more discriminative pairs (e.g.ßdummyTurker
13296ß0.0ßgd >= 6).ßdummyTurker
13297ß0.0ßWe agree that it is indeed more interesting to evaluate the model by checking if the training the model with more well-separated pairs can improve its efficiency for all possibles pairs.ßdummyTurker
13298ß0.0ßWe really thanks for the suggestion.ßdummyTurker
13299ß0.0ßWe do use several sources for the context.ßdummyTurker
13300ß0.0ßWhen you said context dependency, do you mean the discussion on the result when we train the model on each of those context sources?ßdummyTurker
13301ß0.0ßWe did miss this aspect in our current paper.ßdummyTurker
13302ß0.0ßThanks for your constructive and thoughtful comments.ßdummyTurker
13303ß0.0ßWe will give more details to this section.ßdummyTurker
13304ß0.0ßWe were unable to train on giga-20M data due to hardware limitation.ßdummyTurker
13305ß0.0ßWe will try to add the results in the next version.ßdummyTurker
13306ß0.0ßWe did choose the representative examples rather than making a random selection, but the contrast is reasonably systematic.ßdummyTurker
13307ß0.0ßWe will release our outputs.ßdummyTurker
13308ß0.0ßThanks for your constructive and thoughtful comments.ßdummyTurker
13309ß0.0ßWe will proofread the draft again and fix ungrammatical sentences.ßdummyTurker
13310ß0.0ßWe will report the 20M result of Konstas et al.ßdummyTurker
13311ß0.0ß(2017) in our experiments.ßdummyTurker
13312ß0.0ßWe agree and will evaluate the performance of Graph2seq+Anon on the Test set and with (200K) and (2M) examples.ßdummyTurker
13313ß0.0ßWe will modify our description accordingly.ßdummyTurker
13314ß0.0ßFor both baseline and our model, we adopted dropout with 0.2 as the dropout rate on the input representations (x_j in Equations (1) and (2), x^l_{i,j} in Equations (6) and (7))ßdummyTurker
13315ß0.0ßWe will add this to the next version.ßdummyTurker
13316ß0.0ßWe will make changes according to your comments.ßdummyTurker
13317ß0.0ßWe expect the non-NN based method use in their work to not compare favourably to the NN-based ones used in our paper.ßdummyTurker
13318ß0.0ßFor a tentative final paper, we will in addition add results with a CNN, which performed best in Wang (2017).ßdummyTurker
13319ß0.0ßIf you sum up the numbers along the x-axis for each row, those do sum up to 1 (minus rounding errors).ßdummyTurker
13320ß0.0ßThe correct predictions are the diagonal of the matrix.ßdummyTurker
13321ß0.0ßThe x-axis represents the predicted label and the y-axis the correct label, as indicated by the labels.ßdummyTurker
13322ß0.0ßThis is a good suggestion, which we will include in a tentative final version.ßdummyTurker
13323ß0.0ßNote however that Evd gives away the answer, so it was intended as an “upper bound” -- which in practice, it didn’t turn out to be.ßdummyTurker
13324ß0.0ßFor a tentative final paper, we will in addition add results with a CNN, which performed best in Wang (2017) and will also change the random baseline, as suggested.ßdummyTurker
13325ß0.0ßWe'd like to point out that we do have another non-trivial baseline, which is the bag of word embeddings baseline.ßdummyTurker
13326ß0.0ßWe will discuss the surprisingly low performance of models based on ruling text in more depth in a tentative final version.ßdummyTurker
13327ß0.0ßThank you for your suggestion.ßdummyTurker
13328ß0.0ßWe will discuss the issue of mining evidence relevant to a claim and consider adding manual labels for individual evidence pages for future work.ßdummyTurker
13329ß0.0ßWe will add these statistics to the paper, some of which are already in the appendix.ßdummyTurker
13330ß0.0ßWe will increase Figure 1 and align the labels in Figure 2 more clearly, as suggested.ßdummyTurker
13331ß0.0ßWe will add the suggested references and discuss the differences between tasks as suggested.ßdummyTurker
13332ß0.0ßModels using ruling comments were supposed to provide an upper bound of performance (which, in practice, it turned out not to be).ßdummyTurker
13333ß0.0ßWe are aware of the selection bias of sources and agree that this leads to over-estimating “real-world” performance.ßdummyTurker
13334ß0.0ßIn future work, we will consider also using noisy sources.ßdummyTurker
13335ß0.0ßWe will augment our model description with descriptions of attention and skip-connections.ßdummyTurker
13336ß0.0ßThank you.ßdummyTurker
13337ß0.0ßWe’d argue that in practice, an exact label would not be needed.ßdummyTurker
13338ß0.0ßAs stated, we will consider this for future work.ßdummyTurker
13339ß0.0ßIn this work, we propose to model the sequence of attributes instead of the target words.ßdummyTurker
13340ß0.0ßThe intention is to reduce the decoding time, model size and increase the vocabulary size, and improve the translation quality by alleviating the OOVs and rare word problems.ßdummyTurker
13341ß0.0ßWe will revise the paper to improve its readability.ßdummyTurker
13342ß0.0ßPlease refer to the answer of Q2 of Reviewer#1.ßdummyTurker
13343ß0.0ßWe do not need to specifically handle the invalid attribute list, as each of which is associated with a unique word in the vocabulary.ßdummyTurker
13344ß0.0ßThe choice of the number of attributes and attribute values are arbitrary as long as we have enough unique attribute value list to represent all the words in the vocabulary.ßdummyTurker
13345ß0.0ßFor simplicity, we use two attributes for our experiments.ßdummyTurker
13346ß0.0ßWe have investigated the learned clusters.ßdummyTurker
13347ß0.0ßThose words which shared the similar attributes tend to close together.ßdummyTurker
13348ß0.0ßThe following shows an excerpt of the clusters:ßdummyTurker
13349ß0.0ß1st\2nd 	48		50		61		72ßdummyTurker
13350ß0.0ß7		-		-		president	 -ßdummyTurker
13351ß0.0ß32		picture		images		fame		motifsßdummyTurker
13352ß0.0ß87		-		-		headmaster	-ßdummyTurker
13353ß0.0ß135		-		-		chairperson	-ßdummyTurker
13354ß0.0ßIn the final paper, we will add this information.ßdummyTurker
13355ß0.0ßIn this study, we primarily focus on solving the limited vocabulary problem of NMT.ßdummyTurker
13356ß0.0ßThank you for providing the relevant works, we will consider it as our future work.ßdummyTurker
13357ß0.0ßThe proposed model achieves even faster training time in comparison with the training of a conventional NMT model.ßdummyTurker
13358ß0.0ßDue to space limitations, we do report this in the previous manuscript.ßdummyTurker
13359ß0.0ßWe will add this discussion in the final paper.ßdummyTurker
13360ß0.0ß(S3.1a) We follow the common practice of NMT that each word is represented by a learned embedding, a d-dimensional vector.ßdummyTurker
13361ß0.0ßWe adopt the same representation for the proposed latent attributes that each possible attribute value is represented by a learned embedding, a d-dimensional vector.ßdummyTurker
13362ß0.0ß(S3.5) The Beam search algorithm is based on the widely used method in NMT.ßdummyTurker
13363ß0.0ß(F3): \tilde{z} is an intermediate representation which usually is not presented in the architecture of NMT model.ßdummyTurker
13364ß0.0ß(E11): We follow the formulation from the widely cited paper (Sennrich et al., 2017).ßdummyTurker
13365ß0.0ß(E12): Yes, they are jointly trained to minimize the total loss over all parameters.ßdummyTurker
13366ß0.0ß(S3.1b): a_1^3 denotes the third value of first latent attribute, which is represented by a n-dimension vector.ßdummyTurker
13367ß0.0ß(S4&5): Please refer to the answer of Q2 of Reviewer 1 for the supplementary experiments.ßdummyTurker
13368ß0.0ß(S5.2): Our features used for mapping are obtained from the bilingual training process, while those of other methods gained from the monolingual corpus.ßdummyTurker
13369ß0.0ßTherefore, we think ’bilingual’ method is more suitable than ‘monolingual’ method for translating task.ßdummyTurker
13370ß0.0ßWe will rephrase this sentence.ßdummyTurker
13371ß0.0ßThanks for your suggestion.ßdummyTurker
13372ß0.0ß(S1): Subword-NMT usually uses the same vocabulary size with that of the standard word-based NMT.ßdummyTurker
13373ß0.0ßThe computational cost of softmax layer is the same.ßdummyTurker
13374ß0.0ßHowever, raising the number of tokens leads to even more computational cost.ßdummyTurker
13375ß0.0ßIn other words, BPE model does not help in reducing the decoding time.ßdummyTurker
13376ß0.0ß(S4.3): What we would like to express is that our model outperforms all other models in sentences with different length, as shown in Figure 4.ßdummyTurker
13377ß0.0ß(S4.6): In NMT system, the context vector is computed by the attention mechanism, which determines the portion of source context to be attended.ßdummyTurker
13378ß0.0ßFor an unknown word, those units might affect the prediction of the target word.ßdummyTurker
13379ß0.0ß(S4.1): Please refer to our previous answer to Reviewer#1.ßdummyTurker
13380ß0.0ß(T3): The intention of this experiment is to demonstrate that our model is able to use an extreme large vocabulary.ßdummyTurker
13381ß0.0ßTo the best of our knowledge, this is the first model to train on such a large vocabulary and still achieves comparable translation results.ßdummyTurker
13382ß0.0ß(S4.6): Please refer to our previous answer to Reviewer#2.ßdummyTurker
13383ß0.0ßWe greatly appreciate the reviewer for all the efforts.ßdummyTurker
13384ß0.0ßWe believe that our contribution represents a notable extension of prior approaches and will help out with MRS studies.ßdummyTurker
13385ß0.0ß"Our ""supervised"" multi-task learning is not a super novel method in deep learning and NLP, but a novel contribution for MRS."ßdummyTurker
13386ß0.0ßThe distant supervision approaches used in DrQA (Chen+,ACL'17) and R3 (Wang+,AAAI'18) cannot consider the context of passages carefully in the IR task.ßdummyTurker
13387ß0.0ßOur proposed method is simple but effective.ßdummyTurker
13388ß0.0ßIt achieves the state-of-the-art results on a large-scale MRS task.ßdummyTurker
13389ß0.0ßMoreover, using TF-IDF rankers (e.g., the Document Reader of DrQA) as initial rankers for eliminating higher irrelevant passages is very reasonable because of the computational cost of neural networks for large-scale corpora.ßdummyTurker
13390ß0.0ßThis is not a weak point.ßdummyTurker
13391ß0.0ßThe Jp-News is a new dataset that we collected.ßdummyTurker
13392ß0.0ß"The Jp-News dataset was created by crowdworkers in a similar way as SQuAD, as described in the ""Datasets"" (4.1) section."ßdummyTurker
13393ß0.0ßWe cannot describe the details of the dataset due to the limited space.ßdummyTurker
13394ß0.0ßIf our paper is accepted, we will add the data collection procedure.ßdummyTurker
13395ß0.0ßI cannot agree with this comment.ßdummyTurker
13396ß0.0ß"The submission paper described the IR evaluation metrics in the ""evaluation metrics"" (4.2) section and described the negative sampling method in the ""training process"" (4.4) subsection."ßdummyTurker
13397ß0.0ß"We also described experimental designs in the ""Datasets"" (4.1) section."ßdummyTurker
13398ß0.0ßOther reviewers evaluated that our paper is well-written and easy to follow in most places.ßdummyTurker
13399ß0.0ßThis study focuses on supervised multi-task learning, not on distant supervision and domain adaptation.ßdummyTurker
13400ß0.0ßSQuAD is the only dataset that contains both sufficient training and evaluation data for MRS in the same domain (Wikipedia).ßdummyTurker
13401ß0.0ßOther datasets do not contain a large number of evidence passages (e.g., TriviaQA, SearchQA, and Quasar) and do not contain training data for MRS (e.g., CuratedTREC, WebQuestions, and WikiMoviews).ßdummyTurker
13402ß0.0ßWe greatly appreciate the reviewer for all the efforts.ßdummyTurker
13403ß0.0ßWe believe that our paper achieved the state-of-the-art results among the refereed publications and preprints at the submission time.ßdummyTurker
13404ß0.0ßAccording to the new ACL policy, the papers submitted to ACL'18 without preprints should be considered contemporaneous to our submission.ßdummyTurker
13405ß0.0ßOur IR module did not have efficiency problems in practice.ßdummyTurker
13406ß0.0ßOur model with telescoping setting, which processed 200 passages for each question, took 1.5 seconds per question on four GPUs.ßdummyTurker
13407ß0.0ßWe appreciate the feedback, but cannot agree with this comment.ßdummyTurker
13408ß0.0ßWe evaluated state-of-the-art neural IR models such as Match-tensor and Duet that have different architectures and embedding representations from ours.ßdummyTurker
13409ß0.0ßWe appreciate the helpful suggestions.ßdummyTurker
13410ß0.0ßWe will reduce the usage of abbreviations in the camera-ready version.ßdummyTurker
13411ß0.0ßYes.ßdummyTurker
13412ß0.0ßNo.ßdummyTurker
13413ß0.0ßOur system with the telescoping setting is capable of answering questions at a practical speed.ßdummyTurker
13414ß0.0ßWe used a JTAG tokenizer (Fuchi and Takagi, 1998) for the pre-processing of Jp-News.ßdummyTurker
13415ß0.0ßWe will add the description to the paper if accepted.ßdummyTurker
13416ß0.0ßThe Jp-News is a new dataset that we collected.ßdummyTurker
13417ß0.0ßOur procedure was almost the same as the SQuAD's one.ßdummyTurker
13418ß0.0ßThe difference in the data collection procedure is that there are three answers (created by different workers) for each training data of Jp-News, as shown in Table 1.ßdummyTurker
13419ß0.0ßIf our paper is accepted, we will add the data collection procedure.ßdummyTurker
13420ß0.0ßWe greatly appreciate the reviewer for all the efforts.ßdummyTurker
13421ß0.0ßWe believe that our contribution represents a notable extension of prior approaches and will help out with this emerging field.ßdummyTurker
13422ß0.0ßOur supervised multi-task learning is not a super novel method in deep learning, but a novel contribution for MRS.ßdummyTurker
13423ß0.0ßIt is simple but effective.ßdummyTurker
13424ß0.0ßIt achieves state-of-the-art results on a large-scale MRS task.ßdummyTurker
13425ß0.0ßThere were certainly a statistical difference between our components trained with STL and MTL for both the two datasets (Two-way repeated measures ANOVA; p<.05).ßdummyTurker
13426ß0.0ßThis statistical test confirmed the difference between the learning curves of STL and MTL (EM and F1 scores for each epoch), and the test's power was therefore high.ßdummyTurker
13427ß0.0ßWe cannot agree with this comment.ßdummyTurker
13428ß0.0ßMatch-Tensor [Jaech+, Neu-IR'17] and Duet [Mitra+,WWW'17] are ones of the state-of-the-art neural IR baselines.ßdummyTurker
13429ß0.0ßThe Duet paper reported that Duet significantly outperformed the Deep Relevance Matching Model (DRMM) [Guo+,CIKM'16].ßdummyTurker
13430ß0.0ßThey are very competitive baselines when there is a large amount of data; however, they were not able to achieve high accuracy with the small amount of data contained in the reading comprehension datasets.ßdummyTurker
13431ß0.0ßWe will add the missing reference.ßdummyTurker
13432ß0.0ßWe appreciate the reviewer for the helpful suggestions.ßdummyTurker
13433ß0.0ßWe have not evaluate the effects of the lambda value and the fine-tune stage yet.ßdummyTurker
13434ß0.0ß"As described in the ""test process"" section (3.8), our model outputs an answer for each passage in the retrieved set (k passages) and returns a final answer with weighted majority voting from k outputs of the RC component."ßdummyTurker
13435ß0.0ßThe majority voting is not effective for corpora like SQuAD where most descriptions are expected to be stated only once, so the RC performance was degraded when k > 1.ßdummyTurker
13436ß0.0ßMoreover, the RC module has no discriminative power to distinguish between relevant and irrelevant passages because it is trained with only relevant passages.ßdummyTurker
13437ß0.0ßThe IR module has the power because it is trained with both the relevant and irrelevant passages.ßdummyTurker
13438ß0.0ß"Entity extraction is explicitly stated throughout the paper, such as in the title ""named entity analysis and extraction"" (Line 1), the abstract ""to extract named entities from free text"" (Line 21), the introduction (Lines 43-48), the methodology (Lines 594-597, 610-611), and the conclusion (Line 794)."ßdummyTurker
13439ß0.0ßPlease refer to our response to argument 3 of review#1 for the named entity extraction and classification.ßdummyTurker
13440ß0.0ßWe focus on extraction and compare our method with state-of-the-art methods on the extraction results.ßdummyTurker
13441ß0.0ßPOM scheme works at the syntactic level and does not affect the performance at the semantic level.ßdummyTurker
13442ß0.0ßBut the semantic classification requires other works and we leave it to our future work (Line 797-799).ßdummyTurker
13443ß0.0ß"""Non-common words"" is defined in Abstract as ""which appear in named entities and hardly appear in common text"" (Line 14-16) and illustrated in Section 3 ""..."ßdummyTurker
13444ß0.0ß"Such kind of words can be used to predict named entities and we call them as non-common words"" (Line 286-190)."ßdummyTurker
13445ß0.0ßNon-common words is defined on the distinction between named entities and common text, within either the entire dataset or only training set or test set.ßdummyTurker
13446ß0.0ß"In analysis, using training set and test set is to analyzing the phenomenon of non-common words and the main point is ""non-common words in the DEVELOPMENT set and TEST set also hardly appear in the common text of TRAINING set"" (Line 290-292)."ßdummyTurker
13447ß0.0ßThis supports an underlying assumption: words of TEST set that appear in the named entities of TRAINING set and hardly appear in the common text of TRAINING set tend to be (part of) named entities.ßdummyTurker
13448ß0.0ßIn experiments, when deriving non-common words, we use only the TRAINING set.ßdummyTurker
13449ß0.0ß"We state that ""...the word hardly appears in the common text of TRAINING set"" (Line 518-524)."ßdummyTurker
13450ß0.0ßSo the comparison is fair.ßdummyTurker
13451ß0.0ßPlease refer to our response to argument 1 of review#2.ßdummyTurker
13452ß0.0ßCRFs has proved to be more effective in sequence tagging than other statistical models like SVM.ßdummyTurker
13453ß0.0ßPOM is inspired by TOMN but not a simple application.ßdummyTurker
13454ß0.0ßTOMN cannot be simply applied to general named entities because of two differences between time expressions and named entities.ßdummyTurker
13455ß0.0ßFirst, time expressions consist of only a small set of words, which can be collected entirely (Zhong and Cambria, 2018).ßdummyTurker
13456ß0.0ßNamed entities instead contain many and diverse words, which cannot be collected entirely.ßdummyTurker
13457ß0.0ßSecond, POS tags cannot differentiate time expressions from common text; Zhong and Cambria (2018) do not consider POS tags nor other syntactic features in their work.ßdummyTurker
13458ß0.0ßBut POS tags are important features in our work.ßdummyTurker
13459ß0.0ßTo address the impossibility of collecting entire words, we explore and finally find out that the non-common words/proper nouns, which abstract words to a higher syntactic level, have the remarkable power to predict named entities.ßdummyTurker
13460ß0.0ßThe performance of TOMN relies on the collected words.ßdummyTurker
13461ß0.0ßPOM can keep POM pre-tag features without word lexicons because it relies on non-common words/proper nouns.ßdummyTurker
13462ß0.0ßOriginal OntoNotes5's common text contains many ordinals and numbers that should be treated as entities, according to its GUIDELINE (Line 229-233,241-244).ßdummyTurker
13463ß0.0ßIt's not easy to improve the performance since they are already high.ßdummyTurker
13464ß0.0ßIn perspective error reduction, POM reduces much error (Line 679-681).ßdummyTurker
13465ß0.0ßPlease refer to our response to argument 3 of review#1.ßdummyTurker
13466ß0.0ßThe word lexicons derived from wikipedia are from https://en.wikipedia.org/wiki/Lists_of_cities_by_country and https://en.wikipedia.org/wiki/Lists_of_people_by_nationalityßdummyTurker
13467ß0.0ß"""Friday/NNP"" and ""February/NNP"" in CoNLL03 are not named entities."ßdummyTurker
13468ß0.0ß"Some NNP words like ""SUPPRISE/NNP DEFEAT/NNP"" are incorrectly POS tagged."ßdummyTurker
13469ß0.0ß"For ""preference"" please refer to Line 418-429 and (Barabasi and Albert, 1999)."ßdummyTurker
13470ß0.0ßWe collect word lexicons for only CoNLL03's categories (Line 503-507) and will release the code and data (Line 49).ßdummyTurker
13471ß0.0ßThanks for your constructive and insightful comments.ßdummyTurker
13472ß0.0ßWe will revise our draft accordingly.ßdummyTurker
13473ß0.0ßThe building of lexicon is discussed in the experimental settings section (the word embedding part), but without reference to the term D. We will improve our draft according to your suggestion.ßdummyTurker
13474ß0.0ßWe will also include their joint model results in our draft.ßdummyTurker
13475ß0.0ßThe joint result was not chosen earlier due to the use of heterogeneous discrete and neural features.ßdummyTurker
13476ß0.0ßWe appreciate the details and will fix these issues.ßdummyTurker
13477ß0.0ßThanks for your constructive and insightful comments.ßdummyTurker
13478ß0.0ßWe will modify our draft accordingly.ßdummyTurker
13479ß0.0ßWe will add graphics with high-level view where space allows (Indeed we left out some figures due to page limits).ßdummyTurker
13480ß0.0ßWe will give more details on the dataset.ßdummyTurker
13481ß0.0ßThanks for your constructive and insightful comments.ßdummyTurker
13482ß0.0ßWe will modify our draft accordingly.ßdummyTurker
13483ß0.0ßYes the lexicon D includes all possible words, as collected from automatically segmented Chinese GigaWord.ßdummyTurker
13484ß0.0ßWe did not use a segmentor for the NER sentences, but simply match the input with D. We will improve relevant statements in the draft.ßdummyTurker
13485ß0.0ßThis is a mistake in figure drawing and we will fix it.ßdummyTurker
13486ß0.0ßWe will improve the notations to reduce chances of confusion.ßdummyTurker
13487ß0.0ßThis is very reasonable.ßdummyTurker
13488ß0.0ßD does contain noise, as it is obtained from automatically segmented corpora.ßdummyTurker
13489ß0.0ßWe will add relevant discussions to our draft.ßdummyTurker
13490ß0.0ßNevertheless, given the power of disambiguation via supervised NER training, useful information can be actively selected amongst noise.ßdummyTurker
13491ß0.0ßThis is one major advantage as compared with NER systems that use explicit segmentation information.ßdummyTurker
13492ß0.0ßWe appreciate this very nice suggestion.ßdummyTurker
13493ß0.0ßWe will try to discuss and compare with alternative methods that exploit the lexicon.ßdummyTurker
13494ß0.0ßSo far as we can come up with, the lattice structure LSTM is the most natural way for extending character tagging NER with lexicon information.ßdummyTurker
13495ß0.0ßWe will give more discussions where relevant.ßdummyTurker
13496ß0.0ßPlease refer to item 2 above.ßdummyTurker
13497ß0.0ßPlease refer to item 5 above.ßdummyTurker
13498ß0.0ßAs most of the previous studies did not report this result, we followed them and did not report either in our first version.ßdummyTurker
13499ß0.0ßBut this is a nice suggestion, thanks.ßdummyTurker
13500ß0.0ßFor repetition, we presented the percentage of duplicates at sentence level in Figure 1, including 1-gram to 4-gram duplicates, and the results show that our model has clear advantage on reducing repetition compared with the baseline.ßdummyTurker
13501ß0.0ßHuman evaluation may be a more reasonable method, and we will add it in the final version.ßdummyTurker
13502ß0.0ßThe average length of the summaries of Giga is 8.8 words and that of LCSTS is 18.8 Chinese characters.ßdummyTurker
13503ß0.0ßWe evaluate this baseline by selecting the first 10 words for Giga and 20 characters for LCSTS in our experiments (the average lengths of the summaries of the corpora are 8.8 and 18.8), and the results ROUGE-1, 2 and L areßdummyTurker
13504ß0.0ßWe thank the reviewers for their time.ßdummyTurker
13505ß0.0ßWe thank the reviewers for their time.ßdummyTurker
13506ß0.0ßThanks to the valuable comments.ßdummyTurker
13507ß0.0ßWe will further modify or add the contents related to the difference between our work and that in MMT’ task.ßdummyTurker
13508ß0.0ßThe paper will be further checked and corrected by a native writer of English.ßdummyTurker
13509ß0.0ßThe difference between Multi-label keyword prediction network and other works is that our work can extract effectively concrete words.ßdummyTurker
13510ß0.0ßThis is due to our inappropriate description.ßdummyTurker
13511ß0.0ßActually we use the publicly trained model.ßdummyTurker
13512ß0.0ßWe will modify the corresponding content.ßdummyTurker
13513ß0.0ßWe will further give the statistics on concrete and abstract words from these two datasets.ßdummyTurker
13514ß0.0ßAll four dynamic fusion methods achieve the similar performance on Flick8kCN and AIC-ICC datasets.ßdummyTurker
13515ß0.0ßWe will add the related experimental results on AIC-ICC dataset.ßdummyTurker
13516ß0.0ßWe will give the qualitative analysis for experimental results.ßdummyTurker
13517ß0.0ßDifferent from other works listed in weakness 1, the novelty of our work is that our method can represent concrete words (such as objects and attributes) from image modality and abstract words using text modality.ßdummyTurker
13518ß0.0ßDuring the generation process, our dynamic fusion methods can learn to assign different weights for each input modality.ßdummyTurker
13519ß0.0ßOur method is language-independent.ßdummyTurker
13520ß0.0ßIf our paper is accepted, the experimental comparisons on COCO dataset will be further given in the revised version.ßdummyTurker
13521ß0.0ßOur methods are mainly validated on Chinese captioning dataset.ßdummyTurker
13522ß0.0ßBy now there is only one related Chinese captioning papers.ßdummyTurker
13523ß0.0ßSo the experimental results is compared with that in the paper [Xirong Li, Weiyu Lan, Hailong Liu, and Hailong Liu.ßdummyTurker
13524ß0.0ß2016.ßdummyTurker
13525ß0.0ßAdding chinese captions to images.ßdummyTurker
13526ß0.0ßInACM on International Conference on MultimediaRetrieval, pages 271–275.]ßdummyTurker
13527ß0.0ßIn our paper, we give the ensemble of four different fusion methods.ßdummyTurker
13528ß0.0ßExperimental results also show the ensemble method achieved the best performance.ßdummyTurker
13529ß0.0ßActually, our method provides the combination method from image and text modality in Section 2.3.ßdummyTurker
13530ß0.0ß-Weakness argument 1ßdummyTurker
13531ß0.0ßWe are interested in keywords that lead to different responses (as shown in Table1), which may be different from those extracted from existing methods.ßdummyTurker
13532ß0.0ßWe use RL to automatically estimate the keyword probability distributions.ßdummyTurker
13533ß0.0ßThis motivation has been stated in Sec1 & Sec3.1.ßdummyTurker
13534ß0.0ßWe first obtain a candidate keyword set extracted from existing methods.ßdummyTurker
13535ß0.0ßNext, for a candidate keyword related to none of the responses, we will decrease its probability, whereas for that not considered as a keyword but related to the responses, we will increase its probability.ßdummyTurker
13536ß0.0ßThe supervised approach cannot be applied because keywords for our setting are not observed.ßdummyTurker
13537ß0.0ß-Weakness argument 2ßdummyTurker
13538ß0.0ßAs stated in Sec4.1 and Sec4.5, we admit the content information is promising to be used.ßdummyTurker
13539ß0.0ßHowever, the computational efficiency will be problematic if we train our model with encoding the long content jointly.ßdummyTurker
13540ß0.0ßMoreover, from the current results, we find that the generation results are quite good (See the examples in Table 6).ßdummyTurker
13541ß0.0ßThus, we do not treat modeling the content as our focus in our paper.ßdummyTurker
13542ß0.0ßWe only crawled comments that are direct responses to the news.ßdummyTurker
13543ß0.0ßResponses to the comments are not included.ßdummyTurker
13544ß0.0ß-Weakness argument 3ßdummyTurker
13545ß0.0ßWhen the annotators give the diversity score, they need to check whether the same meaning is being conveyed with different words in order to count the number of responses with different meanings.ßdummyTurker
13546ß0.0ßThus, we have already done this.ßdummyTurker
13547ß0.0ß-Question 1 & 2ßdummyTurker
13548ß0.0ßSee the responses to Weakness argument 1.ßdummyTurker
13549ß0.0ß-a real difference...ßdummyTurker
13550ß0.0ßAs stated in Sec1 & Sec3.1, Seq2Multiseq is different from Seq2seq in that:ßdummyTurker
13551ß0.0ß(1)We introduce a latent keyword set for each query and with different sampling of the keyword set, we feed both the encoding outputs of the sampled keywords and the query into the decoder to obtain different responses.ßdummyTurker
13552ß0.0ß(2)To infer a good keyword set, we consider all the responses for a given query simultaneously using RL.ßdummyTurker
13553ß0.0ß-a real benefit of using reinforcement learning...ßdummyTurker
13554ß0.0ßWe are interested in keywords that lead to different responses (as shown in Table1), which may be different from those extracted from existing methods.ßdummyTurker
13555ß0.0ßWe use RL to automatically estimate the keyword probability distributions.ßdummyTurker
13556ß0.0ßThis motivation has been stated in Sec1 & Sec3.1.ßdummyTurker
13557ß0.0ßWe first obtain a candidate keyword set extracted from existing methods.ßdummyTurker
13558ß0.0ßNext, for a candidate keyword related to none of the responses, we will decrease its probability, whereas for that not considered as a keyword but related to the responses, we will increase its probability.ßdummyTurker
13559ß0.0ßThe supervised approach cannot be applied because keywords for our setting are not observed.ßdummyTurker
13560ß0.0ß-learning curvesßdummyTurker
13561ß0.0ßWe will try to add these in our final version.ßdummyTurker
13562ß0.0ß-the proposed approach is no better than previous work...ßdummyTurker
13563ß0.0ßOur work aims at increasing the diversity among multiple generated responses while maintaining each of them to be relative to the query.ßdummyTurker
13564ß0.0ßWe do not argue that our method can obtain higher relevance score per response.ßdummyTurker
13565ß0.0ßFrom our results, our method achieves the highest diversity score, which increases about 50% of the compared methods while obtains comparable relevance score.ßdummyTurker
13566ß0.0ßThus we consider our method outperforms the previous work.ßdummyTurker
13567ß0.0ß-I don't know how much to trust the three human labellers...ßdummyTurker
13568ß0.0ßAs stated in Sec4.3, current automatic evaluations are not appropriate and human annotations are more reliable.ßdummyTurker
13569ß0.0ßIn the experiments of Seq2seq+Diverse Beam Search/Multiple-Mechanism, they also use three/several (no specific number given) annotators.ßdummyTurker
13570ß0.0ßThus, our results should be considered trustworthy.ßdummyTurker
13571ß0.0ßWe thank reviewer 2 for his insightful comments and suggestions on this paper.ßdummyTurker
13572ß0.0ß===ßdummyTurker
13573ß0.0ßThese are all very good questions that we thought a great deal about.ßdummyTurker
13574ß0.0ßAs mentioned in the paper we found that the model is not really sensitive to the order so we just decided on one and kept it.ßdummyTurker
13575ß0.0ßAn interesting alternative approach would be the one proposed by the reviewer, but we chose this model because we wanted to encourage the model to aggregate information from a variety of positions (the lemma-pooling operation), and in our experience attention has trouble learning to pool in this way because by design (standard) attention focuses on one position only.ßdummyTurker
13576ß0.0ßIn any case, it is important to note that our solution works quite well in practice and is very easy to implement.ßdummyTurker
13577ß0.0ß===ßdummyTurker
13578ß0.0ßDynamic simply means that different knowledge can be applied without retraining the system.ßdummyTurker
13579ß0.0ßWe demonstrate specifically the sensitivity to the dynamically provided knowledge in the counterfactual analysis reported in Section 5.3 (see examples in Table 4).ßdummyTurker
13580ß0.0ßWe acknowledge, however, that this ambiguity might arise and try to account for it in the future version of the paper.ßdummyTurker
13581ß0.0ßWe thank Reviewer 1 for his helpful comments and we will try to improve upon clarity of our paper.ßdummyTurker
13582ß0.0ß===ßdummyTurker
13583ß0.0ß- “Unmotivated Approach”ßdummyTurker
13584ß0.0ßOur approach was deliberately motivated by wanting to find an architecture that (1) can integrate information contained in multiple pieces of text and (2) that is able to be used with any kind of neural NLU architecture.ßdummyTurker
13585ß0.0ßSince all neural NLU architectures use word embeddings, refining word embeddings seemed like an excellent choice in service of (2).ßdummyTurker
13586ß0.0ßSince bidirectional LSTMs are good at extracting reading, they seemed to fit the needs of (1) quite well.ßdummyTurker
13587ß0.0ßWe will clarify this.ßdummyTurker
13588ß0.0ßHowever, we do not wish to imply this model is optimal, only that it is reasonable for our goal.ßdummyTurker
13589ß0.0ß- “Unclear what the system actually does, e.g.ßdummyTurker
13590ß0.0ßrange of the max function”ßdummyTurker
13591ß0.0ßWe will clarify the passages identified as unclear.ßdummyTurker
13592ß0.0ßIn words: at each level, every word type is refined based on the refinement document currently being processed.ßdummyTurker
13593ß0.0ßThe max function in equation 3 ranges over all tokens x_i in the input texts of stage l (ell) that have the same lemma as the word w we are updating.ßdummyTurker
13594ß0.0ßWe do this for every word w in the vocabulary at each stage.ßdummyTurker
13595ß0.0ß===ßdummyTurker
13596ß0.0ß- “no motivation for ‘dynamic’ ”, “ ‘dynamic’ is not really evaluated”ßdummyTurker
13597ß0.0ßRegarding motivation, see above.ßdummyTurker
13598ß0.0ßDynamic simply means that different knowledge can be applied without retraining the system.ßdummyTurker
13599ß0.0ßWe demonstrate specifically the sensitivity to the dynamically provided knowledge in the counterfactual analysis reported in Section 5.3 (see examples in Table 4).ßdummyTurker
13600ß0.0ß- “should contrast the approach with including background knowledge in unrefined embeddings”ßdummyTurker
13601ß0.0ßStatic use of the background knowledge would make adaptation without retraining to new or changing knowledge, like the counterfactual reasoning experiments, impossible, which was a key motivation for our work.ßdummyTurker
13602ß0.0ßThus, we would consider a static approach rather orthogonal than competing.ßdummyTurker
13603ß0.0ßMoreover, the GloVe embeddings we use already contain Wikipedia (a source of supplementary knowledge in the QA experiments), albeit from a few years ago, so only the ConceptNet data is missing.ßdummyTurker
13604ß0.0ßFinally, the contextual refinement method (a novelty in this paper) already demonstrates that even without any additional knowledge, contextual refinement is valuable.ßdummyTurker
13605ß0.0ß===ßdummyTurker
13606ß0.0ßThe goal of this work (which we will clarify in the paper) was not to compete with highly tuned task and dataset architectures, but to show that knowledge can be exploited by neural NLU models.ßdummyTurker
13607ß0.0ßAlthough we do, in fact, achieve SotA with a very simple task model on the more recent TriviaQA dataset, we like to mention that achieving SotA results usually requires significant engineering effort, which was not focus and out of scope for this particular work.ßdummyTurker
13608ß0.0ßWe thank reviewer 2 for his comprehensive review and suggestions on improving this paper.ßdummyTurker
13609ß0.0ß===ßdummyTurker
13610ß0.0ßWe like to clarify that both Wikipedia and ConceptNet are integrated in exactly the same way, which shows that our solution is capable of integrating heterogeneous sources of knowledge.ßdummyTurker
13611ß0.0ßWhat we did not explore in this paper, however, is the problem of retrieving contextually relevant knowledge as pointed out quite early in Section 2.ßdummyTurker
13612ß0.0ßThe specific focus of this paper was to propose a simple and effective solution to the problem of integrating background knowledge in free text form assuming we are given relevant knowledge.ßdummyTurker
13613ß0.0ß===ßdummyTurker
13614ß0.0ßWe looked at all changes in the verified subset of the TriviaQA Wiki dataset which is rather small.ßdummyTurker
13615ß0.0ßHowever, we agree that looking at more examples is preferable, although the examples we looked at were quite repetitive.ßdummyTurker
13616ß0.0ß===ßdummyTurker
13617ß0.0ßWe sampled a set of prediction changes from which we picked the 3 illustrative examples.ßdummyTurker
13618ß0.0ßHowever, also here the sampled examples were quite repetitive and we could have picked others.ßdummyTurker
13619ß0.0ßWe thank the reviewer for their thorough work and supporting review.ßdummyTurker
13620ß0.0ßAs noted in our response to reviewer 3 - our results on the Spanish dataset of CoNLL2002 are on par with the Carreras et al 2002 model stated in the NAACL 2016 paper provided by reviewer 3 (which is a SOTA non-neural NER model) - although we used a very basic set of features and apply very limited task-specific tuning to our models.ßdummyTurker
13621ß0.0ßMore generally, the comparison to SOTA in our case is not trivial.ßdummyTurker
13622ß0.0ßFor example this paper: https://arxiv.org/abs/1801.09851 presents SOTA numbers for the JNLPBA and BC2GM datasets.ßdummyTurker
13623ß0.0ßWhile these results are substantially better than those we report, they build on an ensemble of three different models, each capturing a different set of features.ßdummyTurker
13624ß0.0ßIn contrast, our goal in this paper is to demonstrate the applicability of our ideas for a variety of tasks and datasets, we did not aim to select a task specific set of features.ßdummyTurker
13625ß0.0ßOur strategy was hence to select the feature set that we consider standard for sequence labeling tasks in NLP - which indeed provides SOTA and near-SOTA results for the CoNLL2002 NER and the CoNLL2000 chunking datasets.ßdummyTurker
13626ß0.0ßHaving said all that, we realize that a discussion of how our results relate to the SOTA is important for the completeness of our presentation.ßdummyTurker
13627ß0.0ßGiven the opportunity we will provide this discussion in the camera ready version.ßdummyTurker
13628ß0.0ß"We did implement a CRF model (see  lines 284-296) but, as the reviewer says, trained it with ""argmax algorithms"" rather than with a probabilistic log-likelihood approach."ßdummyTurker
13629ß0.0ßWe will add a comparison to that approach to the camera ready version, given the opportunity.ßdummyTurker
13630ß0.0ßWe agree with the reviewer that additional theoretical discussion will improve the paper.ßdummyTurker
13631ß0.0ßOne of the reasons we aimed for a short paper is that we do not have theoretical justification for our method (as stated in the paper) - but a discussion can put our work in a more concrete context and facilitate a future fruitful work.ßdummyTurker
13632ß0.0ßOne thing we do want to note is that the various algorithms we work with do optimize an objective - in our implementation this is the CRF objective described in lines 284-296.ßdummyTurker
13633ß0.0ßIt is true that we do not optimize this objective using a probabilistic log-likelihood approach.ßdummyTurker
13634ß0.0ß"However, perceptron and MIRA (as well as their SWV extensions) do that using an ""argmax-based"" approach."ßdummyTurker
13635ß0.0ßActually, it can be shown that the update rule of the argmax approaches approximates the gradient computation of the log-likelihood model (and hence the argmax approaches are in fact gradient methods over the log-likelihood objective, but with approximated gradients).ßdummyTurker
13636ß0.0ßWe will discuss this in more details in the camera ready version, given the opportunity (some of this discussion is already made in the Collins EMNLP 2002 paper on the structured perceptron) .ßdummyTurker
13637ß0.0ßResponse to additional comments: Thanks for these good comments.ßdummyTurker
13638ß0.0ßWe will devote the extra page given in the camera ready to discuss the efficiency of each weighting scheme and to consider the idea of softmin with a temperature parameter.ßdummyTurker
13639ß0.0ßWe will also improve the writing, particularly with respect to JJ-x and the mentioned theorem.ßdummyTurker
13640ß0.0ßThanks for the comments and suggestions.ßdummyTurker
13641ß0.0ßWe focus on short-text one-liners and attempt to distinguish them from normal short texts.ßdummyTurker
13642ß0.0ßWe didn't pay attentions to specific types of humor such as irony and sarcasm.ßdummyTurker
13643ß0.0ßDifferent types of humor do have different characteristics and we will further study in future to investigate whether our proposal works in any type or some types of humor.ßdummyTurker
13644ß0.0ßQualitative analysis is truly necessary and we should conduct it.ßdummyTurker
13645ß0.0ßIt is right that the features (both the baseline features and new proposed ones) are more related to incongruity, because such features are relatively easier to construct.ßdummyTurker
13646ß0.0ßIn our work, superiority theory motivates us to exploit sentiment associations for humor recognition.ßdummyTurker
13647ß0.0ßWe assume that sentiment can reflect kinds of attitudes towards others and associations may reflect some kinds of incongruity and their combination should be useful for identifying humor.ßdummyTurker
13648ß0.0ßBut precisely modeling superiority is still very hard.ßdummyTurker
13649ß0.0ßNonetheless, qualitative analysis should help us understand better.ßdummyTurker
13650ß0.0ßRelevance theory should be helpful for interpreting the results.ßdummyTurker
13651ß0.0ßAnd such pragmatics theories should help us design more interpretable features.ßdummyTurker
13652ß0.0ßThanks for the suggestion.ßdummyTurker
13653ß0.0ßWe will explain it better.ßdummyTurker
13654ß0.0ßOur observations are 1) contrastive is directly related to unexpectedness, while 2) condition relation is often used to help build a contrastive effect, such as the example,ßdummyTurker
13655ß0.0ß“I 've learned that we are responsible for what we do , unless we are celebrities.”.ßdummyTurker
13656ß0.0ßAnd data analysis also shows that condition relation is often used in humorous text to achieve similar effects.ßdummyTurker
13657ß0.0ßWe will check and fix such problems.ßdummyTurker
13658ß0.0ßPDTB can be also investigated.ßdummyTurker
13659ß0.0ßWe are going to do such experiments and compare the results.ßdummyTurker
13660ß0.0ßThanks a lot for your appreciation and recognition for our paper.ßdummyTurker
13661ß0.0ßP.S., Our paper doesn't involve any human participants, so do you meanßdummyTurker
13662ß0.0ß"""Handling of Human Participants"" is ""N/A"", rather than ""No"" :-)?"ßdummyTurker
13663ß0.0ßThank you.ßdummyTurker
13664ß0.0ßThanks for your valuable comments and suggestions.ßdummyTurker
13665ß0.0ßFollowings are response to specific arguments:ßdummyTurker
13666ß0.0ßIn our experiments we compared our method with sampling and scaling, which are two of most widely used methods when dealing with class inequality in information extraction.ßdummyTurker
13667ß0.0ßAnd Parambath et al.ßdummyTurker
13668ß0.0ß(2014) have shown that scaling, which is one of cost-sensitive learning method, can be regard as a kind of F-measure optimization methods [1].ßdummyTurker
13669ß0.0ßThanks for your valuable suggestion, and we will compare with more F-measure optimization methods in the later version of our paper.ßdummyTurker
13670ß0.0ßCurrently, our work mainly lies in non-sequential, classification based detection models (and this kind of models is also used in NER such as BiLSTM-CNNs [2]).ßdummyTurker
13671ß0.0ßThanks for the insightful comment, and we also agree that how to solve class inequality problem in sequential labeling models is very interesting and important.ßdummyTurker
13672ß0.0ßWe will try to extend our method to solve it in our further research.ßdummyTurker
13673ß0.0ß[1] Parambath, S. P., Usunier, N., & Grandvalet, Y. Optimizing F-measures by cost-sensitive classification.ßdummyTurker
13674ß0.0ßIn NIPS2014.ßdummyTurker
13675ß0.0ß[2] Chiu, J. P., & Nichols, E. Named Entity Recognition with Bidirectional LSTM-CNNs.ßdummyTurker
13676ß0.0ßIn TACL.ßdummyTurker
13677ß0.0ßThanks for your valuable comments and suggestions.ßdummyTurker
13678ß0.0ßFollowings are response to your suggestions and questions:ßdummyTurker
13679ß0.0ßGreat thanks for your proofreading suggestion and your great efforts on the revision of our typos.ßdummyTurker
13680ß0.0ßWe promise that we will carefully proofread our paper in the revision.ßdummyTurker
13681ß0.0ßThank you for reminding us of this important point and we will describe it more specifically in the next version of our paper.ßdummyTurker
13682ß0.0ßWith the rise of the accuracy on either positive or negative class, the importance of negative instances increases.ßdummyTurker
13683ß0.0ßFor that of positive class, it is straightforward.ßdummyTurker
13684ß0.0ßBecause at the beginning if the accuracy of positive class is zero, F-measure will also be zero no matter how high the accuracy on negative class is, i.e., correctly predicting negative instances cannot result in any F-measure increment.ßdummyTurker
13685ß0.0ßTherefore negative instances are inconsequential when the accuracy on positive class is low.ßdummyTurker
13686ß0.0ßAnd with the increment of positive accuracy, the importance of negative class also increases.ßdummyTurker
13687ß0.0ßAdditionally, with the rise of negative accuracy, a number of easy negative instances converge very quickly and the probability of correctly predicting them quickly comes close to 1.0, so their loss comes near zero.ßdummyTurker
13688ß0.0ßThis is analogical to removing easy negative instances out of the training procedure and the hard negative instances remaining become more “balance” proportional to positive instances.ßdummyTurker
13689ß0.0ßFor this reason the importance of remaining hard negative instances are increased to make the model fit them better.ßdummyTurker
13690ß0.0ßExplanation for Equations 7 and 8-ßdummyTurker
13691ß0.0ßOur insignificance distribution captures a general notion of insignificance over all the words in the input sequence.ßdummyTurker
13692ß0.0ßTo understand Equations 7 and 8, we can draw a parallel in the way attention and insignificance distributions are used.ßdummyTurker
13693ß0.0ßTo incorporate attention in the decoding process, we compute a context vector (Equation 3 in the paper).ßdummyTurker
13694ß0.0ßA similar vector is computed for insignificance distribution in Equation 7.ßdummyTurker
13695ß0.0ßFinally, in Equation 8, both these vectors are used to compute the probability of generation carefully.ßdummyTurker
13696ß0.0ßWe will improve this intuition in the final version of the paper.ßdummyTurker
13697ß0.0ßMissing references-ßdummyTurker
13698ß0.0ßWe thank the reviewer for pointing it out.ßdummyTurker
13699ß0.0ßWe will fix the missing references in the camera-ready version.ßdummyTurker
13700ß0.0ßWe apologize if the language in the paper is unclear.ßdummyTurker
13701ß0.0ßWe try to address the reviewer's queries below.ßdummyTurker
13702ß0.0ßWhy is the notion of saliency useful?ßdummyTurker
13703ß0.0ß-ßdummyTurker
13704ß0.0ßThe failure of pointer generator model to generate abstractive summaries motivates us to capture saliency of input words using Part-of-Speech(POS) tags, Named-Entity-Recognition(NER) tags etc.ßdummyTurker
13705ß0.0ß"As Reviewer 1 points out, the term ""saliency"" can have multiple connotations, we have used it to capture the linguistic cues around the word to teach the network to copy only if these cues point to a ""salient"" fact in the input."ßdummyTurker
13706ß0.0ßWe hypothesize that additional linguistic information would allow the model to better differentiate between these factual details (High saliency, must be copied) and non-factual details (Low saliency, can be generated from vocabulary).ßdummyTurker
13707ß0.0ßWhy should a model not copy relevant things from the input?ßdummyTurker
13708ß0.0ß-ßdummyTurker
13709ß0.0ß"Our objective is never to miss out on ""relevant things"" from the input."ßdummyTurker
13710ß0.0ßInfact, the motive behind learning an insignificance distribution over input is to identify these relevant things.ßdummyTurker
13711ß0.0ßThe observation made by [1] is that the model copies much more than just the relevant things from the input, suggesting that there is a huge scope for achieving more abstractiveness without compromising on the relevant information in the source.ßdummyTurker
13712ß0.0ßAre the lexical changes good?ßdummyTurker
13713ß0.0ß-ßdummyTurker
13714ß0.0ßWe understand that the improvement on percentage novel n-grams is only validating the existence of lexical changes.ßdummyTurker
13715ß0.0ßIt is possible to generate totally irrelevant summary and get a perfect score on percentage novel n-grams.ßdummyTurker
13716ß0.0ßWe need some metric to evaluate that we do not compromise the quality of generated summaries.ßdummyTurker
13717ß0.0ßFor this purpose, we turn to ROUGE, which is widely accepted for evaluating summarization tasks.ßdummyTurker
13718ß0.0ßHence, we report our evaluation on ROUGE scores along with percentage of novel n-grams for a comprehensive evaluation of our approach.ßdummyTurker
13719ß0.0ß[1] See, Abigail, Peter J. Liu, and Christopher D. Manning.ßdummyTurker
13720ß0.0ß"""Get to the point: Summarization with pointer-generator networks."""ßdummyTurker
13721ß0.0ßIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.ßdummyTurker
13722ß0.0ßOn insignificance distribution and loss function-ßdummyTurker
13723ß0.0ßWe encode an insignificance distribution over a significance distribution just because it makes our loss function simpler.ßdummyTurker
13724ß0.0ßThe motivation behind our loss function comes from our objective.ßdummyTurker
13725ß0.0ß"The ""copying"" still takes place using attention distribution only."ßdummyTurker
13726ß0.0ßUsing our loss function, we force the model to only copy significant words.ßdummyTurker
13727ß0.0ßPlease refer to Section 2 in the paper for more details.ßdummyTurker
13728ß0.0ßOn improvements in the evaluation metrics -ßdummyTurker
13729ß0.0ßWe employ Fisher-Pitman Permutation Test [1] which attests to the statistical significance of our results for both pre and post coverage scenarios.ßdummyTurker
13730ß0.0ßOur best performing model achieves a 50% higher tendency to generate than the baseline.ßdummyTurker
13731ß0.0ßWe have provided complete details on this test in Section 3 of the paper.ßdummyTurker
13732ß0.0ß[1] Fisher, Ronald Aylmer.ßdummyTurker
13733ß0.0ßThe design of experiments.ßdummyTurker
13734ß0.0ßOliver And Boyd; Edinburgh; London, 1937.ßdummyTurker
13735ß0.0ßUnconvincing Motivation -ßdummyTurker
13736ß0.0ßThe pointer generator framework allows the model to copy factual details such as Person names from the source article.ßdummyTurker
13737ß0.0ßAs pointed out in [1], the model eventually learns to copy non-factual details as well, even whole sentences.ßdummyTurker
13738ß0.0ßThe model fails to generate abstractive summaries, which is essentially the objective behind neural encoder-decoder frameworks.ßdummyTurker
13739ß0.0ßThis motivates our work towards bringing more lexical diversity into the summaries, hypothesizing that additional linguistic information like POS and NER tags allow the model to differentiate between factual (must be copied) and non-factual (can be generated from vocabulary) details.ßdummyTurker
13740ß0.0ßAs the model achieves this, it is simultaneously trained to copy only when required, which is the primary objective of our work.ßdummyTurker
13741ß0.0ßIf accepted, we will incorporate these into the motivation in the final version of the paper.ßdummyTurker
13742ß0.0ß[1] See, Abigail, Peter J. Liu, and Christopher D. Manning.ßdummyTurker
13743ß0.0ß"""Get to the point: Summarization with pointer-generator networks."""ßdummyTurker
13744ß0.0ßIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.ßdummyTurker
13745ß0.0ßMore evaluation metrics apart from ROUGE -ßdummyTurker
13746ß0.0ßThe objective of our work is to improve the novelty in the generated summaries without compromising on other qualities.ßdummyTurker
13747ß0.0ßWe capture this novelty using percentage of novel n-grams in the generated summaries with respect to the source documents.ßdummyTurker
13748ß0.0ßHowever, it is possible to generate a totally irrelevant summary, which has nothing to do with the source document and get a perfect score on percentage of novel n-grams.ßdummyTurker
13749ß0.0ßIn order to evaluate that we do not compromise the quality of generated summaries, we turn to ROUGE, which is a widely accepted metric for evaluating the task of summarization.ßdummyTurker
13750ß0.0ßHence, we report our evaluation on ROUGE scores along with percentage of novel n-grams, which we feel are sufficient to capture the objective of this work.ßdummyTurker
13751ß0.0ßComparison with existing works addressing the same problem for other frameworks-ßdummyTurker
13752ß0.0ßIn this work, we primarily focus on improving the abstractiveness of the generated summaries by the pointer-generator frameworks.ßdummyTurker
13753ß0.0ßThe formulation of our problem setting is specific to pointer generator.ßdummyTurker
13754ß0.0ßHence, we did not feel the necessity to explore other frameworks.ßdummyTurker
13755ß0.0ßHowever, we thank the reviewer for the suggestion; we would explore other frameworks suffering from similar issues in our future explorations.ßdummyTurker
13756ß0.0ßWe thank the reviewer for his comments.ßdummyTurker
13757ß0.0ßIn the primal form, a linear classifier takes as input a feature matrix X of n samples (rows) with m features (columns) and optimizes a set of weights in order to reproduce the n training labels.ßdummyTurker
13758ß0.0ßIn the dual form, the linear classifier takes as input a kernel matrix K of n x n components, where each component K_ij is the similarity between examples x_i and x_j.ßdummyTurker
13759ß0.0ßFor the linear kernel, the kernel matrix K can be obtained as K = X * X', where X' is the transpose of X.ßdummyTurker
13760ß0.0ßIn our case, we have two kinds of features: HISK with the corresponding feature matrix X1 and BOSWE with the corresponding feature matrix X2.ßdummyTurker
13761ß0.0ßWe can combine HISK and BOSWE by first computing the corresponding kernel matrices K1 = X1 * X1' and K2 = X2 * X2', and by summing the matrices K_combined = K1 + K2.ßdummyTurker
13762ß0.0ßThis process is equivalent to feature vector concatenation, i.e.ßdummyTurker
13763ß0.0ßK_combined = [X1 X2] * [X1 X2]', where [X1 X2] is a matrix of n samples with m1 HISK features followed by m2 BOSWE features, hence the size of [X1 X2] is n x (m1 + m2).ßdummyTurker
13764ß0.0ßFinally, we train a dual linear classifier using the K_combined kernel matrix.ßdummyTurker
13765ß0.0ßWe hope this clarifies the weakness identified by the reviewer.ßdummyTurker
13766ß0.0ß"We can extend the ""model fusion"" section in the final paper, since we have an extra page."ßdummyTurker
13767ß0.0ßThe problem being tackled is not clearly described.ßdummyTurker
13768ß0.0ßThe problem being tackled is not clearly motivated: Some general comments are given on it being important to adapt to users interests for tasks such as “adaptive education” and “adaptive mobile”.ßdummyTurker
13769ß0.0ßI can guess what adaptive education might be but not sure about adaptive mobile.ßdummyTurker
13770ß0.0ß>>	Adaptive mobile applications need to invest user interests and needs in a given situation.ßdummyTurker
13771ß0.0ßReferences and principles should be provided to support the assumptions, definition of soundness, and dependency calculation methods.ßdummyTurker
13772ß0.0ß>>	We agree that we should explain how decisions of the proposed assumptions are taken compared to the state of the art approaches.ßdummyTurker
13773ß0.0ßThe applications for this technology need to be more clear and should be related better to NLP.ßdummyTurker
13774ß0.0ßThe paper doesn’t have much computational linguistics.ßdummyTurker
13775ß0.0ßI think it’s still appropriate because the task and method involves semantics but it could be better related to the field.ßdummyTurker
13776ß0.0ß>>	This work aims to resolve conflicts inside the profile by creating a semantic and hierarchical structure of user’s interests.ßdummyTurker
13777ß0.0ßThe current paper is an extension to the previous one as it resolves conflicts over profiles by taking into account their interest structures.ßdummyTurker
13778ß0.0ßThe variable names are not very readable -- using inconsistent differences in italics to represent different meanings (e.g.ßdummyTurker
13779ß0.0ßitalic “E” is the opposite of non-italic “E” but italic “H” is not the opposite of non-italic “H”).ßdummyTurker
13780ß0.0ß>>	Italic “E” and italic “H” are respectively the complements of events E and H.ßdummyTurker
13781ß0.0ßMOTIVATION OF THE APPROACH I was confused by the motivation of the approach, and in particular the idea of conflict resolution in the context of estimating interest profiles.ßdummyTurker
13782ß0.0ßPerhaps it is my unfamiliarity with the task, but I think it would help to provide some concrete examples of what the authors mean with this.ßdummyTurker
13783ß0.0ßTerms like 'conflict', 'true information', 'false interests', etc are not clear and need to be explained.ßdummyTurker
13784ß0.0ß>>	We thank the reviewer for this insightful remark.ßdummyTurker
13785ß0.0ßWe agree that the paper needs some concrete examples in order to explain more the motivation of the approach.ßdummyTurker
13786ß0.0ßIt would also help the structure of the paper to more clearly distinguish between profiles that are 'correct' but do not help enrich a user's profile, perhaps because the users just have different interests, and profiles that are really 'incorrect', e.g., erroneous information.ßdummyTurker
13787ß0.0ßFor example, a statement like this 'especially, with the spread of conflicts (irrelevant interests)' (92) confused me.ßdummyTurker
13788ß0.0ßIt's not clear to me why irrelevant interests are considered conflicts.ßdummyTurker
13789ß0.0ß>>	In general, the enrichment of user’s profile consists in improving its content by adding some relevant interests from different users’ profiles.ßdummyTurker
13790ß0.0ßA relevant interest reflects the user’s needs in a specific period of time.ßdummyTurker
13791ß0.0ßIt cannot be erroneous, duplicated, out-of-date and ambiguous.ßdummyTurker
13792ß0.0ßFor this reason, irrelevant interests are considered conflicts namely false interests.ßdummyTurker
13793ß0.0ß'a profile can contain a copy of false interests coming from a non-reliable profile.ßdummyTurker
13794ß0.0ß'.ßdummyTurker
13795ß0.0ß>>	This sentence means that a profile may be enriched by false interests relative to a profile that is non-reliable.ßdummyTurker
13796ß0.0ß'In fact, a user's profile may be non-reliable as it may provide interests with conflicts, namely false interests.'ßdummyTurker
13797ß0.0ßThe authors try to clarify their approach in section 3 which is based on some strong assumptions.ßdummyTurker
13798ß0.0ßExplaining how they arrive at these assumptions and giving concrete examples of how this would be visible in an online platform would help.ßdummyTurker
13799ß0.0ß>>	We agree that we should explain how decisions of the proposed assumptions are taken compared to the state of the art approaches.ßdummyTurker
13800ß0.0ßBASELINES The authors could include some simple baselines, like just ranking profiles by similarity with the user.ßdummyTurker
13801ß0.0ß>>	Generally, the similarity between two users reflects the similarity between their interests.ßdummyTurker
13802ß0.0ßThis idea corresponds to the principle of the majority voting method (see section 2 and 4.2).ßdummyTurker
13803ß0.0ß"This sentence requires more explanation: ""With the evolutionary aspect ..."ßdummyTurker
13804ß0.0ß"growth."""ßdummyTurker
13805ß0.0ß(abstract)ßdummyTurker
13806ß0.0ßThe discussion of related work could be made more clear by explicitly mentioning which of the references involve conflict resolution in the task of profile enrichment.ßdummyTurker
13807ß0.0ßThe evaluation compares between rankings.ßdummyTurker
13808ß0.0ßThe authors could therefore consider evaluation metrics that focus on comparing rankings rather than MAE of RMSE.ßdummyTurker
13809ß0.0ßResponseßdummyTurker
13810ß0.0ß>>	We admit that some references related to conflict resolution in the context of profile enrichment should be added.ßdummyTurker
13811ß0.0ßIn fact, we have detailed this state of the art in our previous work.ßdummyTurker
13812ß0.0ßThis work aims to resolve conflicts inside the profile by creating a semantic and hierarchical structure of user’s interests.ßdummyTurker
13813ß0.0ßThe current paper is an extension to the previous one as it resolves conflicts over profiles by taking into account their interest structures.ßdummyTurker
13814ß0.0ßWe thank the reviewer for the thoughtful comments and suggestions.ßdummyTurker
13815ß0.0ßBecause we use pre-trained word embeddings, embedding training can be done offline and does not contribute to the computation bottleneck.ßdummyTurker
13816ß0.0ßAt the outset it appears that the SVD might be a bottleneck because of the O(n^3) computation for an nxn matrix.ßdummyTurker
13817ß0.0ßGenerally the complexity of SVD is O(min(d^{2}n, dn^{2})) on the matrix W (dxn).ßdummyTurker
13818ß0.0ßIn our case, d is vector dimension 300, and n is the word count in a document.ßdummyTurker
13819ß0.0ßConsidering that d is a constant, we note that the SVD complexity becomes O(n), which grows only linearly with the document length.ßdummyTurker
13820ß0.0ßAlso the decomposition is one-time processing for each document, so SVD is not an efficiency bottleneck.ßdummyTurker
13821ß0.0ßAs mentioned in the paper (lines 217-220), the topic vectors that we consider are not true words and thus are not directly comparable to the topics generated by LDA.ßdummyTurker
13822ß0.0ßWe will add (tfidf based) baselines from IR in empirical evaluation in the final version.ßdummyTurker
13823ß0.0ßWe thank the reviewer for the thoughtful comments and suggestions.ßdummyTurker
13824ß0.0ßThanks for suggesting related works on retrieval tasks, and we will include discussions about these works.ßdummyTurker
13825ß0.0ßAlthough not our focus, we explore the direct impact of domain-specific information brings to the matching task and hence include that as our contribution.ßdummyTurker
13826ß0.0ßWe will exclude this mention as being our contribution.ßdummyTurker
13827ß0.0ßWe provide a rough argument on the number of topics in the current version of the paper and will make available a more thorough analysis of hyperparameters (the number of topics and embedding dimension) in the final version.ßdummyTurker
13828ß0.0ßAt the outset it appears that the SVD might be a bottleneck because of the O(n^3) computation for an nxn matrix.ßdummyTurker
13829ß0.0ßGenerally the complexity of SVD is O(min(d^{2}n, dn^{2})) on the matrix W (dxn).ßdummyTurker
13830ß0.0ßIn our case, d is vector dimension 300, and n is the word count in a document.ßdummyTurker
13831ß0.0ßConsidering that d is a constant, we note that the SVD complexity becomes O(n), which grows only linearly with the document length.ßdummyTurker
13832ß0.0ßAlso the decomposition is one-time processing for each document, so SVD is not an efficiency bottleneck.ßdummyTurker
13833ß0.0ßThis view is not entirely correct.ßdummyTurker
13834ß0.0ßOur experimental training set test set is separate, and the test data is not involved in the training of the model.ßdummyTurker
13835ß0.0ßThe highest score cannot be obtained only by copying the text of the training data.ßdummyTurker
13836ß0.0ßThe model can only achieve the best performance by fitting the global distribution of corpusßdummyTurker
13837ß0.0ßThank you for your review.ßdummyTurker
13838ß0.0ßThe article has some typographic and writing questions.ßdummyTurker
13839ß0.0ßWe will improve it for future revisions.ßdummyTurker
13840ß0.0ßThe goal of our model is to fit the corpus and generate coherent and semantically meaningful sentences.This model is a preliminary language generation task.ßdummyTurker
13841ß0.0ßWe can apply it with external conditions to tasks such as automatic comment generation and product description generation.ßdummyTurker
13842ß0.0ßWe only show the text output of the COCO image description.ßdummyTurker
13843ß0.0ßThe characteristic of this data set is that the sentence structure is very simple.ßdummyTurker
13844ß0.0ßSo the output text looks like simple word and phrase substitutions.ßdummyTurker
13845ß0.0ßFuture versions of our article will add other dataset output samplesßdummyTurker
13846ß0.0ßThank you very much for your review, our article has some writing and typographical errors, we will improve it in future versionsßdummyTurker
13847ß0.0ß15 sentences are a bit small, we will increase the number of evaluation sentences in future experiments.ßdummyTurker
13848ß0.0ßThere is no overlapping data between the training set and the test set, which is completely separate.ßdummyTurker
13849ß0.0ßTest set data does not participate in training and generation process.ßdummyTurker
13850ß0.0ßWe tested the proposed model on the open Chinese poetry dataset.ßdummyTurker
13851ß0.0ßThere is no detailed description of this dataset in the article.ßdummyTurker
13852ß0.0ßWe will add a description of the poetic dataset described in the future versionßdummyTurker
13853ß0.0ßWe follow the BLEU-2 to BLEU-5 evaluation guidelines for text generation models such as RankGAN (httpsßdummyTurker
13854ß0.0ßThe seed sentence is still extracted from the training data, and the test data does not participate in the generation process.ßdummyTurker
13855ß0.0ßVolunteers are all international students in the English sentence generation test.ßdummyTurker
13856ß0.0ßTheir mother tongue is English.ßdummyTurker
13857ß0.0ßWe will add human evaluation details in future revisions.ßdummyTurker
13858ß0.0ßThank you for your comments.ßdummyTurker
13859ß0.0ßIt is arguably a little incremental over G&H.ßdummyTurker
13860ß0.0ßOur approach is relatively simple.ßdummyTurker
13861ß0.0ßHowever, its effectiveness is shown by a considerable accuracy improvement over G&H and a 10-fold reduction in training time.ßdummyTurker
13862ß0.0ßConceptually, modeling multiple relations is substantially different from simply modeling coherence (as in G&H).ßdummyTurker
13863ß0.0ßIn this way we also hope it will lead to interesting follow-up work, as individual relations can be informed by injecting prior knowledge (e.g., by training jointly with relation extraction models).ßdummyTurker
13864ß0.0ßNo satisfying answer to why ment-norm works better than rel-norm.ßdummyTurker
13865ß0.0ß(*) This is not easy to answer.ßdummyTurker
13866ß0.0ßFigure 4 shows that when using the oracle, rel-norm and ment-norm achieve quite similar performance.ßdummyTurker
13867ß0.0ßIt suggests that ment-norm is less sensitive to LBP’s imperfection thanks to its ability of selecting proper mentions (i.e., attention over mentions).ßdummyTurker
13868ß0.0ßRel-norm however does not have this ability: its attention is over relations.ßdummyTurker
13869ß0.0ßThe induced relation types are not very interpretable.ßdummyTurker
13870ß0.0ßWe believe that it may be possible to make relations more interpretable (e.g., by integrating syntactic or discourse information).ßdummyTurker
13871ß0.0ßMoreover, the dataset is not large (1000 documents), so inducing more fine-grain relations would likely result in overfitting.ßdummyTurker
13872ß0.0ßStill, the induced relations are beneficial from the performance perspective and also reduce training time.ßdummyTurker
13873ß0.0ßAs the reviewer pointed out, this suggests that they introduce a useful structural bias.ßdummyTurker
13874ß0.0ßIs it purely an empirical fact that dedicating a distinct 'none' relation doesn't fix rel-norm?ßdummyTurker
13875ß0.0ßTheoretically, rel-norm can assign a relation to 'none'.ßdummyTurker
13876ß0.0ßHowever, we did not observe that behaviour in the experiments.ßdummyTurker
13877ß0.0ß- It's not clear if the choice of ment-norm over rel-norm is really absolutely critical.ßdummyTurker
13878ß0.0ßBased on (*), we believe that ment-norm is preferable when the used inference method is potentially not accurate.ßdummyTurker
13879ß0.0ßThank you for your comments and suggestions.ßdummyTurker
13880ß0.0ßFirst of all, we would like to clarify two misunderstandings that lead to weaknesses 1 and 3.ßdummyTurker
13881ß0.0ßA) The reviewer states (weakness 1) that “the basic assumption [is]… that a pair of entity usually has multiple relations” and that “the assumption is not justified”.ßdummyTurker
13882ß0.0ßWe do not make this assumption.ßdummyTurker
13883ß0.0ßInstead, we assume that different relations can occur in a document for different entity pairs.ßdummyTurker
13884ß0.0ßIndeed, these relations can encode coreference, various types of semantic (e.g., located_in) and syntactic relations (e.g., apposition), and general (topical) coherence.ßdummyTurker
13885ß0.0ßSome previous work already relied on multiple relations between entities (e.g., Cheng and Roth (2013)), though they focused on the supervised setting.ßdummyTurker
13886ß0.0ßWe believe that this assumption is certainly reasonable.ßdummyTurker
13887ß0.0ßAlso recall that training of our model converges much faster than for the relation-agnostic model, suggesting that our modeling assumption introduces a valid and useful structural bias.ßdummyTurker
13888ß0.0ßIn fact, we agree that the assumption “mention pair should have multiple relations” is not justified.ßdummyTurker
13889ß0.0ßRel-norm can be regarded as a form of a mixture model over relation types, which assumes that exactly one (but unknown) relation is active between two mentions, and relation weights encode our confidence which relation it is (i.e.ßdummyTurker
13890ß0.0ßthe posterior probabilities).ßdummyTurker
13891ß0.0ßMent-norm can in principle assign multiple relations to the same pair but regularization in 496-499 prevents this from happening.ßdummyTurker
13892ß0.0ßSo we actually already address the reviewer’s concern.ßdummyTurker
13893ß0.0ßB) The reviewer mentions that “technical contribution is incremental” and “the model seems only add some normalization to” G&H (weakness 3).ßdummyTurker
13894ß0.0ßNormalization is not our main contribution, although it is used to define two variants of our framework.ßdummyTurker
13895ß0.0ßOur main contribution is introduction of multiple latent relations without requiring neither supervision nor feature engineering.ßdummyTurker
13896ß0.0ßThis is different from G&H and other previous work.ßdummyTurker
13897ß0.0ßThough our approach is relatively simple, its effectiveness is shown by a considerable accuracy improvement over G&H and a 10-fold reduction in training time.ßdummyTurker
13898ß0.0ßConceptually, modeling multiple relations is substantially different from simply modeling coherence (as in G&H).ßdummyTurker
13899ß0.0ßIn this way we also hope it will lead to interesting follow-up work, as individual relations can be informed by injecting prior knowledge (e.g., by training jointly with relation extraction models).ßdummyTurker
13900ß0.0ßThe improvement in accuracy is marginal.ßdummyTurker
13901ß0.0ßIn fact 0.85% is substantial compared to other improvements.ßdummyTurker
13902ß0.0ßTable 1 shows the improvements over time — 0.3%, 2%, 0.5%, 0.72%.ßdummyTurker
13903ß0.0ßOur improvement over G&H is the second best.ßdummyTurker
13904ß0.0ßOur model is also ten times faster to train.ßdummyTurker
13905ß0.0ßThe coefficient (1e-7) seems too small.ßdummyTurker
13906ß0.0ßHow is the effect of this regularization?ßdummyTurker
13907ß0.0ßWe observe that the regularization term constitutes about 5-10% of the optimization objective.ßdummyTurker
13908ß0.0ßWithout it, the performance of the multi-relational model drops to the level of the mono-relational model.ßdummyTurker
13909ß0.0ßHow to derive the relation embedding is not clear.ßdummyTurker
13910ß0.0ßRelation embeddings R_k are part of the model parameters.ßdummyTurker
13911ß0.0ßTherefore they are learnt jointly with the other parameters.ßdummyTurker
13912ß0.0ßWhy not combine ment-norm and rel-norm?ßdummyTurker
13913ß0.0ßThis is an interesting suggestion.ßdummyTurker
13914ß0.0ßCombining them is possible and may lead to a better performance.ßdummyTurker
13915ß0.0ßThank you for your comments and suggestions.ßdummyTurker
13916ß0.0ßDoes the pairwise score function work on all entity combinations in a document?ßdummyTurker
13917ß0.0ßYes.ßdummyTurker
13918ß0.0ßHowever, if we keep only mention pairs within 5 sentences, we obtain similar performance.ßdummyTurker
13919ß0.0ßThis means we can further speed up the method.ßdummyTurker
13920ß0.0ßWhy two sets of selected candidates?ßdummyTurker
13921ß0.0ßOur candidate selection consists of two pruning steps, using the mention-entity prior p(e|m) and using the contextual prior.ßdummyTurker
13922ß0.0ßThe former is widely used.ßdummyTurker
13923ß0.0ßThe latter was proposed by Ganea & Hofmann.ßdummyTurker
13924ß0.0ßDoes this mean the relation representations learned in entity linking are helpful for relation extraction?ßdummyTurker
13925ß0.0ßThis is an interesting suggestion.ßdummyTurker
13926ß0.0ßFor this we may need a larger number of latent relations, which might not seem beneficial for entity linking but may be useful in this transfer scenario: the induced relations would be more fine-grain and interpretably / semantically-coherent.ßdummyTurker
13927ß0.0ßIndeed, we have made comparisons to show the effectiveness of the model rather than data.ßdummyTurker
13928ß0.0ß"We can see from Table 1, the model named with ""w/o DG"" could be regarded as a traditional machine reading model (which is similar with DrQA and R3), which is also trained on the bootstrapped data, and the advantage of our DGR model is very clear (about two points improvement)."ßdummyTurker
13929ß0.0ß"Thus, we don't think ""the results are likely to rely on data rather than the proposed model"", but we will emphasize it in the next version."ßdummyTurker
13930ß0.0ßWe don’t think the complexity is a big issue.ßdummyTurker
13931ß0.0ßActually, our model only selects several (1-5) candidate documents for normalization during each epoch, which is same to other models, such as SR2 or SR3.ßdummyTurker
13932ß0.0ßSo it may not introduce a heavy burden in training, let alone our proposed DGR is relatively simpler (we do not use sophisticated attention mechanism, or more ad-hoc input word embedding, etc.).ßdummyTurker
13933ß0.0ßIn addition, as our model achieves significant improvement over the un-normalized one (Figure 4).ßdummyTurker
13934ß0.0ßDuring inference period, our model has the same complexity as other models (go through every retrieved document to make predictions).ßdummyTurker
13935ß0.0ßThus, we believe the extra training time is worthwhile for open-domain question answering.ßdummyTurker
13936ß0.0ßTo our best knowledge, our data generation scheme is the first one that use a dynamic bootstrapping method to generate positive data in ODQA setting.ßdummyTurker
13937ß0.0ßFor the negative documents, it has been clearly stated in Section 3.2 that they are selected from the documents pool-which is retrieved by IR model, so these negative documents have strong lexical overlap (as we use tf-idf based Elasticsearch IR tool ) with the question.ßdummyTurker
13938ß0.0ßWhich is, from your point, highly topical paragraph.ßdummyTurker
13939ß0.0ßThanks for your valuable comments!ßdummyTurker
13940ß0.0ß1) The reason for this advantage is two-folds.ßdummyTurker
13941ß0.0ß(1) Although some dataset has provided the ground-truth documents (in the training period), our proposed data generation scheme could consistently produce high-quality documents to enhance the training data, which will strengthen the performance of our model (Section 4.4).ßdummyTurker
13942ß0.0ß(2) The baseline models do not apply global normalization, which will result in a wrong answer span with high score.ßdummyTurker
13943ß0.0ßWe have explained it in the introduction and experiment section 4.3.ßdummyTurker
13944ß0.0ß2) It has been shown in Figure 5 that when we add more positive document in addition to the original ground-truth document.ßdummyTurker
13945ß0.0ßThe behavior is consistently better.ßdummyTurker
13946ß0.0ß3) The global normalization means that we normalize the answer score in every document, rather than a single document (which is formulated in Equation 9).ßdummyTurker
13947ß0.0ß4) Most (if not all) of the previous machine comprehension works use this objective function for training in machine comprehension, and we will adopt your advice to cite some of them.ßdummyTurker
13948ß0.0ß5) The reason we use Relu is that in this way the product of the two scores is also positive in this way.ßdummyTurker
13949ß0.0ßFor example, if both scores are negative, say -2.0 and -2.0, it is very low and should not be selected.ßdummyTurker
13950ß0.0ßHowever, their product is 4.0.ßdummyTurker
13951ß0.0ßAnother candidate with start and end score 1.0 and 2.0 respectively, which is larger than the previous one, but their product is only 2.0, which is less than the first one.ßdummyTurker
13952ß0.0ßThus, we clamp the value to be positive.ßdummyTurker
13953ß0.0ß6) We use off-the-shelf tri-gram Elasticsearch (Gormley and Tong, 2015) as the IR model (page 4, footnote 4).ßdummyTurker
13954ß0.0ß7) We use the unweighted average of each model as an ensemble model, which is also referred as committee method in machine learning community.ßdummyTurker
13955ß0.0ßWe use ensemble because in this ablation study the main focus is to evaluate the data quality, so we use a comparatively better model (ensemble in this work) to make this evaluation more indicative.ßdummyTurker
13956ß0.0ßWe appreciate your insightful comments!ßdummyTurker
13957ß0.0ßIn this paper, the model is initialized by the ground truth documents, which contain no distractor documents.ßdummyTurker
13958ß0.0ßThen, we use the model to select more positive documents (Algorithm 1).ßdummyTurker
13959ß0.0ßTherefore, even a document contains the right answer, it may not be selected if the model gives it a low document selection score.ßdummyTurker
13960ß0.0ßFor the limit of the space, we do not explain/illustrate the example, but we will make it clearer in the final version.ßdummyTurker
13961ß0.0ßThanks for your scrupulous indication of our grammatical errors.ßdummyTurker
13962ß0.0ßThere are some typos because of our carelessness.ßdummyTurker
13963ß0.0ßWe will make a throughout revision on them and invite a native English speaker to polish the whole paper.ßdummyTurker
13964ß0.0ßYour understanding is exactly right!ßdummyTurker
13965ß0.0ßwhich is also demonstrated in the analysis part of Section4.2 (628-636), but we will rephrase it to make it clearer.ßdummyTurker
13966ß0.0ß1) This is the original query obtained from the query log, we will make a revise on them to make is less distractive.ßdummyTurker
13967ß0.0ß2) The interactive way means we use ATTENTION mechanism to represent the document and question.ßdummyTurker
13968ß0.0ßConcretely, we use the embedding of the question to determine the weight of each word in the document (Equation (2)), which results in an attentive representation of the documents w.r.t.ßdummyTurker
13969ß0.0ßquestion.ßdummyTurker
13970ß0.0ß3) Yes, we select the spans with start and end position that has the highest start and end probability respectively.ßdummyTurker
13971ß0.0ßDuring inference period, we only consider those spans in which the end position >= start position.ßdummyTurker
13972ß0.0ß4) The current work only considers the documents that do not contain the answer as the negative documents, because these documents have a higher probability to be negative.ßdummyTurker
13973ß0.0ßBut your advice is really inspiring, so we will analyze this issue in-depth in the future work.ßdummyTurker
13974ß0.0ß5) The processing means that the questions that are either too long to proceed or the questions are obviously not a question.ßdummyTurker
13975ß0.0ßWe remove these questions so it is in accordance with previous methods on WebQuestions.ßdummyTurker
13976ß0.0ßThank you for the insightful comments!ßdummyTurker
13977ß0.0ß**Response-#1:Linearization**ßdummyTurker
13978ß0.0ßWe apologize for the confusion regarding linearization.ßdummyTurker
13979ß0.0ßFirst, note that our inference model q(z|x), the most important component of StructVAE, does not use linearization and indeed guarantees the syntactic well-formedness of MRs using the transition system(Sec.ßdummyTurker
13980ß0.0ß3.2.1).ßdummyTurker
13981ß0.0ßWe use linearization for the reconstruction model p(x|z) because it (1) allows us to use standard sequence-to-sequence models instead of additionally implementing a tree-structured encoder for z, and (2) we assume that enforcing strict tree-structure is less important when computing the probability of the NL using p(x|z).ßdummyTurker
13982ß0.0ßThis is in line with existing works of modeling structured inputs (e.g., Konstas et al.ßdummyTurker
13983ß0.0ß(2017) for AMR-to-text generation).ßdummyTurker
13984ß0.0ßPlease see Response-#3 for details on the prior.ßdummyTurker
13985ß0.0ß**Response-#2:Parameters**ßdummyTurker
13986ß0.0ß$\phi$ and $\theta$ don’t overlap.ßdummyTurker
13987ß0.0ß**Response-#3:Prior**ßdummyTurker
13988ß0.0ßThe prior p(z) is a standard LSTM-based LM over tokens in the linearized MR. We apologize and will present more details besides Appendix-D. We could have used a tree-structured model as well, but we chose not to simply for convenience.ßdummyTurker
13989ß0.0ßAdmittedly sampling from p(z) might not yield well-formed structures, but note that our core method **does not require** sampling from p(z).ßdummyTurker
13990ß0.0ßIn E.q.ßdummyTurker
13991ß0.0ß(4), latent MRs are sampled from the **inference model** during semi-supervised learning, which guarantees syntactic well-formedness (see Response-#1).ßdummyTurker
13992ß0.0ßFor the KL term (as part of the learning signal), since p(z) is trained on well-formed MRs, empirically we found it to be reliable enough in its core function of identifying “unnatural” code (illustrated in Example-3 in Tab.3).ßdummyTurker
13993ß0.0ßWe can also sample well-formed examples from p(z) using rejection sampling, checking well-formedness using an off-the-shelf syntax checker and rejecting any syntactically-incorrect samples.ßdummyTurker
13994ß0.0ßThis is useful when generating new examples (as in Appendix A).ßdummyTurker
13995ß0.0ßWe found most samples from p(z) are indeed well-formed (98.9% (95.3%) well-formed samples out of 100K samples on the ATIS (Django) domain).ßdummyTurker
13996ß0.0ß**Response-#4:Unlabeled-Data**ßdummyTurker
13997ß0.0ßWe totally agree that evaluating on extra-collected NL utterances besides those in existing datasets would be highly beneficial.ßdummyTurker
13998ß0.0ßWhile it might be relatively easy to get unlabeled queries in practical settings(e.g.ßdummyTurker
13999ß0.0ßthrough queries to a search engine), unfortunately most academic semantic parsing datasets do not feature large sets of *in-domain* unlabeled data, so experimentation in this setting is not trivial.ßdummyTurker
14000ß0.0ßThus, to perform controlled experiments without confounds, we had to resort to simulated experiments using existing datasets by holding out some labeled examples.ßdummyTurker
14001ß0.0ßWe do agree that experiments with actual additional unlabeled data would add value, however, and we’ll consider ways to add such experiments in the camera ready.ßdummyTurker
14002ß0.0ßTo C1 (baselines): We do have two strong baselines.ßdummyTurker
14003ß0.0ßOne baseline referred to as Seq2Seq-Copy in our paper is pointer-generator network (See et al, ACL 2017), which is one of the state-of-the-art models on summarization.ßdummyTurker
14004ß0.0ßThe residual LSTM (Prakash et al, arXiv: 1610.03098) referred by reviewer is another neural architecture for text generation.ßdummyTurker
14005ß0.0ßFrom the results reported in Gupta et al.ßdummyTurker
14006ß0.0ß(AAAI, 2018), one can see that Seq2Seq-Copy outperforms VAE-SVG (METEOR: 24.31 vs 22.2) on Quora datasets and VAE-SVG outperforms the residual LSTM (METEOR: 31.0 vs 27.0) on MSCOCO datasets.ßdummyTurker
14007ß0.0ßWe think, therefore, that the copy mechanism is an important mechanism for paraphrase generation (and thus a strong baseline), which is not included in the residual LSTM and VAE-SVG.ßdummyTurker
14008ß0.0ßThere are possibly other architectures for generation.ßdummyTurker
14009ß0.0ßBut the main motivation of our work is, with the generator-evaluator learning framework, to investigate whether we can achieve better performance than training a single generator (despite its specific architecture).ßdummyTurker
14010ß0.0ßAnother stronger baseline referred as to RbR in our paper is the generator reinforce-trained by ROUGE (Ranzato et al., ICLR 2016), which is exactly the core algorithm employed by the other paper referred by reviewer, Paulus et al.ßdummyTurker
14011ß0.0ß(2017, arXiv: 1705.04304).ßdummyTurker
14012ß0.0ßTo C2 (introduction of IRL): Yes, we will add more introduction of IRL in our revised version.ßdummyTurker
14013ß0.0ßTo C3 (RL training): Yes, we did encounter difficulties in the training of RL models in our experiments.ßdummyTurker
14014ß0.0ßWith a lot of effort, we find that reward rescaling and curriculum learning are particularly effective to make the RL training more efficient and stable, and we believe these techniques can be beneficial on other sequence learning tasks using RL.ßdummyTurker
14015ß0.0ßTo Q1 (intermediate reward): The reward given by the evaluator is only received at the end of sequence.ßdummyTurker
14016ß0.0ßFor the intermediate words, we run Monte-Carlo simulations to estimate the intermediate reward (value function) as shown in line 300, or detailed in the algorithm 1 in the appendix.ßdummyTurker
14017ß0.0ßTo Q2 (RbR in line 591): The RbR (Reinforced by ROUGE) refers to one of our baselines, which is an implementation of Ranzato et al.ßdummyTurker
14018ß0.0ß(ICLR 2016).ßdummyTurker
14019ß0.0ßTo C1: As stated in the first response to review #1, we have two reasonably strong baselines, both of which have superior performances than the Residual LSTM (COLING 2016) (we have cited it in the paper).ßdummyTurker
14020ß0.0ßIt should be emphasized that the main contribution of this work is the generator-evaluator framework for seq2seq learning, but not the design of generator itself.ßdummyTurker
14021ß0.0ßFor the rest of papers, EACL 2017, ICLR 2018 and EMNLP 2017, mentioned by the reviewer, we will definitely cite them in our revised version.ßdummyTurker
14022ß0.0ßHowever, since they have different problem settings and rely on different types of supervision, it is difficult to make a direct comparison (NOTE: none of them is compared with Residual LSTM either).ßdummyTurker
14023ß0.0ßEACL 2017: Their approach relies on bilingual data to produce paraphrase by pivoting over a shared translation in another language, while our work focus on learning to paraphrase on monolingual data.ßdummyTurker
14024ß0.0ßICLR 2018, EMNLP 2017: They tackle the problem of QA-specific paraphrasing with the guidance from an external QA system and associated evaluation metric, while we consider general purpose paraphrasing trained only with paraphrase pairs.ßdummyTurker
14025ß0.0ßTo C2: We fully agree with the reviewer that the lexical-based evaluation is not ideal, and that is exactly our motivation of proposing a machine learned evaluator.ßdummyTurker
14026ß0.0ßWe report our results in terms of the measures for reproducibility of our results by others.ßdummyTurker
14027ß0.0ßFurthermore, BLEU and METEOR are empirically proved to be not perfect but reliable in paraphrase identification by Madnani et al.ßdummyTurker
14028ß0.0ß(NAACL 2012).ßdummyTurker
14029ß0.0ßFor the human evaluation, we have two human annotators to score each pair of paraphrase, and their average agreement is moderate (kappa coefficient=0.44).ßdummyTurker
14030ß0.0ßTo C3: That is very good advice.ßdummyTurker
14031ß0.0ßWe have collected some paraphrase examples generated by different models given the same source sentence.ßdummyTurker
14032ß0.0ßOur finding is that RbM-SL can capture more precise paraphrase patterns and RbM-IRL generates more readable paraphrases, which is consistent to the human evaluation results.ßdummyTurker
14033ß0.0ßWe will include them in the revised version.ßdummyTurker
14034ß0.0ßTo Q1: With the copying mechanism of the generator, the current vocabulary size works well on the experimental data.ßdummyTurker
14035ß0.0ßThe generator can definitely scale to domain with a larger vocabulary when necessary.ßdummyTurker
14036ß0.0ßTo Q2: Both H and G are feed-forward networks (Parikh et al., EMNLP 2016).ßdummyTurker
14037ß0.0ßTo Q3: The classification accuracy of the evaluator in RbM-SL on Quora dataset is 87.10%, which is comparable to the results reported by Quora team (goo.gl/dPZ3Zd).ßdummyTurker
14038ß0.0ßWe did not test performance of the evaluator in RbM-IRL since it is for ranking paraphrase pairs rather than classification.ßdummyTurker
14039ß0.0ßTo Q4: The human annotators are intern students.ßdummyTurker
14040ß0.0ßDear reviewer, we appreciate your time and effort on our paper.ßdummyTurker
14041ß0.0ßThe reason to adopt the proposed algorithm for word embedding has mainly two as discussed in the paper: (1) new words/phrases are continuously generated after base model was generated, and also domain specific words are usually not available in the base model; (2) corpus of the base model is usually not accessible.ßdummyTurker
14042ß0.0ßTherefore, an incremental model generation methodology is necessary both for efficiency and feasibility of new model generation.ßdummyTurker
14043ß0.0ßThe vectors from two models cannot be assembled by simple concatenation because both models are not in the same vector space, as proven in paper’s section 2.ßdummyTurker
14044ß0.0ßVector space of one of the models has to be transformed to the other model’s space first to perform concatenation.ßdummyTurker
14045ß0.0ßWe believe that this orthogonality is not just due to random initialization.ßdummyTurker
14046ß0.0ßIn fact, even with constant weight initialization, the result models from two runs still support the observation: vectors of a same word from two models tend to be orthogonal.ßdummyTurker
14047ß0.0ßIt seems to us that weights of this network tend to settle in a vector space defined by the eigenvectors of the system serving as a local minima during training, and therefore the vectors appear to be orthogonal from separate runs.ßdummyTurker
14048ß0.0ßIn our approach, vectors for the common words both in the base model and new model are calculated by Eq.ßdummyTurker
14049ß0.0ß(23) and controlled by parameter r. So, practically, the resulting vectors could be perfectly matched with the original vectors, as demonstrated in Table 10 where the error comparing with Google model is 0.0%.ßdummyTurker
14050ß0.0ßAs the response to Weakness argument 1, simple concatenation without vector space transformation is not a correct approach because two models are in two different vector spaces.ßdummyTurker
14051ß0.0ßAs the response to Weakness argument 2, we believe that it is not due to different random initialization.ßdummyTurker
14052ß0.0ßWe observed same vector orthogonality with identical random initialization or even constant initialization.ßdummyTurker
14053ß0.0ßWe like this idea and would like to do experiment on it and add the result to the final version.ßdummyTurker
14054ß0.0ßWe anticipate that the combined model will have better performance because the assembling process will suppress the noise, assuming the split individual small corpus has large enough size to generate stabilized vectors.ßdummyTurker
14055ß0.0ßIf the split corpus is too small, of course the result will be mainly noise.ßdummyTurker
14056ß0.0ßDear reviewer, we appreciate your time and effort as well as advices on our paper.ßdummyTurker
14057ß0.0ßWe appreciate the reviewer’s comments and definitely we should expand “Introduction” section of the current paper to include more related work review, including Artetxe’s research.ßdummyTurker
14058ß0.0ßThank you for bringing Artetxe’s work to our attention.ßdummyTurker
14059ß0.0ßThough their approach is based on minimizing difference on matrix transformed vectors in the same way as many other studies do, our method is unique in two folds: (1) vector orthogonality is used as constraint in obtaining the transformation of two vector spaces; (2) we proposed incremental model generation algorithm which does not require the corpus used to generate base models.ßdummyTurker
14060ß0.0ßIndeed, our approach is applicable on bilingual settings and it could be an interesting task to benchmark our methodology against Artetxe and other researchers’ approaches.ßdummyTurker
14061ß0.0ßWe studied many similar approaches, such as Bamler and Mandt (2017), Hamilton et al.ßdummyTurker
14062ß0.0ß(2016), etc., our methodology is novel in exploiting vector orthogonality and incremental generation.ßdummyTurker
14063ß0.0ßAs demonstrated in the paper, our approach efficiently trains the new corpus only and adds new word vectors while preserving the accuracy of base model.ßdummyTurker
14064ß0.0ßWe surely will perform more extensive proof-reading and improve quality of final version of the current paper.ßdummyTurker
14065ß0.0ßIt is embarrassing.ßdummyTurker
14066ß0.0ßTable 8 was copied with wrong data.ßdummyTurker
14067ß0.0ßIt should be listed as below and will be updated in the final version.ßdummyTurker
14068ß0.0ßMinimum    Maximum    Mean     Std.ßdummyTurker
14069ß0.0ßDev.ßdummyTurker
14070ß0.0ß0.03            10.64          0.53.ßdummyTurker
14071ß0.0ß0.46ßdummyTurker
14072ß0.0ßDear reviewer, we appreciate your time and effort on our paper.ßdummyTurker
14073ß0.0ßWe do not agree with the reviewer that the random initialization is the reason of result vectors showing orthogonality.ßdummyTurker
14074ß0.0ßIn fact, even with constant initialization we still observed the same behavior.ßdummyTurker
14075ß0.0ßUsing the equation that the reviewer gave in the comments as an example, even we initialize the weights with random series of non-zero mean values, the resulting vectors of a word from two runs still tend to have zero dot product and show vector orthogonality.ßdummyTurker
14076ß0.0ßSo, the equation does not hold for word embedding training.ßdummyTurker
14077ß0.0ßBut we do agree with the reviewer that the local minima formed by the training process might play a role in generation of this general behavior.ßdummyTurker
14078ß0.0ßWe cannot agree with the reviewer that the method is implying transformation from a random vector to another random vector.ßdummyTurker
14079ß0.0ßThe vectors in different models hold invariant properties for example cosine similarity among words and distance between vectors, which is actually the foundation that one model can be transformed to another model’s space, as demonstrated in the Artetxe’s paper referenced by the reviewer’s argument.ßdummyTurker
14080ß0.0ßWe cannot agree on reviewer’s comment that vector orthogonality is not related with the proposed algorithm.ßdummyTurker
14081ß0.0ßWe use this property of vector orthogonality as extra constraint to form the process to solve the transformation matrix, which ensures the uniqueness of the solution as presented in the paper.ßdummyTurker
14082ß0.0ßThe raw articles are collected through paid service providers as listed in the “Tool” column of Table 1.ßdummyTurker
14083ß0.0ßThe sources for each corpus has one domain focus.ßdummyTurker
14084ß0.0ßFor example, articles for C7 are collected from telecommunication related websites.ßdummyTurker
14085ß0.0ßThe raw articles are then performed pre-processing steps such as cleaning, phrase segmentation etc.ßdummyTurker
14086ß0.0ßto form the final corpora.ßdummyTurker
14087ß0.0ßOnly 8 relations is used due to the limitation of entity pairs.ßdummyTurker
14088ß0.0ßWe will investigate more relations in future work.ßdummyTurker
14089ß0.0ßFigure 1 shows the process of our method and you can ignore the data it contents.ßdummyTurker
14090ß0.0ßWe will be improved soon.ßdummyTurker
14091ß0.0ßWell, in fact these works are different with ours because our purpose is to find useful rules from both KBs and texts.ßdummyTurker
14092ß0.0ßThese works mainly leverage KB facts and text jointly for a specific task such a relation extraction while RE is just one application of our mined rules.ßdummyTurker
14093ß0.0ßPRA mines rules on a KB via random walk and we don’t think it is able to mining rules by combining KBs and Texts (As said in PRAßdummyTurker
14094ß0.0ßReply to weakness argumentßdummyTurker
14095ß0.0ßour statement is “BoD is a better choice as atoms of rules.”, not “best”.ßdummyTurker
14096ß0.0ßHere we mean that BoD is better than word sequence because word sequence can not cover similar expressions.ßdummyTurker
14097ß0.0ßIn order to find the sentences, there are four heuristic matching rulesßdummyTurker
14098ß0.0ßWe chose the relations which contains enough entity pairs in Wikidata.ßdummyTurker
14099ß0.0ßSince we collect sentence from Wikipeida articles where most of the entities are already linked to wikidata or can be found using string match, NER is not necessary in this work.ßdummyTurker
14100ß0.0ßHowever, NER and entity linking is needed if applying our method to free texts collected from the Internet.ßdummyTurker
14101ß0.0ßThe BoD pattern are approximated using the locations of words.ßdummyTurker
14102ß0.0ßIn other words, it is the center point of a cluster of relation mentions.ßdummyTurker
14103ß0.0ßBoD has nothing to do with similarity between relations.ßdummyTurker
14104ß0.0ßIt is the representation of the clustering results and relation mention similarity is used for mention clustering.ßdummyTurker
14105ß0.0ßThis will be dealt with in next version.ßdummyTurker
14106ß0.0ßFigure 1 shows only the process of our method and the contents are not actual data.ßdummyTurker
14107ß0.0ßA flat clustering with larger number of clusters should work as well but we haven’t investigated the difference comparing with hierarchical clustering yet.ßdummyTurker
14108ß0.0ßIn practice, there is always a cluster that occupy a very large portion of the data while the others usually contain a few samples, therefore, we cluster the big cluster again to find more fine-grained clusters.ßdummyTurker
14109ß0.0ßThe entity information is not included in BoD.ßdummyTurker
14110ß0.0ßHere we use match mean the words and their locations in a sentence matches the BoD patterns of a relation.ßdummyTurker
14111ß0.0ß0.8 is selected empirically.ßdummyTurker
14112ß0.0ßA threshold is need here anyway (all association rule mining methods need such thresholds).ßdummyTurker
14113ß0.0ßHigher threshold lead to high precision but low recall.ßdummyTurker
14114ß0.0ßThe precision of mined facts shows that 0.8 is a not bad choice.ßdummyTurker
14115ß0.0ß3 levels is chosen because we find there is no sufficient transactions/sentences in a cluster when we go deeper.ßdummyTurker
14116ß0.0ßT is the transaction set of the sentences in a cluster.ßdummyTurker
14117ß0.0ßThe support and confidence can be described by the following examples.ßdummyTurker
14118ß0.0ßIf a cluster contains 160 sentences (transactions).ßdummyTurker
14119ß0.0ßThere are 100 transactions contain the antecedent(subj_Citizenship_America), 120 transactions contain the consequent's property (obj_Citizenship), and 30 transactions contain the antecedent and the consequent(obj_Citizenship_America).ßdummyTurker
14120ß0.0ßThe support and confidence of the rule(BoD∧subj_Citizenship_America --> obj_Citizenship_America) are respectively 30/120 and 30/100.ßdummyTurker
14121ß0.0ßIf two rule share the same consequent, the one with higher confidence will preserved.ßdummyTurker
14122ß0.0ßThis improvement is described as section 4.ßdummyTurker
14123ß0.0ßIn RLSW, mentions of relations are words between e1 and e2, and the beta distribution is used in fit the distribution of words.ßdummyTurker
14124ß0.0ßNEARM uses the words before and after the entities and uses the Gauss distribution to fit the distribution of the words.ßdummyTurker
14125ß0.0ßFirstly texts are transformed into vectors according to their fitness w.r.t BoDs and then properties values are concatenated to these vectors in a one-hot manner.ßdummyTurker
14126ß0.0ßIn practice, there is no adequate sentences support long rules.ßdummyTurker
14127ß0.0ßS1-S5 are in Table 9 and Table 10.ßdummyTurker
14128ß0.0ßFigure 2 is not a real search box but a just a description of the variability in definitions seen.ßdummyTurker
14129ß0.0ßWe emphasize that our models allow for definitions of variable thus enabling real-world deployability.ßdummyTurker
14130ß0.0ßThis is indeed a challenging problem and it is precisely why we introduce this task and provide the first set of non-trivial models that significantly improve over established competing models which demonstrate superior performance on standard varieties.ßdummyTurker
14131ß0.0ßWe clarify that our IR baseline has the same setting where the query is a definition sought to be matched against <def, word> tuples to return the appropriate words -- so it is a fair comparison.ßdummyTurker
14132ß0.0ßPlease see responses to Argument 1 (for reviewer 1) and Question 3 (for reviewer 1) where we discuss this as well.ßdummyTurker
14133ß0.0ßWhile techniques like CCA (and for that matter LSTMS) might have wide applicability, we believe the novelty of our method is holistic -- we show how to construct slang embeddings, embed slang definitions and then learn to rank using CCA, KernelCCA, and DeepCCA as a unified framework.ßdummyTurker
14134ß0.0ßThe test set is close to $2000$ definitions.ßdummyTurker
14135ß0.0ßMoreover, we establish the statistical significance of all of our results using randomization tests which we explicitly mention in the paper (along with p-values).ßdummyTurker
14136ß0.0ßIt is worth noting that while previous works used a much smaller test set of $500$ definitions we have a test-set that is at-least 3 times larger.ßdummyTurker
14137ß0.0ßYes, we use the same evaluation setting as Hill et.al for consistency.ßdummyTurker
14138ß0.0ßWe agree that incorporating such semantics would be useful but we see that as a separate research question.ßdummyTurker
14139ß0.0ßIndeed, we acknowledge that our proposed method might also be used in the case of standard reverse dictionary when the number of definitions per average is not only noisy but lower than that in the data used by Hill et.al.ßdummyTurker
14140ß0.0ßAcknowledging this broader view, in this work, our primary goal was to show the applicability of our method on slang where the use case is quite real rather than artificially concocted.ßdummyTurker
14141ß0.0ßThat said, we will investigate this broader view-point as well.ßdummyTurker
14142ß0.0ß"While Wiktionary might contain some slang, it does not contain a large fraction of other slang words like ""sushit"" (which is slang for bad sushi)."ßdummyTurker
14143ß0.0ßIt is precisely our motivation for using Urban Dictionary which almost exclusively focuses on such non-standard usages and expressions.ßdummyTurker
14144ß0.0ßLine 129 elaborates some current approaches to the problem and is not a criticism.ßdummyTurker
14145ß0.0ßIndeed.ßdummyTurker
14146ß0.0ßWe are aware of mean reciprocal rank (MRR) and other metrics like NDCG, but as you noted we stuck to what was used in prior work.ßdummyTurker
14147ß0.0ßWe do not, however, believe our conclusions on the relative performance of various models would be different even if we used other ranking metrics like NDCG or MRR.ßdummyTurker
14148ß0.0ßWe can, of course, add these metrics given the opportunity.ßdummyTurker
14149ß0.0ßWhile Twitter undoubtedly models informal language, we noted that the coverage of slang is actually quite poor.ßdummyTurker
14150ß0.0ßAs we noted in Lines 144-152, twitter embeddings trained on 7B tokens only captured 1.5% of the top 100K slang words in Urban Dictionary.ßdummyTurker
14151ß0.0ßThis is precisely the reason why we propose learning slang embeddings for this task.ßdummyTurker
14152ß0.0ßFirst, we clarify that we tried extensive hyper-parameter tuning on the LSTM models including varying the number of layers, adding dropout layers, as well as multiple optimization algorithms including ADAM, AdaDelta and SGD (with/without momentum) and multiple loss functions including ranking loss.ßdummyTurker
14153ß0.0ßHowever, in all these cases the LSTM models performance was similar to the best possible result we reported.ßdummyTurker
14154ß0.0ßWhile in theory an LSTM which can approximate arbitrarily complex functions are in general more powerful, we believe they suffer from poor sample efficiency.ßdummyTurker
14155ß0.0ßThis suggests that in the wake of limited and noisy training data, models like CCA which are less complex and in some sense impose an inductive bias will likely perform better.ßdummyTurker
14156ß0.0ßYou’re right, there are more straightforward methods to solve the echoing problem than hard negative mining.ßdummyTurker
14157ß0.0ßOne of such methods could be just filtering out on test time responses that have high word/semantic overlap with the context.ßdummyTurker
14158ß0.0ßBut we believe that hard negative mining considering contexts as possible negative responses (HN_{r+c}) is the most natural way to learn the following things: 1) how to respond relevant 2) how to respond appropriately, i.e.ßdummyTurker
14159ß0.0ßso that responses are not just “copies” or paraphrases of the context when it is bad from the natural conversation’s point of view.ßdummyTurker
14160ß0.0ßStrategies like random negative sampling (RN) or hard negative mining considering only responses as possible negative samples (HN_r) produce the dialog model that ranks responses mostly by similarity/relevance (see Table 3).ßdummyTurker
14161ß0.0ßOn the other hand, filtering on test time (mentioned above) does not allow to return responses very similar to the context when appropriate (for instance, it is quite natural to answer “Hello” to “Hello”).ßdummyTurker
14162ß0.0ßYou’re also right about the lack of evidence that responses semantically similar to the context are often negative and are highly ranked by the dialog model.ßdummyTurker
14163ß0.0ßHowever, we believe that responses that have _very high_ word/semantic overlap with the context in most cases are bad from the natural conversation’s point of view and generally affect the model quality (see Table 2 and Table 3).ßdummyTurker
14164ß0.0ßThat’s why as the first step we use just contexts as negative examples and not use phrases that are somehow semantically similar to the contexts.ßdummyTurker
14165ß0.0ßIndeed, on the one hand we consider cases where the model returns the context itself as the response, on the other hand – we consider cases where the model returns inappropriate responses that just have high lexical/semantic similarity to the context.ßdummyTurker
14166ß0.0ßIt was correctly noticed in the Weakness argument 1 that there's no enough evidence to claim that any semantically similar response is necessary a bad one, so our model training was based only on negative responses equivalent to the context.ßdummyTurker
14167ß0.0ßNevertheless, judging by obtained results (Table 3) we hypothesize that our model filters out not only copies of the context, but inappropriate responses having high lexical and/or semantic similarity.ßdummyTurker
14168ß0.0ßThe problem we're trying to solve is to force the model to return responses that are not just relevant/similar to the given context (a property that a copy/paraphrase of the context satisfies, too), but are actually _appropriate_ as a response in a dialogue.ßdummyTurker
14169ß0.0ß"To the best of our knowledge, in papers on conversation systems researchers prefer to avoid the term ""relevant"" (which came from IR) when describing positive responses  because appropriate response is not just information/lexically/semantically similar to the context."ßdummyTurker
14170ß0.0ß"We'd propose that the word/literal overlap is a special case of semantic similarity, while ""relevance between the context and response"" could just mean ""appropriateness"" in dialogs and differs from other metrics you mentioned."ßdummyTurker
14171ß0.0ßIndeed, we should provide a comparison to the suggested baseline as well.ßdummyTurker
14172ß0.0ßWe actually already did this study and found several problems: 1) one needs to carefully adjust thresholds on semantic or lexical similarity, 2) this method misses a lot of false positives – for example, interrogative and affirmative forms of the given context, 3) overall, it doesn't improve quality significantlyßdummyTurker
14173ß0.0ßYes, HN_{r+c} denotes the model that trained with hard negative mining approach that considers both responses and contexts as possible negative examples.ßdummyTurker
14174ß0.0ßWe tried to train the model with different m values and achieved one of the best results with 0.05 value.ßdummyTurker
14175ß0.0ßm has the same meaning in Eq 2,3,4.ßdummyTurker
14176ß0.0ßThe main hypothesis in this paper is: “with the domain difference between source and target minimized, the performance of cross-domain sentiment classification can be significantly improved by exploiting unlabeled target with semi-supervised learning techniques”.ßdummyTurker
14177ß0.0ßWe do not intend to argue that each component in the current setting is the best possible choice.ßdummyTurker
14178ß0.0ßInstead, the proposed framework is general in which different components can be improved independently in future research.ßdummyTurker
14179ß0.0ßWe have tested our hypothesis with a series of experiments under different experimental settings.ßdummyTurker
14180ß0.0ßThe results support our hypothesis and demonstrate the advantages of the proposed framework in comparison with other state-of-the-art methods.ßdummyTurker
14181ß0.0ßAs mentioned in section 3.2, any neural network can be potentially used as encoder.ßdummyTurker
14182ß0.0ßComputational efficiency is one reason why we use CNN instead of LSTM.ßdummyTurker
14183ß0.0ßIn addition, CNN has proven to be very effective in previous works on document-level sentiment classification as long-term sequential information is not very necessary for this task.ßdummyTurker
14184ß0.0ßAgain, this is not the main focus of our work, and the encoder component can be replaced by another more advanced method.ßdummyTurker
14185ß0.0ß\beta=1-\alpha can be used when the feature adaptation loss is numerically similar to the entropy loss.ßdummyTurker
14186ß0.0ßHowever, our case is different where the cross-entropy loss is numerically similar to entropy loss while the feature adaption loss tends to be very small due to the choice of the objective function Eq.ßdummyTurker
14187ß0.0ß(2) (few hundred times smaller than the entropy loss and cross-entropy loss).ßdummyTurker
14188ß0.0ßWe want to set \alpha and \beta to certain values such that the loss from each component contributes similarly to the final loss.ßdummyTurker
14189ß0.0ßTherefore, we adopted grid search, tuning \alpha from 50 to 500 with step size of 50, and \beta from 0.1 to 1 with step size of 0.1 on the development set.ßdummyTurker
14190ß0.0ßThis issue can be easily addressed by adding the comparison of computation times in the final version.ßdummyTurker
14191ß0.0ßThanks for pointing out these related works.ßdummyTurker
14192ß0.0ßWe will include them and adjust our claims in the final version.ßdummyTurker
14193ß0.0ßBelow are the details on how the baseline models (mSDA, AuxNN, ADAN) were re-implemented.ßdummyTurker
14194ß0.0ß1) For mSDA, we used the code provided by the author.ßdummyTurker
14195ß0.0ßmSDA is a close-form solution with no randomness, thus it is widely used as a baseline in domain adaption works.ßdummyTurker
14196ß0.0ßThere are two parameters that need to be specified — the number of stacked layers L and the corruption probability P. As suggested by the author, we experimented with L = {2,3,4,5}, and P = {0.2, 0.3, 0.4, 0.5, 0.6, 0.7} on the development set of one task, and set L=3 and P=0.5 empirically.ßdummyTurker
14197ß0.0ßAlso, we can reproduce its results on the Amazon benchmark as reported in many previous papers.ßdummyTurker
14198ß0.0ß2) For AuxNN, we modified the code provided by the author.ßdummyTurker
14199ß0.0ßThe CNN encoder used in this model is the same as ours, thus the implementation of our CNN encoder is exactly the same as theirs.ßdummyTurker
14200ß0.0ßWe did not modify this part of their code.ßdummyTurker
14201ß0.0ßFor fair comparison, we used the same optimization algorithm (RMSProp) for our model and all other neural baselines with default hyper-parameters without specific tuning.ßdummyTurker
14202ß0.0ßThere are no other hyper-parameters for this model to be tuned.ßdummyTurker
14203ß0.0ßWe only modified their training procedure minimally.ßdummyTurker
14204ß0.0ßIn their original code, a development set from the target domain is used to determine whether the model should be saved for evaluation.ßdummyTurker
14205ß0.0ßAs discussed in our supplementary material, this setting is problematic as target labels are not available in unsupervised domain adaption.ßdummyTurker
14206ß0.0ßWe modified this part such that the model selection procedure is the same as ours (described in Section A.1 of the supplementary material).ßdummyTurker
14207ß0.0ß3) For ADAN, we reimplemented the model by ourselves as the source code is not available.ßdummyTurker
14208ß0.0ßFor fair comparison, we replace its encoder (a feedforward network) with our CNN encoder.ßdummyTurker
14209ß0.0ßThe optimization algorithm and the model selection procedure are the same as our method and other neural baselines.ßdummyTurker
14210ß0.0ßFor other hyper-parameters and training details, we strictly follow the settings in their paper as those hyper-parameter values are commonly used which should be robust on a different dataset.ßdummyTurker
14211ß0.0ßAs you mentioned, many proposed methods normally skip the mapping procedure or replace the mapping induced by a characteristic kernel by a neural network as what we did.ßdummyTurker
14212ß0.0ßThis may not precisely capture the information of distributions, but has proven to be effective in previous works even though the performance is not guaranteed (we will phrase our claims in careful terms).ßdummyTurker
14213ß0.0ßWe intend to estimate the distance between distributions as the distances between the mean vectors (though this may not be a precise estimation).ßdummyTurker
14214ß0.0ßOnce we obtain these two mean vectors, different distance metrics for vectors can be applied.ßdummyTurker
14215ß0.0ßWe have also experimented with other metrics such as Euclidean distance and cosine distance, and found that symmetric KL distance performs the best empirically.ßdummyTurker
14216ß0.0ßReply_to_weakness_argument_1: RNNs are not suitable for modeling the distance information regarding the target because they do not have a special mechanism to give more emphasis on certain words; unlike the max-pooling of CNNs.ßdummyTurker
14217ß0.0ßRNNs receive every input word consecutively, and thus the model training can be affected by distant words that are not relevant to the target.ßdummyTurker
14218ß0.0ßHowever, by adopting another CNN whose max-pooling gives a special attention to the target word and words nearby as in TCNN-hard, we relieve the adverse effects of irrelevant words on a given target.ßdummyTurker
14219ß0.0ßEven the attention mechanism cannot resolve this issue because entirely resorting to the attention mechanism to capture the relevance of words with regard to the target is less straightforward than explicitly including the distance information between each word and the target.ßdummyTurker
14220ß0.0ßReply_to_weakness_argument_2: Refer to Table 2 of supplementary material.ßdummyTurker
14221ß0.0ßWe used ReLU and the size of the pooling layer is '# word in text - kernel size + 1'.ßdummyTurker
14222ß0.0ßReply_to_question_1: In TCNN-soft, the distance of each word from a target is embedded as a vector.ßdummyTurker
14223ß0.0ßWe do not impose any constraint on the distance embedding vectors, and we train them by first randomly initializing them.ßdummyTurker
14224ß0.0ßThrough our learning process, the distance-concatenated inputs are fed into a CNN, and we expect the CNN to be trained so that the distance information regarding the target is reflected in the distance embedding vectors.ßdummyTurker
14225ß0.0ßSince we optimize this CNN by minimizing the cross-entropy loss, we do not need to modify the loss function.ßdummyTurker
14226ß0.0ßReply_to_question_2: The slope of TCNN-soft around 90~100% in Figure 2(right) is steeper than that of TCNN-hard.ßdummyTurker
14227ß0.0ßTherefore, we anticipate that as data get larger, the performance of TCNN-soft will improve in terms of macro-F1.ßdummyTurker
14228ß0.0ßThis is not as obviously displayed in Figure 2(left), but as macro-F1 is a measure of how well sentiments are predicted in a balanced way, it is more practical to improve macro-F1 compared with accuracy.ßdummyTurker
14229ß0.0ßReply_to_weakness_argument_1: In the paper that proposed TD-LSTM, the model is tuned on test data, but not on validation data.ßdummyTurker
14230ß0.0ßThis can be inferred not only from their paper and their published source code.ßdummyTurker
14231ß0.0ßWe tuned all baselines and our methods on validation data while holding out test data.ßdummyTurker
14232ß0.0ßThis explains the reason for relatively lower performance of TD-LSTM, BILSTM-ATT-G, and TDparse compared with the original paper.ßdummyTurker
14233ß0.0ßReply_to_weakness_argument_2: To save space, we squeezed the figures and tables, while not violating the rules for the font size.ßdummyTurker
14234ß0.0ßReply_to_weakness_argument_3:We could not include notational details due to page limits.ßdummyTurker
14235ß0.0ßWe will add a reference to the CNN on which our model is based (Kim,2014)ßdummyTurker
14236ß0.0ßReply_to_weakness_argument_4: Figure 4(a) and 4(b) are representations of the distance embedding vectors learned from single-target dataset and multi-target dataset, respectively; not averaged.ßdummyTurker
14237ß0.0ßTCNN-soft is used because it is the only method that models the distance embeddings.ßdummyTurker
14238ß0.0ßAlthough it is not feasible to perform such analyses with TCNN-hard, we can indirectly infer similar observations by analyzing accuracy fluctuations according to the size of the “neighbor window”.ßdummyTurker
14239ß0.0ßRefer to line 195-217 and Figure 2(a) in our supplementary material.ßdummyTurker
14240ß0.0ßReply_to_weakness_argument_5: Looking at the entire sentence, we agree with the reviewer’s comment that the author is being sarcastic about the price of PSP being too expensive, but the sentiment toward the target PSP itself is neutral.ßdummyTurker
14241ß0.0ßBILSTM-ATT-G showed incorrect prediction for this example because the model is affected by a distant but powerful word ‘epic’.ßdummyTurker
14242ß0.0ßHowever, as our proposed method explicitly considers the distance information and focuses more on the words that are closer to the target, our method could avoid an adverse effect from the distant word ‘epic’.ßdummyTurker
14243ß0.0ßWe will fix the references according to your advice.ßdummyTurker
14244ß0.0ßThanks.ßdummyTurker
14245ß0.0ßReply_to_weakness_argument_1: We can conduct experiments on other dataset such as reviews, but sizes of English data are relatively small (2000~4000) (dataset used in SemEval-2016 task 5) to train deep learning models.ßdummyTurker
14246ß0.0ßTherefore, we selected the most widely used benchmark dataset for TLSA.ßdummyTurker
14247ß0.0ßReply_to_weakness_argument_2: Assuming that “the sentiment of the target itself” refers to the sentiment bias on the target, the answer to the question depends on the dataset.ßdummyTurker
14248ß0.0ßIn the single-target dataset, CNN performs better than TCNN-hard^nbr, which means that the sentiment of the target itself is not very crucial in the dataset.ßdummyTurker
14249ß0.0ßIn contrast, in the multi-target dataset, TCNN-hard^nbr shows superior performance than CNN, implying that the sentiment bias for the target itself in the multi-target dataset should not be overlooked.ßdummyTurker
14250ß0.0ßWe conjecture that this is because the multiple-target data were collected from tweets related to UK elections; single-target data are from general topics.ßdummyTurker
14251ß0.0ßReply_to_weakness_argument_3: We used the embedding vectors as evidence to support our claim that closer words to the target are more influential to the sentiment on the target.ßdummyTurker
14252ß0.0ßFurthermore, we could project the distance embedding vectors to see whether the vectors are clustered according to the corresponding distance.ßdummyTurker
14253ß0.0ßReply_to_weakness_argument_4: We reported in supplementary material the hyper parameters of our models for reproducibility.ßdummyTurker
14254ß0.0ßWe select the best performing model of TDparse for each dataset: TDParse+ (m) for the single-target dataset and TDPWindow-12 for the multi-target dataset.ßdummyTurker
14255ß0.0ßRegarding the low performance of TDParse, we wrote comments about this issue for the first comment of Reviewer 1.ßdummyTurker
14256ß0.0ßCheck our comment there.ßdummyTurker
14257ß0.0ßReply_to_question_1: We initialize the vectors using the uniform distribution with a range from -0.05~0.05.ßdummyTurker
14258ß0.0ßThe number of data having larger distance than 20 is 1,142/6,940 for the single-target dataset, and 4,051/12,587 for the multi-target dataset.ßdummyTurker
14259ß0.0ßIn the single-target dataset, the similarity between distance embedding vector 2 and 4 is 0.478 and that between 2 and 24 is -0.739.ßdummyTurker
14260ß0.0ßIn the multi-target dataset, the similarity between 2 and 4 is -0.019 and that between 2 and 24 is -0.128.ßdummyTurker
14261ß0.0ßBinning may not be a good choice because of the potential information loss.ßdummyTurker
14262ß0.0ßIt is better to let the model choose the important features rather than manually binning the features.ßdummyTurker
14263ß0.0ßReply_to_question_2: We conduct paired t-test between the results of TCNN-hard and the highest results of the comparison models.ßdummyTurker
14264ß0.0ßReply_to_question_3: Refer to Table 2 of supplementary material.ßdummyTurker
14265ß0.0ßThe percentage of the document outside the neighbor window is 79.3%.ßdummyTurker
14266ß0.0ßAll our ROUGE scores are reported by the official ROUGE script and have a 95% confidence interval with at most ±0.33 in table 1 and ±0.24 in table 2.ßdummyTurker
14267ß0.0ßIf the non-news article does not have severe out-of-vocabulary issue, we expect our method to do a reasonable job.ßdummyTurker
14268ß0.0ßWe will show some examples in the final paper.ßdummyTurker
14269ß0.0ßOur contribution is to combine the extractive summarization and the abstractive summarization.ßdummyTurker
14270ß0.0ßHence our method can be used on any other abstractive summarization model that has attention mechanism, not only for See et al.ßdummyTurker
14271ß0.0ß2017.ßdummyTurker
14272ß0.0ßAll the results in table 1 and 2 are evaluated using full-length ROUGE.ßdummyTurker
14273ß0.0ßWe truncate the ground-truth summaries at 100 tokens for training which results in only 4% summaries being truncated.ßdummyTurker
14274ß0.0ßSo our model can predict complete sentences to generate summaries.ßdummyTurker
14275ß0.0ßFor the human evaluation, we didn’t truncate the summaries so the readability will not be affected.ßdummyTurker
14276ß0.0ß1.ßdummyTurker
14277ß0.0ßStandard extractive summarization aims to select sentences to form a final summary; while our extractor only provides fixed sentence-level attention to facilitate the pointer-generator model for abstractive summarization.ßdummyTurker
14278ß0.0ßWe use ground-truth maximizing Rough-recall which is different from the ground truth of standard extractive summarization.ßdummyTurker
14279ß0.0ßHence, it is less meaningful to compare the standard extractive summarization performance.ßdummyTurker
14280ß0.0ß2.ßdummyTurker
14281ß0.0ßAs mentioned above, we use ROUGE recall to encourage recall of important sentence.ßdummyTurker
14282ß0.0ßOn the other hand, the abstracter needs to generate most important information without being redundant so ROUGE F-1 score is more appropriate for evaluating abstractive results.ßdummyTurker
14283ß0.0ß3.ßdummyTurker
14284ß0.0ßAll our ROUGE scores are reported by the official ROUGE script and have a 95% confidence interval with at most ±0.33 in table 1 and ±0.24 in table 2.ßdummyTurker
14285ß0.0ßWe show the article and 6 summaries (reference summary, 4 generated summaries and a random summary) to each human evaluator.ßdummyTurker
14286ß0.0ßThe random summary is a reference summary randomly picked from other articles and is used as a trap.ßdummyTurker
14287ß0.0ßWe reject all the evaluations that score the informativity of the random summary as 3, 4 and 5.ßdummyTurker
14288ß0.0ßBy using this trap mechanism, we can ensure a much better quality of our human evaluation.ßdummyTurker
14289ß0.0ßFor each example, we first ask 5 human evaluators to evaluate.ßdummyTurker
14290ß0.0ßHowever, for those articles that are too long, which are always skipped by the evaluators, it is hard to collect 5 reliable evaluations.ßdummyTurker
14291ß0.0ßHence, we collect at least 3 evaluations for every example.ßdummyTurker
14292ß0.0ßFor each summary, we average the scores over different human evaluators.ßdummyTurker
14293ß0.0ßYes, the evaluator evaluated the reference summary as well.ßdummyTurker
14294ß0.0ßThe scores are 3.43, 3.61 and 3.62 for informativity, conciseness and readability, respectively.ßdummyTurker
14295ß0.0ßThe reference summaries get the best score on conciseness since the recent abstractive models tend to copy sentences from the input articles.ßdummyTurker
14296ß0.0ßHowever, our model learns well to select important information and form complete sentences so we even get slightly better scores on informativity and readability.ßdummyTurker
14297ß0.0ßThank you for your review.ßdummyTurker
14298ß0.0ßWe did not work on French, but our results clearly show that ourßdummyTurker
14299ß0.0ßsystem works.ßdummyTurker
14300ß0.0ßIt will clearly not work as well as the previouslyßdummyTurker
14301ß0.0ßpublished supervised systems, we don't believe that comparing theßdummyTurker
14302ß0.0ßexact numbers is very interesting.ßdummyTurker
14303ß0.0ßThank you for your suggestion about bolding the best figures, weßdummyTurker
14304ß0.0ßwill do this for the camera ready.ßdummyTurker
14305ß0.0ßThanks for your advice, I will add the necessary examples in the future paper.ßdummyTurker
14306ß0.0ßSince the baseline paper used the NYT data set, for comparison purposes, we only use this data set.ßdummyTurker
14307ß0.0ßIn the following work, we will try to verify the validity of our method on other data sets.ßdummyTurker
14308ß0.0ßThe hyper-parameters in this paper are determined by referenceing  the choice of  baseline papers combined with our  verifying in experiment .ßdummyTurker
14309ß0.0ßWe will redraw the two figures and remove the different colors in the figure to make it suitable for printed paper.ßdummyTurker
14310ß0.0ßThe hyper-parameters in this paper are determined by referenceing  the choice of  baseline papers combined with our  verifying in experiment.ßdummyTurker
14311ß0.0ßBaseline papers used a grid search to determine the optimal parameters.ßdummyTurker
14312ß0.0ßThe grid search is an exhaustive search to specify parameter values.ßdummyTurker
14313ß0.0ßThe optimal learning algorithm is obtained by optimizing the parameters of the estimation function through a cross-validation method.ßdummyTurker
14314ß0.0ß"That is, combine the possible values of each parameter and list all possible combinations to generate a ""grid."""ßdummyTurker
14315ß0.0ßEach combination was then used for model training and performance was assessed using cross-validation.ßdummyTurker
14316ß0.0ßAfter the fitting function tries all the parameter combinations, it returns an appropriate classifier and automatically adjusts to the best combination of parameters.ßdummyTurker
14317ß0.0ßThe method we proposed is effective, because of the following aspectsßdummyTurker
14318ß0.0ßIn the future, we will verify the effectiveness of our method on more data sets and conduct further experiments to verify the contribution of our method in removing the influence of noise.ßdummyTurker
14319ß0.0ßThank you so much for your great efforts and time on reviewing our paper.ßdummyTurker
14320ß0.0ßThe attention model of Lin can only assign different weights to each sentence according to some degree of credibility.ßdummyTurker
14321ß0.0ßWeigh information alone does not prove which sentence is noise.ßdummyTurker
14322ß0.0ßOur IATT method not only assigns different weights, but also assembles the sentence vectors one by one in descending order of weight, and then tests each combination vector for classification to find the optimal combination vector.ßdummyTurker
14323ß0.0ßAlthough we assign weights as the same way as Lin, we does not make all the sentences to participate in the model training, because we believe that the optimal combination vector we have found contains all positive sentences without noise.ßdummyTurker
14324ß0.0ßThe data of the PCNN+ATT method in our precision-recall curves comes from the code provided by Lin (the author) on GitHub.ßdummyTurker
14325ß0.0ßWe run his code over and over again, but we got the result showed in our paper.ßdummyTurker
14326ß0.0ßWe are also confused about the disparity with the result reported by Lin et al.ßdummyTurker
14327ß0.0ßWe will proofread the English writing and grammar carefully and turn to a native English speaker for help if necessary.ßdummyTurker
14328ß0.0ßWe designed a comparison of Precision @ N for different baselines in our original paper.ßdummyTurker
14329ß0.0ßConsidering the space of paper, we did not show it in the submitted paper.ßdummyTurker
14330ß0.0ßThank you for your very nice comments on our work.ßdummyTurker
14331ß0.0ßThe attention model of Lin can only assign different weights to each sentence in the sentence set according to some degree of credibility.ßdummyTurker
14332ß0.0ßThe purpose of their model is to alleviate the influence of noise sentences, but can’t exclude them completely, because all sentences including the noise sentences participate in the training process.ßdummyTurker
14333ß0.0ßOur IATT method not only assigns different weights to each sentence on the basis of Lin, but also assembles the sentence vectors one by one in descending order of weight, and then tests each combination vector for classification to find the optimal combination vector.ßdummyTurker
14334ß0.0ßWe believe that the optimal combination vector we have found contains  all positive sentences without noise sentences.ßdummyTurker
14335ß0.0ßIn a nutshell, the method of Lin is to reduce the influence of the noise sentences, and our method is to remove the influence of the noise sentences.ßdummyTurker
14336ß0.0ßThe query vector “r” refers to the distributed representation of the relation r, which is used to score how each sentence and the relation r matches.ßdummyTurker
14337ß0.0ßWe will proofread the typos carefully and turn to a native English speaker for help if necessary.ßdummyTurker
14338ß0.0ßIn the method of Mintz and MIML, the sentence features are derived from NLP tools, which inevitably brings errors, and these errors will propagate during the training.ßdummyTurker
14339ß0.0ßour method have adopted PCNN to extract sentence features automatically avoiding errors propagating caused by NLP tools.ßdummyTurker
14340ß0.0ßIn addition, our method focus on trying to reduce the impact of error labeling, both of the previous methods have encountered this problem.ßdummyTurker
14341ß0.0ßThe method proposed by Zeng only selects the most likely one sentence for each entity pair in training.ßdummyTurker
14342ß0.0ßThis method is free from the influence of noise sentences, but it will lose a large amount of information containing in neglected sentences.ßdummyTurker
14343ß0.0ßThe ATT method proposed by Lin assigns different weights to each sentence in the sentence set according to some degree of credibility.ßdummyTurker
14344ß0.0ßThis method is effective, but it only alleviates the influence of noise sentences, can't exclude them completely.ßdummyTurker
14345ß0.0ßThe data of PCNN+ONE method in our precision-recall curves comes from the code we write by ourselves according to the paper of Zeng et al (2015); The data of the PCNN+ATT and the PCNN+AVE in our precision-recall curves comes from the code provided by the author on GitHub; The data of Mintz and MIML method comes from Lin’s experiment.ßdummyTurker
14346ß0.0ßWe propose a general method to convert a semantic network (where concepts are represented as discrete nodes in graphs) into a semantic space (where concepts are represented as vectors in a real-valued muti-dimensional space).ßdummyTurker
14347ß0.0ßIn the latter, the similarity between two words is given by the cosine between the respective vectors.ßdummyTurker
14348ß0.0ßIt is not given by any metric of distance/path-length in a graph.ßdummyTurker
14349ß0.0ßThe conversion method is presented in Section 2, and its evaluation in Sections 3-4.ßdummyTurker
14350ß0.0ßSection 1-Introduction provides only an outline of the paper.ßdummyTurker
14351ß0.0ßThe semantic similarity task over word pairs in a test set can be performed only if the trained model contains the words in that test set.ßdummyTurker
14352ß0.0ßThat is the case also for Google's word2vec, whose training set (100Btoken) contains words in the pairs of whatever test set is used to evaluate it under the similarity task.ßdummyTurker
14353ß0.0ßThe goal of the paper is not to obtain the best similarity score reported so far in the literature; or to explore hybrid methods, where embeddings are obtained from (the best methods over) text and (the best methods over) semantic networks, which will likely together will perform better than any of its components.ßdummyTurker
14354ß0.0ßThe key goal of the paper is to demonstrate that it is feasible to convert from semantic networks to semantic spaces, and furthermore to demonstrate that the proposed conversion method, when trained over a limited size vocabulary (60kwords), is very effective, substantially outperforming a very strong baseline, Google's state-of-the-art word2vec, where embbedings were gathered from huge masses of text (100Btokens/3Mvocabulary).ßdummyTurker
14355ß0.0ßWe do that by resorting to a range of case studies from WordNet and observing that that baseline is outperformed by 20%.ßdummyTurker
14356ß0.0ßThe several experiments with different subsets of WordNet reported here also indicate that better results can be obtained beyond the cases studied considered, namely with semantic networks that will be larger and denser than the ones that happened to be used here.ßdummyTurker
14357ß0.0ßResponse_to_Question#1=Response_to_Obs#1ßdummyTurker
14358ß0.0ßResponse_to_Question#2= Response_to_Obs#4ßdummyTurker
14359ß0.0ßAll WordNet relation are used, as indicated (Section4.2).ßdummyTurker
14360ß0.0ßFor the sake of comparability, the two semantic spaces, wnet2vec and word2vec, are evaluated under the same approach, the similarity task.ßdummyTurker
14361ß0.0ßBetter results were obtained when all types of semantic relation were assigned the same weight (Section4.2).ßdummyTurker
14362ß0.0ßThe definition of WordNet subsets used for the experiments took into account only size and/or frequency.ßdummyTurker
14363ß0.0ßWe find it above average.ßdummyTurker
14364ß0.0ßWe agree the approach is new.ßdummyTurker
14365ß0.0ßOur model outperforms in 20% a very strong baseline (state-of-the-art word2vec) in the standard similarity task (SimLex-999).ßdummyTurker
14366ß0.0ßSections1 and 5 cite 35 related works.ßdummyTurker
14367ß0.0ßSimLex-999 is specifically designed for the evaluation of models in the similarity task, largely surpassing in reliability other datasets collected in other research contexts.ßdummyTurker
14368ß0.0ßOur model outperforms state-of-the-art baseline in 20% when run over this standard dataset.ßdummyTurker
14369ß0.0ßThe mainstream approach for the intrinsic evaluation of word embeddings in lexical semantics, and thus to evaluate them on their own merits of capturing lexical semantic information, is to test them under the semantic similarity task.ßdummyTurker
14370ß0.0ßIn order to asses the intrinsic quality of the embeddings obtained by the proposed method, we present an extensive range of evaluations and respective results in several different experiments with a range of different settings and data sets, in a comparative way, in Section 3, and discuss them in Section 4.ßdummyTurker
14371ß0.0ßAs we noted in Section 1, word embeddings are instrumental in virtually any natural processing task or application, where they can be integrated.ßdummyTurker
14372ß0.0ßLike any other versatile language resource, their impact in the performance of a myriad of NLP systems can be assessed.ßdummyTurker
14373ß0.0ßThis will count as their extrinsic evaluations.ßdummyTurker
14374ß0.0ßThis type of evaluation though is much less informative about their inherent merits given that many extraneous factors and other components can interact to inflate or blur their contribution to the final performance of systems.ßdummyTurker
14375ß0.0ßGiven its much less informativeness regarding the inherent strengths of word embeddings, and the myriad of NLP systems where embeddings can be integrated, their extrinsic evaluation has to be left out of the scope/limits of a conference submission like the present one.ßdummyTurker
14376ß0.0ßAll relations, including antonymy, are not weighted (Section 4.2).ßdummyTurker
14377ß0.0ßThanks for your comment.ßdummyTurker
14378ß0.0ßNote that in case of Yang et al., the relational network is given/known, whereas in our setting such link information (reply-to links) is not available.ßdummyTurker
14379ß0.0ßUsing similarity to sample the contextual nodes made more sense in our setting since the graph is constructed based on similarity.ßdummyTurker
14380ß0.0ßWe wanted to compare our similarity-based approach with the random walk approach on the datasets used by Yang et al.ßdummyTurker
14381ß0.0ßWe contacted the original author of those datasets.ßdummyTurker
14382ß0.0ßUnfortunately, those datasets are not available in the textual format, which limits us to apply our method there and do the comparison.ßdummyTurker
14383ß0.0ßWe plan to do the multi-class classification in our future study, which includes 13 humanitarian classes such as affected individuals, infrastructure and utility damage, injured or dead people and so on.ßdummyTurker
14384ß0.0ßIn this scenario, we plan to use some weak supervision (e.g., presence of class-specific key phrases) to construct the similarity graph.ßdummyTurker
14385ß0.0ßThank you for pointing this out.ßdummyTurker
14386ß0.0ßWe had these results in an extended version of the paper.ßdummyTurker
14387ß0.0ßWe could not accommodate them (table with detailed discussion) in the paper due to the space limitations.ßdummyTurker
14388ß0.0ßThe table below presents the results using different amounts of labeled data – 100, 500, 1,000, 2,000, and all labels.ßdummyTurker
14389ß0.0ßExp		100	        500	        1000	2000	All LßdummyTurker
14390ß0.0ßL		43.63	52.89	56.37	60.11	60.89ßdummyTurker
14391ß0.0ßL+50kU	52.32	59.95	61.89	64.05	65.11ßdummyTurker
14392ß0.0ßL		48.97	76.62	80.62	79.16	80.16ßdummyTurker
14393ß0.0ßL+50kU	75.08	85.54	89.08	91.54	93.54ßdummyTurker
14394ß0.0ßL represents results using labelled instances and +50kU represents results after including 50,000 unlabeled tweets.ßdummyTurker
14395ß0.0ßIt is clear that model’s performance improve as we include more labeled data — from 43.63 to 60.89 for NEQ (first two rows) and from 48.97 to 80.16 for QFL (last two rows) in the case of labeled only (L).ßdummyTurker
14396ß0.0ßWhen we compare supervised vs. semi-supervised (L vs. L+50kU), we notice significant improvements in F1 scores (5% to 26% absolute) for the semi-supervised model.ßdummyTurker
14397ß0.0ßWe will include these results in the final version of the paper.ßdummyTurker
14398ß0.0ßYang et al.ßdummyTurker
14399ß0.0ß(2016) compare the graph-based semi-supervised embedding model with other semi-supervised methods and show state-of-the-art results.ßdummyTurker
14400ß0.0ßIn our study, we mainly focus on extending their work with similarity-based graph embedding to make it applicable to textual content.ßdummyTurker
14401ß0.0ßSimilarly, Ganin et al, (2016) compare the domain adversarial neural model with existing domain adaptation models and demonstrates state-of-the-art results.ßdummyTurker
14402ß0.0ßSince our proposed model does both semi-supervised learning and domain adaptation simultaneously, methods that do only semi-supervised learning or domain adaptation are not directly comparable.ßdummyTurker
14403ß0.0ßPlease see our response to Reviewer 1 (Reply to Weakness argument 1).ßdummyTurker
14404ß0.0ßThanks for pointing this paper.ßdummyTurker
14405ß0.0ßWe will definitely cite it.ßdummyTurker
14406ß0.0ßOur main contribution lies in the proposal of two different objective functions for Seq2Seq.ßdummyTurker
14407ß0.0ßWith these new objective functions, especially CVaR, the optimization process is different from the original Seq2Seq, which has its technical contribution.ßdummyTurker
14408ß0.0ßTheir evaluation targets are different.The distinct, PPL, Bleu and human evaluation are used for specific evaluation,while the diver and overlap are used for diverse evaluation.From the results, the MGL model obtains the best results in terms of specific measures, and CVaR performs the best in terms of diverse measures.Therefore, these experimental results are consistent with our proposals.ßdummyTurker
14409ß0.0ßWe are sorry that we did not explain the MGL model in details, due to space limitation.Thank you for your suggestions, and we will add some detailed analysis to our paper.ßdummyTurker
14410ß0.0ßWe are sorry that we made the writing mistake.It should be “numerically effective”.ßdummyTurker
14411ß0.0ßThe targets of CVaR and “minimum risk training”(Shen et al.ßdummyTurker
14412ß0.0ß2016) are totally different, though they both have the name “risk”.For minimum risk training, the risk is defined as the expected error, and the advantage of using this approach is that it can directly optimize arbitrary evaluation metrics.ßdummyTurker
14413ß0.0ßCVaR is a risk-sensitive measure, where the “risk” is aimed at considering the worst generated responses.ßdummyTurker
14414ß0.0ßTherefore we can see that they have no relation with each other.ßdummyTurker
14415ß0.0ßCVaR has been widely used in many other areas, such as finance and auto-control, while in NLP, there is few work to consider the risk (worst case) of the system.ßdummyTurker
14416ß0.0ßTherefore, we would like to introduce this risk-sensitive measure to the area of NLP, and hope to inspire more future work to consider the risk of a NLP system.ßdummyTurker
14417ß0.0ßWe are sorry that we used an unsuitable example in our paper, and we will revise it to the following example.ßdummyTurker
14418ß0.0ßGiven the post ‘Can you recommend me a tourist city?’, if a user prefers the magnificent mountains and rivers, it is better to reply ‘You maybe like the Bernina Express to the Alps’.ßdummyTurker
14419ß0.0ßWhile if a user loves literature, it is better to reply ‘Paris is a beautiful city with full of the literary atmosphere.’.ßdummyTurker
14420ß0.0ßAdditionally, we will follow your advice and revise our diverse measure to ‘divrs’ in the modified version.ßdummyTurker
14421ß0.0ßWe are sorry that we didn’t describe the human evaluation in details.ßdummyTurker
14422ß0.0ßThe three annotators for human evaluation are randomly selected from a class of computer science majored students (48 students), and all the test examples are randomly sampled to the annotators.ßdummyTurker
14423ß0.0ßIn fact, the kappa numbers of ~0.4 are significant for the Chinese Weibo dataset.ßdummyTurker
14424ß0.0ßFor example, the kappa number is 0.247~0.448 on average in (Shang et al.ßdummyTurker
14425ß0.0ß2015) who used the same dataset as we did.ßdummyTurker
14426ß0.0ßThat’s maybe caused by the difficulty of Chinese dialogue for human evaluation.ßdummyTurker
14427ß0.0ßIn fact, we used the standard train/dev/test split and the default script for the Ubuntu dataset.ßdummyTurker
14428ß0.0ßWhile for the Chinese Weibo dataset, there are no standard split data, and many existing works(Shang et al.ßdummyTurker
14429ß0.0ß2015; Zhou et al.ßdummyTurker
14430ß0.0ß2018; Luo et al.ßdummyTurker
14431ß0.0ß2016) also used their own splits of the data.ßdummyTurker
14432ß0.0ßWe will release our splits of dataset, so that our results can be reproduced.ßdummyTurker
14433ß0.0ßThe main contribution of our work is to propose two tailored seq2seq models for different scenarios.ßdummyTurker
14434ß0.0ßTheir targets are different.ßdummyTurker
14435ß0.0ßIn the specific-requirement scenario, the distinct, PPL, Bleu and human evaluation are used for evaluation, while in the diverse-requirement scenario, the diver and overlap are used for evaluation.ßdummyTurker
14436ß0.0ßFrom the results, the MGL model obtains the best results in terms of specific measures, and CVaR performs the best in terms of diverse measures.ßdummyTurker
14437ß0.0ßTherefore, these experimental results are consistent with our proposals.ßdummyTurker
14438ß0.0ßActually, it is very difficult to define which is qualitatively better for the dialogue generation task.ßdummyTurker
14439ß0.0ßIn this paper, we focus on the specific and diverse requirements.ßdummyTurker
14440ß0.0ßFrom the results on Ubuntu, MGL indeed produced more specific results.ßdummyTurker
14441ß0.0ßTake the post “install grub … boot” for example, the response of MGL, i.e., “I have boot with the window from the grub and it go from the boot menu” conveys more specific information as compared with other baseline models, such as ‘How do I install grub?’, ` Do you delete grub?’ and ` What do you mean?’.ßdummyTurker
14442ß0.0ßIn addition, from the results on STC dataset, CVaR produced diverse responses.ßdummyTurker
14443ß0.0ßTake the post “waiting for Wade final games” for example, CVaR’s responses are related to different topics.ßdummyTurker
14444ß0.0ßFor example, “I must go and see the final games” focuses on the game, while “James is so fast”  focuses on the person, James.ßdummyTurker
14445ß0.0ßHowever, the responses of the baseline model, i.e., Mechanism, are all related to the person, Wade.ßdummyTurker
14446ß0.0ßThose results are not cherry-picked, but we cannot give more results due to space limitation.ßdummyTurker
14447ß0.0ßAs pointed by many previous work (Zhou et al.ßdummyTurker
14448ß0.0ß2017; Xing et al.ßdummyTurker
14449ß0.0ß2017), one characteristic of conversation is one to many, which means that there are many responses for each post.ßdummyTurker
14450ß0.0ßTherefore, most conversation datasets have the multiple responses for each post, such as Twitter dataset(Xing et al.ßdummyTurker
14451ß0.0ß2017), Baidu Tieba(Zhou et al.ßdummyTurker
14452ß0.0ß2017) and Open Subtitle dataset(Li et al.ßdummyTurker
14453ß0.0ß2016).ßdummyTurker
14454ß0.0ßThat is why we did not consider the case of only one response for each post.ßdummyTurker
14455ß0.0ßWe are sorry that we did not explain the CVaR model in details, due to space limitation.ßdummyTurker
14456ß0.0ßAccording to the definition of CVaR, at each time step, 1-a percentages of the ground-truth responses for each post are optimized, based on the computation of the generated likelihood function.ßdummyTurker
14457ß0.0ßWe will add the detailed optimization process to the modified version.ßdummyTurker
14458ß0.0ßPlease also refer to our response to the weakness argument 3.ßdummyTurker
14459ß0.0ßPlease also refer to our response to the weakness argument 2.ßdummyTurker
14460ß0.0ßThese features are mainly in Table 3, which is the summary of non-textual features, here we use “non-textual features” to make a distinction from the linguistic feature, as we also use linguistic feature as another kind of feature and made performance comparison (see table 4 and 5).ßdummyTurker
14461ß0.0ßWe have given the feature importance analysis in the following reply to weakness argument 2.ßdummyTurker
14462ß0.0ßWe give the feature importance orders of two datasets as follows.ßdummyTurker
14463ß0.0ßHere we use the following widely used sklearn feature importance evalulation program:ßdummyTurker
14464ß0.0ßhttp://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.htmlßdummyTurker
14465ß0.0ßThe sum of all the feature importantce values is equal to 1.ßdummyTurker
14466ß0.0ßFor the meaning of these features, please refer to the table 3 in my paper.ßdummyTurker
14467ß0.0ßIt is noted the feature importance order of these two data sets are slightly different.ßdummyTurker
14468ß0.0ßWhereas feature”rank” is most important in both datasets.ßdummyTurker
14469ß0.0ßRank:	0.226529099ßdummyTurker
14470ß0.0ßISR:	0.093641039ßdummyTurker
14471ß0.0ßDLu:	0.076408403ßdummyTurker
14472ß0.0ßavgRL:	0.075990139ßdummyTurker
14473ß0.0ßL:	0.072791844ßdummyTurker
14474ß0.0ßDLb:	0.06707119ßdummyTurker
14475ß0.0ßPOS:	0.062420471ßdummyTurker
14476ß0.0ßBST:	0.047657959ßdummyTurker
14477ß0.0ßNEG:	0.045995458ßdummyTurker
14478ß0.0ßavgRD:	0.030512495ßdummyTurker
14479ß0.0ßWRD:	0.030094895ßdummyTurker
14480ß0.0ßRD:	0.027134447ßdummyTurker
14481ß0.0ßETF:	0.026463208ßdummyTurker
14482ß0.0ßRES:	0.026134687ßdummyTurker
14483ß0.0ßETG:	0.025329248ßdummyTurker
14484ß0.0ßPP1:	0.017447365ßdummyTurker
14485ß0.0ßMNR:	0.015230848ßdummyTurker
14486ß0.0ßEXT:	0.011969164ßdummyTurker
14487ß0.0ßERD:	0.009084346ßdummyTurker
14488ß0.0ßPR:	0.00778452ßdummyTurker
14489ß0.0ßNR:	0.004309176ßdummyTurker
14490ß0.0ßRank:	0.212755604ßdummyTurker
14491ß0.0ßDLu:	0.094470465ßdummyTurker
14492ß0.0ßavgRL:	0.078795829ßdummyTurker
14493ß0.0ßDLb:	0.076699851ßdummyTurker
14494ß0.0ßPOS:	0.072985285ßdummyTurker
14495ß0.0ßL:	0.072230072ßdummyTurker
14496ß0.0ßEXT:	0.071971763ßdummyTurker
14497ß0.0ßNEG:	0.038321868ßdummyTurker
14498ß0.0ßETF:	0.038212149ßdummyTurker
14499ß0.0ßavgRD:	0.034286109ßdummyTurker
14500ß0.0ßWRD:	0.031577023ßdummyTurker
14501ß0.0ßETG:	0.028620377ßdummyTurker
14502ß0.0ßRD:	0.02638407ßdummyTurker
14503ß0.0ßISR:	0.024122093ßdummyTurker
14504ß0.0ßBST:	0.021090261ßdummyTurker
14505ß0.0ßRES:	0.020690605ßdummyTurker
14506ß0.0ßPR:	0.018258353ßdummyTurker
14507ß0.0ßERD:	0.014015949ßdummyTurker
14508ß0.0ßPP1:	0.01135609ßdummyTurker
14509ß0.0ßMNR:	0.007751888ßdummyTurker
14510ß0.0ßNR:	0.005404298ßdummyTurker
14511ß0.0ßAccording to the suggestions, we will give the significance test (confidence scores) in the revised edition.ßdummyTurker
14512ß0.0ßIt is noted that the neural networks have great potential in many NLP tasks.ßdummyTurker
14513ß0.0ßIn this paper we mainly focused dataset construction.ßdummyTurker
14514ß0.0ßAlthough in our primary experiments we didn’t see clear superiority of NN, NN based research is undoubtedly one direction of future work.ßdummyTurker
14515ß0.0ßplease see the feature importance order in the reply to Weakness argument 2.ßdummyTurker
14516ß0.0ßIt is noted that in work of Huayi Li et al.ßdummyTurker
14517ß0.0ß(2014), one author is from dianping.ßdummyTurker
14518ß0.0ßThey argued that “First, Dianping has a team of expert evaluators whose duty is to evaluate its detection algorithm.ßdummyTurker
14519ß0.0ßEvery week, a random sample of detected fake reviews is manually evaluated by them based on all the data they collected (e.g., reviews, side information, IP addresses, click data, etc).ßdummyTurker
14520ß0.0ßSecond, an even stronger evidence is that for each detected fake review, Dianping sends an email to its reviewer with reasons.”ßdummyTurker
14521ß0.0ß[Huayi Li et al.ßdummyTurker
14522ß0.0ß2014.ßdummyTurker
14523ß0.0ßSpotting Fake Reviews via Collective Positive-Unlabeled Learning.ßdummyTurker
14524ß0.0ßICDM '14]ßdummyTurker
14525ß0.0ßPlease see the feature importance orders in the reply to Weakness argument 2, where we have given the feature comparisons.ßdummyTurker
14526ß0.0ßW.r.t the linguistic vs. metadata features, we have made comparison in table 4 and table 5, we found metadata features perform better than linguistic feature.ßdummyTurker
14527ß0.0ßMoreover, TBF outperforms BF alone in many cases.ßdummyTurker
14528ß0.0ß(N/A)ßdummyTurker
14529ß0.0ß(1).ßdummyTurker
14530ß0.0ßBy referring to many other datasets/corpus (and papers) released in ACL communities, (e.g., Hu B et al.ßdummyTurker
14531ß0.0ßEMNLP 2015; Suhr A et al.ßdummyTurker
14532ß0.0ß"ACL 2017), we have prepared  one agreement called ""Terms and Application Form of use of our datasets”, which includes copyright information, privacy issues and other information of the corpus."ßdummyTurker
14533ß0.0ßThe users must sign the agreement before the use.ßdummyTurker
14534ß0.0ß(2) In the current form of the datasets, we have made necessary data masking process, thus reviewers identities are not provided.ßdummyTurker
14535ß0.0ßW.r.t two sample reviews in the original paper, we will mask the revierer ID in the revised edition.ßdummyTurker
14536ß0.0ß[1] Hu B, Chen Q, Zhu F. LCSTS: A Large Scale Chinese Short Text Summarization Dataset.ßdummyTurker
14537ß0.0ßEMNLP 2015.ßdummyTurker
14538ß0.0ßPages 2667-2671.ßdummyTurker
14539ß0.0ß[2] Suhr A, Lewis M, Yeh J, et al.ßdummyTurker
14540ß0.0ßA Corpus of Natural Language for Visual Reasoning.ßdummyTurker
14541ß0.0ßACL 2017.ßdummyTurker
14542ß0.0ßPages 217-223.ßdummyTurker
14543ß0.0ßIt is noted that in reference (Huayi Li et al.ßdummyTurker
14544ß0.0ß2014), they said “ First, Dianping has a team of expert evaluators whose duty is to evaluate its detection algorithm.ßdummyTurker
14545ß0.0ßEvery week, a random sample of detected fake reviews is manually evaluated by them based on all the data they collected (e.g., reviews, side information, IP addresses, click data, etc).ßdummyTurker
14546ß0.0ßSecond, an even stronger evidence is that for each detected fake review, Dianping sends an email to its reviewer with reasons.”.ßdummyTurker
14547ß0.0ßAccording to the suggestions, we will provide and add discussion about Dianping's filtering algorithm.ßdummyTurker
14548ß0.0ßThank you very much!ßdummyTurker
14549ß0.0ß(N/A)ßdummyTurker
14550ß0.0ß(N/A)ßdummyTurker
14551ß0.0ß(N/A)ßdummyTurker
14552ß0.0ßplease refer to part 1 of the reply to Weakness argument 2ßdummyTurker
14553ß0.0ßplease refer to part 2 of the reply to Weakness argument 2.ßdummyTurker
14554ß0.0ß(N/A)ßdummyTurker
14555ß0.0ßAccording to the suggestion, we will add reference to the work of Huayi Li et al.ßdummyTurker
14556ß0.0ßin 2014.ßdummyTurker
14557ß0.0ßIt is noted that Other researchers (Mukherjee et al., 2013a; Rayana et al., 2015) also leverage the filtered review list in e-commerce websites such as Yelp as spams; however, these datasets are not publicly available for possible academic comparison.ßdummyTurker
14558ß0.0ß(N/A)ßdummyTurker
14559ß0.0ß(N/A)ßdummyTurker
14560ß0.0ß(N/A)ßdummyTurker
14561ß0.0ß(N/A)ßdummyTurker
14562ß0.0ßIt is noted that there are some information in reference (Huayi Li et al.ßdummyTurker
14563ß0.0ß"2014) ""First, Dianping has a team of expert evaluators whose duty is to evaluate its detection algorithm."ßdummyTurker
14564ß0.0ßEvery week, a random sample of detected fake reviews is manually evaluated by them based on all the data they collected (e.g., reviews, side information, IP addresses, click data, etc).ßdummyTurker
14565ß0.0ß"Second, an even stronger evidence is that for each detected fake review, Dianping sends an email to its reviewer with reasons"""ßdummyTurker
14566ß0.0ß.ßdummyTurker
14567ß0.0ßAccording to the suggestion, we will provide information regarding Dianping's filtering algorithm.ßdummyTurker
14568ß0.0ß(N/A)ßdummyTurker
14569ß0.0ß(N/A)ßdummyTurker
14570ß0.0ßWe completely agree with your opinion.ßdummyTurker
14571ß0.0ßHowever, in our consideration, the “low-resource” means lacking the manually annotated datasets for relation classification task, which would make it difficult to train a supervised model.ßdummyTurker
14572ß0.0ßThus we still need a bridge, such as an MT system, to transfer some information across language.ßdummyTurker
14573ß0.0ßIn fact, we employ the whole ACE05 multilingual training corpus as our experimental dataset (Table 1).ßdummyTurker
14574ß0.0ßOne of our comparative work “Min et al., Learning transferable representation for bilingual relation extraction via convolutional neural networks (Table 4)” could proof this, which utilizes the same entity type settings.ßdummyTurker
14575ß0.0ßWe will correct the wrong description in the revised version.ßdummyTurker
14576ß0.0ßThanks for your suggestion, and we will clarify the training procedure in the revised version.ßdummyTurker
14577ß0.0ßAbout the difference between the prior work and ours: In Qin et al.’s work, they drive an implicit discourse relation network to learn from its corresponding sample with a manual-annotated connective, in which the latent feature space of the “teacher” network is almost exactly the same as the “student” network.ßdummyTurker
14578ß0.0ßIn our framework, the latent feature spaces of HRRC and LRRC are from completely different resource.ßdummyTurker
14579ß0.0ßWe will clarify these points and correct typos in the revised version.ßdummyTurker
14580ß0.0ßThanks for the suggestion.ßdummyTurker
14581ß0.0ßWe will consider and attempt to evaluate our model on the opposite order (CH to EN) or on other language (Arabic).ßdummyTurker
14582ß0.0ßActually, we directly concatenate the entity type embedding to the output of the max-pooling layer (that is, Figure 1 is correct).ßdummyTurker
14583ß0.0ßWe will correct the description in the next version.ßdummyTurker
14584ß0.0ßYes, we used the same labels which are inherited from high-resource data to train the low-resource model.ßdummyTurker
14585ß0.0ßThe optimization terminates when there is no promotion through thirty iterations.ßdummyTurker
14586ß0.0ßThe BiLSTM-MT-Source system should be similar to the CNN-MT-Source rather than CNN-MT-Target.ßdummyTurker
14587ß0.0ßWe will correct this typo in the revised version.ßdummyTurker
14588ß0.0ßIn fact, the performance of CNN-MT-Target is better than BiLSTM-MT-Target, just like CNN-MT-Source vs. BiLSTM-MT-Source.ßdummyTurker
14589ß0.0ßFor complete comparison, we will include the results in the revised version.ßdummyTurker
14590ß0.0ßThe NER tags is provided by the ACE 2005 multilingual training corpus.ßdummyTurker
14591ß0.0ßThe PoS tags of English and Chinese datasets are from the corenlp toolkit (https://nlp.stanford.edu/software/corenlp.shtml) and the LTP toolkit (https://www.ltp-cloud.com), respectively.ßdummyTurker
14592ß0.0ßReply-to-weakness-argument#1: ‌ßdummyTurker
14593ß0.0ßDiscussed in response to reviewer#1: “Reply-to-weakness-argument#2”ßdummyTurker
14594ß0.0ßIn none of the mentioned papers in weakness#2 there are “better” results than what we achieved here.ßdummyTurker
14595ß0.0ßDehghani-et.al-2017a is not a relevant paper for comparison, Dehghani-et.al-2017b is a short version of our submission with identical results, and we beat the Dehghani-et.al-2017c significantly (baseline-1&4 in Table1).ßdummyTurker
14596ß0.0ßDiscussed in response to reviewer#1: “Reply-to-weakness-argument#2”ßdummyTurker
14597ß0.0ßDiscussed in  “Reply-to-weakness-argument#1”ßdummyTurker
14598ß0.0ß$c_j$ is the gold label for the output of the conf-net using which the loss of the conf-net is calculated (Eq.1) and as it is stated in the paper, it indicates how similar the true and the weak labels are.ßdummyTurker
14599ß0.0ßThe way we calculate $c_j$ depends on the task (lines 318 and 366).ßdummyTurker
14600ß0.0ßFor example, for the sentiment, $c_j$ is the mean absolute difference of true labels and weak labels, on different classes, for the instances in $V$.ßdummyTurker
14601ß0.0ßGP is the Gaussian process which is related to the technique employed for the hyper-parameter tuning that we have reference to (Golovin et al.2017;Desautels et al.2014).ßdummyTurker
14602ß0.0ßFurther explanations are out of the scope of this paper.ßdummyTurker
14603ß0.0ßReply-to-weakness-argument#1: ‌ßdummyTurker
14604ß0.0ßWe have done some analysis that are excluded due to the space limit like feeding the model with completely correct and incorrect labels as weak annotations to study the behavior of the conf-net.ßdummyTurker
14605ß0.0ßMany discussions in the paper have been driven from this analysis (e.g.ßdummyTurker
14606ß0.0ßline 525).ßdummyTurker
14607ß0.0ßWe will add a summary on this in the camera-ready version.ßdummyTurker
14608ß0.0ßTo clarify on this, Dehghani-et.al-2017a is a reference related to the data-preprocessing and there is no connection between this paper and our submission in terms of model and tasks at all!ßdummyTurker
14609ß0.0ßDehghani-et.al-2017b is a short version of this submission presented in a NIPS non-archival workshop and we declared this at the time of submission and this is allowed according to the ACL submission policy.ßdummyTurker
14610ß0.0ßWe propose a task/architecture independent model for semi-supervised learning, while Dehghani-et.al-2017c proposed an architecture for the ranking.ßdummyTurker
14611ß0.0ßThe proposed ranking model in Dehghani-et.al-2017c is meant to learn from “only” weakly supervised data and no true labels are used (unless in a single fine-tuning experiment), while we have the semi-supervised learning setup.ßdummyTurker
14612ß0.0ßWe use the architecture proposed by Dehghani-et.al-2017c as the SOTA of neural ranking for the target network and we have “direct comparison” with their model (Table1, baselines-1&4) with significant improvements over these baselines.ßdummyTurker
14613ß0.0ßDehghani-2018 is accepted in ICLR2018 for which the notification date was after the ACL submission deadline.ßdummyTurker
14614ß0.0ßRegardless of this point, there are significant differences, e.g.ßdummyTurker
14615ß0.0ßin Dehghani-2018 there is a neural network and a Gaussian-Process and there are three stages for training the model, while we have two networks jointly trained,  end-to-end,  in a single stage which makes the model super efficient compared to Dehghani-2018.ßdummyTurker
14616ß0.0ßAs Dehghani-2018 is officially published, we will add their results as a new baseline along with discussions in the camera-ready version.ßdummyTurker
14617ß0.0ßThe idea proposed is a general framework and not an  approach to beat SOTA of a single task.ßdummyTurker
14618ß0.0ßThe model applies on any task and allows to plug in any architecture as the target network (which can be SOTA for that task) and it is meant to help the target network to be trained where data with true labels is limited.ßdummyTurker
14619ß0.0ßNevertheless, in tables-1&3, we have strong new models like NLI, SOTA in neural ranking and the winner of the SemEval plus some baselines that one would immediately ask for.ßdummyTurker
14620ß0.0ßIt depends on the task, quality of weak annotator and the |V|:|U| ratio.ßdummyTurker
14621ß0.0ßFor ranking, we observed higher average and a more uniform distribution for the conf-scores since the weak annotator is a good signal of relevance, while for the sentiment conf-scores are lower in average with an exponential distribution showing that only a small portion of the weak labels are taken into account.ßdummyTurker
14622ß0.0ßWe think conducting the statistical test based on Semeval^{1th} would be fair if we use their propose architecture/model as the target network to have an equally strong core, but different learning strategy.ßdummyTurker
14623ß0.0ßDiscussed in “Reply-to-weakness-argument#3”.ßdummyTurker
14624ß0.0ßDiscussed in “Reply-to-weakness-argument#2”.ßdummyTurker
14625ß0.0ßThe 1:10 ratio, which is the sampling/alternating ratio in our model is a rough approximation for a good ratio for |V|:|U|.ßdummyTurker
14626ß0.0ßThe conf-score can be considered as a multiplicative factor for the learning rate of the gradient descent which forms the final step-size during the back-propagation.ßdummyTurker
14627ß0.0ßNLI is one of our baselines and is based on knowledge-distillation.ßdummyTurker
14628ß0.0ßWe thank the reviewer for his/her thoughtful remarks.ßdummyTurker
14629ß0.0ßThe method is evaluated on the other domains only in terms of precision as in the intrinsic evaluation experiment, and not in terms of language modeling and perplexity.ßdummyTurker
14630ß0.0ßIt would be interesting to evaluate the other domains from a language modeling perspective as well, and that would be considered for the final version.ßdummyTurker
14631ß0.0ßYour suggestion is interesting at the outset, but I can quickly think of two downsides to your suggestion of targeting publication venues.ßdummyTurker
14632ß0.0ßFirst, there are many domains, which do not have specialized publications, such as music in the commercial sense.ßdummyTurker
14633ß0.0ßSecond, automatically scraping these pdf documents or web pages would incur enormous amounts of manual post-processing to make them usable, which is what we expect to eliminate/reduce in the first place.ßdummyTurker
14634ß0.0ßThe domain and genre both affect word distributions, which is the underlying signal source of our tool.ßdummyTurker
14635ß0.0ßUnder that assumption, we expect Dexter to detect nuances in genre similar to how it detects nuances in domain.ßdummyTurker
14636ß0.0ßWe did not find other tools that are as user-friendly and readily available to use under the same conditions that we operate on, such as having as an input a seed set of in-domain texts.ßdummyTurker
14637ß0.0ßAddressed above in response to argument 3.ßdummyTurker
14638ß0.0ßIndeed that is another problem to be addressed by the NLP community but not the focus of this paper.ßdummyTurker
14639ß0.0ßOur focus is on automatic extraction of unannotated in-domain monolingual corpora for purposes such as estimating word embeddings and language models.ßdummyTurker
14640ß0.0ßMost NLP algorithms with their underlying deep learning (Seq2Seq) framework utilize word embeddings, which are usually pre-trained on a monolingual corpus.ßdummyTurker
14641ß0.0ßWe thank the reviewer for his/her thoughtful remarks.ßdummyTurker
14642ß0.0ßThat is a good suggestion.ßdummyTurker
14643ß0.0ßThank you for the reference.ßdummyTurker
14644ß0.0ßWe used cosine similarity for its computational efficiency on GPUs, but it would be interesting to see if the added value of the weighted overlap metric outweighs the drop in speed.ßdummyTurker
14645ß0.0ßThe comparison with BootCaT is not as straightforward as mentioned.ßdummyTurker
14646ß0.0ßBootCaT takes in a set of keywords for a certain domain rather than a set of seed articles.ßdummyTurker
14647ß0.0ßTransforming this set of seed articles to a set of seed keywords is doable but not straightforward.ßdummyTurker
14648ß0.0ßOne can add a preprocessing stage of keyword extraction on the seed set and then utilize BootCaT, but the performance would be affected by the keyword extraction algorithm.ßdummyTurker
14649ß0.0ßUpon trying out BootCaT, one can see the added noise due to the automatic scraping of web pages.ßdummyTurker
14650ß0.0ßHere is a noisy example among many from BootCaT when using nlp-related keywords as an input “The boy put the tortoise on the rugROOT put rugthe ontortoise put boy The the Dependency LabelsDependency Labels Argument dependencies:g p • Subject (subj), object (obj), indirect object (iobj)...”.ßdummyTurker
14651ß0.0ßIt is telling us how compact the corpus is, but given that the root labels are scientific, this compactness is that of science as defined by the categories of Wikipedia.ßdummyTurker
14652ß0.0ßTo further expand on our justification (page 4, l. 380-386), domains are loosely defined and subdomains exist.ßdummyTurker
14653ß0.0ßFor example, under science, there will be Wikipedia pages of rigorous theorems as well as pages of scientists and tackling their personal background.ßdummyTurker
14654ß0.0ßThus it could be the case that the seed corpus used to extract the scientific corpus is more representative of the held-out corpus used to test language models on than the gold scientific corpus.ßdummyTurker
14655ß0.0ßWe did not look deeper into that.ßdummyTurker
14656ß0.0ßIndeed this is a key insight, and will be explicitly stated both in the abstract and introduction.ßdummyTurker
14657ß0.0ßThere were improvements: while for DAM and ESIM there was no significant improvement on SNLI (Fig.ßdummyTurker
14658ß0.0ß1), for cBiLSTM there was a sensible improvement: from 79% to 81% accuracy.ßdummyTurker
14659ß0.0ßAn explanation for this phenomenon is that models such DAM and ESIM learn to capture annotation artifacts in the SNLI data, as shown in [1].ßdummyTurker
14660ß0.0ßFor such a reason, we created a set of adversarial datasets that test whether an NLI model is consistent with respect to a set of logic rules that every NLI model should comply with.ßdummyTurker
14661ß0.0ßIn our experiments, we show that, in our models, (1) performance does not degrade on SNLI, and (2) on adversarial datasets, it yields up to a 79% improvement in predictive accuracy.ßdummyTurker
14662ß0.0ß[1]  Annotation Artifacts in Natural Language Inference Data (NAACL 2018) - https://arxiv.org/abs/1803.02324ßdummyTurker
14663ß0.0ßDuring experiments we used parse sub-tree introduction/removal, but we missed to mention that in 622-625.ßdummyTurker
14664ß0.0ßWe will correct this in the final version of the paper, and provide an ablation study on the generation process.ßdummyTurker
14665ß0.0ßWe do not really think that also reporting negative results together with successes should be considered “weakness”.ßdummyTurker
14666ß0.0ßHowever, we can devote less space describing this method, and use such space for describing our results more thoroughly.ßdummyTurker
14667ß0.0ßIn earlier versions we had more intuitions and examples, later removed due to space constraints.ßdummyTurker
14668ß0.0ßWe will include more examples in the final version.ßdummyTurker
14669ß0.0ßThat’s correct: the probabilities are associated to the perturbed versions of the entailments, but the writing is opaque.ßdummyTurker
14670ß0.0ßWe will make it more clear.ßdummyTurker
14671ß0.0ßWe think \triangleq or := are useful for distinguishing between identity and “defined as”, but just using the = symbol should be more clear.ßdummyTurker
14672ß0.0ßThe condition in (6) means that, for every “variable to s_i” mapping in S, s_i belongs to the space of sentences.ßdummyTurker
14673ß0.0ßWe show in Fig.ßdummyTurker
14674ß0.0ß1 that for DAM and ESIM, there was no significant improvement on the SNLI test set for appropriate \lambda, while there was a sensible improvement for cBiLSTM.ßdummyTurker
14675ß0.0ßPlease note that the paper is in British English.ßdummyTurker
14676ß0.0ßAdversarial examples in vision are about adding imperceptible, semantically-invariant changes to images, that cause a model to “hallucinate” and see things that are not there.ßdummyTurker
14677ß0.0ß"Our case is similar: we make the model ""hallucinate"" relationships between sentences that are not there, such as “entailments” or “contradictions”."ßdummyTurker
14678ß0.0ßFrom our analysis and results, we argue that the result of the classification is highly correlated with the presence of some surface patterns and annotation artifacts (e.g.ßdummyTurker
14679ß0.0ßwords such as “dog” or “man”), which can be used for hallucinating relations between sentences, violating the background assumptions.ßdummyTurker
14680ß0.0ßWe will clarify this in the final version of our paper.ßdummyTurker
14681ß0.0ßAdversarial training did not reduce the prediction accuracy on SNLI: in DAM and ESIM, the predictive accuracy stays the same for appropriate \lambda values; in cBiLSTM we see a sensible improvement (79% to 81%).ßdummyTurker
14682ß0.0ßIn [1] authors show that models like DAM and ESIM tend to overfit some annotation artifacts in the SNLI data.ßdummyTurker
14683ß0.0ßFor such a reason, a better NLI model is not guaranteed to yield better results on SNLI.ßdummyTurker
14684ß0.0ßThus, in our experiments, we show that, in our regularized models, (1) performance does not degrade on SNLI, and (2) it significantly improves on adversarial datasets.ßdummyTurker
14685ß0.0ß[1] https://arxiv.org/abs/1803.02324ßdummyTurker
14686ß0.0ßThank you for your insightful feedback and comments!ßdummyTurker
14687ß0.0ßWe agree that WikiSQL queries are simple in the space of possible SQL queries.ßdummyTurker
14688ß0.0ßIn this work we wanted to emphasize the ability to generalize to unseen tables.ßdummyTurker
14689ß0.0ßThere is a tradeoff between number of tables and query complexity because annotators do not have time to familiarize themselves with one table on which to create complex queries.ßdummyTurker
14690ß0.0ßFor other datasets, there is usually only one (or a few few) table whose schema is easily memorized.ßdummyTurker
14691ß0.0ßIn the future, we plan on expanding this work to address more complex queries that involve joins, groupings, and nestings.ßdummyTurker
14692ß0.0ßThe main contribution of this work is the WikiSQL dataset/task, which emphasizes the ability to generalize to unseen tables.ßdummyTurker
14693ß0.0ßOur methods serve only as baselines for WikiSQL and are not meant to compete on other semantic parsing tasks.ßdummyTurker
14694ß0.0ßFor reference on how our baselines perform on other tasks, the seq2seq baseline obtains state-of-the-art performance on the JOBS, GeoQuery, ATIS, and IFTTT datasets with fixed schema (https://arxiv.org/abs/1601.01280).ßdummyTurker
14695ß0.0ßBy ‘relational learning’, we assume you mean ‘reinforcement learning’?ßdummyTurker
14696ß0.0ßWe tried two other means of achieving order invariance, including the shuffle data augmentation method you suggested.ßdummyTurker
14697ß0.0ßWe show that policy learning achieves better performance and faster convergence (Table 3).ßdummyTurker
14698ß0.0ßThank you for your insightful feedback and comments!ßdummyTurker
14699ß0.0ßThe question type distribution is table-dependent (e.g.ßdummyTurker
14700ß0.0ßa table with all dates will trigger more ‘when’ questions).ßdummyTurker
14701ß0.0ßFor most of the tables in Wikipedia, humans naturally compose ‘what’ questions.ßdummyTurker
14702ß0.0ßThis distribution is consistent between splits for WikiSQL.ßdummyTurker
14703ß0.0ßIn training, we can sample balanced question types for each batch.ßdummyTurker
14704ß0.0ßWe agree that order invariance captures one form of query equivalence but not all.ßdummyTurker
14705ß0.0ßWe evaluate using this form of equivalence because it is the most common one in WikiSQL.ßdummyTurker
14706ß0.0ßOur baselines do not rely on this assumption and can be used with any measure of query equivalence.ßdummyTurker
14707ß0.0ßThank you.ßdummyTurker
14708ß0.0ßWe did not include this due to space constraints.ßdummyTurker
14709ß0.0ßWe will add these analyses to the paper.ßdummyTurker
14710ß0.0ßThe result for some of these analysis are below.ßdummyTurker
14711ß0.0ßThis difference is due to the random sampling of tables for splits.ßdummyTurker
14712ß0.0ßWe did not notice significant performance differences.ßdummyTurker
14713ß0.0ßIn Section 6 we show that the model struggles with ambiguous aggregations over numerical columns which correlate with “how many” or “what is the total” questions.ßdummyTurker
14714ß0.0ßWe will add a detailed figure to the paper.ßdummyTurker
14715ß0.0ßThe performance slightly decreases with more conditions.ßdummyTurker
14716ß0.0ßWe will add a detailed figure to the paper.ßdummyTurker
14717ß0.0ßWe set hyperparameters with grid search on the development split.ßdummyTurker
14718ß0.0ßThere is one paraphrase for each SQL query.ßdummyTurker
14719ß0.0ßWe are pleased to read that the reviewer has noted our good results.ßdummyTurker
14720ß0.0ßWe would appreciate if the reviewer could give more detailed feedback about the lack of innovation in our work.ßdummyTurker
14721ß0.0ßWith respect to the comment about the GO token in the reviewer's summary, we do not believe that the use of 2 tokens is standard practice.ßdummyTurker
14722ß0.0ßFor example, consider this example from Google's sequence-to-sequence tutorialßdummyTurker
14723ß0.0ßhttps://www.tensorflow.org/images/seq2seq/seq2seq.jpgßdummyTurker
14724ß0.0ß"They use a scheme of ""I am a student < s > Je suis étudiant""."ßdummyTurker
14725ß0.0ß"In our scheme it would be ""I am a student GO < s > Je suis étudiant""."ßdummyTurker
14726ß0.0ßWe admit that inclusion of this token started as an accident after misreading Bahdanau et al., who refer to the start-of-sentence token as the GO token.ßdummyTurker
14727ß0.0ßBut our results still hold.ßdummyTurker
14728ß0.0ßWe do understand that our contributions are simple, however the process of identifying and isolating these simple techniques was time consuming.ßdummyTurker
14729ß0.0ßWe humbly compare our work with batch normalization which is also a simple technique that has been widely applied in other domains.ßdummyTurker
14730ß0.0ßWe think that there is scope for simple and reproducible techniques that yield good results.ßdummyTurker
14731ß0.0ßWe agree it would be interesting to look at the sigmoid, but that does not change our assertion that attention does not need to be an expectation.ßdummyTurker
14732ß0.0ßThe abnormality comes from the fixed transition between the GO token and the start-of-sentence token.ßdummyTurker
14733ß0.0ßWe thank the reviewer for their suggestion of introducing a trainable init state.ßdummyTurker
14734ß0.0ßF1 scores are only applicable to the binary classification tasks with unbalanced class distributions.ßdummyTurker
14735ß0.0ßIn our case they would be the metaphor and pun tasks.ßdummyTurker
14736ß0.0ßWe found that recall is similar to the BiLSTM but ABOW had better precision.ßdummyTurker
14737ß0.0ßWe are happy to include F1, precision and recall scores in the camera ready version.ßdummyTurker
14738ß0.0ßLine 81: The significance of this interpretation is that a simple rearrangement of the same parameters allows the optimizer to converge to a better solution.ßdummyTurker
14739ß0.0ßWe would be happy to add a new section with a review of related works on reparameterization from the machine learning community.ßdummyTurker
14740ß0.0ß"Line 441: The ""Fermi"" system is cited in line 359."ßdummyTurker
14741ß0.0ßLine 556: It is regrettable that the datasets are not comparable.ßdummyTurker
14742ß0.0ßWe only realized that Yang et al.ßdummyTurker
14743ß0.0ß(2016) filtered their datasets after we had implemented our own system and were trying to compare our results.ßdummyTurker
14744ß0.0ßIt would have been very difficult to reproduce the unsupervised filtering technique employed by Tang et al.ßdummyTurker
14745ß0.0ß(2015).ßdummyTurker
14746ß0.0ßThis filtered dataset has been shared by a number of researchers from different institutions, but for some reason they are unable to share it with us.ßdummyTurker
14747ß0.0ßThis is a rich dataset and it would be a shame to close it off to benchmarking because of a procedural mistake in a preceding work.ßdummyTurker
14748ß0.0ßWe are surprised by the reviewer's assertion that we do not test our model on sentiment classification given that the Yelp task in Section 4.2 is a sentiment classification task.ßdummyTurker
14749ß0.0ß"We state - ""we believe that modeling sequence, and other structure, is important""."ßdummyTurker
14750ß0.0ßOur more limited assertion is that the BiLSTM does not always adequately capture sequence information.ßdummyTurker
14751ß0.0ßWe note that there are many other works that study generation from non-sequential inputs (Wiseman et al.ßdummyTurker
14752ß0.0ß2017, Lebret et al.ßdummyTurker
14753ß0.0ß2016, and Mirella Lapata's last year's ACL keynote).ßdummyTurker
14754ß0.0ßWe do not make the claim that our model is applicable to machine translation (MT).ßdummyTurker
14755ß0.0ßWeakness 3 and Question 2 are the same)ßdummyTurker
14756ß0.0ßWe accept that we may not have given a strong enough definition of generalization.ßdummyTurker
14757ß0.0ßWe use it in the context of Bishop (2006), whereby models with a small number of parameters perform strongly on small data problems and scale well to large data problems.ßdummyTurker
14758ß0.0ßWe agree that task-specific models may be stronger than ABOW.ßdummyTurker
14759ß0.0ßWe are happy to improve the discussion of generalization in the introduction to address the reviewer's concerns.ßdummyTurker
14760ß0.0ßYes, we did do sentiment classification in Section 4.2.ßdummyTurker
14761ß0.0ßAs mentioned in the paper, we are unsure about the exact mechanics of how the GO token affects performance.ßdummyTurker
14762ß0.0ßThis, however, does not detract from our main point that we can build performant models with far fewer parameters.ßdummyTurker
14763ß0.0ßWe did not run Mei et al.ßdummyTurker
14764ß0.0ß's system in the same way that they did not re-run all the prior systems in their work.ßdummyTurker
14765ß0.0ßInstead we compared their reported numbers with our results.ßdummyTurker
14766ß0.0ßThank for your valuable comments.ßdummyTurker
14767ß0.0ßWe will give our response as follows.ßdummyTurker
14768ß0.0ßFor weakness1: In fact, we have tackled the task of aspect-level polarity classification.ßdummyTurker
14769ß0.0ßThe goal of this work is not only to extract the targets, but also to identify the sentiments of each extracted target.ßdummyTurker
14770ß0.0ßWe name our task as targeted sentiment analysis, which is essentially composed of two subtasks: target detection and aspect-level polarity classification.ßdummyTurker
14771ß0.0ßAs Section 1 states, given a sentence ``ESPN poll says Michael Jordan is the greatest basketball athlete'',ßdummyTurker
14772ß0.0ßwe aim to first extract the targets (i.e., ESPN and Michael Jordan) and then judge their sentiment classes (i.e., Neutral and Positive) respectively.ßdummyTurker
14773ß0.0ßWe hope that our explanation can clarify your confusion.ßdummyTurker
14774ß0.0ßFor weakness2: Thank you for pointing out the related work.ßdummyTurker
14775ß0.0ßThe work of (Yang and Cardie, 2013) , (Klinger and Cimiano,2013) and (Jebbara et al.ßdummyTurker
14776ß0.0ß2016) are related to this tasks.ßdummyTurker
14777ß0.0ßWe will add them to the revised paper.ßdummyTurker
14778ß0.0ßFor weakness3: Due to space limitation, we just give the qualitative analysis in the draft.ßdummyTurker
14779ß0.0ßWe will give qualitative discussion and error analysis in the extra page if this paper is accepted.ßdummyTurker
14780ß0.0ßQuestion1 and additional comments: Here we clarify that the task of sentiment classification means to give each target a polarity class (e.g., positive, negative and neutral).ßdummyTurker
14781ß0.0ßPerhaps you can understand that our proposed models perform target recognition (i.e., aspect recognition) and polarity classification (i.e., sentiment classification).ßdummyTurker
14782ß0.0ßWe will carefully proofread the paper according to your comments.ßdummyTurker
14783ß0.0ßThank for your insightful comments.ßdummyTurker
14784ß0.0ßWe will give our response as follows.ßdummyTurker
14785ß0.0ßFor weakness1: Thanks for pointing out the novelty problem.ßdummyTurker
14786ß0.0ßWe have to argue for this paper about its novelty, which aims to explore a good way of jointly learning target extraction and sentiment classification.ßdummyTurker
14787ß0.0ßWe think that our work is a good attempt to design appropriate methods for targeted sentiment analysis.ßdummyTurker
14788ß0.0ß"Compared with ""Collobert et al 2011"", ""Model1"" and ""Model2"" share deeper representations because they use SBi-GRU to learn representations for both target detection and sentiment identification, but ""Collobert et al 2011"" only share the word embeddings and non-linear transform layer."ßdummyTurker
14789ß0.0ß"""Model2"" and ""Model4"" are also different from ""Collobert et al 2011” because we introduce target detection information to sentiment identification."ßdummyTurker
14790ß0.0ßThe reason is that labels of target and sentiment have more constraints.ßdummyTurker
14791ß0.0ß"For example, in ""LeBron James is my favorite baseketball player"", the target labels and sentiment labels are [B-Person, I-Person, O,O,O,O,O] and [B-Positive, I-Positive, O,O,O,O,O]."ßdummyTurker
14792ß0.0ß"From the example, we can see that target labels and sentiment labels have same boundary information: ""B"" and ""I""."ßdummyTurker
14793ß0.0ßTherefore, it is useful to introduce target detection information to sentiment identification.ßdummyTurker
14794ß0.0ßThe experimental results also show the effectiveness of our models.ßdummyTurker
14795ß0.0ßWe also thank you for providing the related work.ßdummyTurker
14796ß0.0ß"For weakness2: The reason why we don't compare with ""Mitchel et al., 2013"" is that ""Mitchel et al., 2013"" mainly evaluate the beginning of spans but we take full-span metrices for evaluation used in ""Zhang et al."ßdummyTurker
14797ß0.0ß"2015""."ßdummyTurker
14798ß0.0ß"As for the results of ""Zhang et al."ßdummyTurker
14799ß0.0ß"2016"", they first merge three datasets :""Mitchel et al., 2013"", MPQA and ""Dong et al."ßdummyTurker
14800ß0.0ß"2014""."ßdummyTurker
14801ß0.0ßThen, they shuffle the whole data and divide them into training, development and testing set.ßdummyTurker
14802ß0.0ß"Therefore, I think it's unfair to compare with ""Zhang et al."ßdummyTurker
14803ß0.0ß"2016""."ßdummyTurker
14804ß0.0ßFor weakness3: Due to space limitation, we don't give insightful discussion and visual representation of features.ßdummyTurker
14805ß0.0ßWe will add more discussions and visualization in the revised version.ßdummyTurker
14806ß0.0ß"For weakness4: The difference between ""Serial"" and ""Parallel"" is that we introduce target detection information to sentiment identification."ßdummyTurker
14807ß0.0ß"From the results of ""Serial"" and ""Parallel"" model, we can find that ""Serial"" outperforms ""Parallel""."ßdummyTurker
14808ß0.0ßWe do the paired t-test for the best and second-best methods, and the P-values are 0.36, 0.40, 0.41, 0.52 for English Target, English Sentiment, Spanish Target and Spanish Sentiment respectively.ßdummyTurker
14809ß0.0ßWe will carefully proofread the paper and add related references you provide.ßdummyTurker
14810ß0.0ßThanks for your comments.ßdummyTurker
14811ß0.0ßTo address the weaknesses 1 and 3, we’d like to make more thorough comparisons between our model and a hybrid approach for handling factoid and recommendation questions (and other types of dialogues).ßdummyTurker
14812ß0.0ßTheoretically, our model is capable of handling all these types of conversations with a single unified structure.ßdummyTurker
14813ß0.0ßWeaknesses 2, question 1 to 4 are about the dataset.ßdummyTurker
14814ß0.0ßWe will provide more details about our dataset.ßdummyTurker
14815ß0.0ßWe build the knowledge grounded dataset through two steps: 1) Automatic matching through regular expression and information retrieval based approaches; 2) Manually data cleaning, i.e., the dataset is manually checked and corrected by native speaking experts.ßdummyTurker
14816ß0.0ßAbove two steps provide assurance for the quality of our dataset.ßdummyTurker
14817ß0.0ßIn future work, we will continue to update the dataset to support future research on knowledge grounded conversation.ßdummyTurker
14818ß0.0ßOur dataset contains factoid questions (can be answered by a clear and concise response) and non-factoid questions (subjective and psychological questions, etc.).ßdummyTurker
14819ß0.0ßFactoid questions are about 25%, and others are about 75%.ßdummyTurker
14820ß0.0ßWe’d like to deep into the analysis of different types of the non-factoid dialogues in future work.ßdummyTurker
14821ß0.0ßRecommendation can be viewed as a subset of diffusion.ßdummyTurker
14822ß0.0ßFor example, “Can you recommend me some romantic films?”.ßdummyTurker
14823ß0.0ßNKD tries to solve it through entity diffusion.ßdummyTurker
14824ß0.0ßThanks again.ßdummyTurker
14825ß0.0ßThanks for your comments.ßdummyTurker
14826ß0.0ßWe try to understand why NKD model seems to struggle to formulate well-formed sentences by re-examining the corpus.ßdummyTurker
14827ß0.0ßWe find that these social networking dialogues contain a large number of phrases pieces, such as abbreviations, slang, and idioms.ßdummyTurker
14828ß0.0ßThis may lead our model tends to generate short responses that seem to be not well-formed.ßdummyTurker
14829ß0.0ßWe will verify the scalability.ßdummyTurker
14830ß0.0ßIn general, the time complexity can be constrained by limiting the number of retrieved facts and entities.ßdummyTurker
14831ß0.0ßTherefore, there’s a tradeoff between the speed and performance.ßdummyTurker
14832ß0.0ßWe’d like to further explore the scalability on large-scale KBs in the future.ßdummyTurker
14833ß0.0ßTo weaknesses 3, we will ensure that all the released data are in accordance with proper authorizations.ßdummyTurker
14834ß0.0ßFour evaluators rated the scores independently.ßdummyTurker
14835ß0.0ßThe pairwise Cohen's Kappa agreement scores are 0.67 on fluency, 0.54 on appropriateness, and 0.60 on entire correctness, which indicates a strong annotator agreement.ßdummyTurker
14836ß0.0ßThe knowledge triplets in the form of (subject, relation, object) represent facts.ßdummyTurker
14837ß0.0ßBoth subject and object are denoted as entities.ßdummyTurker
14838ß0.0ßIn facts matching ( left part of Figure 2 ), entity “Titanic” is recognized, and all the facts, with “Titanic” as subject, are also extracted.ßdummyTurker
14839ß0.0ßThese entities and retrieved facts are regarded as relevant facts.ßdummyTurker
14840ß0.0ßWe limit the number of facts to 100.ßdummyTurker
14841ß0.0ßWe will describe GenDS, add the agreement rate, and reference the paper of [Dhingra et al] .ßdummyTurker
14842ß0.0ßThanks again.ßdummyTurker
14843ß0.0ßThanks for your comments.ßdummyTurker
14844ß0.0ßWeaknesses 1 and question 1 are about knowledge retriever.ßdummyTurker
14845ß0.0ßWe will make it more clear.ßdummyTurker
14846ß0.0ßThe knowledge triplets in the form of (subject, relation, object) represent facts.ßdummyTurker
14847ß0.0ßBoth subject and object are denoted as entities.ßdummyTurker
14848ß0.0ßIn facts matching (left part of Figure 2), entity “Titanic” is recognized.ßdummyTurker
14849ß0.0ßThen, all the facts, with “Titanic” as subject, are also retrieved.ßdummyTurker
14850ß0.0ßBoth the matched entities and the retrieved facts are denoted as relevant facts.ßdummyTurker
14851ß0.0ßWe list three options for facts matching.ßdummyTurker
14852ß0.0ßWhen implementing, one can use any one of them or combinations with regard to different applications.ßdummyTurker
14853ß0.0ßIn our scenario, we find that string matching is comparable with NER.ßdummyTurker
14854ß0.0ßTherefore, we carry out experiments based on string matching for simplicity.ßdummyTurker
14855ß0.0ßWeaknesses 2 and 3 are about the decoder.ßdummyTurker
14856ß0.0ßWe will make more clear to the model.ßdummyTurker
14857ß0.0ßThe response is generated in an end-to-end manner, while the encoder interacts with the KB through external operations.ßdummyTurker
14858ß0.0ßThe knowledge retriever is jointly trained with the response generation by sharing the entity embeddings.ßdummyTurker
14859ß0.0ßTherefore, the entity embeddings capture both the entity relation in the knowledge base and the information within the dialogue context.ßdummyTurker
14860ß0.0ßWe’d like to verify the performance of such configuration.ßdummyTurker
14861ß0.0ßEmpirically, when the gated decoder works together with the coefficient attenuation tracker, the response will be generated with more flexibility.ßdummyTurker
14862ß0.0ßMeanwhile, both modules need to learn to collaborate with each other.ßdummyTurker
14863ß0.0ßWe would like to apply automatic metrics for fluency when available.ßdummyTurker
14864ß0.0ßWe notice that [arxiv.org/abs/1605.06069] used a language model like entropy-based metric for response evaluation.ßdummyTurker
14865ß0.0ßHowever, instead of measuring the fluency,  they used it to measure the informativeness.ßdummyTurker
14866ß0.0ßThey explained that a response with a higher entropy implies that it contains more information.ßdummyTurker
14867ß0.0ßWe use GUID  for all the entities and relations.ßdummyTurker
14868ß0.0ßIn facts matching, for a fact in the form of a triplet, we average the embedding of its subject and relation to formulating a fact embedding.ßdummyTurker
14869ß0.0ßThen, the embeddings of GUIDs are learned during the joint training of knowledge retriever and response generation.ßdummyTurker
14870ß0.0ßLines 236-249 (right) is just an example to intuitively show how facts matching and entity diffusion collaborate.ßdummyTurker
14871ß0.0ßWith facts matching, NKD is capable of answering factoid questions ( Table 3 ).ßdummyTurker
14872ß0.0ßWith entity diffusion, NKD also handles the facts related chit-chats much better than the baselines ( Table 4 ).ßdummyTurker
14873ß0.0ßThe model automatically learns to utilize the retrieved facts and diffused entities using h^K_{N_X} (line 223).ßdummyTurker
14874ß0.0ßIf it fails to encode the knowledge entities and relations within the inputs, then it may fail to answer factoid questions and facts related chit-chats.ßdummyTurker
14875ß0.0ßTherefore, the interpretation learning is naturally guided by the response generation.ßdummyTurker
14876ß0.0ßWe choose the recurrent unit LSTM, the dimensionality of embeddings empirically.ßdummyTurker
14877ß0.0ßDuring training, we find that the performance of 2-layer DNN and 3-layer DNN are comparable and both better than 1-layer DNN for tracker in the decoder.ßdummyTurker
14878ß0.0ßTherefore, we utilize the 2-layer setting for efficiency.ßdummyTurker
14879ß0.0ßDetailed comparisons will be released if there are new discoveries.ßdummyTurker
14880ß0.0ßWe will cite Dhingra et al.ßdummyTurker
14881ß0.0ß(2017) as suggested.ßdummyTurker
14882ß0.0ßThanks again.ßdummyTurker
14883ß0.0ßWe thank the reviewers suggestion and comments.ßdummyTurker
14884ß0.0ßPlease see the motivations in the first paragraph of the reply to reviewer #2, due to word limitation.Thanks.ßdummyTurker
14885ß0.0ßDesign choices.ßdummyTurker
14886ß0.0ßWe are devoted to design a general and effective framework to fit the motivation.ßdummyTurker
14887ß0.0ßFirstly, making object-centric, each object has a unique memory to store the extracted knowledge of the document.ßdummyTurker
14888ß0.0ßInspired by NTM[Graves et al., 2014], Matrix Memory keeps the knowledge that is fuzzy, uncertain or incomplete.ßdummyTurker
14889ß0.0ßWe also add Action History to store the action sequence for summarizing some regularity of action sequence when needed.ßdummyTurker
14890ß0.0ßSecondly, the actions of OONP are divided into three categories, namely, New-assign, Update.X, Update2what.ßdummyTurker
14891ß0.0ßThe action 'New-assign' is to determine whether the current object is a new object or an existing object.ßdummyTurker
14892ß0.0ßInspired by the content addressing of NTM, we use similarity function to handle the dynamic multi-class classification for “New-assign” action.ßdummyTurker
14893ß0.0ßAfter determining the object, it is naturally needed to determine which property should be updated and updated to what.ßdummyTurker
14894ß0.0ßSuch designed structure and actions are designed naturally to fit the demands in narrative logic of human.ßdummyTurker
14895ß0.0ßFinally, a series of specific actions of “Update.X” and “Updata2what” that are not constrained by our framework can be designed, to fit the downstream applications.ßdummyTurker
14896ß0.0ßOONP parser is not prone to overfitting and in the last two experiments, about 2000 documents can train a strong model.ßdummyTurker
14897ß0.0ßWe did the additional experiments of task III.ßdummyTurker
14898ß0.0ßOONP parser can achieve 94.7/95.2/96.3% assign accuracy when given 500/1000/2000 training samples, with the same size of validation/testing set.ßdummyTurker
14899ß0.0ßThe OONP parser does not demand large size of parameters and are easy to learn due to the high reusability of components such as similarity function.ßdummyTurker
14900ß0.0ßA similar setting without the constrains on grammar could be find in SeqGAN (L. Yu 2017).ßdummyTurker
14901ß0.0ßWe additionally consider to generate convincing-looking code that obey a pre-defined CFG.ßdummyTurker
14902ß0.0ßThe contributions to do so are as follows:ßdummyTurker
14903ß0.0ß(1) By incorporating the CFG, syntax errors could be eliminated and the generation quality improves.ßdummyTurker
14904ß0.0ß(2) The search space for the generator could be dramatically reduced.ßdummyTurker
14905ß0.0ß(3) The studied problem has many important applications.ßdummyTurker
14906ß0.0ßFor example, in database optimization, a large collection of valid SQL queries is required to test the performance of scheduler and optimizer.ßdummyTurker
14907ß0.0ßBesides, generating such convincing-looking sequence/code could address the imbalance problem in some classification problems (G. Douzas et al.ßdummyTurker
14908ß0.0ß2018).ßdummyTurker
14909ß0.0ßWe random split 20% of the data in each dataset as the test data (references) to evaluate the generations.ßdummyTurker
14910ß0.0ßWe omit the details of the synthetic data generation due to the space limitation.ßdummyTurker
14911ß0.0ßThe palindrome dataset is generated by following the CFG rules described in Sec.ßdummyTurker
14912ß0.0ß3.ßdummyTurker
14913ß0.0ßIn each generation, the data generator keeps generate operations with uniform probability distribution, and it terminates when the corresponding sequence become a valid palindrome.ßdummyTurker
14914ß0.0ßThe SQL data are generated in the same way, with two separate set of grammatical rules select from the SQL-99 standard BNF (https://ronsavage.github.io/SQL/sql-99.bnf.html).ßdummyTurker
14915ß0.0ßThe grammar has a collection of rules that yield terminal symbols.ßdummyTurker
14916ß0.0ßThe goal of sec 3 is to introduce the preliminaries and related concepts about CFGs.ßdummyTurker
14917ß0.0ßWe follow (E. Hopcroft et al.ßdummyTurker
14918ß0.0ß2006) to present the definitions of context-free grammars and parse trees.ßdummyTurker
14919ß0.0ßWe introduced these concepts to make the paper self-contained and more accessible for the general audience.ßdummyTurker
14920ß0.0ßWe will make it clearer in the extended version.ßdummyTurker
14921ß0.0ßWith these definitions, we could transform the sequence generation task into the parse tree generation task.ßdummyTurker
14922ß0.0ßWe were not able to include all the details on synthetic data generation due to the space limitation.ßdummyTurker
14923ß0.0ßThe palindrome dataset is generated by following the CFG rules described in Sec.3.ßdummyTurker
14924ß0.0ßIn each generation, the generator keeps generating operations with uniform probability distribution, and it terminates when the corresponding sequence become a valid palindrome (stop condition described in Sec 4.2).ßdummyTurker
14925ß0.0ßThe SQL data are generated in the same way, with two separate set of grammatical rules from the SQL-99 standard BNF (https://ronsavage.github.io/SQL/sql-99.bnf.html).ßdummyTurker
14926ß0.0ßTreeGAN employs tree generator and TreeLSTM (NOT CNN) as the discriminator, and TreeGAN- replaces the TreeLSTM with LSTM for discriminator.ßdummyTurker
14927ß0.0ßWe do not consider CNN as the discriminator because: (1) CNN is mainly designed for image data instead of text data.ßdummyTurker
14928ß0.0ß(2) follow the success of DCGANs, which employs a convolutional network for decoding and a de-convolutional network for encoding, we employ a tree-structure generator and a tree-structure as the discriminator.ßdummyTurker
14929ß0.0ßWe believe such symmetry in the structure of encoder and decoder make the model neater and easier to understand.ßdummyTurker
14930ß0.0ßBy comparing TreeGAN with TreeGAN-, we know whether employing a tree-structured classifier helps; By comparing TreeGAN with SeqGAN, we know whether employ a tree-structured, syntax-aware generator helps; By comparing TreeGAN with TreeGen, we know whether the adversarial training helps.ßdummyTurker
14931ß0.0ßWe summarized the differences between our work and the closely related code generation works (line 221 - 227).ßdummyTurker
14932ß0.0ßAnd we discussed the differences between our model and other existing GAN frameworks.ßdummyTurker
14933ß0.0ßBesides, we provided an illustration in Figure 2 to compare TreeGAN with DCGAN and SeqGAN (or MaskGAN).ßdummyTurker
14934ß0.0ßIn the quantitative results, we presented the SYNTAX score to show the ability of capturing the grammar, and for synthetic SQL datasets we also reported the SCHEMA score to show how well could each model learn the underlying schema.ßdummyTurker
14935ß0.0ßThese metrics are not used in previous work, and we think they provide an approach to evaluate the generated code grammatically.ßdummyTurker
14936ß0.0ßIn line 696-730, we provided analysis on the generations of synthetic SQL datasets.ßdummyTurker
14937ß0.0ßIn line 768-788, we provided analysis on the generations for the real python code datasets.ßdummyTurker
14938ß0.0ßMore results will be available in the extended version.ßdummyTurker
14939ß0.0ßThank you for the review, the detailed questions/suggestions and the insightful comments!ßdummyTurker
14940ß0.0ß"Weakness_1:""notations_and_descriptions_are_not_well-defined"""ßdummyTurker
14941ß0.0ßN(t) is defined at the beginning of 2.4.2.ßdummyTurker
14942ß0.0ßIn line 240ßdummyTurker
14943ß0.0ß"""units"" are defined as ""words or ngrams""."ßdummyTurker
14944ß0.0ßWe will changeßdummyTurker
14945ß0.0ß"""surface unit"" to ""unit"" in section 2.4.2 and define N in"ßdummyTurker
14946ß0.0ßTable2 better (see 530-535).ßdummyTurker
14947ß0.0ß"Weakness_2:""it_is_not_clear_why_and_how_the_authors_choose_pivot_languages."""ßdummyTurker
14948ß0.0ßWe choose pivot-languages to reduce the number of dictionaryßdummyTurker
14949ß0.0ßgraphs.ßdummyTurker
14950ß0.0ßWith 1664 editions it is infeasible to work with theßdummyTurker
14951ß0.0ßfull dictionary graph (it contains >2,000,000 bilingualßdummyTurker
14952ß0.0ßdictionaries and probably 1,000,000,000,000 edges).ßdummyTurker
14953ß0.0ßChoosingßdummyTurker
14954ß0.0ß10 pivot-languages seems a reasonable trade-off betweenßdummyTurker
14955ß0.0ßcomputation time and multilinguality.ßdummyTurker
14956ß0.0ß"Weakness_2:""What_does_""type""_mean_in_section_2.1?"""ßdummyTurker
14957ß0.0ßType means word type.ßdummyTurker
14958ß0.0ß"Our objective was to choose ""simple"""ßdummyTurker
14959ß0.0ßpivot-languages, i.e., languages that encode linguisticßdummyTurker
14960ß0.0ßconcepts directly on the surface, making it unnecessary toßdummyTurker
14961ß0.0ßbuild a complex language-specific preprocessing pipeline forßdummyTurker
14962ß0.0ßeach of the 1259 languages.ßdummyTurker
14963ß0.0ß"Weakness_2:""pivot_languages_..._are_all_with_limited_spoken"""ßdummyTurker
14964ß0.0ß"""populations._I_am_curious_about_these_languages_could"""ßdummyTurker
14965ß0.0ß"""actually_achieve_better_results."""ßdummyTurker
14966ß0.0ßWe selected the pivot-languages based on mathematical andßdummyTurker
14967ß0.0ßcomputational criteria.ßdummyTurker
14968ß0.0ßWe would be interested in argumentsßdummyTurker
14969ß0.0ßfor taking the number of speakers into account.ßdummyTurker
14970ß0.0ß"Weakness_3:""What's_the_difference_between_strict_and_relaxed?"""ßdummyTurker
14971ß0.0ßOn the word-level there is no difference.ßdummyTurker
14972ß0.0ßOn theßdummyTurker
14973ß0.0ß"ngram-level, relaxed includes ""ambiguous"" ngrams (e.g.,"ßdummyTurker
14974ß0.0ß"""roun"" for ""round"", even though ""ground"" also contains it)."ßdummyTurker
14975ß0.0ßDetails can be found in supplementary.ßdummyTurker
14976ß0.0ßThank you for the review, the detailed questions/suggestions and the insightful comments!ßdummyTurker
14977ß0.0ß"Weakness_1:""limited_to_vocabulary_found_in_the_New_Testament"""ßdummyTurker
14978ß0.0ßWe agree, but we have the best coverage of languagesßdummyTurker
14979ß0.0ßachieved to date for this type of study.ßdummyTurker
14980ß0.0ßObviously, there is a trade-off between coverage ofßdummyTurker
14981ß0.0ßvocabulary and coverage of languages.ßdummyTurker
14982ß0.0ß"Weakness_2/Question_1:""How_many_different_concepts_for_one_language_do_you_learn?"""ßdummyTurker
14983ß0.0ßWe learn multilingual concepts, so there is only one set of conceptsßdummyTurker
14984ß0.0ßacross all languages.ßdummyTurker
14985ß0.0ßE.g., there are 187572 CLIQUE-conceptsßdummyTurker
14986ß0.0ß(they overlap and no concept spans all 1259 languages).ßdummyTurker
14987ß0.0ßExample (KJV): 3581 unique English words and 987 uniqueßdummyTurker
14988ß0.0ß4-grams are covered by CLIQUE-concepts.ßdummyTurker
14989ß0.0ß"Weakness_2:""number_of_CC_were'too_low'"""ßdummyTurker
14990ß0.0ßSee lines 322-327.ßdummyTurker
14991ß0.0ß"Weakness_3:""measures_S1,_S16_and_R1"""ßdummyTurker
14992ß0.0ßWe will expand the explanation.ßdummyTurker
14993ß0.0ß"Question_2:""How_would_you_extend_the_embedding_spaces_to_new_concepts"""ßdummyTurker
14994ß0.0ßhow to extend concepts (presumably if the underlyingßdummyTurker
14995ß0.0ßdictionary graph is enlarged by new nodes and edges) and howßdummyTurker
14996ß0.0ßto fold new vectors into an existing embedding space.ßdummyTurker
14997ß0.0ßBothßdummyTurker
14998ß0.0ßseem interesting research issues in itself, but are beyondßdummyTurker
14999ß0.0ßthe scope of this paper.ßdummyTurker
15000ß0.0ß"Question_3/8:""some_languages_suffer_comparatively_little"ßdummyTurker
15001ß0.0ß"from_sparseness_semantic_information...in_a_simply_accessible_form"""ßdummyTurker
15002ß0.0ßSome languages are easily tokenizable using basicßdummyTurker
15003ß0.0ßheuristics.ßdummyTurker
15004ß0.0ßWhat's really important is that this is not trueßdummyTurker
15005ß0.0ßfor many languages: they do not have any whitespacesßdummyTurker
15006ß0.0ßbetween words and the semantics (of types that are oftenßdummyTurker
15007ß0.0ßhapax legomena) are not simply accessible.ßdummyTurker
15008ß0.0ßTheßdummyTurker
15009ß0.0ßpivot-languages do not have this problem.ßdummyTurker
15010ß0.0ß"Question_4-6:""verse_empty"",""new_testaments_are_not_complete"""ßdummyTurker
15011ß0.0ßThe corpus contains some errors, some verses are empty.ßdummyTurker
15012ß0.0ß"Question_10:_BOW:_""Would_it_have_taken_more_time_to_train?"""ßdummyTurker
15013ß0.0ßThis would take weeks.ßdummyTurker
15014ß0.0ßWe can run this experiment if theßdummyTurker
15015ß0.0ßreviewer thinks it's important.ßdummyTurker
15016ß0.0ß"Question_11:""training_corpus_size_of_50_GB"""ßdummyTurker
15017ß0.0ßWe write each line out several times.ßdummyTurker
15018ß0.0ßSee line 546.ßdummyTurker
15019ß0.0ß"Question_7/9/12:""concerns_about_round-trip""/""Give_examples_of_chosen_and_reject_concepts."ßdummyTurker
15020ß0.0ß"""/""weak_and_strong_embeddings"""ßdummyTurker
15021ß0.0ßWe don't have space for answering these questions here, butßdummyTurker
15022ß0.0ßwould do so in the paper in case of acceptance.ßdummyTurker
15023ß0.0ßOur proposed multi-view learning framework actually outperforms InferSent on all unsupervised evaluation tasks, and some supervised evaluation tasks, including TREC and SUBJ.ßdummyTurker
15024ß0.0ßOur focus here is learning from unlabelled corpora which are massively and freely available online, while the InferSent model was trained on a human-annotated dataset SNLI.ßdummyTurker
15025ß0.0ßWorking with unlabelled corpora is desirable since it doesn’t require costly and time-consuming human labour to annotate the data.ßdummyTurker
15026ß0.0ßThe trainable temperature term \tau decreases through training, and another model trained with fixed \tau set to the final value (0.046) from the previous experiment performed similarly.ßdummyTurker
15027ß0.0ßTraining with a static value of \tau=0.05 worked similarly.ßdummyTurker
15028ß0.0ßWe could expand the Section 5 discussion of this.ßdummyTurker
15029ß0.0ßRemoving the first principal component during testing provides performance gain on unsupervised evaluation tasks even though it is not incorporated in training.ßdummyTurker
15030ß0.0ßThis finding is first explored in Arora et al.ßdummyTurker
15031ß0.0ß(2017), later in Mu et al.ßdummyTurker
15032ß0.0ß(2018).ßdummyTurker
15033ß0.0ßSince we tuned our model based on STS14, and we want to minimize the discrepancy between training and testing, the method is also incorporated in training, and it provides an additional performance gain.ßdummyTurker
15034ß0.0ßOur model indeed uses a classification loss.ßdummyTurker
15035ß0.0ßWe are computing softmax over the cosine similarity and then use the cross-entropy loss.ßdummyTurker
15036ß0.0ßIn word2vec, they compute softmax over the dot-product and then use the cross-entropy loss (but in practice, skip-gram and CBOW both use the negative sampling to approximate the cross-entropy loss on the softmax function).ßdummyTurker
15037ß0.0ßThe QT paper reported the performance of their model only on STS14 among all unsupervised evaluation tasks presented in our paper.ßdummyTurker
15038ß0.0ßThere is no publicly available implementation of the QT model, and we spent roughly a month trying to reproduce their model but were unable to replicate their results.ßdummyTurker
15039ß0.0ßWe emailed the authors of the QT paper for clarification/assistance at the time, but they still haven’t replied to us.ßdummyTurker
15040ß0.0ßWe have already copied results of all evaluation tasks which the QT model was evaluated on, in order to compare with our model.ßdummyTurker
15041ß0.0ßSince we are still not able to reproduce the results of their model, we think our current comparison is sufficient.ßdummyTurker
15042ß0.0ßAll the hyperparameter settings are listed in the supplemental material submitted with the paper at the same time separately, as required by ACL policy.ßdummyTurker
15043ß0.0ßThe main hyperparameters: hidden size d = 800, batch size N = 1024, and kernel size of the single-layer ConvNet K=3, could be added to the paper.ßdummyTurker
15044ß0.0ßWe want to keep the networks that create z^g and z^f different.ßdummyTurker
15045ß0.0ßThe z^g is from a simple linear network (which is why we use average pooling).ßdummyTurker
15046ß0.0ßWe try to keep the linearity in computing the z^g, since it helps increase the difference to the nonlinearly computed z^f.ßdummyTurker
15047ß0.0ßAlg.1 is the power iteration algorithm, and we will add the reference to von Mises & Pollaczek-Geiringer (1929) [1].ßdummyTurker
15048ß0.0ßThanks for pointing it out.ßdummyTurker
15049ß0.0ßIn our experiments, the power method converges before the last iteration since the vectors are not zero-centred, and the resulting vector is extremely close to the first singular vector.ßdummyTurker
15050ß0.0ßBy removing that, we enforce the vectors to be isotropic around a certain point, and this point is very close to the origin (verified empirically but also a result of the first singular vector being dominated by the mean vector).ßdummyTurker
15051ß0.0ßAs you suggest, other methods could be used instead, but this method is simple and consistent with Arora et al.ßdummyTurker
15052ß0.0ß(2017) and Mu et al.ßdummyTurker
15053ß0.0ß(2018).ßdummyTurker
15054ß0.0ßThe cosine similarity measure is used on unsupervised evaluation tasks in order to be consistent with and comparable to prior work (e.g.ßdummyTurker
15055ß0.0ßArora et al.ßdummyTurker
15056ß0.0ß(2017) and Hill et al.ßdummyTurker
15057ß0.0ß(2016)).ßdummyTurker
15058ß0.0ßSince we want to improve the performance on unsupervised evaluation tasks while maintaining good performance on the supervised ones, learning with the cosine similarity (the unsupervised measure) is a reasonable choice for us.ßdummyTurker
15059ß0.0ßAs suggested in Logeswaran & Lee (2018), a minibatch is constructed using N contiguous sentences in a corpus.ßdummyTurker
15060ß0.0ßThe positive samples are the adjacent 2 sentences, as indicated in Eq.ßdummyTurker
15061ß0.0ß5, and the negative samples are the remaining sentences in the minibatch.ßdummyTurker
15062ß0.0ßLogeswaran & Lee (2018) found that this sampling strategy performed similarly to other methods such as random sampling.ßdummyTurker
15063ß0.0ßThe single-view baseline mentioned in our paper is using separately trained z^f and z^g and comparing the performance of their concatenated vector.ßdummyTurker
15064ß0.0ßIn our paper, the results show that, by learning through multi-view, each view, z^f or z^g, gets improved, and an ensemble of 2 views provide even stronger results.ßdummyTurker
15065ß0.0ßAnother baseline which you suggested is to compare an ensemble of 2 “multi-view” learnt (separately parameterised) z^f or z^g.ßdummyTurker
15066ß0.0ßThe ensemble of 2 z^f provides similar results on supervised evaluation tasks excluding SICK-R and SICK-E as our multi-view model does, while it has much worse results on all unsupervised evaluation tasks and SICK-R and SICK-E.ßdummyTurker
15067ß0.0ßThe ensemble of 2 z^g gives us comparable results on all unsupervised evaluation tasks, while it doesn’t provide good results on supervised evaluation tasks.ßdummyTurker
15068ß0.0ß[1] Richard von Mises and H. Pollaczek-Geiringer, Praktische Verfahren der Gleichungsauflösung, ZAMM - Zeitschrift für Angewandte Mathematik und Mechanik 9, 152-164 (1929)ßdummyTurker
15069ß0.0ß# Please refer to Reply to weakness argument3 in Reviewer#2ßdummyTurker
15070ß0.0ßWe appreciate the thoughtful discussion.ßdummyTurker
15071ß0.0ß"There exist pairs of nouns that have a high cosine and at the same time are not semantically equivalent as in ""A car is a truck."""ßdummyTurker
15072ß0.0ßIn theory, such combinations would be problematic.ßdummyTurker
15073ß0.0ßIn practice, however, they rarely appear in learner English because such expressions are not valid even in their native languages; we actually found no such errors in the test data.ßdummyTurker
15074ß0.0ßThe problem is that learners apply other usages derived from their native languages to the English be-verb (copula) usage.ßdummyTurker
15075ß0.0ßThe above fact that Reviewer points out and the above discussion are very important to the present work.ßdummyTurker
15076ß0.0ßWe will add them to the revised paper.ßdummyTurker
15077ß0.0ßWe thank again for the crucial advice.ßdummyTurker
15078ß0.0ßThe performance of the proposed method seems low, but compared to other grammatical error detection tasks, it is relatively high.ßdummyTurker
15079ß0.0ßFor example, the-state-of-the-art error detection method (Kaneko et al.ßdummyTurker
15080ß0.0ß2017) achieves F=37.7 in general and F=0.416 for overgeneralized be-verbs (as described in Reply to weakness argument1 in Review#2).ßdummyTurker
15081ß0.0ß"The majority baseline always tells ""correct"", which means that no errors are detected, which makes recall zero, precision undefined."ßdummyTurker
15082ß0.0ßWe agree with the argument.ßdummyTurker
15083ß0.0ßAt the same time, the crude approximation work well in practice; learners make overgeneralized be-verb errors because they apply other usages derived from their native languages to English.ßdummyTurker
15084ß0.0ß"For example, they don't normally make an error such as ""A car is a truck"" as Reviewer#3 (below) points out because such expressions are not valid even in their native languages."ßdummyTurker
15085ß0.0ßIf they often made such errors, the crude approximation would cause a problem, but it is not the case as shown in the evaluation.ßdummyTurker
15086ß0.0ßRather, application of other usages from their native language is the major cause of this type of error, which can be detected by the approximation.ßdummyTurker
15087ß0.0ßAs Reviewer #1 points out, the reader will likely wonder at this point.ßdummyTurker
15088ß0.0ßWe will make this point clear in the revised paper.ßdummyTurker
15089ß0.0ßWe thank the reviewer for letting us notice about this.ßdummyTurker
15090ß0.0ßSince we did not show the detailed statistics of the test data, it was confusing.ßdummyTurker
15091ß0.0ßThe test data is not particular small compared to other test data sets used in the literature.ßdummyTurker
15092ß0.0ßPrecisely, it consists of 233 essays on 10 prompts, 3,260 sentences, which is comparable to, for example, the CoNLL-2014 test data (50 essays on two prompts, 1,312 sentences).ßdummyTurker
15093ß0.0ßThe number of overgeneralized be-verbs is 83 in the test data, which is less than the most frequent errors such as preposition errors (390) in the CoNLL-2014 test data, but comparable with other major error types such as word form (103) and word order (54).ßdummyTurker
15094ß0.0ßThe statistics is similar in the CoNLL-2013 test data.ßdummyTurker
15095ß0.0ßWe will make it clearer in the revised paper.ßdummyTurker
15096ß0.0ßSome part of this comment is missing?ßdummyTurker
15097ß0.0ßYes, it would be beneficial to the method, especially for the reversal.ßdummyTurker
15098ß0.0ßPart of them is indirectly considered in one of the baseline methods because it uses the WordNet hierarchy.ßdummyTurker
15099ß0.0ßWe will mention this, citing Dagan&Rei in the revised paper.ßdummyTurker
15100ß0.0ßIn theory, metonymic usage might cause a problem.ßdummyTurker
15101ß0.0ßIn practice, learners of English (especially, beginning to intermediate ones) tend to use literal expressions more often (as shown in the accompanied test data).ßdummyTurker
15102ß0.0ß"Also, metonymic usage is often with an adjective as in ""The village is conservative."ßdummyTurker
15103ß0.0ß""", which is excluded from detection."ßdummyTurker
15104ß0.0ßIt is not straightforward how to apply the state-of-the-art methods to overgeneralized be-verb detection.ßdummyTurker
15105ß0.0ß"For example, from ""Paris is rain."ßdummyTurker
15106ß0.0ß""", the NMT-based methods might detect/correct either ""Paris"", ""is"", or ""rain"", or two/three of them, but it is not known which one(s) should be treated as correct detection."ßdummyTurker
15107ß0.0ß"Furthermore, they will likely detect/correct ""style"" from the following (actual) example ""Gardening is many style."ßdummyTurker
15108ß0.0ß"""; it is not sure whether they detected/corrected it as a number error or an overgeneralized be-verb."ßdummyTurker
15109ß0.0ßThese become problematic in evaluation.ßdummyTurker
15110ß0.0ßThis is why we did not include the state-of-the-art methods in the evaluation.ßdummyTurker
15111ß0.0ßHaving said that, we tested the-state-of-the-art neural-based method (Kaneko et al.ßdummyTurker
15112ß0.0ß2017) with the following three conditions: judged as overgeneralized be-verb (1) if either the subject, be-verb, or complement is detected as an error (EITHER); (2) if the be-verb is detected (BE); if all three are detected (ALL).ßdummyTurker
15113ß0.0ßThe results are far worse than the baselines:ßdummyTurker
15114ß0.0ßEITHER: A:0.599,R:0.506,P:0.353,F:0.416ßdummyTurker
15115ß0.0ßBE: A:0.714 (only one error was detected which was a false positive)ßdummyTurker
15116ß0.0ßALL: A: 0.713 (no error detected)ßdummyTurker
15117ß0.0ßBesides, the proposed method has two advantages over the-state-of-the-art methods: (i) it does not require manually created training data (e.g., original-corrected sentence pairs); (ii) it can distinguish overgeneralized be-verb errors from the other error types.ßdummyTurker
15118ß0.0ßThese points were crucial and we will make them clear in the revised one.ßdummyTurker
15119ß0.0ßAs Reviewer #2 points out, it is a weakness of the proposed method.ßdummyTurker
15120ß0.0ßHowever, it outperforms the baselines that use the same information.ßdummyTurker
15121ß0.0ßConsidering this, it will likely outperform them with the error frequency estimated from a development set.ßdummyTurker
15122ß0.0ßAs language learning assistance, it is crucial to be able to explain to the learner why the word in question is wrong.ßdummyTurker
15123ß0.0ßThere has been very limited work in this regard in the literature and almost no method is capable of this.ßdummyTurker
15124ß0.0ßThe proposed method achieves this, distinguishing this type of error from the others, which in turn can be used to explain the rule that the subject and complement are semantically equivalent.ßdummyTurker
15125ß0.0ßIn this regard, this work sets the first step to realizing feedback for learners.ßdummyTurker
15126ß0.0ß#Please refer to Reply to weakness argument2 in Reviewer#1ßdummyTurker
15127ß0.0ßComment on statistics of error and Table1.ßdummyTurker
15128ß0.0ßWe thank the reviewer for the advice.ßdummyTurker
15129ß0.0ßWe will add the information and fix the problem.ßdummyTurker
15130ß0.0ßWe thank the reviewer for highlighting these issues, which we will address in the next revision.ßdummyTurker
15131ß0.0ßThe fact that supervised trees for this task were suboptimal is on the one hand unfair, but on the other hand shows that our method is applicable to many languages without high-quality parsers.ßdummyTurker
15132ß0.0ßOur model has three advantages over LSTMsßdummyTurker
15133ß0.0ß"""u"" is indeed a vector parameter."ßdummyTurker
15134ß0.0ß"We used ""parameter"" in the way it is used in PyTorch/DyNet, where the term does not preclude something from being a vector."ßdummyTurker
15135ß0.0ßAs this is not how everyone in NLP uses this term, we will improve the wording.ßdummyTurker
15136ß0.0ßTemperature is sometimes used in reinforcement learning.ßdummyTurker
15137ß0.0ßSetting it to 1 yields standard softmax.ßdummyTurker
15138ß0.0ßLowering it makes the output closer and closer to one-hot.ßdummyTurker
15139ß0.0ßWe will improve the explanation, and add references.ßdummyTurker
15140ß0.0ßA good one is Sutton and Barto's bookßdummyTurker
15141ß0.0ßSNLI mainly consists of short single sentences.ßdummyTurker
15142ß0.0ßThe Dictionary data has unpunctuated, untokenized multisentence definitions, including usage examples.ßdummyTurker
15143ß0.0ßThere are differences between the trees induced by these models for the same sentence, but we suspect this is mostly because they struggle with language that is very different from their training data.ßdummyTurker
15144ß0.0ßWe are in the uncomfortable position of being unable to replicate Hill et al, even after extensive discussion with the authors.ßdummyTurker
15145ß0.0ßOur reimplementation of their BoW model, using their vectors, performed worse than what they report.ßdummyTurker
15146ß0.0ßWe hint at this in footnote 3, but will state it explicitly in future revisions.ßdummyTurker
15147ß0.0ßWe even found their embeddings to lack coverage of several words in their test set (this is easily verifiable as their data is online).ßdummyTurker
15148ß0.0ßLe & Zuidema's model is indeed similar to ours, but with three main differences, which we will discuss more throughly in our next revision.ßdummyTurker
15149ß0.0ß(1) We don't use an automatic parser.ßdummyTurker
15150ß0.0ßNot biasing the model with prior information on the structure of trees allows it to be used as an empirical test for syntactic theory.ßdummyTurker
15151ß0.0ß(2) We use TreeLSTMs instead CNNs, and (3) soft gating instead of pooling.ßdummyTurker
15152ß0.0ßThese small differences have important consequencesßdummyTurker
15153ß0.0ßWe acknowledge that the comparison is lacking, and will aim to do a better job.ßdummyTurker
15154ß0.0ßWe find our method is interesting for two main reasons.ßdummyTurker
15155ß0.0ß(1) Yogatama et al.ßdummyTurker
15156ß0.0ßuse shift-reduce parsing for tree induction.ßdummyTurker
15157ß0.0ßChoi et al.ßdummyTurker
15158ß0.0ßuse easy-first parsing.ßdummyTurker
15159ß0.0ßChart parsing is another established parsing strategy, and we show it is viable and performs equally well or better on NLI.ßdummyTurker
15160ß0.0ß(2) Yogatama et al.ßdummyTurker
15161ß0.0ßuse reinforcement learning.ßdummyTurker
15162ß0.0ßChoi et al.ßdummyTurker
15163ß0.0ßuse a sampling method.ßdummyTurker
15164ß0.0ßWe show it is possible to have a fully-differentiable model instead.ßdummyTurker
15165ß0.0ßReinforcement learning is known to suffer from high variance.ßdummyTurker
15166ß0.0ß"This is probably why Yogatama et al perform even below LSTMs, and fail to induce ""real"" trees (they are almost entirely left-branching) (Williams et al)."ßdummyTurker
15167ß0.0ßChoi et al.ßdummyTurker
15168ß0.0ßis based on a differentiable estimator, but the results of the sampling are used in a non-differentiable way to drive the composition.ßdummyTurker
15169ß0.0ßThe training signal cannot propagate to all possible composition paths, as most are never attempted at all.ßdummyTurker
15170ß0.0ßThis may play a role in the high inconsistency of their trees across reruns (Williams et al).ßdummyTurker
15171ß0.0ßWe agree that an error analysis with Choi et al would have been desirable.ßdummyTurker
15172ß0.0ßTheir code was only recently published, following acceptance of their paper into AAAI-18.ßdummyTurker
15173ß0.0ßA detailed comparison and additional experiments were not possible due to the short timeframe.ßdummyTurker
15174ß0.0ß"We initially tried a multilinear perceptron which looked at both constituents being considered for composition, nicely interpretable as ""should these constituents be merged?""."ßdummyTurker
15175ß0.0ßWe found that a simple dot product between u and the composed vector was faster and led to comparable downstream performance, so we chose it for empirical reasons.ßdummyTurker
15176ß0.0ß"Similarity to an ""ideal vector"" is one interpretation, but would be more accurately achieved with cosine."ßdummyTurker
15177ß0.0ßThe dot product is similar, but the lack of normalisation affords more representational power.ßdummyTurker
15178ß0.0ßThis is analogous to linear predictor functions, where feature vectors are dotted with weight vectors to produce scores.ßdummyTurker
15179ß0.0ßWe note Choi et al.ßdummyTurker
15180ß0.0ßalso take this approach.ßdummyTurker
15181ß0.0ßbullet 1: We will improve the characterization of `nextword' in the final revision.ßdummyTurker
15182ß0.0ßbullet 2: We will make section 2 easier to follow by adding citations to the relevant RNNG literature.ßdummyTurker
15183ß0.0ßSection 3 reviews beam search (e.g.ßdummyTurker
15184ß0.0ßchapter 4 of Russell and Norvig's AI textbook) as applied by Stern et al 2017; the presentation is necessarily condensed, compared to these works.ßdummyTurker
15185ß0.0ßSimilarly, the presentation of complexity metrics is condensed compared to the cited review article, Hale 2016.ßdummyTurker
15186ß0.0ßThe results in Table 1 are on the Penn Treebank, a standard benchmark (see caption).ßdummyTurker
15187ß0.0ß"The beam formalizes the ""rank"" in the sense of e.g."ßdummyTurker
15188ß0.0ßGibson’s dissertation.ßdummyTurker
15189ß0.0ßThanks, will look into Demberg's model and see whether it has implications for this study.ßdummyTurker
15190ß0.0ß"- Reply to weakness argument 1 (""more related work should be included""):"ßdummyTurker
15191ß0.0ß- We will add more relevant citations in the revision.ßdummyTurker
15192ß0.0ß"- Reply to weakness argument 2 (""not in terms of situational (semantic and pragmatic) appropriateness""):"ßdummyTurker
15193ß0.0ß- We believe this claim may not be that clear, as the language model should capture some aspects of semantic and pragmatics.ßdummyTurker
15194ß0.0ß"- Reply to weakness argument 3 (""semantic and pragmatic coherence are much more important""):"ßdummyTurker
15195ß0.0ß- We agree that the current approach is unable to tease apart these factors, but contend that language model sentence probability will still capture some aspect of intents and content.ßdummyTurker
15196ß0.0ßHaving said that, we agree it'd be interesting to model them separately as a future work.ßdummyTurker
15197ß0.0ß- We will define this in the revision.ßdummyTurker
15198ß0.0ß"- ""examples of the sentences and the judgements"":"ßdummyTurker
15199ß0.0ß- We will include examples of sentences and ratings in the revision.ßdummyTurker
15200ß0.0ß- We will include the necessary citations (Lau et al.ßdummyTurker
15201ß0.0ß(2015, 2016)).ßdummyTurker
15202ß0.0ß"- Reply to weakness argument 1 (""The hypotheses put forward are not supported by previous work""):"ßdummyTurker
15203ß0.0ß- We will describe the motivations that lead us to these hypotheses, and include more relevant papers.ßdummyTurker
15204ß0.0ßWe note that understanding how document context influences human acceptability judgments is crucial to success in the task of modelling and predicting human acceptability judgments.ßdummyTurker
15205ß0.0ßThis task is important for several NLP applications, such as MT evaluation, style and coherence assessment, and NL generation.ßdummyTurker
15206ß0.0ß"- Reply to weakness argument 2 (""The notion of acceptability is not explicitly defined""):"ßdummyTurker
15207ß0.0ß- We measure acceptability because grammaticality is a theoretical concept that is difficult to elicit from non-expert assessors.ßdummyTurker
15208ß0.0ßWe are interested in predicting pre-theoretical naturalness ratings, rather than grammaticality judgments as such.ßdummyTurker
15209ß0.0ß- We agree that we do not separate syntactic or semantic factors in naturalness ratings.ßdummyTurker
15210ß0.0ßThis is a design choice.ßdummyTurker
15211ß0.0ßAs language models capture both syntax and semantics, acceptability constitutes an appropriate property to focus on for our experiments.ßdummyTurker
15212ß0.0ßIn follow up work we intend tease apart syntactic and semantic elements of acceptability judgments.ßdummyTurker
15213ß0.0ß"- ""whether participants actually took the time to read the context?"""ßdummyTurker
15214ß0.0ß- We briefly looked at this but did not see any striking anomalies in individual annotation time.ßdummyTurker
15215ß0.0ßWe will include timing statistics in the revision.ßdummyTurker
15216ß0.0ß"- Reply to weakness argument 1 (""better outlined the task""):"ßdummyTurker
15217ß0.0ß- The document contexts were not round-trip translated, and so they are always coherent/acceptable.ßdummyTurker
15218ß0.0ßWe asked workers to judge how natural they find a sentence.ßdummyTurker
15219ß0.0ßWe will include more description for the MTurk task, e.g.ßdummyTurker
15220ß0.0ßparticipant statistics, and add a screenshot of the HIT.ßdummyTurker
15221ß0.0ß"- Reply to question 1 (""specify the number of participants""):"ßdummyTurker
15222ß0.0ß- Each HIT was annotated by 20 MTurkers.ßdummyTurker
15223ß0.0ßWe will include these numbers and statistics in the revision.ßdummyTurker
15224ß0.0ß"What do the numbers associated with h- and h+ at line 193 mean?"")"ßdummyTurker
15225ß0.0ß- The standard deviation is computed over the multiple human ratings for a sentence, and then we take its mean (i.e.ßdummyTurker
15226ß0.0ßmean of sd) to aggregate over all 500 sentences.ßdummyTurker
15227ß0.0ßThis value provides a metric for measuring the average variability of human judgements over our two test sets.ßdummyTurker
15228ß0.0ß"- Reply to question 3 (""multilevel modeling approach""):"ßdummyTurker
15229ß0.0ß- An interesting paper.ßdummyTurker
15230ß0.0ßThanks.ßdummyTurker
15231ß0.0ß"- Reply to question 4 (""get Human Subjects approval...""):"ßdummyTurker
15232ß0.0ß- Yes.ßdummyTurker
15233ß0.0ß"- Reply to question 5 (""make the ratings dataset public?"ßdummyTurker
15234ß0.0ß"""):"ßdummyTurker
15235ß0.0ß- Yes, we intend to publish the dataset (fn 1).ßdummyTurker
15236ß0.0ß"- ""I might refer the authors to general studies of discourse processing..."""ßdummyTurker
15237ß0.0ß- Yes, thanks.ßdummyTurker
15238ß0.0ß"- ""documenting informed consent and protecting participant anonymity"":"ßdummyTurker
15239ß0.0ß- We will clarify this in the revision.ßdummyTurker
15240ß0.0ß1.ßdummyTurker
15241ß0.0ß"""how 'acceptability' was defined to the users"": we ask workers to judge how natural they find a sentence."ßdummyTurker
15242ß0.0ßWe will clarify this in the revision.ßdummyTurker
15243ß0.0ß2.ßdummyTurker
15244ß0.0ß"""why context helps the LSTMs model human judgments"": Context puts the sentence into the right 'topic' space, and our positive TDLM results clearly support this view."ßdummyTurker
15245ß0.0ß"- Reply to weakness argument 2 (""why people should care about the effect of context on grammaticality""):"ßdummyTurker
15246ß0.0ß- A strong correlation between unsupervised language model sentence probability and acceptability demonstrates the plausibility that linguistic knowledge can be represented as a probabilistic system.ßdummyTurker
15247ß0.0ßThis is discussed in Lau et al.ßdummyTurker
15248ß0.0ß(2016).ßdummyTurker
15249ß0.0ß- Also, this task is important for several NLP applications, such as MT evaluation, style and coherence assessment.ßdummyTurker
15250ß0.0ß"- Reply to weakness argument 3 (""MTurk experiments was not spelled out very clearly""):"ßdummyTurker
15251ß0.0ß- We will include more details in the revision.ßdummyTurker
15252ß0.0ß"- ""What do you mean by ""filter""?"ßdummyTurker
15253ß0.0ß""":"ßdummyTurker
15254ß0.0ß- By filter we meant 'removing' workers based on criteria defined in fn 3.ßdummyTurker
15255ß0.0ß- We will define this in the revision.ßdummyTurker
15256ß0.0ßIn our work, we also proposed a multi-level comparison.ßdummyTurker
15257ß0.0ßThis scheme performs the word-word comparison, sentence-sentence comparison, and sentence-word comparison.ßdummyTurker
15258ß0.0ßWe also do experiments to evaluate the efficiency of this multi-level comparison.ßdummyTurker
15259ß0.0ßBecause the performance of M-Project-Average is not strong, we did not report in this manuscript.ßdummyTurker
15260ß0.0ßThank for your recommendation, we will compare with the TF-KLD work.ßdummyTurker
15261ß0.0ßThe performance of Tai et al.ßdummyTurker
15262ß0.0ß(ACL 2015) model is not competitive compared to others, so we did not mention it to save space.ßdummyTurker
15263ß0.0ßn/aßdummyTurker
15264ß0.0ßThank you for pointing this out.ßdummyTurker
15265ß0.0ßWe will clarify the terminology in the beginning to prevent any confusion.ßdummyTurker
15266ß0.0ßThank you very much for the suggestion.ßdummyTurker
15267ß0.0ßWe will incorporate the references into the final draft.ßdummyTurker
15268ß0.0ßWe use stochastic gradient descent to optimize our loss function.ßdummyTurker
15269ß0.0ßWe will add the optimization details in the final draft.ßdummyTurker
15270ß0.0ßThanks for your question.ßdummyTurker
15271ß0.0ßWe do not assume that every word has two distinct meanings.ßdummyTurker
15272ß0.0ßMost words have one distinct meaning, very few have more than two.ßdummyTurker
15273ß0.0ßBy using K=2, the model can represent words with two distinct meanings, as well as provide richer representations for words with one distinct meaning than if we were used to use the K=1 model.ßdummyTurker
15274ß0.0ßFor example, for a word with a single distinct meaning, but with similarities to a broad range of other words, the two component model can learn a unimodal but more heavy tailed representation than the K=1 model corresponding to a Gaussian.ßdummyTurker
15275ß0.0ßIn the case that a single Gaussian is the best representation for a word, the K=2 model will collapse to a single Gaussian representation.ßdummyTurker
15276ß0.0ßIn the experiments, we generally see that the flexibility of K=2 does indeed lead to better quality embeddings.ßdummyTurker
15277ß0.0ßThe overall value of this added flexibility diminishes for K>2 for word similarity evaluation; while in principle K>2 allows even greater flexibility, most words can be very flexibly modelled with a mixture of two Gaussians, leading to K=2 representing a good balance between flexibility and Occam’s razor.ßdummyTurker
15278ß0.0ßWe also note that K=2 can capture more than two distinct meanings.ßdummyTurker
15279ß0.0ßIn general, if a word has multiple meanings, these meanings are usually `compressed’ into the embeddings.ßdummyTurker
15280ß0.0ßFor instance, the word pairs (cell, jail) and (cell, biology) and (cell, phone) will all have positive similarity scores based on K=1 model.ßdummyTurker
15281ß0.0ßHowever, some of the pairs often have lower similarities than others because they’re not dominant meanings.ßdummyTurker
15282ß0.0ßUsing higher K allows different meanings to have disentangled representations, which can better reflect the true similarity scores.ßdummyTurker
15283ß0.0ßNote that is possible a high K would lead to better performance in Word Sense Disambiguation compared to lower K. In fact, different tasks might need different levels of fine-grained meaning separation.ßdummyTurker
15284ß0.0ßFor word similarity evaluation, K=2 strikes a good balance.ßdummyTurker
15285ß0.0ßThank you for pointing this out.ßdummyTurker
15286ß0.0ßWe plan to use our embeddings in downstream tasks in future work.ßdummyTurker
15287ß0.0ßThank you for the comment.ßdummyTurker
15288ß0.0ßWe will try to improve on the clarity of the probabilistic aspect in the final draft.ßdummyTurker
15289ß0.0ßAn existing work by Neelakantan et al.ßdummyTurker
15290ß0.0ß(2014) uses a non-parametric approach to estimate the number of senses for each word.ßdummyTurker
15291ß0.0ßInterestingly, this approach does not outperform the model with fixed number of senses.ßdummyTurker
15292ß0.0ßIn our opinion, inferring the number of senses purely from an unsupervised text corpus remains a challenge.ßdummyTurker
15293ß0.0ßFor our model, it is possible with train with K>2; however, it has been observed in Athiwaratkun et al.ßdummyTurker
15294ß0.0ß(2017) that the dictionary-level Gaussian mixtures with K=3 does not overall improve the word similarity results, even though it can discover 3 distinct senses for certain words.ßdummyTurker
15295ß0.0ßMost words only have one distinct meaning, and very few have more than two.ßdummyTurker
15296ß0.0ßK=2 does not constrain each word to have two distinct meanings, and can learn richer representations for words with single meanings than a K=1 model.ßdummyTurker
15297ß0.0ßFor example, the two mixture components can learn to cluster together to form a more heavy tailed unimodal distribution, that captures a word with one dominant meaning but with close relationships to a wide range of other words.ßdummyTurker
15298ß0.0ßK=2 can perform better for words with single meanings.ßdummyTurker
15299ß0.0ßThe value of such flexibility in practice is not significant for K>2.ßdummyTurker
15300ß0.0ßIn cases where a single Gaussian is the best description of a word, the two components model can learn to use a single Gaussian.ßdummyTurker
15301ß0.0ßThank you for the question!ßdummyTurker
15302ß0.0ßProbabilistic models are useful when the representation helps with (1) the training process or (2) the downstream tasks.ßdummyTurker
15303ß0.0ßIn this paper, we present a probabilistic approach for training which offers highly interpretable sense separation for word embeddings.ßdummyTurker
15304ß0.0ßIn downstream tasks, the probabilistic representation can be useful such as in distribution regression networks (Kou, 2018) which takes a distribution as input and outputs another distribution.ßdummyTurker
15305ß0.0ßOn the other hand, the mean vectors can also be used in a non-probabilistic approach as the usual vector word embeddings such as GloVe or word2vec.ßdummyTurker
15306ß0.0ßThanks for the comments.ßdummyTurker
15307ß0.0ßWe report only one language pair due to space limitation.ßdummyTurker
15308ß0.0ßActually, we got similar and consistent observations on two additional tasks: WMT14 En-De and WMT17 En-Zh, whose baseline results with MLE training using Transformer is 27.12 and 37.02 respectively.ßdummyTurker
15309ß0.0ßWe observed: 1) RL with beam search/multinomial sampling for reward computation achieves 27.31/27.48 for En-De, and 37.23/37.41 for En-Zh.ßdummyTurker
15310ß0.0ßThis shows that multinomial sampling is better than beam search.ßdummyTurker
15311ß0.0ß2) For reward shaping and variance reduction, similarly, we found they are not critical to the final performance for both tasks, e.g., 27.48/27.47 for En-De and 37.41/37.43 for En-Zh w/o.ßdummyTurker
15312ß0.0ßreward shaping.ßdummyTurker
15313ß0.0ß27.45/27.48 for En-De and 37.40/37.41 for En-Zh w/o.ßdummyTurker
15314ß0.0ßvariance reduction; 3) with MLE objective combination, En-Zh can achieve 37.54.ßdummyTurker
15315ß0.0ßThese results are similar to WMT17 Zh-En task investigated in the current manuscript.ßdummyTurker
15316ß0.0ßFurthermore, for WMT17 En-Zh task, with the proposed method of leveraging monolingual data, we obtain 37.75/37.68 with source-side/target-side monolingual data using RL training.ßdummyTurker
15317ß0.0ßWith all the combination of monolingual data from both sides, we obtain 38.22 BLEU, setting a new SOTA on this task.ßdummyTurker
15318ß0.0ßWe will add the results on En-De/En-Zh tasks in the final version, to make the conclusions more reliable.ßdummyTurker
15319ß0.0ßWe aim at providing a comprehensive study on how to effectively use RL to train competitive NMT models on large-scale datasets.ßdummyTurker
15320ß0.0ßWe also try to answer whether monolingual data is useful in RL training for NMT, which has never been explored before.ßdummyTurker
15321ß0.0ßTowards this end, our solution implies a new method of leveraging source-side monolingual data, as described in subsection 4.1, which achieves promising results.ßdummyTurker
15322ß0.0ßThank the reviewer for deep suggestions and comments.ßdummyTurker
15323ß0.0ßWe will correct type errors.ßdummyTurker
15324ß0.0ßThe term 'rating' itself means that it is personalized.ßdummyTurker
15325ß0.0ßThe individual attributes are correlated, but we found that for a given text, different attributes addressing its qualitative ratings tend to be correlated even in similar fields, such as essay grading.ßdummyTurker
15326ß0.0ßIn the ASAP dataset for automatic essay grading, most of the attributes are highly correlated with Pearson correlations in excess of 0.7 for each text, given the same rater.ßdummyTurker
15327ß0.0ßOn the other hand, our dataset has correlations of about 0.60 - 0.66 between the 3 basic attributes, showing a greater independence as compared to attributes often used for automatic essay grading.ßdummyTurker
15328ß0.0ßTo take into account any reader's preferences or domain knowledge, we used a diverse set of articles, across multiple domains, such as politics (Reagan Era), law (Copyright law), sports (Chess), entertainment (Animated Films), biographical profiles (Saori Yoshida - one the the best women's wrestlers in history), etc.ßdummyTurker
15329ß0.0ßBased on this wide range of domains, we felt that it would counter any reader's preference for a given domain.ßdummyTurker
15330ß0.0ßWe consider using the text features as the baseline.ßdummyTurker
15331ß0.0ßWe have 30 instances with the same input feature vector, but with 20 different scores as class labels.ßdummyTurker
15332ß0.0ßWe assume that the NN will pick the majority class from the training instances corresponding to the same input vector.ßdummyTurker
15333ß0.0ßWe report this as a baseline with using only text features.ßdummyTurker
15334ß0.0ßAs we stated in our hypothesis, the gaze features ADDED to the text features give the massive improvement, and the improvement is more consistent if the reader fully understood the text.ßdummyTurker
15335ß0.0ßIf we were to randomly pick values out of 1 to 4, we'd get correlations between 0.04 and 0.10 for each of the properties.ßdummyTurker
15336ß0.0ßOn the other hand, in our experiments, we observed a pairwise correlation between 0.6 and 0.7.ßdummyTurker
15337ß0.0ßWith respect to the agreements, we ran a simulation and found similar differences with regard to QWK.ßdummyTurker
15338ß0.0ßThe QWK for a random pick ranged between 0.01 to 0.04, about 10 - 15% of the text features.ßdummyTurker
15339ß0.0ß1 hidden layer containing 100 neurons mean 1 layer with latent size 100.ßdummyTurker
15340ß0.0ßThe size of input vector is 361.ßdummyTurker
15341ß0.0ßOut of these, 349 are from text features (including 300 word embedding dimensions), 11 are from gaze features, and 1 is the class label.ßdummyTurker
15342ß0.0ßWe used the Mean Squared Error (MSE)ßdummyTurker
15343ß0.0ßA vast majority of the scores were 3 and 4 for individual tasks.ßdummyTurker
15344ß0.0ßIf we were to replace scores with polarity (good vs. bad), the texts will be highly skewed towards the good category, with about 24 - 28 (out of 30) texts being rated good by each reader.ßdummyTurker
15345ß0.0ßThanks for your valuable comments.ßdummyTurker
15346ß0.0ßWe will try to explain your concerns point by point.ßdummyTurker
15347ß0.0ßAs we know, previous state-of-art neural model dealing with (slot filling/name entity recognition) is basically build by Bi-LSTM followed by a CRF layer.ßdummyTurker
15348ß0.0ßLiu and lane propose two types of recurrent models, its main idea is to use sequence-to-sequence with attention frame work, its attention is like translation model which using an hidden vector in the decoder to attend to encoder.ßdummyTurker
15349ß0.0ßOur architecture computes attention not in a word-by-word style, and each vector attends to the other vectors in an parallel way which do not use an decoder to extract features to learn an discriminative model, more like a CRF layer.ßdummyTurker
15350ß0.0ßMaybe it was the main difference.ßdummyTurker
15351ß0.0ßAs mentioned above, our model utilize a different attention mechanism to extract features in parallel.ßdummyTurker
15352ß0.0ßAnd another difference lies in the word-embedding layer, which contains four parts, so when we process self-attention in four vector spaces independently which encourages our model to learn diverse features.ßdummyTurker
15353ß0.0ßThe last parts contribute to the way to combine two tasks, we not just share the Bi-LSTM and embedding features in the down level, and utilize the intent embedding of current sequence as an gate to help slot-filling task.ßdummyTurker
15354ß0.0ßBecause we find some case that slot tag is heavily rely on the intent.ßdummyTurker
15355ß0.0ßMaybe it was improper to say statistically, we means to emphasize that the improvements of our results is much more than previous work.ßdummyTurker
15356ß0.0ßThanks for your kind advice.ßdummyTurker
15357ß0.0ß(1)	We changed our third contribution as “Combination of count-based and prediction-based approaches for sentiment-context word embedding” according to your kind suggestion.ßdummyTurker
15358ß0.0ß(2)	We cited the distinction between count-based and prediction-based methods in paper(Baroni et al 2014)ßdummyTurker
15359ß0.0ß(3)ßdummyTurker
15360ß0.0ß[1] We re-organized the first paragraph in Section 3:ßdummyTurker
15361ß0.0ßIn Section 3.1we introduce the definition of Sentiment-Context Vector to describe the word representations learned in our model.ßdummyTurker
15362ß0.0ßSection 3.2 introduces our two-phase algorithm in detail.ßdummyTurker
15363ß0.0ßThe first phase proposes a count-based method for sentiment words.ßdummyTurker
15364ß0.0ßThe second phase is an prediction-based method learning SentiVec for both sentiment and non-sentiment words.ßdummyTurker
15365ß0.0ß[2] If necessary, we are ready to add some formalization as you kindly suggested.ßdummyTurker
15366ß0.0ß[3] We are sorry for the trouble brought about to you.ßdummyTurker
15367ß0.0ßWe’ve changed x_j into o in the formula.ßdummyTurker
15368ß0.0ß(4)ßdummyTurker
15369ß0.0ß[1] We defined the si on line310.ßdummyTurker
15370ß0.0ßsi is the intrinsic sentiment score for word i.ßdummyTurker
15371ß0.0ß[2] line215-217 mentions we need word occurrences in a corpus and calibrated sentiment scores in training process.ßdummyTurker
15372ß0.0ßAnd we introduce the real observation samples like SentiWordNet as calibrated sentiment scores, TripAdvisor dataset and 2018 Wikipedia dump as corpus in Section4.2.ßdummyTurker
15373ß0.0ßWe are ready to introduce the observation samples more formally in Section 3.2.ßdummyTurker
15374ß0.0ß(5).ßdummyTurker
15375ß0.0ßSize 2.4B Dimension 100ßdummyTurker
15376ß0.0ßp = precision, r = recallßdummyTurker
15377ß0.0ßExperiment:sentiment analysis  libSVMßdummyTurker
15378ß0.0ß----------------------------------------------------------------------------------------------------ßdummyTurker
15379ß0.0ß|model                      p1        p2.ßdummyTurker
15380ß0.0ßp3.ßdummyTurker
15381ß0.0ßp4.ßdummyTurker
15382ß0.0ßp5      |  r1        r2.ßdummyTurker
15383ß0.0ßr3.ßdummyTurker
15384ß0.0ßr4.ßdummyTurker
15385ß0.0ßr5       |  Acc.|ßdummyTurker
15386ß0.0ß----------------------------------------------------------------------------------------------------ßdummyTurker
15387ß0.0ß|HPCA                       0         0.386    0        0.283   0         | 0         0.528   0       0.745    0        | 0.323|ßdummyTurker
15388ß0.0ß|GloVe                       0.3      0.421   0.242  0.331   0.470   | 0.065  0.570  0.116  0.537.ßdummyTurker
15389ß0.0ß0.328 | 0.375|ßdummyTurker
15390ß0.0ß|Word2vec(sg)          0.351  0.496  0.219    0.297  0.461  | 0.211   0.107   0.249  0.633  0.434 |0.326|ßdummyTurker
15391ß0.0ß|Word2vec(cbow)     0.318  0.416   0.211    0.314  0.431  | 0.122   0.542   0.090  0.449  0.414 |0.365|ßdummyTurker
15392ß0.0ß|SentiVec(sg)            0.305  0.369  0.316   0.380  0.257  | 0.154.ßdummyTurker
15393ß0.0ß0.766.ßdummyTurker
15394ß0.0ß0.108.ßdummyTurker
15395ß0.0ß0.261.ßdummyTurker
15396ß0.0ß0.361  |0.383|ßdummyTurker
15397ß0.0ß|SentiVec(cbow)       0.362  0.441  0.317.ßdummyTurker
15398ß0.0ß0.328  0.459 | 0.168.ßdummyTurker
15399ß0.0ß0.581.ßdummyTurker
15400ß0.0ß0.113.ßdummyTurker
15401ß0.0ß0.482  0.411  |0.393|ßdummyTurker
15402ß0.0ß-----------------------------------------------------------------------------------------------------ßdummyTurker
15403ß0.0ßExperiment:sentiment analysis  CNN-LSTM  (sig.ßdummyTurker
15404ß0.0ß: SentiVec is better than others with 0.99 confidence level )ßdummyTurker
15405ß0.0ß-----------------------------------------------------------------------------------------------------ßdummyTurker
15406ß0.0ß|model                      p1        p2.ßdummyTurker
15407ß0.0ßp3.ßdummyTurker
15408ß0.0ßp4.ßdummyTurker
15409ß0.0ßp5      |  r1       r2.ßdummyTurker
15410ß0.0ßr3.ßdummyTurker
15411ß0.0ßr4.ßdummyTurker
15412ß0.0ßr5       |  Acc.|ßdummyTurker
15413ß0.0ß----------------------------------------------------------------------------------------------------ßdummyTurker
15414ß0.0ß|HPCA                       0         0          0          0.381   0.455 | 0         0         0        0.999    0.008 | 0.382|ßdummyTurker
15415ß0.0ß|GloVe                        0        0          0.333   0.383   0.472 | 0         0        0.006  0.815    0.339 | 0.402|ßdummyTurker
15416ß0.0ß|Word2vec(sg)         0          1           1          0.400   0.472 | 0         0.004  0.003  0.736  0.490 |0.422|ßdummyTurker
15417ß0.0ß|Word2vec(cbow)     0.5.ßdummyTurker
15418ß0.0ß0.3       0.333  0.408    0.526 | 0.011   0.032  0.019  0.757   0.465 |0.437|ßdummyTurker
15419ß0.0ß|SentiVec(cbow)      0.5      0.448   0.288   0.445    0.595 |0.041   0.110   0.116  0.742   0.527 | 0.475|ßdummyTurker
15420ß0.0ß-----------------------------------------------------------------------------------------------------ßdummyTurker
15421ß0.0ßExperiment:word analogy(google)ßdummyTurker
15422ß0.0ß----------------------------------------ßdummyTurker
15423ß0.0ß|model                   |   semantic  |  syntactic|ßdummyTurker
15424ß0.0ß----------------------------------------ßdummyTurker
15425ß0.0ß|HPCA                    |   0.4968    |0.4721|ßdummyTurker
15426ß0.0ß|GloVe                     |  0.7299    |0.5706|ßdummyTurker
15427ß0.0ß|Word2vec(sg)       |   0.6651    |0.5702|ßdummyTurker
15428ß0.0ß|Word2vec(cbow)   |   0.7180    |0.6285|ßdummyTurker
15429ß0.0ß|SentiVec                |    0.7306  |0.6618|ßdummyTurker
15430ß0.0ß----------------------------------------ßdummyTurker
15431ß0.0ßWe show that end-to-end training is not necessarily always beneficial, when compared, for example to seq2seq models.ßdummyTurker
15432ß0.0ßThank you for the rest of the comments, they will be handled in the paper either as ideas for future work (for example, classification for the learning algorithm) or by including the proper citations.ßdummyTurker
15433ß0.0ßit seems like long words which are well-formed are more likely to appear in the dictionary, while short combinations of letters that appear in the data are not in the dictionary (such as im, ur, etc.)ßdummyTurker
15434ß0.0ßWe compare to many baselines, including previous work in the literature.ßdummyTurker
15435ß0.0ßIf possible, we would appreciate it if the reviewer took a look at all the baselines we compare against.ßdummyTurker
15436ß0.0ßWe believe we conduct quite a thorough ablation study in the experimental section.ßdummyTurker
15437ß0.0ß"We do perform oracle experiments in which we see exactly what happens when we feed the algorithm with the perfect ""insertions""."ßdummyTurker
15438ß0.0ßPlease see starting at line 743.ßdummyTurker
15439ß0.0ßWe chose this morphological evaluation as it enables comparison to previous work in different flavors.ßdummyTurker
15440ß0.0ßThis dataset has been a standard dataset in the evaluation of morphology models.ßdummyTurker
15441ß0.0ßWe thank the reviewer for all other minor comments that we will be addressed if the paper is accepted.ßdummyTurker
15442ß0.0ßWhile the idea of inserting new characters marking empty slots is not new, *learning* this insertion function is a novelty which we introduce.ßdummyTurker
15443ß0.0ßIn addition, this method has not been extensively studied as we did in this paper.ßdummyTurker
15444ß0.0ßWe are sorry the reviewer thinks we misrepresent our paper, we had no intention to do that.ßdummyTurker
15445ß0.0ßWe will tone down claims if the paper is accepted.ßdummyTurker
15446ß0.0ßThank you for all the relevant work.ßdummyTurker
15447ß0.0ßWe will include that in the paper if it is accepted.ßdummyTurker
15448ß0.0ßWhile we focused on character-based string transduction, there is nothing in principle that prevents the use of the method on a larger vocabulary.ßdummyTurker
15449ß0.0ßI agree that, we need  to improve our writing.ßdummyTurker
15450ß0.0ßNeed to separate important factors from this and need to rewrite this paper.ßdummyTurker
15451ß0.0ßPhrase based approach (used MOSES)ßdummyTurker
15452ß0.0ßYes.ßdummyTurker
15453ß0.0ßI will add NMT apprach to the paperßdummyTurker
15454ß0.0ßThere is only few research works were conducted for Sinhala and Tamil language pair.ßdummyTurker
15455ß0.0ßAll of them are conducted using limited datasets (less than 5000 parallel sentences) since unavailability of datasets.ßdummyTurker
15456ß0.0ßSince both Sinhala and Tamil languages are morphologically rich and low resourced, pure phrase based (word level) approach gives very low BLEU score value and analysis shows very high (50%) out of vocabulary rate.ßdummyTurker
15457ß0.0ßTherefore we tried to apply morphological segmentation and transliteration techniques to this.ßdummyTurker
15458ß0.0ßSince we are in the early stage we wanted to identify problems in Sinhala-Tamil language pair when do translationsßdummyTurker
15459ß0.0ßYes, I agree with it.ßdummyTurker
15460ß0.0ßThis is the first time we submitted a paper for ACL.ßdummyTurker
15461ß0.0ßThanks for giving these ideas.ßdummyTurker
15462ß0.0ßYes.ßdummyTurker
15463ß0.0ßI can correct those errors.ßdummyTurker
15464ß0.0ßWhen we applied unsupervised segmentation to our datasets, we got segmented outputs with additional segmentation.ßdummyTurker
15465ß0.0ßThose segmentations do not have any meaning.ßdummyTurker
15466ß0.0ßSometimes words were segmented to separate characters.ßdummyTurker
15467ß0.0ßTo remove those additional segmentations, we have used several techniques to identify best approach to get better results.ßdummyTurker
15468ß0.0ßHowever, over segmentation helped to translate named-entities since it translates character level.ßdummyTurker
15469ß0.0ßYes.ßdummyTurker
15470ß0.0ßI need to compare those with previous worksßdummyTurker
15471ß0.0ßActually I wanted to show the difficulty of doing translation between both morphologically rich language and low resourced language pairs.ßdummyTurker
15472ß0.0ßOther language pairs that I selected both are not morphologically rich and one language is morphologically rich.ßdummyTurker
15473ß0.0ßI have extracted 1000 test sets from extracted 26000 parallel datasets and separately extracted 1000 dataset for developmentßdummyTurker
15474ß0.0ßAccording to Durrani's work, both languages have the same alphabet (most of the times).ßdummyTurker
15475ß0.0ßHowever, for Sinhala and Tamil its is not like that.ßdummyTurker
15476ß0.0ßYes.ßdummyTurker
15477ß0.0ßThanks for pointing me that.ßdummyTurker
15478ß0.0ßI will check that again and will write it as you saidßdummyTurker
15479ß0.0ßI have not used factored based MT approach since we do not have POS taggers and Morphologically rich analyzers.ßdummyTurker
15480ß0.0ßUsed MOSES phrase based approachßdummyTurker
15481ß0.0ßWe kindly appreciate your constructive feedback and suggesting improvements for the work.ßdummyTurker
15482ß0.0ßOn paper layout.ßdummyTurker
15483ß0.0ßThis paper is aimed at an audience that is a bit familiar with word embeddings, with the definition of a probabilistic model that is from anßdummyTurker
15484ß0.0ßexponential family, and with the challenges that are posed by the normalisation of such probabilistic models.ßdummyTurker
15485ß0.0ßTherefore we refrainedßdummyTurker
15486ß0.0ßourselves to explain what a exponential family word vector space model is.ßdummyTurker
15487ß0.0ßHowever, given more space, we aim to explain band construction more throughfully.ßdummyTurker
15488ß0.0ßOn more tasks.ßdummyTurker
15489ß0.0ßWe believe the vital justification point for this sampler was the demonstration of what it promises: faster learning and the lower variance (thus more guaranteed) sampling.ßdummyTurker
15490ß0.0ßWe think that is shown by the experiments.ßdummyTurker
15491ß0.0ßWe can show it also on other datasets, and alternative performance measures, but we don’t expect a dramatic change in the conclusions of the samplerßdummyTurker
15492ß0.0ßWe agree that the explanation of the band construction is a bit short, and weßdummyTurker
15493ß0.0ßwill extend it in the final version of the paper (considering the page limit).ßdummyTurker
15494ß0.0ßThanks for your feedback on the article.ßdummyTurker
15495ß0.0ßWe are happy to read that youßdummyTurker
15496ß0.0ßappreciate the idea of the paper, and find it well-written andßdummyTurker
15497ß0.0ßstructured.ßdummyTurker
15498ß0.0ßOn bottleneck.ßdummyTurker
15499ß0.0ßOur claim here is that simply using a large number of negative samples, in the light of minimizing the approximation gap, is infeasible.ßdummyTurker
15500ß0.0ßIn negative sampling, Z(w) is not a bottleneck, because it will beßdummyTurker
15501ß0.0ßestimated by just a single negative sammple.ßdummyTurker
15502ß0.0ßWe want to improve thisßdummyTurker
15503ß0.0ßpoor estimate by a stronger estimator, which is still tractable.ßdummyTurker
15504ß0.0ßOn statistical significance.ßdummyTurker
15505ß0.0ßWe believe that the differences are statistically significant.ßdummyTurker
15506ß0.0ßTable 1 may give the impression that we have a marginal improvement, butßdummyTurker
15507ß0.0ßthat is because we did not fully optimize parameter M. For a fairßdummyTurker
15508ß0.0ßcomparison, we set M two orders of magnitude larger than the number of negative samples.ßdummyTurker
15509ß0.0ßFurthermore, the point of this proposed sampler is that it is givingßdummyTurker
15510ß0.0ßfaster learning and lower variance.ßdummyTurker
15511ß0.0ßCoverage of results.ßdummyTurker
15512ß0.0ßOur aim for this short conference paper was to present an approximation to the optimisation that would speed up the convergence.ßdummyTurker
15513ß0.0ßWe think that is shown by the experiments.ßdummyTurker
15514ß0.0ßWe can show it also on other datasets, and alternative performance measures, but we don’t think that that will change the conclusions of the paper dramatically.ßdummyTurker
15515ß0.0ßOn complexityßdummyTurker
15516ß0.0ßWe thought we could omit real training times as these completely depend on the machine specifications, and usually outdated in a few years.ßdummyTurker
15517ß0.0ßNevertheless, we can certainly add these to the paper.ßdummyTurker
15518ß0.0ßWe thank the reviewer for helpful comments.ßdummyTurker
15519ß0.0ßThanks for your constructive and thoughtful comments!ßdummyTurker
15520ß0.0ßWe will add more comments to explain Figure 2.ßdummyTurker
15521ß0.0ßWe set the value of t to 5 for all experiments according to the development set of one data split.ßdummyTurker
15522ß0.0ßAlthough our focus of this paper is relation extraction, our graph-state LSTM model can be applied to encode other structures, such as AMR.ßdummyTurker
15523ß0.0ßWe will give such discussion in the later version.ßdummyTurker
15524ß0.0ßYes, the basic principle is to use global information in the sense that we do not want to split the original graph structure.ßdummyTurker
15525ß0.0ßGiven our baseline LSTM model, the current model is the best that we can come up with using similar LSTM.ßdummyTurker
15526ß0.0ßWe foresee that there can be alternative structures that utilize full graph information.ßdummyTurker
15527ß0.0ßWe will leave exploration of such structures to future work.ßdummyTurker
15528ß0.0ßWe will mention this in the draft.ßdummyTurker
15529ß0.0ßIn fact, edge type has been included into each x_{i,j}^l.ßdummyTurker
15530ß0.0ßPlease see Equation 2, where e_l represents the edge type embedding.ßdummyTurker
15531ß0.0ßThanks for your constructive and thoughtful comments!ßdummyTurker
15532ß0.0ßWe realized that we have caused a big confusion by missing out some details.ßdummyTurker
15533ß0.0ßWe actually did not tune on test.ßdummyTurker
15534ß0.0ßThere are five folds in our cross validation setting, which strictly follow Peng et al., (2017).ßdummyTurker
15535ß0.0ßTheir corpus does not contains a held-out devset in each fold.ßdummyTurker
15536ß0.0ßTherefore, for the first fold, we take 200 sentences out of the training data as the devset, using the remaining for development training.ßdummyTurker
15537ß0.0ßIn the end the 200 sentences are merged back into the training set.ßdummyTurker
15538ß0.0ßThe number of iterations is decided on the devset, and used for all five folds.ßdummyTurker
15539ß0.0ßFor all the other hyper parameters, we simple followed Peng et al., (2017) and did not do tuning.ßdummyTurker
15540ß0.0ßBelow are the numbers of different transition iterations on the devset of that fold.ßdummyTurker
15541ß0.0ß#transition iter: 1  2  3  4  5ßdummyTurker
15542ß0.0ß#F1: 72.0  77.5  80.5  84.0  84.5ßdummyTurker
15543ß0.0ßWe used this fold for showing the effectiveness of iterations and single-direction information exchange, where the figure is drawn on the test portion of that fold.ßdummyTurker
15544ß0.0ßWe mentioned that ``we used 5 as the number of iterations for remaining experiments'' at the end of the section, which sounded as if we tuned on test.ßdummyTurker
15545ß0.0ßWe did not realize this confusing statement in our draft.ßdummyTurker
15546ß0.0ßWe will give all the above details in our revision!ßdummyTurker
15547ß0.0ßWe studied each individual case to reach this conclusion.ßdummyTurker
15548ß0.0ßNevertheless, richer analysis, such as the average entity distance along with sentence length, should be informative, and we will follow your suggestion.ßdummyTurker
15549ß0.0ßWe thank the reviewer for the thoughtful comments.ßdummyTurker
15550ß0.0ßPlease find our replies below:ßdummyTurker
15551ß0.0ßWe agree.ßdummyTurker
15552ß0.0ßTraining large-scale multi-lingual NMT models is a very slow process, and we could not provide results for both the WMT and UN corpora at the time of submission.ßdummyTurker
15553ß0.0ßHowever, we have now obtained the zero-shot translation results for a model trained directly on the UN corpus, which we will add to the manuscript.ßdummyTurker
15554ß0.0ßPivot-based translation translates each source sentence twice, which is expensive in a large-scale production system.ßdummyTurker
15555ß0.0ßPivoting doubles the latency for translation requests that need pivoting, which impacts real-time translation applications.ßdummyTurker
15556ß0.0ßAlso, the increased computational cost of pivot translation means that the production service needs to be hosted on a larger fleet to serve the same level of traffic.ßdummyTurker
15557ß0.0ßWe do not compare direct zero-shot methods to pivot-based methods because we are looking for techniques that avoid this latency and cost.ßdummyTurker
15558ß0.0ßWe hope that pivot-based methods can be replaced with direct neural zero-shot methods, but (Ha et al.ßdummyTurker
15559ß0.0ß2016), (Miura et al.ßdummyTurker
15560ß0.0ß2017) and our work all indicate that these methods have a long way to go.ßdummyTurker
15561ß0.0ßOur model gets somewhat further ahead than the neural zero-shot experiments in (Miura et al.ßdummyTurker
15562ß0.0ß2017).ßdummyTurker
15563ß0.0ßWe will explain this reasoning more clearly in the manuscript.ßdummyTurker
15564ß0.0ßWe will strengthen the bilingual baseline models with the copied corpus idea from (Currey et al 2017).ßdummyTurker
15565ß0.0ßAs for trying deeper models, the reason that we did not run more experiments in this direction was based on the guidance from (Britz et al 2017) in EMNLP.ßdummyTurker
15566ß0.0ßIn Section 4.3, they showed that varying the encoder and decoder depths yielded small BLEU improvements versus the shallowest model.ßdummyTurker
15567ß0.0ßWe will add the omitted references.ßdummyTurker
15568ß0.0ßSee Weakness 2.ßdummyTurker
15569ß0.0ßToday, direct zero-shot methods are not competitive with pivot-based methods, but our work is an additional step in the right direction.ßdummyTurker
15570ß0.0ßWe saw that (Ha et al.ßdummyTurker
15571ß0.0ß2016) incorporated external monolingual data along the lines that you suggested in Sections 4.2 and 4.3 of their work.ßdummyTurker
15572ß0.0ßHowever, our aim was to show that a neural interlingua could be learned without a multi-parallel corpus; we did not look for potential performance benefits.ßdummyTurker
15573ß0.0ßWe meant that the interlingual embeddings were useful for crosslingual classification; we did not mean that it outperformed the classifier trained and tested on the same language.ßdummyTurker
15574ß0.0ßWe will rewrite the crosslingual classification section for clarity.ßdummyTurker
15575ß0.0ßWe hope that your concerns have been adequately addressed above.ßdummyTurker
15576ß0.0ßWritingßdummyTurker
15577ß0.0ßWe will correct the typos in the manuscript.ßdummyTurker
15578ß0.0ßExperiments on interlingua lengthßdummyTurker
15579ß0.0ßWe experimented with the interlingua length (i.e.ßdummyTurker
15580ß0.0ß40, 50 and 60), and saw no meaningful difference versus 50.ßdummyTurker
15581ß0.0ßWe chose a length of 50 to match the maximum length of the source sentence.ßdummyTurker
15582ß0.0ßIn this paper, we wanted to show that we learned an interlingual embedding that was useful for various tasks, including direct zero-shot translation, crosslingual classification, and multilingual translation, rather than tune the system for maximum performance.ßdummyTurker
15583ß0.0ß"""attention types"""ßdummyTurker
15584ß0.0ß(Britz et al 2017) in EMNLP compare multiplicative and additive attention, (Martins et al 2016) in ICML compare sparse and dense attention, and (Raffel et al 2017) in ICML compare monotonic vs non-monotonic attention on various tasks, including translation and multilabel classification.ßdummyTurker
15585ß0.0ßBroadly speaking, the results are similar.ßdummyTurker
15586ß0.0ß"""other hyper-parameters"""ßdummyTurker
15587ß0.0ßAs with all neural approaches to machine learning, the number of hyperparameters to tune is quite large.ßdummyTurker
15588ß0.0ßIt is not clear to the authors which hyperparameters the reviewer thinks would be especially influential on the results.ßdummyTurker
15589ß0.0ßWe also want to reiterate that the focus of the paper is to show that the architecture we proposed can learn an interlingua which can be used for zero-shot classification and crosslingual classification, not that we achieve a state-of-the-art BLEU score for a specific MT task.ßdummyTurker
15590ß0.0ß"""text classification has no comparison to baselines"""ßdummyTurker
15591ß0.0ßWe show that using the interlingual embeddings for crosslingual classification is better than simply choosing the majority class, but worse than the usual baseline where the training and test languages are the same.ßdummyTurker
15592ß0.0ßmultiple runs and no significance resultsßdummyTurker
15593ß0.0ßFor shallow recurrent architectures like our model, the effect of multiple runs is quite small.ßdummyTurker
15594ß0.0ßSee Section 4.3 of (Britz et al 2017) for evidence of this.ßdummyTurker
15595ß0.0ßWe note that several recent, well-cited papers (Vaswani et al 2017), (Melvin et al 2017), (Gehring et al 2017) and (Firat et al 2016) do not include confidence intervals.ßdummyTurker
15596ß0.0ßNonetheless, it is straightforward to add confidence intervals to our results.ßdummyTurker
15597ß0.0ßevaluation of clusteringßdummyTurker
15598ß0.0ßIt is not clear how one would compare the quality of clusters quantitatively.ßdummyTurker
15599ß0.0ßFor example, an interlingual encoder that mapped every sentence to the zero matrix would be unhelpful for translation, but would achieve a very low negative log-likelihood with any probabilistic clustering technique (e.g.ßdummyTurker
15600ß0.0ßGMM's).ßdummyTurker
15601ß0.0ßThank you for your advice!ßdummyTurker
15602ß0.0ßWe propose the grounded paradigm to explore a new potential solution for discourse relation recognition and we already show the superiority of our framework on proposed corpora.ßdummyTurker
15603ß0.0ßSorry for the poor intuition of our paper, we are going to improve it in future.ßdummyTurker
15604ß0.0ßMeanwhile, given that this paradigm is still at the stage of theoretical exploration, we can't directly apply it on PDTB corpora.ßdummyTurker
15605ß0.0ßHowever, we did run the textual approaches mentioned in our paper on PDTB and they can achieve comparable performance.ßdummyTurker
15606ß0.0ßWe can add all these vital information (the detail of our corpora and our simulation) into our paper if there is an additional page.ßdummyTurker
15607ß0.0ßThank you for your advice again!ßdummyTurker
15608ß0.0ßThank you for your advice!ßdummyTurker
15609ß0.0ßWe propose the grounded paradigm to explore a new potential solution for discourse relation recognition and we already show the superiority of our framework on proposed corpora.ßdummyTurker
15610ß0.0ßGiven that this paradigm is still at the stage of theoretical exploration, we can't directly apply it on PDTB corpora.ßdummyTurker
15611ß0.0ßHowever, we did run the textual approaches mentioned in our paper on PDTB and they can achieve comparable performance.ßdummyTurker
15612ß0.0ßWe can add all these vital information into our paper if there is an additional page.ßdummyTurker
15613ß0.0ßThank you for your advice again!ßdummyTurker
15614ß0.0ßYour advice do help us a lot, we are going to improve our work in the future as you suggest!ßdummyTurker
15615ß0.0ßThank you for your advice!ßdummyTurker
15616ß0.0ßWe propose the grounded paradigm to explore a new potential solution for discourse relation recognition and we already show the superiority of our framework on proposed corpora.ßdummyTurker
15617ß0.0ßGiven that this paradigm is still at the stage of theoretical exploration, we can't directly apply it on PDTB corpora.ßdummyTurker
15618ß0.0ßHowever, we did run the textual approaches mentioned in our paper on PDTB and they can achieve comparable performance.ßdummyTurker
15619ß0.0ßWe can add all these vital information into our paper if there is an additional page.ßdummyTurker
15620ß0.0ßThank you for your advice again!ßdummyTurker
15621ß0.0ßThis paper is an extension for the Percy Liang’s paper.ßdummyTurker
15622ß0.0ßIt was pretty hard to provide all the details in 4 pages.ßdummyTurker
15623ß0.0ßWe expect to do that when we receive one additional page.ßdummyTurker
15624ß0.0ß"Line 368: ""Auperscript"" -> ""Superscript"""ßdummyTurker
15625ß0.0ßcorrectßdummyTurker
15626ß0.0ßBe consistent with style in the bibliography.ßdummyTurker
15627ß0.0ßSome of the references write out conference names entirely, some use abbreviations, and the abbreviating is not always done the same way.ßdummyTurker
15628ß0.0ßcorrectßdummyTurker
15629ß0.0ßThe bibliography should not include URLs.ßdummyTurker
15630ß0.0ß(It is trivial to find the papers using a web search.)ßdummyTurker
15631ß0.0ßThis is correct; although we used ACM bibtex with ACL bibliography style.ßdummyTurker
15632ß0.0ß"""Zettlemoyer and Collins, UAI 2005"" appears twice in the bibliography, once correctly and once as ""arXiv 2012"", which is not correct."ßdummyTurker
15633ß0.0ßThis is correct.ßdummyTurker
15634ß0.0ßThis is correct.ßdummyTurker
15635ß0.0ßIn sequence level accuracy we got marginal results since it depends on the statistics of the training data and the number of examples taking advantage of out-of-domain tokens.ßdummyTurker
15636ß0.0ßThe ATIS dataset is a challenging dataset and even Liang did not provide the sequence level accuracy.ßdummyTurker
15637ß0.0ßThe results are too low both for us and the baseline in this metric.ßdummyTurker
15638ß0.0ßIf this paper get accepted we will provide these values for all of these setsßdummyTurker
15639ß0.0ßWe will fix it.ßdummyTurker
15640ß0.0ßTo make it clear, we don’t need to further depend on i.ßdummyTurker
15641ß0.0ßWe put t to make it different from the source sequence.ßdummyTurker
15642ß0.0ßWe are going to give chance for the history tokens or all the source vocabulary.ßdummyTurker
15643ß0.0ßIt’s hidden in z_t which contains W_h.ßdummyTurker
15644ß0.0ßW_h is a mapping from current state to all the source vocabulary and being captured in the softmax layer as well.ßdummyTurker
15645ß0.0ßYes.ßdummyTurker
15646ß0.0ßit can be a reason.ßdummyTurker
15647ß0.0ßGetting better results after augmenting by the publication domain supports this idea.ßdummyTurker
15648ß0.0ßIf Ai,j >\tau, concept i is a prerequisite of concept j; If Ai,j<-\tau, concept j is a prerequisite of concept i; if |Ai,j|<\tau, concept i and j has no prerequisite relations.ßdummyTurker
15649ß0.0ßIn our experiments, \tau=0.6.ßdummyTurker
15650ß0.0ßFurthermore, we will introduce the model more progressively.ßdummyTurker
15651ß0.0ßMOOC and CPR-Recover are two representative methods in the learning based model and recovery based model respectively.ßdummyTurker
15652ß0.0ßMOOC proposed semantic, contextual and structural features for concept pairs in Massive Open Online Courses, and then trained binary classifiers such as Naïve Bayes, Logistic Regression, SVM with linear kernel and Random Forest for the relation learning.ßdummyTurker
15653ß0.0ßCPR-Recover proposed an unsupervised optimization-based method based on causality and sparsity assumptions to learn concept-level prerequisite relations from course dependencies.ßdummyTurker
15654ß0.0ßPrerequisite relations essentially can be considered as the learning dependency among concepts, which means a concept is necessary to learn before one can proceed to understand more advanced knowledge.ßdummyTurker
15655ß0.0ß"For example, we should learn ""binary tree"" before ""AVL tree"" when learning data structure."ßdummyTurker
15656ß0.0ßWe make the corpus publicly available at httpsßdummyTurker
15657ß0.0ßYes, we extracted candidate concepts from textbooks by AutoPhrase, and then manually selected domain concepts from these candidate concepts.ßdummyTurker
15658ß0.0ßWe will indicate the MOOC data is in English.ßdummyTurker
15659ß0.0ßThe iPRL is quite different and more practical than the existing methods for the prerequisite relation discovery among concepts in educational data.ßdummyTurker
15660ß0.0ßMOOC[1] and CPR-Recover[2] are two representative methods in the learning based model and recovery based model respectively.ßdummyTurker
15661ß0.0ßMOOC proposed semantic, contextual and structural features for concept pairs in Massive Open Online Courses, and then trained binary classifiers such as Naïve Bayes, Logistic Regression, SVM with linear kernel and Random Forest for the relation learning.ßdummyTurker
15662ß0.0ßHowever, the labeling of concept pairs for the training data is a grueling and time-consuming work, which makes the approach impractical in real applications.ßdummyTurker
15663ß0.0ßWhile for CPR-Recover, it doesn’t need the training data.ßdummyTurker
15664ß0.0ßIt utilized the assumption that the dependency among courses is caused by sufficient evidence provided by prerequisite relations among concepts representing the courses.ßdummyTurker
15665ß0.0ßThus, CPR-Recover can learn concept-level prerequisite relations from course dependencies without any labeling data of concept pairs.ßdummyTurker
15666ß0.0ßHowever, this method ignores useful features.ßdummyTurker
15667ß0.0ßThe iPRL both utilizes the advantages of these methods.ßdummyTurker
15668ß0.0ßAt first, it can discover the prerequisite relations among concepts from the dependencies of educational data as in the recovery-based model, and then learn a classifier by leveraging semantic, contextual and structural features to judge the prerequisite relation for a concept pair.ßdummyTurker
15669ß0.0ßThe classifier can further improve the relation discovery from the dependencies of educational data.ßdummyTurker
15670ß0.0ßThis procedure can iteratively improve the performance of the prerequisite relation discovery.ßdummyTurker
15671ß0.0ßThe iPRL also doesn't need any labeling data of concept pairs and has higher precision and recall than CPR-Recover, so it can be used in real applications.ßdummyTurker
15672ß0.0ß[1] Liangming Pan, et al.ßdummyTurker
15673ß0.0ß2017.ßdummyTurker
15674ß0.0ßPrerequisite relation learning for concepts in moocs.ßdummyTurker
15675ß0.0ßIn ACL.ßdummyTurker
15676ß0.0ßpages 1447–1456.ßdummyTurker
15677ß0.0ß[2] Chen Liang, et al.ßdummyTurker
15678ß0.0ß2017.ßdummyTurker
15679ß0.0ßRecovering concept prerequisite relations from university course dependencies.ßdummyTurker
15680ß0.0ßIn AAAI.ßdummyTurker
15681ß0.0ßpages 4786–4791.ßdummyTurker
15682ß0.0ßPrerequisite relations essentially can be considered as the learning dependency among concepts, which means a concept is necessary to learn before one can proceed to understand more advanced knowledge.ßdummyTurker
15683ß0.0ßWe did the experiments in both Textbook and MOOC datasets to check whether the following assumption be rightßdummyTurker
15684ß0.0ßF is a classifier to judge whether a concept pair has a prerequisite relation.ßdummyTurker
15685ß0.0ßWhile W is used to measure the relatedness among two concepts.ßdummyTurker
15686ß0.0ßThat is, if two concepts are semantically or positionally relevant, then they should have similar prerequisites.ßdummyTurker
15687ß0.0ßThus, there are two different feature sets.ßdummyTurker
15688ß0.0ßTo compare the MOOC method fairly, we used same features.ßdummyTurker
15689ß0.0ßActually, MOOC data includes the feature Wikipedia refd, which is from Wikipedia.ßdummyTurker
15690ß0.0ßWe will elaborate the motivation more clearly and write the related works briefly.ßdummyTurker
15691ß0.0ßAnd we make the Chinese textbook dataset publicly available at httpsßdummyTurker
15692ß0.0ßTable 2 shows the best performance when hop is 1 that is JUPMN(1)  We did show the effect of different experiment settings to performance in Table 3 including parameters of single/both memory networks and number of hops(K).ßdummyTurker
15693ß0.0ßIn Figure 4, we also show the effect of different memory sizes The analysis of the settings related to Table 3 is discussed from Line 544 to Line598.ßdummyTurker
15694ß0.0ßThe analysis of Figure 4 is given in the last paragraph of Section 4.3 from Line 630 to Line 649.ßdummyTurker
15695ß0.0ßAs for the significance of improvement, we conducted t-test by running 10 random tests for our model against the state-of-the-art to obtain p-value of 0.04,0.009.0.012 which statistically indicate significant(IMDB,Yelp14) or very significant (Yelp13) improvement.ßdummyTurker
15696ß0.0ßThe analysis of the two components including the user memory network and product memory network from Line 544 to Line605.ßdummyTurker
15697ß0.0ßFor case study, we only listed examples in JUPMN due to the limitation of pages.ßdummyTurker
15698ß0.0ßWord-cloud analysis can help to understand the characteristics of comments related to different users and products.ßdummyTurker
15699ß0.0ßFor example, in Yelp 13, lenient users might share many common sentiment words to the finicky users.ßdummyTurker
15700ß0.0ßThus, product information is more useful.ßdummyTurker
15701ß0.0ßThe different settings are considered and discussed in the paper in terms of  number of hops (K), use of single/both memory networks, and different memory sizes.ßdummyTurker
15702ß0.0ßWe analyze the impact of different hop size.ßdummyTurker
15703ß0.0ßThe computation cost is O(KxN) where K is the number of hops.ßdummyTurker
15704ß0.0ßTable 3 shows JUPMN-U (1) and JUPMN-P (1), is comparable to the fully proposed system  JUPMN which suggests the need to utilize both memory networks.ßdummyTurker
15705ß0.0ßO(N) is also the cost of the full JUPMN model according to big-O analysis.ßdummyTurker
15706ß0.0ßAnother configuration is on memory network size.ßdummyTurker
15707ß0.0ßThis is given in Figure 4 and analysed from Line 630 to Line 649.ßdummyTurker
15708ß0.0ßFigure 4 shows that when memory size increases from 10 to 75, the performance of JUPMN steadily increases.ßdummyTurker
15709ß0.0ßOnce it goes beyond 75, JUPMN is no longer sensitive to memory size.ßdummyTurker
15710ß0.0ßTo have a fair comparison to other baseline methods which all used LSTM to learn documentation representation, we did not include other representations.ßdummyTurker
15711ß0.0ßThis can remove additional variants to prove the significance of our two-memory network model.ßdummyTurker
15712ß0.0ßThe improvement by absolute value is from 0.3% - 0.6% to the state-of-the-art model(Chen 2016).. We select the hard-to-beat Chen(2016) paper as comparison model because Chen(2016)’s work outperformed nearly all newly proposed models in ACL 2017 and EMNLP 2017 in document classification tasks.ßdummyTurker
15713ß0.0ßTo further prove the significance of our method, we use t-test to obtain the p-value which is 0.04,0.009,0.012 to the three datasets used, indicating that the improvement is either significant(< 0.05)) or very significant(<0.01).ßdummyTurker
15714ß0.0ßYes, the novelty of our model is mainly in the use of two-memory networks built from user profiles and product information separately.ßdummyTurker
15715ß0.0ßCompare to Dou (2017), our result shows that using a single memory network to process both user and product information together is not sufficient because user profile and product information are fundamentally different.ßdummyTurker
15716ß0.0ßUsing two separate networks can best capture the useful features in the two kinds of input data.ßdummyTurker
15717ß0.0ßThis is proven by our analysis from Line 544 to Line 605 on the different contributions in different type of datasets.ßdummyTurker
15718ß0.0ßThe improvement of our methods to single unified memory network model demonstrated by net improvement is 7.3%, 4.2% and 3.1% respectively.ßdummyTurker
15719ß0.0ßIn related work, we introduced Chen(2016) in Line 158 to Line 176 and Dou(2016)’s work in in Line 186 to Line 199.ßdummyTurker
15720ß0.0ßWe will highlight our differences to the two papers you mentioned in the related work.ßdummyTurker
15721ß0.0ßThe main contribution is that different external information need to be modelled separately.ßdummyTurker
15722ß0.0ßA unified memory network cannot best leverage the different contributions of user profile and product information in the learning process.ßdummyTurker
15723ß0.0ßThis realization is proven based our evaluation of different datasets.ßdummyTurker
15724ß0.0ßSince use profile is more subjective than product information, user memory network performs well in movie reviews whereas product information plays a more signicant role in Yelp13 datasets.ßdummyTurker
15725ß0.0ßBut overall, our full method has the most consistent and top performance in all datasets.ßdummyTurker
15726ß0.0ßThe significant value (p-value) comes from t-test by randomly running experiment 10 times for both our model and the state-of-the-art as the baseline.ßdummyTurker
15727ß0.0ßThe p-value is not only decided by the net improvement of headline performance to other state-of-the-art models (which is listed in the table 2), but also on the consistence of improvement over different times of random tests.ßdummyTurker
15728ß0.0ßYes, the novelty of our model is mainly in the use of two-memory networks built from user profiles and product information separately.ßdummyTurker
15729ß0.0ßCompare to Dou (2017), our result shows that using a single memory network to process both user and product information together is not sufficient because user profile and product information are fundamentally different.ßdummyTurker
15730ß0.0ßUsing two separate networks have different contributions in different type of datasets.ßdummyTurker
15731ß0.0ßOur proposed method also proves that the improvement to Chen(2016), the state-of-the-art method is significant or very significant with p-value of 0.04,0.009,0.012 respectively.ßdummyTurker
15732ß0.0ßTable 3 shows JUPMN-U (1) and JUPMN-P (1), is comparable to the fully proposed system JUPMN which suggests the need to utilize both memory networks.ßdummyTurker
15733ß0.0ßBut JUPMN-P (1) did not perform well (relatively) in IMDB movie review dataset than JUPMN-U.ßdummyTurker
15734ß0.0ßBy contrast, JUPMN-P (1) outperforms JUPMN-U(1) in Yelp13.ßdummyTurker
15735ß0.0ßUtilizing both user profile and product information in two separate memory networks is an overall good choice for a variety of datasets.ßdummyTurker
15736ß0.0ßThe significance value is coming from t-test by randomly running experiment in 10 times for both our model and state-of-the-art baseline.ßdummyTurker
15737ß0.0ßThe p-value smaller than 0.05 means the improvement is significant while the p-value smaller than 0.01 means the improvement is very significant.ßdummyTurker
15738ß0.0ßThanks for the reviewer’s comments.ßdummyTurker
15739ß0.0ßThanks for the reviewer’s constructive suggestion.ßdummyTurker
15740ß0.0ßThe motivation behind sharing high-level layers is straightforward, mapping the latent representations from different languages to a shared-latent space.ßdummyTurker
15741ß0.0ßWith the development of NMT, the specific implementation of the encoder module varies among the simple RNN, LSTM, GRU, CNN and the newly emerged self-attention.ßdummyTurker
15742ß0.0ßAlthough they have different architectures, they perform identically or similarly, i.e., encoding the context information from the raw input sentence.ßdummyTurker
15743ß0.0ßTherefore, we believe that the high-level sharing mechanism works similarly whatever the specific architecture of the encoder is.ßdummyTurker
15744ß0.0ßHowever, we will conduct experiments to verify our conjecture and consider including this in our final camera-ready if accepted.ßdummyTurker
15745ß0.0ßAutomatic selecting what to share is an interesting and promising direction for our work.ßdummyTurker
15746ß0.0ßSince it needs elaborate design and it is very complex, we decide to investigate this in our future work.ßdummyTurker
15747ß0.0ßIn this paper, we propose some additional components, which are well structured in the whole model.ßdummyTurker
15748ß0.0ßThe motivation is either to enforce the shared-latent space or to improve the translation performance directly.ßdummyTurker
15749ß0.0ßSince the Transformer is a very strong baseline model, it is very hard to achieve some improvement on this baseline.ßdummyTurker
15750ß0.0ßThe improvement about 0.5 BLEU score is viewed as significant in other works (see the paper “improving NMT with conditional sequence generative adversarial nets” and “weighted Transformer network for machine translation”).ßdummyTurker
15751ß0.0ßTherefore, our improvement is not minor.ßdummyTurker
15752ß0.0ßAdditionally, it is not trivial to make all of the components work together for the unsupervised NMT.ßdummyTurker
15753ß0.0ßThanks for the reviewer’s comments.ßdummyTurker
15754ß0.0ßDue to the page limit, we did not present the qualitative study in our submission.ßdummyTurker
15755ß0.0ßWe will take the reviewer’s suggestion and consider including the qualitative study in our final camera-ready if accepted.ßdummyTurker
15756ß0.0ßThanks for the suggestion.ßdummyTurker
15757ß0.0ßWe have presented our data pre-processing in detail in the section of data sets and pre-processing.ßdummyTurker
15758ß0.0ßWe have explicitly presented that we lowered case the training data (see line 502).ßdummyTurker
15759ß0.0ßWe will give more direct explanations about this problem in our final camera-ready if accepted.ßdummyTurker
15760ß0.0ßThanks for this suggestion.ßdummyTurker
15761ß0.0ßWe have noticed this editing error and we have corrected it.ßdummyTurker
15762ß0.0ßSelf-attention is used to find relationships between words.ßdummyTurker
15763ß0.0ßThe network does not inherently identify entities.ßdummyTurker
15764ß0.0ßBy finding the most important relationships between words the most salient words in a given sentence/document (based off patterns in the training set), the HCAN can create an embedding representation capturing the core content of a sentence/document.ßdummyTurker
15765ß0.0ßThe CNN performs worse than the HAN and the HCAN.ßdummyTurker
15766ß0.0ßWe show that we can match state-of-the-art performance (HAN) in half the time; we show that on all tasks, CNN may be faster but it gets worse performance than the state-of-the-art (HANs).ßdummyTurker
15767ß0.0ßThe lower hierarchy of the hierarchical structure acts like a for-loop across each sentence.ßdummyTurker
15768ß0.0ßThe same network weights/structure are applied to every sentence in a document to create a sentence embedding.ßdummyTurker
15769ß0.0ßThis cannot be done using a standard feedforward structure because the same weights/structure have to be recycled for each sentence--in a feedforward structure each entry/sentence would be assigned a new weight.ßdummyTurker
15770ß0.0ßThis lower hierarchy is more like a convolution across each sentence, as in a convolution operation the weights are preserved; however, our attention-based operations are much more complex than a convolution operation.ßdummyTurker
15771ß0.0ßWe will add a comparison of target attention with maxpool and mean-pool to show its effectiveness.ßdummyTurker
15772ß0.0ßTo evaluate the modification, we will conduct more experiments in the future.ßdummyTurker
15773ß0.0ßThank you for the comment.ßdummyTurker
15774ß0.0ßThanks for the suggestion, we will use this set of standards later.ßdummyTurker
15775ß0.0ßActually, the ideal target of this work is to identify co-hyponyms automatically.ßdummyTurker
15776ß0.0ßHowever, as the reviewer points out, our model (Equation 5) may affect other types of related words.ßdummyTurker
15777ß0.0ßWe think the proposed method in this paper partially addresses the problem mentioned in the Introduction.ßdummyTurker
15778ß0.0ßMaybe we should introduce external knowledge to identify co-hyponyms.ßdummyTurker
15779ß0.0ßThanks for the question.ßdummyTurker
15780ß0.0ßWe also wonder the explanation for the improvement.ßdummyTurker
15781ß0.0ßWe should study the reason further.ßdummyTurker
15782ß0.0ßActually, our model doesn’t need any external source to identify co-hyponyms.ßdummyTurker
15783ß0.0ßThe target of this work is to identify co-hyponyms automatically and to optimize them to be close directly.ßdummyTurker
15784ß0.0ßThe results demonstrate the effectiveness of the proposed optimization objective.ßdummyTurker
15785ß0.0ßHowever, it should be clarified that our model (Equation 5) may affect other types of related words.ßdummyTurker
15786ß0.0ßSo we think the proposed method in this paper partially address the problem mentioned in the Introduction.ßdummyTurker
15787ß0.0ßMaybe we should introduce external knowledge to improve word embeddings.ßdummyTurker
15788ß0.0ßThe worse performance on German and Spanish is not easy to explain.ßdummyTurker
15789ß0.0ßWe should conduct more experiments to understand the results.ßdummyTurker
15790ß0.0ßThanks for the comment.ßdummyTurker
15791ß0.0ßIt would be better to provide statistical significance.ßdummyTurker
15792ß0.0ßBesides, better and all-around evaluation sets are more convincing.ßdummyTurker
15793ß0.0ßWe will add some experiments in the futures according to this suggestion.ßdummyTurker
15794ß0.0ßThe motivation we intend to state is that we should treat the co-hyponymy and other relationships separately.ßdummyTurker
15795ß0.0ßWe should conduct experiments to demonstrate the effectiveness of this idea on downstream tasks.ßdummyTurker
15796ß0.0ßTherefore, we suppose to treat different relationships between words differently, hoping to improve the quality of embeddings.ßdummyTurker
15797ß0.0ßFor all our experiments, we actually do balance the training sets by default as outlined in lines 429-437.ßdummyTurker
15798ß0.0ßWe actually do employ character n-grams as style features (among others).ßdummyTurker
15799ß0.0ßSee line 411.ßdummyTurker
15800ß0.0ßWe refrain from using neural networks as we believe that they capture more than style.ßdummyTurker
15801ß0.0ßWe used the most widespread measures for binary classification experiments; but you are right that a discussion would have been appropriate.ßdummyTurker
15802ß0.0ßWe will try to squeeze one in.ßdummyTurker
15803ß0.0ßWe did not consider a regression experiment so far, but the corpus annotations allow this.ßdummyTurker
15804ß0.0ßSpace permitting, we will try to add one.ßdummyTurker
15805ß0.0ßUnfortunately, not with the current annotation.ßdummyTurker
15806ß0.0ßThe journalists viewed the original articles, since they had to do this as part of their daily work, anyway.ßdummyTurker
15807ß0.0ßIntroducing a special tool into this work pipeline and integrating it with the systems used by the journalists was not feasible, so that more fine-grained judgments could not be collected in a sufficiently reliable way.ßdummyTurker
15808ß0.0ßIn fact, we found it rather impressive that the style models were competitive already, given that the corpus covers only a short time frame.ßdummyTurker
15809ß0.0ßConsider how little the style model has to go on compared to the topic-based model.ßdummyTurker
15810ß0.0ßWe would have loved to report an experiment training a topic-based model in one time interval and applying it to another one much later.ßdummyTurker
15811ß0.0ßIn such an experiment, topic-based models would likely fail, since topics change drastically in news, whereas style does not so much.ßdummyTurker
15812ß0.0ßAlas, the journalists did not have the resources to conduct another annotation study.ßdummyTurker
15813ß0.0ßReconstruction error is in fact what Unmasking measures, quantified as accuracy of discriminating the two document sets.ßdummyTurker
15814ß0.0ßWe will try to clarify the term.ßdummyTurker
15815ß0.0ßApologies for the confusionßdummyTurker
15816ß0.0ßThe class-wise precision, recall, and consequently F1 of the left and right class are pretty low (0.20 and 0.57, respectively).ßdummyTurker
15817ß0.0ßThis is much lower than when just predicting hyperpartisanship (Table 3, F1 = 0.78).ßdummyTurker
15818ß0.0ßCorrect, since Rubin combines style and topic, their approach performs a bit better.ßdummyTurker
15819ß0.0ßAnd incorporating topic into satire detection is indeed inappropriate, since just as the topics of news change drastically as time passes, so do the topics of satire, which generally follow the trending news topics.ßdummyTurker
15820ß0.0ßIn a corpus like ours, spanning only one week, this may not surface, but such a classifier would not generalize to larger time scales.ßdummyTurker
15821ß0.0ßUnfortunately, the journalists were not available for an additional annotation study.ßdummyTurker
15822ß0.0ßThe motivation for our linearization method is stated below.ßdummyTurker
15823ß0.0ßInspired by the success of linearization methods of syntactic trees used in NMT, we want to linearize packed forests in a similar manner.ßdummyTurker
15824ß0.0ßFixed traversal order exists in a tree but not in a forest, so we proposed a sorting method which is like topological ordering while preserving linguistic information in the forest.ßdummyTurker
15825ß0.0ßFurthermore, different parts of constituent trees should be combined in different ways, so we proposed four operators to achieve this.ßdummyTurker
15826ß0.0ßWe will add the above description about motivation in the Introduction section in the final paper.ßdummyTurker
15827ß0.0ßWe use Adadelta optimizer because it is widely applied in past research papers such as Neural Machine Translation by Jointly Learning to Align and Translate written by Bahdanau et al.ßdummyTurker
15828ß0.0ß(ICLR, 2015).ßdummyTurker
15829ß0.0ßWe will report the significance test results as you said in the final paper.ßdummyTurker
15830ß0.0ßWe will briefly introduce the pruning algorithm.ßdummyTurker
15831ß0.0ßWe will evaluate on other evaluation metrics such as RIBES and report the results.ßdummyTurker
15832ß0.0ßWe will compare our method with other linearization methods.ßdummyTurker
15833ß0.0ßSince our method is the first attempt of linearize forests, to compare with other methods, we will apply our method to 1-best constituent tree, and show the difference to other methods.ßdummyTurker
15834ß0.0ßAs for the issue of reversibility, we will try to modify our method, removing part of syntactic information encoded in the linearized forest, to make our method reversible.ßdummyTurker
15835ß0.0ßThen, we will compare the reversible version of our method with other linearization methods, and show the merits and demerits of our method.ßdummyTurker
15836ß0.0ßAs far as we know, there are no previous work on linearization of packed forests, although there are some work on linearization of constituent trees.ßdummyTurker
15837ß0.0ßDifferent operators connect different kinds of symbols.ßdummyTurker
15838ß0.0ßCircle_c means that the node is a child node of some other node.ßdummyTurker
15839ß0.0ßCircle_product is used to connect two leaf words, circle_dot is used to connect a constituent label and a leaf word, while circle_plus is used to connect two linearization segments.ßdummyTurker
15840ß0.0ßDifferent operators reflect different properties of combination, which make the linearized sequence encode rich syntactic information.ßdummyTurker
15841ß0.0ßWe will add the above explanation in the final version.ßdummyTurker
15842ß0.0ßWe evaluate the BLEU score on the development set every 10,000 updates.ßdummyTurker
15843ß0.0ßIf BLEU score is not increased in ten consecutive evaluations, then training is stopped.ßdummyTurker
15844ß0.0ßWe will reorganize the logic of our paper to avoid the redundant description of other tree-based NMT systems.ßdummyTurker
15845ß0.0ßThanks for pointing them out.ßdummyTurker
15846ß0.0ßWe will reflect your comments.ßdummyTurker
15847ß0.0ßThanks a lot for the insightful comments.ßdummyTurker
15848ß0.0ßPlease see our replies below:ßdummyTurker
15849ß0.0ßWe appreciate your valuable comments.ßdummyTurker
15850ß0.0ßWe agree that the interaction framework is not new.ßdummyTurker
15851ß0.0ßHowever, it is non-trivial to successfully incorporate knowledge into the interaction framework.ßdummyTurker
15852ß0.0ßThe key problem is how to effectively build entity embeddings with both knowledge bases and documents.ßdummyTurker
15853ß0.0ßIn our work, we successfully composed semantics from knowledge graphs for entity embeddings and leveraged duet framework to incorporate these entity embeddings.ßdummyTurker
15854ß0.0ßWe believe our work will be a nice start of incorporating structural knowledge bases into data-driven neural models for document ranking and analysis.ßdummyTurker
15855ß0.0ßWe will carefully check and correct these errors in the revision.ßdummyTurker
15856ß0.0ßFirst, Our work is not solely a retrieval model but more focuses on how to bring in structured semantics from knowledge graphs to neural networks.ßdummyTurker
15857ß0.0ßSecond, retrieval models do help for NLP tasks such as data denoising.ßdummyTurker
15858ß0.0ßThird, for question answering over knowledge base, many works (such as Yih et al., ACL 2015) have a reranking process.ßdummyTurker
15859ß0.0ßIn above aspects, retrieval models are also interesting for ACL community.ßdummyTurker
15860ß0.0ßThank you for the suggestion, we will make our figures and model descriptions more clear in the revision.ßdummyTurker
15861ß0.0ßWe will cite the paper in the revision.ßdummyTurker
15862ß0.0ßIn the future work, we will consider more improvement for EDRM.ßdummyTurker
15863ß0.0ßThanks a lot for the insightful comments.ßdummyTurker
15864ß0.0ßPlease see our replies below:ßdummyTurker
15865ß0.0ßOur experimental configurations are same as K-NRM and Conv-KNRM.ßdummyTurker
15866ß0.0ßWe did not put all experimental configurations in this paper, because of the limitation of space.ßdummyTurker
15867ß0.0ßThrough previous papers, readers can have better comprehensions for the experimental setup.ßdummyTurker
15868ß0.0ßWe will also revise our experiment descriptions to include as many details as possible and make them easier for readers who are unfamiliar with this topic.ßdummyTurker
15869ß0.0ßHere we explain these confusions as followßdummyTurker
15870ß0.0ß(1) In this work, CTR was mapped to 5 relevance grades.ßdummyTurker
15871ß0.0ßThe thresholds were chosen so that our relevance grades had the same distribution as TREC Web Track 2009-2012 qrels.ßdummyTurker
15872ß0.0ß(2) Time-aware click model (TACM), a more sophisticated click model, used both clicks and dwell times.ßdummyTurker
15873ß0.0ßTACM aligned extremely well with expert annotations and illustrated deep underlying relevance.ßdummyTurker
15874ß0.0ß(3) In the training process, we utilized CTR and pairwise loss.ßdummyTurker
15875ß0.0ßWe treated the one with higher CTR as a positive example and the other was a negative example.ßdummyTurker
15876ß0.0ß(4) LeToR represents learning to rank in information retrieval.ßdummyTurker
15877ß0.0ß(5) Soft-TF is the key to our baseline K-NRM, which indicates the match score between word pairs are not exactly same.ßdummyTurker
15878ß0.0ß(6) The cross-in-the-circle operator is concatenation.ßdummyTurker
15879ß0.0ßWe thank your suggestion that current descriptions are not so friendly to NLP audience, and we will extensively polish these confusing parts in our revision.ßdummyTurker
15880ß0.0ßThank you for pointing out our writing problems.ßdummyTurker
15881ß0.0ßWe will perform careful proofreading, check grammatical errors and revise incomprehensible parts in our paper to improve readability of this work.ßdummyTurker
15882ß0.0ßThank you for your valuable suggestions and insightful feedback!ßdummyTurker
15883ß0.0ßWe will carefully revise our paper in the camera-ready version accordingly.ßdummyTurker
15884ß0.0ß1) To improve the write-up quality, we will talk more about the new concepts at the first time we introduce them.ßdummyTurker
15885ß0.0ßAlso, we would consider renaming “truth word sets” to “truth sets” or “truth” to refer the concept in the final version.ßdummyTurker
15886ß0.0ßGiven more space in the camera-ready version, we are going to move a few illustrative examples forward and add some restatements.ßdummyTurker
15887ß0.0ßWe will continue using our consistent examples to connect the new concepts and terms throughout the paper.ßdummyTurker
15888ß0.0ß2) We will include the parameters in the final version.ßdummyTurker
15889ß0.0ßWhat’s more, many of these details are actually in our submitted supplementary material, and we excluded them just due to the page limit.ßdummyTurker
15890ß0.0ß3) The KBCOM paper was only publicly available after the submission deadline of ACL 2018.ßdummyTurker
15891ß0.0ßTherefore, we were unable to read it or compare with it in our submission.ßdummyTurker
15892ß0.0ßPlease also note that it is a non-archival workshop paper, which is not in any proceedings or on the arXiv preprint server.ßdummyTurker
15893ß0.0ßAfter carefully reading the paper, we found that the core idea of this preliminary study is actually the same as one of our baseline methods, BLex (Section 4.2, line 447), which performs substantially worse than ours.ßdummyTurker
15894ß0.0ßWe will cite and mention the KBCOM work in the final version.ßdummyTurker
15895ß0.0ßThank you for your valuable suggestions and constructive feedback!ßdummyTurker
15896ß0.0ßIn this paper, we position our work as a straightforward, effective method particularly for mining cross-cultural differences between terms in different languages, which is an important open problem.ßdummyTurker
15897ß0.0ßOur method can potentially help other systems mine differences from two domains/languages.ßdummyTurker
15898ß0.0ßIn such real application scenarios, we think the requirement of a domain-specific vocabulary is necessary.ßdummyTurker
15899ß0.0ßOur method indeed uses a bilingual lexicon as a cheap parallel resource like many other cross-lingual NLP works.ßdummyTurker
15900ß0.0ßNonetheless, our method does not look up a dictionary like most classical dictionary-based methods do.ßdummyTurker
15901ß0.0ßInstead, we use a small set of seed translation pairs as the bridge (BSL) between two embedding spaces, so that all other terms are aligned according to the seed pairs in the SocVec space.ßdummyTurker
15902ß0.0ßThen, we run the following mining steps with the SocVec space.ßdummyTurker
15903ß0.0ßSimply put, we only treat the dictionary as an external resource for preprocessing instead of a major component of our method.ßdummyTurker
15904ß0.0ßAs mentioned in the paper, we tune all the tunable parameters of baseline methods like MultiCCA and Duong to obtain their highest performances in the comparison.ßdummyTurker
15905ß0.0ßWe also conduct ablation tests to verify our performance (Table 2 and Table 3).ßdummyTurker
15906ß0.0ßPlus, we think our increments about 5~25% in both correlation and precision are quite significant over strong baseline methods.ßdummyTurker
15907ß0.0ßPlease also consider the illustrative qualitative comparison results mentioned in our paper (there are more in the submitted supplementary materials).ßdummyTurker
15908ß0.0ßThanks for your comments and suggestions.ßdummyTurker
15909ß0.0ßThe task of generating remaining sentences according to given first sentences in a story is very challenging and the task will become more challenging if we provide less sentences but require to generate more sentences.ßdummyTurker
15910ß0.0ßWe will consider to conduct the experiments with various numbers of input sentences and present the results in the next edition.ßdummyTurker
15911ß0.0ßWe pre-train the generator and the discriminators for 15 epochs and then they are trained adversarially for about 5 epochs.ßdummyTurker
15912ß0.0ßThe convergence for adversarial training is determined based on the BLEU score on the dev set.ßdummyTurker
15913ß0.0ßBoth pre-training and teacher forcing are useful in our model.ßdummyTurker
15914ß0.0ßThe performance will be harmed if pre-training or teacher forcing is not used.ßdummyTurker
15915ß0.0ßWe will clearly mention this in the next edition.ßdummyTurker
15916ß0.0ßWe will consider to do human evaluation for KA-Seq2Seq.ßdummyTurker
15917ß0.0ßThanks for your comments and suggestions.ßdummyTurker
15918ß0.0ßThe works of (Mostafazadeh et al., 2016), (Chaturvedi et al., 2017), (Wang et al., 2017) focused on choosing the correct ending from two candidates, which is totally different from our generation task.ßdummyTurker
15919ß0.0ßWe will make this clearer in the next edition.ßdummyTurker
15920ß0.0ßIn the evaluation, we have more than 9800 stories for test, which is a test set with big size for drawing convincing conclusions, so we didn’t adopt cross-validation in the experiments.ßdummyTurker
15921ß0.0ßWe will try to find another data set to test the robustness of our model.ßdummyTurker
15922ß0.0ßWe will compute and add the significance test results, the inter-annotator agreement and present some wrong examples in the next edition.ßdummyTurker
15923ß0.0ßGive Story: Krista was organizing her office.ßdummyTurker
15924ß0.0ßShe bought some new shelves.ßdummyTurker
15925ß0.0ßThey were big and heavy.ßdummyTurker
15926ß0.0ßShe assembled them carefully.ßdummyTurker
15927ß0.0ßTFIDF:	She bought it and put all her supplies in it.ßdummyTurker
15928ß0.0ßSeq2Seq:		She was proud of herself.ßdummyTurker
15929ß0.0ßSeq2SeqMMI:		She was happy to have done a great job.ßdummyTurker
15930ß0.0ßOurs:	She was glad she 'd done such a lot of work.ßdummyTurker
15931ß0.0ßHuman:	When she put all her books on them , they collapsed!ßdummyTurker
15932ß0.0ßThanks for your comments and suggestions.ßdummyTurker
15933ß0.0ß1)  As we mentioned in the paper (in line 427), the discriminators only provide a reward value for a finished sequence.ßdummyTurker
15934ß0.0ßWe should not only consider the fitness of previous tokens (prefix) but also the resulted future outcome.ßdummyTurker
15935ß0.0ßThus we apply MC search with the generator (G_{\theta}) to sample the unknown last T-t words in time t. In other words, the MC search result is based on G_{\theta} and the current state in time t.ßdummyTurker
15936ß0.0ß2) The object function is defined in equation (9) when we consider the whole generation process as a single step.ßdummyTurker
15937ß0.0ßThe reward Q* for action in time t in is defined in equation (10).ßdummyTurker
15938ß0.0ßUse it to replace the reward Q in equation (9) we could get the new object function which considers the process as a T-step decision sequence.ßdummyTurker
15939ß0.0ßThen it becomes easy to derive the gradient in equation (11).ßdummyTurker
15940ß0.0ß3) The nature of approximation is policy gradient.ßdummyTurker
15941ß0.0ßIn Sutton’s words, ``Rather than approximating a value function and using that to compute a deterministic policy, we approximate a stochastic policy directly using an independent function approximator with its own parameters.” It is detailed in the paper (Sutton et al., 2000).ßdummyTurker
15942ß0.0ßWe will try to make the equations and their explanations easier to understand in next edition.ßdummyTurker
15943ß0.0ßWe pre-train the generator and the discriminators for 15 epochs and then they are trained adversarially for about 5 epochs.ßdummyTurker
15944ß0.0ßThe statistical makeup of the convergence process for adversarial training is based on the BLEU score on the dev set.ßdummyTurker
15945ß0.0ßWe will release our code and data on github to make our experiments more repeatable.ßdummyTurker
15946ß0.0ßThe responses for these questions are explained above.ßdummyTurker
15947ß0.0ßWithout a keyword selection step, the seq2seq model tends to generate dull and generic endings (mentioned in section 4.5).ßdummyTurker
15948ß0.0ßThe results of our experiments (both automatic and human-based evaluations) have shown that KA-Seq2Seq performs better than the standard Seq2Seq in this task.ßdummyTurker
15949ß0.0ßWe have checked top-3 keywords selected by the algorithm manually and found that sometimes the second word and the third word are not very related to the theme of the story.ßdummyTurker
15950ß0.0ßChoosing one word is just an empirical setting.ßdummyTurker
15951ß0.0ß1) Based on the distribution p mentioned in equation (5).ßdummyTurker
15952ß0.0ß2) \theta represents the parameters in generator G. \phi represents the parameters in discriminators D.ßdummyTurker
15953ß0.0ß3) The explanations for equations (9) (10) (11) are mentioned above.ßdummyTurker
15954ß0.0ßWe estimate the reward baseline b with the expectation reward of the current generator and it is detailed in Appendix A of the paper (Zaremba and Sutskever, 2015).ßdummyTurker
15955ß0.0ß4) \lambda is the tradeoff parameter for both D2 and D3.ßdummyTurker
15956ß0.0ßTo guarantee the sum of Qs belongs to [0,1], the tradeoff parameter for D1 is 1- \lambda - \lambda.ßdummyTurker
15957ß0.0ßAs reviewer 1 pointed out, one strength of our model is its robustness against noise on token boundaries and that it does not rely on a tokenizer/pipeline model.ßdummyTurker
15958ß0.0ßEspecially in the current era of end-to-end modeling with neural networks, it is an important step to show that the traditional NLP pipeline consisting of tokenizer and POS tagger model can be replaced by a single network which is considerably more robust against noise.ßdummyTurker
15959ß0.0ßAs mentioned in the paper, there is no dataset available which we could use to verify our assumptions.ßdummyTurker
15960ß0.0ßThis is why we have created the artificial dataset.ßdummyTurker
15961ß0.0ßIn fact, we assume that the artificial dataset is much more challenging than user-generated content since it considers different levels of noise.ßdummyTurker
15962ß0.0ßWe agree that in future work, it would be interesting to label user-generated content accordingly.ßdummyTurker
15963ß0.0ßHowever, this goes beyond the scope of a case study in a short paper.ßdummyTurker
15964ß0.0ßIf we used the automatic tokenized data in UD, we could not have modeled the effect of different levels of noise which was an important part of our evaluation.ßdummyTurker
15965ß0.0ß"""it_seems_the_author_is_using_word-level_accuracy_as_evaluation_[...]_common_metric_is_F-score"""ßdummyTurker
15966ß0.0ßTo be able to compare with previous work in Table 1, we use the same evaluation measure as they do which is accuracy [Müller et al.ßdummyTurker
15967ß0.0ß2015, Plank et al.ßdummyTurker
15968ß0.0ß2016, Yu et al.ßdummyTurker
15969ß0.0ß2017].ßdummyTurker
15970ß0.0ßHowever, we also have F1 score results and are happy to extend the paper with this: The F1 score of our models (Table 1) are: our: 92.86, our with SRNN: 92.62.ßdummyTurker
15971ß0.0ßNote that these numbers are not comparable anymore to the other numbers of Table 1.ßdummyTurker
15972ß0.0ß"The ""gold"" data in the noisy scenario is the corrupted dataset with labels derived from the original test set without corruption (see description in Section 3.2 ""Results"")."ßdummyTurker
15973ß0.0ßWe would be happy to use the additional space of the final paper to extend the description on that and give examples.ßdummyTurker
15974ß0.0ßThere are two main differences of our model to the original model by Zhou:ßdummyTurker
15975ß0.0ß1.ßdummyTurker
15976ß0.0ßThe original model in Zhou et al.ßdummyTurker
15977ß0.0ß(2016) is word-based and would therefore suffer from the same problems as MarMot in the noisy data experiment.ßdummyTurker
15978ß0.0ß2.ßdummyTurker
15979ß0.0ßOur model uses an LSTM layer for creating character-based representations.ßdummyTurker
15980ß0.0ßWhen we run the model without this additional LSTM layer, the performance drops significantly to around 88%.ßdummyTurker
15981ß0.0ßWe will add the ablation study to the final version if the paper gets accepted.ßdummyTurker
15982ß0.0ßWe consider our work a case study on how much errors in tokenization a model can handle, i.e.ßdummyTurker
15983ß0.0ßwhich level of tokenization corruption affects the results how much.ßdummyTurker
15984ß0.0ßIn future work, we will also integrate different languages.ßdummyTurker
15985ß0.0ßWe expect that our models also works for those languages since we consider space only as an additional feature to our model but not as a central component for tokenization.ßdummyTurker
15986ß0.0ßThis is confirmed by our experiments on the corrupted tokenization: The model can handle both splitted words (like words splitted into syllables in Vietnamese) and merged words (like merged tokens in Chinese).ßdummyTurker
15987ß0.0ßWe followed your suggestion and ran UDpipe on the text.ßdummyTurker
15988ß0.0ßThe results are:ßdummyTurker
15989ß0.0ß- clean text: P = 98.23, R = 98.74, F1 = 98.48ßdummyTurker
15990ß0.0ß- noisy LOW: P = 72.90, R = 69.01, F1 = 70.90ßdummyTurker
15991ß0.0ß- noisy MED: P = 21.23, R = 20.05, F1 = 20.62ßdummyTurker
15992ß0.0ß- noisy HIGH: P = 21.06, R = 19.91, F1 = 20.47ßdummyTurker
15993ß0.0ßThus, the quality of the segmentation of our model is better even on the clean text and, by a very large margin, on the noisy text.ßdummyTurker
15994ß0.0ßWe will add those results to the final version of the paper if the paper gets accepted.ßdummyTurker
15995ß0.0ßActually, this is what we did: We first used a word segmenter and then applied MarMot to its output.ßdummyTurker
15996ß0.0ßWe could not explain this in detail due to space constraints but will add it to the final version.ßdummyTurker
15997ß0.0ßOur paper is a case study on how much corruption on tokenization leads to which performance decreases in the models.ßdummyTurker
15998ß0.0ßTo the best of our knowledge, there is no labeled dataset available which provides different levels of tokenization corruption.ßdummyTurker
15999ß0.0ßThis is why we created the artificial dataset and performed our analysis on it.ßdummyTurker
16000ß0.0ßHowever, thank you for suggesting the CoNLL2017 shared task data.ßdummyTurker
16001ß0.0ßWe will use this dataset when we do multilingual experiments in the future (see response to next argument).ßdummyTurker
16002ß0.0ßWe plan to do multilingual experiments in the future.ßdummyTurker
16003ß0.0ßSince we do not use any language-specific information in our model, we are confident that the model can be applied to other languages as well.ßdummyTurker
16004ß0.0ßOur experiments on noisy text shows that the model is robust even in the case of difficult tokenization.ßdummyTurker
16005ß0.0ßWe will improve our wording.ßdummyTurker
16006ß0.0ßWhat we mean is that their are no OOVs when modeling characters (given that all characters are in the char-vocabulary).ßdummyTurker
16007ß0.0ßHowever, we agree that the models still perform better on words they know from training than on words they have never seen before.ßdummyTurker
16008ß0.0ßNevertheless, it is arguably easier for character-based models to guess the meaning of unknown words when they find similarities in the character sequence to other words they know.ßdummyTurker
16009ß0.0ß"- ""typo_on_Figure_1(?"ßdummyTurker
16010ß0.0ß")_<4,6>_instead_of_<4,3>"":"ßdummyTurker
16011ß0.0ß"<4,3,NN> is correct since we define the sequence as ""<start position, length, label>"" and 3 is the length of the depicted sequence."ßdummyTurker
16012ß0.0ßSorry for the confusion.ßdummyTurker
16013ß0.0ß"For questions that contain multiple sentences, we concatenate them as ""one sentence"" in order to derive the entity position feature."ßdummyTurker
16014ß0.0ßWe will make this clear in the revision.ßdummyTurker
16015ß0.0ßThanks for this suggestion!ßdummyTurker
16016ß0.0ß"We will modify those places as ""entity pairs"" in the new version."ßdummyTurker
16017ß0.0ßThanks for this comment, and we will move the relevant information in Section 3 to Section 2.1 to make it clear that entity may contain multiple words.ßdummyTurker
16018ß0.0ß"Sorry about the confusion, we will distinct ""entity pairs"" and ""entity relations"" in the revision."ßdummyTurker
16019ß0.0ß"About the decision to consider only one direction for the entity pair: To confirm this point, we calculate the percentage of questions that contain ""symptom entity first and then drug entity""."ßdummyTurker
16020ß0.0ßFor symptom2drug intention type, the percentage is 88.67%, while for drug2side_effect intention, that number is 44.36%.ßdummyTurker
16021ß0.0ßWe will include more such statistics to support the in-order entity pair setting.ßdummyTurker
16022ß0.0ßThanks for this insightful comment.ßdummyTurker
16023ß0.0ßTo give a deep understanding about the English Medical Question Dataset, here we provide some examples, which will also be included in the new version.ßdummyTurker
16024ß0.0ß1）Can anyone please tell me if Tramadol is a narcotic ?ßdummyTurker
16025ß0.0ßI have random drug testing and also cronic pain.ßdummyTurker
16026ß0.0ßWhen i started taking Tramadol it was not considered a narcotic.ßdummyTurker
16027ß0.0ßCan anyone tell me if it is?ßdummyTurker
16028ß0.0ßThanksßdummyTurker
16029ß0.0ß2）How strong is tramadol 50mg?ßdummyTurker
16030ß0.0ßwhen I take them for my back I get very happy.ßdummyTurker
16031ß0.0ßIs this normal?ßdummyTurker
16032ß0.0ß1) I feel dopey in the morning and wonder if it is better taken in the evening?ßdummyTurker
16033ß0.0ßWhen is the best time to take Zoloft ?ßdummyTurker
16034ß0.0ß2) Could anyone tell me from their experience with zoloft... how long it takes to start feeling the benefits of taking this medicine ?ßdummyTurker
16035ß0.0ßThank you very much !ßdummyTurker
16036ß0.0ßThanks for the suggestions about Table 4, Table 5 and Figure 4, we will modify them according to your suggestions in the new version.ßdummyTurker
16037ß0.0ßWe will add the details about entity detection based on medical dictionaries.ßdummyTurker
16038ß0.0ßGenerally speaking, we utilize an existing package Jieba (https://github.com/fxsjy/jieba), which is developed for Chinese text segmentation.ßdummyTurker
16039ß0.0ßThis package allows users to upload new dictionary, and it will segment texts based on the added dictionaries.ßdummyTurker
16040ß0.0ß"This package provides different options for matching, and we choose ""exact match"" option."ßdummyTurker
16041ß0.0ßMedical corpus is used to learn word and entity embeddings.ßdummyTurker
16042ß0.0ßFor Chinese dataset, corpus includes medical textbooks, contents from the medical forum, medical relevant news, and medical wikipedia.ßdummyTurker
16043ß0.0ßFor English dataset, such corpus includes news and text information about drugs, for example: https://www.drugs.com/news.html and https://www.drugs.com/drug_information.htmlßdummyTurker
16044ß0.0ßFor Table 4 and 5, we have all the results (Precision, Recall and F1).ßdummyTurker
16045ß0.0ßWe only report precision due to the space limit and the similar trends in recall and F1.ßdummyTurker
16046ß0.0ßWe will include more results in the revision.ßdummyTurker
16047ß0.0ßWe really appreciate your carefully checking and great suggestions!ßdummyTurker
16048ß0.0ßWe will change those corresponding places in the revision.ßdummyTurker
16049ß0.0ßMany thanks for your insightful suggestions about baselines!ßdummyTurker
16050ß0.0ßWe will include both hierarchical LSTM (Yang et al.ßdummyTurker
16051ß0.0ß2016) and another variant of the proposed method (where only individual entities are modeled) as baselines in the revision.ßdummyTurker
16052ß0.0ßSorry that we cannot report the results of these two baselines due to the very limited time of rebuttal period.ßdummyTurker
16053ß0.0ßa) About the entity order at the end of section 2.2.ßdummyTurker
16054ß0.0ß"To confirm this point, we calculate the percentage of questions that contain ""symptom entity first and then drug entity""."ßdummyTurker
16055ß0.0ßFor symptom2drug intention type, the percentage is 88.67%, while for drug2side_effect intention, that number is 44.36%.ßdummyTurker
16056ß0.0ßWe will include more such statistics to support the in-order entity pair setting.ßdummyTurker
16057ß0.0ßb) Thanks for pointing out this claim about language independency of the proposed model.ßdummyTurker
16058ß0.0ßOur claim might be strong, and we will modify this accordingly in the revision.ßdummyTurker
16059ß0.0ßThanks for the suggestion about hyper-parameters.ßdummyTurker
16060ß0.0ßWe will use the additional page given in the new version to report the effect of hyper-parameters.ßdummyTurker
16061ß0.0ßWe have all the results (Precision, Recall and F1) for the experiments in Section 4.3.2.ßdummyTurker
16062ß0.0ßWe only report precision due to the space limit and the similar trends in recall and F1.ßdummyTurker
16063ß0.0ßWe will include more results in the revision.ßdummyTurker
16064ß0.0ßThanks for the suggestion about learning entity embeddings from knowledge graph (KG).ßdummyTurker
16065ß0.0ßHowever, for medical questions, most of the involved entities are medical terminologies, and the representations learned based on KG are not very meaningful due to the sparseness of corresponding entity links in KG.ßdummyTurker
16066ß0.0ßWe will add explanations about this in the revision.ßdummyTurker
16067ß0.0ßThanks for pointing out the problems and for your insightful and valuable suggestions, your concerns are answered as follow.ßdummyTurker
16068ß0.0ßThe setup of the experiment is not exactly the same.ßdummyTurker
16069ß0.0ßThe main difference lies in the vocabulary size for encoder-decoder network, the size we used is 45,000, whereas that of Kim and Lee used is 70,000 or 100,000.ßdummyTurker
16070ß0.0ßAnd we only used the training data of WMT17 sentence-level QE task, we did not used the word-level and phrase-level training data.ßdummyTurker
16071ß0.0ßBesides, we combined six likelihood-trained UNQE models by changing the dimension of the word embedding from 500 to 700 and the dimension of the RNN from 100 to 200 at intervals of 50.ßdummyTurker
16072ß0.0ßWe averaged six predictions from each UNQE models with six types of dimensionality settings.ßdummyTurker
16073ß0.0ßWhereas Kim and Lee averaged 15 predictions from each single model with five types of dimensionality settings.ßdummyTurker
16074ß0.0ßWe used the same parallel corpora to pre-train the encoder-decoder network.ßdummyTurker
16075ß0.0ßAnd we only used the training data of the WMT17 sentence-level QE task to train the whole network, without using the training data for word-level and phrase-level QE task.ßdummyTurker
16076ß0.0ßThe system performance is improved because we combined the two sub-networks into one and trained the network based on sentence-level QE training data.ßdummyTurker
16077ß0.0ßThe dimension of word embedding we used is 500 and 700, which is the same as that of Kim and Lee’s.ßdummyTurker
16078ß0.0ßWe combined six likelihood-trained UNQE models by changing the dimension of the word embedding from 500 to 700 and the dimension of the RNN from 100 to 200 at intervals of 50.ßdummyTurker
16079ß0.0ßWe averaged six predictions from each UNQE models with six types of dimensionality settings.ßdummyTurker
16080ß0.0ßWhereas Kim and Lee averaged 15 predictions from each single model with five types of dimensionality settings.ßdummyTurker
16081ß0.0ßBecause the sigmoid function is a nonlinear activation function, it will decrease the gap between the QE scores for the translation outputs.ßdummyTurker
16082ß0.0ßThus the result will be worse after adding the sigmoid function.ßdummyTurker
16083ß0.0ßWe found that most of the predicted QE scores are among [0, 1] when used the naive score, but a little scores are not, so we used simple truncation approach.ßdummyTurker
16084ß0.0ßFurther analysis revealed that the predicted scores are closer to the human judgments.ßdummyTurker
16085ß0.0ßThanks for pointing out the problems and for your insightful and valuable suggestions, your concerns are answered as follow.ßdummyTurker
16086ß0.0ßBecause the model is designed to predict the quality score of translation output according to the HTER score of human judgment, which is among [0, 1].ßdummyTurker
16087ß0.0ßThus the score predicted by our model should also be among [0, 1], so we clipped the output of the network to [0, 1].ßdummyTurker
16088ß0.0ßWe found that this approach is more effective than using the sigmoid function to normalize the output scores in the experiment.ßdummyTurker
16089ß0.0ßWell, we used formula (5) to calculate the final QE score.ßdummyTurker
16090ß0.0ßTo get the summary unit vn, the hidden state vj employing p gated hidden units for the target word vj is computed by: vj = f (qyj ,vj−1).ßdummyTurker
16091ß0.0ßThe GRU for the activation function f is used to learn long-term dependencies of translation qualities for target words.ßdummyTurker
16092ß0.0ßWe consider the QE score as the integrated/condensed value reflecting the sequential quality information of sequential target words.ßdummyTurker
16093ß0.0ßBecause the last hidden state vn is a summary of the sequential quality vectors, we defined the summary units as the last hidden state vn of QE RNN.ßdummyTurker
16094ß0.0ßWell, we only used the training data of the WMT17 sentence-level QE task to train the whole network, without using the training data for word-level and phrase-level QE task.ßdummyTurker
16095ß0.0ßThanks for pointing out the problems and for your insightful and valuable suggestions, your concerns are answered as follow.ßdummyTurker
16096ß0.0ß(1)We implemented the neural network based on the open source “Nematus” package, which was employed to extract quality vectors given the translation context, and then we developed a GRU-based QE RNN network which was employed to combined the quality vectors into a QE score, that is the quality vectors were taken regards as mediation input nodes for the hidden states of GRU nodes.ßdummyTurker
16097ß0.0ßIn the Theano framework, the parameters of the whole network can automatically tune when fed into QE training instances.ßdummyTurker
16098ß0.0ßBecause the size of the training data for sentence-level QE task is very small, 23,000 instances for en-de task and 25,000 instances for de-en task, whereas the number of the parameters of the whole network is very big, we pre-training the encoder-decoder network based on bilingual parallel data, released for MT task, and pre-training the QE RNN network based on the training data for sentence-level QE task.ßdummyTurker
16099ß0.0ßAfter that, we re-trained the whole network based on the training data for sentence-level QE task to get a fine-grained network for sentence-level QE task.ßdummyTurker
16100ß0.0ßRico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel Läubli, Antonio Valerio Miceli Barone, Jozef Mokry and Maria Nadejde, 2017.ßdummyTurker
16101ß0.0ßNematus: a Toolkit for Neural Machine Translation.ßdummyTurker
16102ß0.0ßProceedings of the EACL 2017 Software Demonstrations, pages 65-68, Valencia, Spain, Association for Computational Linguistics.ßdummyTurker
16103ß0.0ß(2)We pre-trained the QE RNN based on the training data released by WMT17 sentence-level QE task (https://lindat.mff.cuni.cz/repository/xmlui/handle/11372/LRT-1974), and re-trained the whole network based on the same training data.ßdummyTurker
16104ß0.0ßOnly the parameters of the QE RNN were tuned in pre-training, whereas the parameters of the whole network were tuned in re-training in order to make the encoder-decoder network aware of the sentence-level QE task.ßdummyTurker
16105ß0.0ß(3)Well, we compared the parameters of the encoder-decoder network after pre-training with the parameters after re-training, and found the changes between them are small, which might lead to the unified network is only slightly better than training both networks separately.ßdummyTurker
16106ß0.0ßBy the way, we will add the statistical significance test in the experiment and include the missed references in the updated version.ßdummyTurker
16107ß0.0ßBecause the network was implemented based on the “Nematus” package, we hope to open source our system after asking for the consent of Sennrich et al.ßdummyTurker
16108ß0.0ßThe term classification was used in the work due to the methodology being a supervised binary classification problem, determining the status of a word as metaphor/non-metaphor.ßdummyTurker
16109ß0.0ßThe results obtained from the test set were overall unmodified with respect to hyper-parameter tuning, with the exception of dropout values (discussed in the Results section).ßdummyTurker
16110ß0.0ßWhile other configurations of these hyper-parameters were tested, the original arrangement was found to be very effective, which can in part be explained by the inherent benefit of these hyper-parameters over others (eg.ßdummyTurker
16111ß0.0ßAdam, ReLU algorithms).ßdummyTurker
16112ß0.0ßThe inclusion of a separate cross-validation set would certainly be an advantage to the current study.ßdummyTurker
16113ß0.0ßAnalysis of the results is provided in the Discussion section regarding some aspects of the classification.ßdummyTurker
16114ß0.0ßAn example is the pattern that certain metaphors that would be ambiguous to a human annotator seem to be reflected in the discrepancy of the classifier results, with sentences that arguably contain metaphors but are not annotated as such being classified positively.ßdummyTurker
16115ß0.0ßFurther analysis such as a t-SNE plot would definitely give additional insight into the status of each sentence.ßdummyTurker
16116ß0.0ßThe justification of the use of the Paragraph Vector can be seen in the importance of utilizing contextual elements in metaphor processing, since surface realizations of metaphor tend to appear in the form of semantic anomalies within a given context.ßdummyTurker
16117ß0.0ßThe use of NN models allows to examine different combinations of these latent contextual features.ßdummyTurker
16118ß0.0ßThe current approach is also data-driven, not requiring any hand-coded knowledge when applied to other datasets.ßdummyTurker
16119ß0.0ßSequences of the RNN/LSTM models were done at the level of one sentence per input, meaning a 300-dimensional representation of that sentence per timestep, allowing for patterns in metaphorical instances to be retained at a certain level.ßdummyTurker
16120ß0.0ßIt would not make sense to retain memory of multiple sentences for the current task, since the VUAMC contains a variety of different categories and selections from the British National Corpus.ßdummyTurker
16121ß0.0ßSince the test sentences were not included in the training of the Paragraph Vector model, their representations are obtained by freezing the existing word vectors but training on the new documents, minimizing log-likelihood with gradient descent for several iterations.ßdummyTurker
16122ß0.0ßSee response to #5.ßdummyTurker
16123ß0.0ßThere were 8220 positive and 7982 negative labels in the full dataset.ßdummyTurker
16124ß0.0ßA simple majority baseline provides an F-score of 0.67, with a precision of 0.50 and recall of 1.00.ßdummyTurker
16125ß0.0ßThank you for your thorough review!ßdummyTurker
16126ß0.0ßWe will clarify the paper accordinglyßdummyTurker
16127ß0.0ßIndeed, DALPHI itself is an important contribution of this work, as we felt the existing annotation software platforms do not provide a comparably modern interface, structured open source implementation, and easy extensibility as DALPHI.ßdummyTurker
16128ß0.0ßWhile the results of the experiment are interesting by themselves and most likely translate to other annotation platforms as well, they also specifically support our claim that our annotation framework is useful in practice.ßdummyTurker
16129ß0.0ßWe did not include the real AL component in the experiments as the pre-annotations need to be of controlled accuracy to get comparable and interpretable results.ßdummyTurker
16130ß0.0ßWe chose the 10/50/90 settings to test the outer bounds of possible model accuracies; since annotations are especially needed in domains or for languages with little annotated data available already, the first models trained on very little data might indeed yield accuracies well below 50%.ßdummyTurker
16131ß0.0ßOf course, the evaluation of more settings would be extremely interesting, however, due to the large number of study participants required to get statistically meaningful results, this had to be left for future work.ßdummyTurker
16132ß0.0ßThank you for your thorough review and insightful comments, and especially for providing references to further related work, which indeed we should not have missed!ßdummyTurker
16133ß0.0ßWhile individual features of our annotation software might not be novel, we felt that none of the existing software platforms provide a comparably modern interface, structured open source implementation, and easy extensibility as DALPHI.ßdummyTurker
16134ß0.0ßTherefore, we believe that our framework will add value to the NLP community; a claim we further backed up by conducting an extensive experiment.ßdummyTurker
16135ß0.0ßWe chose the 10/50/90 settings to test the outer bounds of possible model accuracies; since annotations are especially needed in domains or for languages with little annotated data available already, the first models trained on very little data might indeed yield accuracies well below 50%.ßdummyTurker
16136ß0.0ßOf course, the evaluation of more settings would be extremely interesting, however, due to the large number of study participants required to get statistically meaningful results, this had to be left for future work.ßdummyTurker
16137ß0.0ßIndeed in the case where pre-annotations were made at a level of 90% recall, the performance of the human annotators was slightly below 90%.ßdummyTurker
16138ß0.0ßThis is due to the annotators erroneously changing some of the pre-annotations, thereby making them wrong (e.g.ßdummyTurker
16139ß0.0ßfor person names extending the annotation span to additionally include the person's title).ßdummyTurker
16140ß0.0ßThe annotators were all students without prior annotation experience and probably should have been instructed more carefully on the task.ßdummyTurker
16141ß0.0ßNever the less, the point we were trying to make was that the performance is still better than with no pre-annotations (baseline).ßdummyTurker
16142ß0.0ßBut this is indeed an interesting result that we will point out in the discussion.ßdummyTurker
16143ß0.0ßWe created the gold standard manually by ourselves and will further clarify this in the paper as well as make these annotated documents available for review.ßdummyTurker
16144ß0.0ßThank you, we will extend our related work section accordingly!ßdummyTurker
16145ß0.0ßNo, when the documents are merged back with the rest of the corpus, it can not happen that the documents are annotated again or manual annotations are overwritten.ßdummyTurker
16146ß0.0ßIt is indeed a rather technical description of what is going on under the hood, i.e.ßdummyTurker
16147ß0.0ßhow the data is stored internally, and we will further clarify this in the paper!ßdummyTurker
16148ß0.0ßYes, r_0 is the BiLSTM, we will add this.ßdummyTurker
16149ß0.0ßThank you very much for suggesting the connection to the Peters et al NAACL 2018 paper on deep contextualized embeddings.ßdummyTurker
16150ß0.0ßWe will definitely discuss connections to it  in the paper.ßdummyTurker
16151ß0.0ßWe will include more ablations in the final version.ßdummyTurker
16152ß0.0ßIn particular, we plan to include an ablation that uses the overall meta model, but uses word-only context for the character embeddings rather than sentence-level context.ßdummyTurker
16153ß0.0ßTable 5 includes ablations of the character model and the word model, compared with the full meta-model.ßdummyTurker
16154ß0.0ßThe differences between just using the character model and the meta-model is significant (and between using just the words and the meta-model even more so).ßdummyTurker
16155ß0.0ßWe also plan to include ablations related to the multiple losses.ßdummyTurker
16156ß0.0ßWe have experimented with training the full network end-to-end (rather than incorporating the different losses for the word, character, and meta model) and without multiple losses backpropagating through the network we saw drops between 0.2 and 0.4 points on development sets for English.ßdummyTurker
16157ß0.0ß“The proposed extension of cross-stitch networks seems marginal.ßdummyTurker
16158ß0.0ßDiffering from cross-stitch networks, the proposed method employs skip-connections from each layers, but the extension is inspired by (He et al., 2016).”ßdummyTurker
16159ß0.0ß“The methodological originality of the proposed method is unclear.”ßdummyTurker
16160ß0.0ßWe would like to emphasize that this is---to our knowledge---the first time cross-stitch networks have been applied to NLP.ßdummyTurker
16161ß0.0ßOur contribution is thus both a) showing that this model is useful for NLP and b) that it can be improved upon.ßdummyTurker
16162ß0.0ßOur improvements include skip-connections that can be learned and the orthogonality constraint, which are both theoretically motivated and whose usefulness we show through an ablation analysis.ßdummyTurker
16163ß0.0ßIn addition, we believe it is important to think more rigorously about how we do multi-task learning for NLP.ßdummyTurker
16164ß0.0ßFor this reason, we think that highlighting how commonly used multi-task and transfer learning approaches relate to each other---and how they can be generalized---is an important theoretical contribution.ßdummyTurker
16165ß0.0ß“The performance improvement from cross-stitch networks looks small.”ßdummyTurker
16166ß0.0ßWe will add statistical significance results to the camera-ready version.ßdummyTurker
16167ß0.0ßWe would also like to note that multi-task learning has been observed to improve upon single-task learning only under certain conditions, e.g.ßdummyTurker
16168ß0.0ßfor 1/5 tasks (Martinez Alonso and Plank, 2017) and 50/90 combinations (Bingel and Søgaard, 2017), whereas we consistently improve upon single-task learning, hard parameter sharing, and the previous state-of-the-art approach for NLP tasks, low supervision.ßdummyTurker
16169ß0.0ßWe will elaborate on this in the camera-ready version.ßdummyTurker
16170ß0.0ßMartinez Alonso, H., & Plank, B.ßdummyTurker
16171ß0.0ß(2017).ßdummyTurker
16172ß0.0ßWhen is multitask learning effective?ßdummyTurker
16173ß0.0ßMultitask learning for semantic sequence prediction under varying data conditions.ßdummyTurker
16174ß0.0ßIn EACL.ßdummyTurker
16175ß0.0ßRetrieved from http://arxiv.org/abs/1612.02251ßdummyTurker
16176ß0.0ßBingel, J., & Søgaard, A.ßdummyTurker
16177ß0.0ß(2017).ßdummyTurker
16178ß0.0ßIdentifying beneficial task relations for multi-task learning in deep neural networks.ßdummyTurker
16179ß0.0ßIn EACL.ßdummyTurker
16180ß0.0ßRetrieved from http://arxiv.org/abs/1702.08303ßdummyTurker
16181ß0.0ßWe use Naïve Bayes because it is a classic and widely used method, and also the first linguistic model of our online system.ßdummyTurker
16182ß0.0ßBesides, it was meant to show that traditional methods do not work  well any more on spammers who have become more sophisticated and capricious.ßdummyTurker
16183ß0.0ßIndeed, it is not appropriate to use Naïve Bayes here as a baseline.ßdummyTurker
16184ß0.0ßWord-level CNN (a popular method for text classification in recent years, which is also included in our work) should be seen as a baseline.ßdummyTurker
16185ß0.0ßAs shown in Table 2, our model gained an increase of 3.97% on the F1 score relative to Word-level CNN, reaching 92.03%.ßdummyTurker
16186ß0.0ßDue to limited space, we did not describe the settings clearly.ßdummyTurker
16187ß0.0ßPlease see the answers below in the question part.ßdummyTurker
16188ß0.0ßYes, our method is not suitable for all kinds of messages.ßdummyTurker
16189ß0.0ßTo be precise, our model is suitable for websites where user behaviors can be collected, such as product review, tweet, and Facebook post.ßdummyTurker
16190ß0.0ßFor e-mails and texting via cellular networks, the proposed method can be always degraded to a linguistic model (by switching off the behavioral and temporal part which can be easily done in the integration stage, as shown in Figure 2).ßdummyTurker
16191ß0.0ßThere are a variety of behaviors in our dataset, such as login, search, click on various titles/pictures/buttons/..., move the mouse, send actions, typing, open a specific URL, exit, and so on.ßdummyTurker
16192ß0.0ßThe unit of a session is Day.ßdummyTurker
16193ß0.0ßThe start of the session is defined as the first time visiting the website, and the end is 23:59:59 of the same day.ßdummyTurker
16194ß0.0ßThat is, as shown in Table 1, the maximum time interval between two actions is defined as 86400 seconds (1 day).ßdummyTurker
16195ß0.0ßWe used results (spam or not spam labels) from old models (such as rules, graph models, and statistical models)，which had been deployed online before, as gold-labels.ßdummyTurker
16196ß0.0ßThese models had been evaluated manually to guarantee high precision (at least 99%).ßdummyTurker
16197ß0.0ßOf course, the recall rates were not very high (about 80%).ßdummyTurker
16198ß0.0ßThe proposed model used these labels for training and testing, and managed to identify more spam messages that were not identified by old models.ßdummyTurker
16199ß0.0ßThe new spam messages were then evaluated manually again for calculating precision and recall.ßdummyTurker
16200ß0.0ßYes, a development (or so-called “validation”) set is needed.ßdummyTurker
16201ß0.0ßWe sampled 10% from the training set as the development set.ßdummyTurker
16202ß0.0ßAs shown in our source code, there is an input variable for the development set.ßdummyTurker
16203ß0.0ßFurthermore, we actually combine the iteration parameter and the loss curve on the development set to determine when to stop training.ßdummyTurker
16204ß0.0ßSorry but the wording of the table caption is misleading.ßdummyTurker
16205ß0.0ßThe “length” values in Table 3 means length of raw behavioral steps (for example number of clicks) in model B+T, rather than the vector length of B+T.ßdummyTurker
16206ß0.0ßWe could have provided predictions, but due to limited space, we only provided number of steps, which is somehow corresponding to prediction time (fewer steps means less prediction time).ßdummyTurker
16207ß0.0ßYes, some actions can’t be observed in server side directly, such as the trace of the mouse.ßdummyTurker
16208ß0.0ßTherefore, some scripts are deployed on client side legally to collect these actions and send them back to server side.ßdummyTurker
16209ß0.0ßFor security reasons, the dataset used in the paper cannot be disclosed at the moment.ßdummyTurker
16210ß0.0ßWe are trying to find public datasets to test our model.ßdummyTurker
16211ß0.0ßTo obtain better results, Bleualign requires the translated text too.ßdummyTurker
16212ß0.0ßHowever, in general, you are right that reliance on a third-party service is not a scalable solution.ßdummyTurker
16213ß0.0ßThis is the reason why we included the off-line mode.ßdummyTurker
16214ß0.0ßWe agree that some literature on splitting compounds could be included.ßdummyTurker
16215ß0.0ßHowever, the theme of the paper is not how to split compounds.ßdummyTurker
16216ß0.0ßThere are quite a lot of exhaustive researches on this subject matter out there.ßdummyTurker
16217ß0.0ßThe point we trying to make is that an overly simplistic linguistic preprocessing can enhance the neural model which is evident in this work by comparing the German_v1.ßdummyTurker
16218ß0.0ßWe are doing an extrinsic evaluation using a Seq2Seq MT model.ßdummyTurker
16219ß0.0ßHowever, considering the complexity of the scenario which we defined, still, we think that the improvement is statistically significant.ßdummyTurker
16220ß0.0ßThe output of the models is a distribution over the target sentences in their window given the source sentence.ßdummyTurker
16221ß0.0ßSo the target sentence with the highest probability is checked against the ground truth answers.ßdummyTurker
16222ß0.0ßThe statistics of the system performance includes all types of alignments including 1 to 1 and many to many.ßdummyTurker
16223ß0.0ßWe agree that some literature on splitting compounds could be included.ßdummyTurker
16224ß0.0ßHowever, the theme of the paper is not how to split compounds.ßdummyTurker
16225ß0.0ßThere are quite a lot of exhaustive researches on this subject matter out there.ßdummyTurker
16226ß0.0ßThe point we trying to make is that an overly simplistic linguistic preprocessing (exactly as you mentioned) can enhance the neural model which is evident in this work by comparing the German_v1 and German_v2 models.ßdummyTurker
16227ß0.0ßFor training data, we preferred noisy data over clean data, so yes.ßdummyTurker
16228ß0.0ßBut for testing they are checked manually, otherwise, as you mentioned, it wouldn't make sense.ßdummyTurker
16229ß0.0ßPlease let me answer this question in this way.ßdummyTurker
16230ß0.0ßFailure in cross-alignment detection is a big concern in linguistic/contrastive studies where the quality is the main concern.ßdummyTurker
16231ß0.0ßEven in applied studies like Machine Translation where the slight decrease in data quality caused by the inability in cross-alignment identification is compensated by large quantities of data, new accurate cross-alignment methods may extend the capacity of the algorithms beyond sentence boundary and lead to more cohere and naturally structured translations by capturing inter-sentence relations.ßdummyTurker
16232ß0.0ßEuroparl does contain many-to-many alignments (empty lines in source or target language).ßdummyTurker
16233ß0.0ßAlthough their proportion is not noticeable, as we mentioned, we manually checked them and our system obtained a high accuracy over them.ßdummyTurker
16234ß0.0ßThat's right.ßdummyTurker
16235ß0.0ßWe need to describe the way our approach leads to the improvement.ßdummyTurker
16236ß0.0ßAlthough embedding sentences into low-dimensional vectors helps us to deal with sparsity hence dealing with domain disparity and this idea is checked in many other works, in general, we agree with you that this argument needs a statistical proof.ßdummyTurker
16237ß0.0ßNo.ßdummyTurker
16238ß0.0ßEach of them is a matrix with dimensions (# of samples in each batch, steps, dimensions)ßdummyTurker
16239ß0.0ßWe consider 1-0, 0-1, 1-2, 2-1, 2-3, 3-2, and all other combination except for 1-1 as a many-to-many alignment.ßdummyTurker
16240ß0.0ßSo yes.ßdummyTurker
16241ß0.0ßIn our hyperparameter tunings, we used Cosine, GESD, exponential and AESD and they did differ statistically in their results.ßdummyTurker
16242ß0.0ßThis is a nice idea to include the results in an ablation analysis.ßdummyTurker
16243ß0.0ßWe are aware that currently some potential readers are not familiar with Reinforcement Learning (RL), thus we provided a mapping between RL terminology and NLP/Information Extraction terminology in Table 1 and attempted to bridge the gap.ßdummyTurker
16244ß0.0ßWe will further replace RL terms with expressions more accessible to NLP readers.ßdummyTurker
16245ß0.0ßAlong with writing, we will further elaborate the explanations on figures and polish these figures to improve readability.ßdummyTurker
16246ß0.0ßClarification on line:60:90: In the “campaign” examples, supervised approaches tend to classify both as triggers as Attack type because there are more Attack-“campaign” instances than Demonstrate-“campaign” instances in the corpus.ßdummyTurker
16247ß0.0ßHuman readers’ decisions will not be biased by such distribution and they can correctly determine a Demonstrate-“campaign” regardless of its less frequent occurrence.ßdummyTurker
16248ß0.0ßWe assume that human readers put “weights” on ambiguous cases and we model such a weight as a “reward”.ßdummyTurker
16249ß0.0ßMotivation: Our motivation of adopting IRL/GAIL is to tackle ambiguous cases.ßdummyTurker
16250ß0.0ßAmbiguous cases are impacted by biased label/feature distribution (e.g., in ACE2005 training split: Attack-“campaign”:Demonstrate-“campaign”=2:1), so some supervised models introduce contextual or external features/data and attempt to indirectly influence ambiguous cases with changes in distribution.ßdummyTurker
16251ß0.0ßIn our work, instead of focusing on distribution, we directly assess ambiguous cases with a dynamic mechanism -- in Demonstrate-“campaign” example, we penalize wrong Attack label with strong negative reward and promote correct Demonstrate label with strong positive reward.ßdummyTurker
16252ß0.0ß"GAN: In [Goodfellow2014], a well-optimized discriminator maximizes the margins between estimated probabilities with regard to real and fake images and stimulates the generator to generate images as ""real""."ßdummyTurker
16253ß0.0ßIn our work, the discriminator assesses labels from ground-truth (and correct labels from the extractor) with high rewards and wrong ones with low rewards; and the discriminator is iteratively optimized.ßdummyTurker
16254ß0.0ßFor ambiguous cases, if the extractor repeatedly attempts with wrong labels, the discriminator outputs lower and lower rewards and eventually stimulates the extractor to output correct labels.ßdummyTurker
16255ß0.0ßWe also elaborated the discriminator’s output with the Demonstrate-“campaign” example in Line:674:686ßdummyTurker
16256ß0.0ßWe appreciate the recommendation on other competing approaches and we recognize them as top approaches.ßdummyTurker
16257ß0.0ßSome of them have been cited in our submission.ßdummyTurker
16258ß0.0ßOur goal is to develop an end-to-end event extraction framework.ßdummyTurker
16259ß0.0ßFor fair comparison, we require a competitive approach which labels both triggers and argument roles and is trained upon the same data split.ßdummyTurker
16260ß0.0ß[Yang2017,Feng2016,Nguyen2018] only detected event triggers.ßdummyTurker
16261ß0.0ßTherefore, we choose [Li2013], which is a non-Neural Networks (NN) approach that achieves the best performance, and [Nguyen2016], which is a state-of-the-art NN approach, for the setting of perfect entity annotation.ßdummyTurker
16262ß0.0ßAnd [Li2014] is the best choice for settings with system generated entities.ßdummyTurker
16263ß0.0ßMoreover, as mentioned in line:628:630, we tune the parameters according to F1 scores of argument role labeling instead of trigger labeling to align with [Li] papers and [Nguyen2016].ßdummyTurker
16264ß0.0ßWe acknowledge the trade-off in the scores among different concentrations and that we need to further elaborate and clarify these differences.ßdummyTurker
16265ß0.0ßGiven more space we will provide more comprehensive comparison and analysis on other datasets as suggested.ßdummyTurker
16266ß0.0ßWe will release source code and make resources publicly available.ßdummyTurker
16267ß0.0ßSee reply to weakness argument 2.ßdummyTurker
16268ß0.0ßSee reply to weakness argument 2.ßdummyTurker
16269ß0.0ßSee reply to weakness argument 2.ßdummyTurker
16270ß0.0ßRL resembles seq2seq in terms of considering historical outputs.ßdummyTurker
16271ß0.0ßAt least in our opinion, a core step to apply RL on other tasks is to re-define loss functions into reward functions.ßdummyTurker
16272ß0.0ßAs mentioned in Section 2, end-to-end event extraction includes trigger labeling and argument role labeling.ßdummyTurker
16273ß0.0ßThe former is a sequence task and the latter involves relation classification, so our framework is tackling a more complicated scenario than [Yang2017] on the same dataset.ßdummyTurker
16274ß0.0ßMore details on motivation are in our reply to Review 1’s weakness argument 2.ßdummyTurker
16275ß0.0ßD is a binary classifier, while Q in Equation (16) can represent multiple actions (e.g., 33 event types), we need the output from D to further guide the update of probability distribution in Q.ßdummyTurker
16276ß0.0ßMore details are in our reply to Review 1’s weakness argument 2.ßdummyTurker
16277ß0.0ßPlease see our reply to Review 1’s weakness argument 3.ßdummyTurker
16278ß0.0ßYou are right.ßdummyTurker
16279ß0.0ßThey will be includedßdummyTurker
16280ß0.0ßThe reported results for German is for the task of alignment.ßdummyTurker
16281ß0.0ßThis model is used for parallel data extraction for German and other language pairs whose results are reported in the human evaluation section.ßdummyTurker
16282ß0.0ßWe are planning an extrinsic evaluation using a Seq2Seq NMT system.ßdummyTurker
16283ß0.0ßIn our hyperparameter tunings, we used average pooling and regular concatenation too among which the max pooling produced better results.ßdummyTurker
16284ß0.0ßWe will include an ablation study on different hyperparameters to support the paper.ßdummyTurker
16285ß0.0ßIt was the best architecture we came up with.ßdummyTurker
16286ß0.0ßAdding attention on the source sentences caused overfitting in the early steps of training which prevent the model to advance more.ßdummyTurker
16287ß0.0ßYour argument is valid.ßdummyTurker
16288ß0.0ßWe are working on including an evaluation of the model on the second shared task of BUCC 2017.ßdummyTurker
16289ß0.0ßThe sequences are fixated on 40 tokens.ßdummyTurker
16290ß0.0ßLonger sentences were not affected so much by this decision.ßdummyTurker
16291ß0.0ßHowever almost half of the errors are from too short sentences (less than 10 tokens).ßdummyTurker
16292ß0.0ßWell, that's an interesting architecture.ßdummyTurker
16293ß0.0ßI would definitely like to try it out on my data to see the performance.ßdummyTurker
16294ß0.0ßthe probability is estimated over the logits of the target sentences in the window given each source sentence.ßdummyTurker
16295ß0.0ßEach source sentence comes with a window of target sentences.ßdummyTurker
16296ß0.0ßeach item in the Cartesian product of these two sets has a logit and the probability is a normalized softmax over all possible combinations.ßdummyTurker
16297ß0.0ßThe translation for both Bleualign and our model is done using Google translation service.ßdummyTurker
16298ß0.0ßIt is accuracy.ßdummyTurker
16299ß0.0ßThe MT system here is a third-party service and is not scalable for large or running datasets.ßdummyTurker
16300ß0.0ßAfter training our model, we use the offline mode which is a dictionary-based method and has no reliance on external services.ßdummyTurker
16301ß0.0ßThanks for your meticulous comments and valuable time in making our work thorough.ßdummyTurker
16302ß0.0ßWe appreciate your detail comments and address reviewer-specific clarifications below.ßdummyTurker
16303ß0.0ßW1: Thanks for pointing out Iyer's work in ACL 2016.ßdummyTurker
16304ß0.0ßWe will discuss his work in the Related Works of our paper.ßdummyTurker
16305ß0.0ßOur method is different with Bastings et al.ßdummyTurker
16306ß0.0ß17, and we compare our method with theirs in our response to Q11 (see below).ßdummyTurker
16307ß0.0ßW2: Copying strategy is not that important in our model since we have replaced the values in SQL conditions with abstract symbols like val_0, val_1.ßdummyTurker
16308ß0.0ßThanks for pointing out this issue.ßdummyTurker
16309ß0.0ßWe had to omit many details due to page limit.ßdummyTurker
16310ß0.0ßSQL: SUM Played WHERE first game < val_0 and lost < val_1 and drawn < val_2.ßdummyTurker
16311ß0.0ß"gold: ""what is the sum of matches played that has a year of first match before val_0 fewer than val_1 lost and val_2 drawn"""ßdummyTurker
16312ß0.0ß"prediction: ""how many matches played where first game is before val_0 and lost is less than val_1 and drawn is less than val_2"""ßdummyTurker
16313ß0.0ßSQL: select avg silver where rank < val_0 and gold > val_1ßdummyTurker
16314ß0.0ß"gold: ""what is the average silver with a rank smaller than <val_0> and more than <val_1> gold."""ßdummyTurker
16315ß0.0ß"prediction: ""what is the average silver that has less than val_0 rank and val_1 gold."""ßdummyTurker
16316ß0.0ßWe will provide more examples in the later version of the paper.ßdummyTurker
16317ß0.0ßW3: Thanks.ßdummyTurker
16318ß0.0ßWe will find an native English speaker to help us polish the paper.ßdummyTurker
16319ß0.0ßW4: The shortest length distribution (length/percent) is: 10/73%; (11-15)/15%; (16-20)/6%; 20/6%.ßdummyTurker
16320ß0.0ßThe K is set 15.ßdummyTurker
16321ß0.0ßQ1: Thanks for the advice.ßdummyTurker
16322ß0.0ßWe will refer and cite this dataset earlier in our paper.ßdummyTurker
16323ß0.0ßQ2: We absolutely agree that, with the recent progress of deep learning technology, more and more deep learning based NLG approached are proposed.ßdummyTurker
16324ß0.0ßWe will revise the introduction to make it more clear.ßdummyTurker
16325ß0.0ßQ3: From our experimental results, we conclude that the sequential models may have information loss when imposing an order on the graph structured data.ßdummyTurker
16326ß0.0ßAt least on the SQL-to-text task, our experimental results could demonstrate this finding.ßdummyTurker
16327ß0.0ßIndeed, on a more general graph structured dataset such as the shortest path task, we have similar observations.ßdummyTurker
16328ß0.0ßWe also hope in the future, other researchers could utilize our proposed Graph2Seq model in their tasks to verify this conclusion.ßdummyTurker
16329ß0.0ßWe will release our code and data soon.ßdummyTurker
16330ß0.0ßQ4: As discussed in session 3.1 (line 244-247) we use the LSTM to generate the initial node embedding from the text attribute of a node.ßdummyTurker
16331ß0.0ßQ5: We use the same dimensions for the seq2seq.ßdummyTurker
16332ß0.0ßWe did experiment with higher values, e.g.ßdummyTurker
16333ß0.0ßword dimension from [50, 100, 200, 300], decoder hidden size from [80, 100, 150, 200] The performance can be better, but not significantly.ßdummyTurker
16334ß0.0ßWe can provide the performance with different settings of parameters in later version.ßdummyTurker
16335ß0.0ßWe select the top frequent 20,000 words as our vocabulary and treat other words as unknown words.ßdummyTurker
16336ß0.0ßQ6: Thanks for your idea.ßdummyTurker
16337ß0.0ßWe quickly implement your idea as a new variant of Tree2Seq model, the BLEU score on WikiSQL is 27.59, which is better than our Tree2Seq model.ßdummyTurker
16338ß0.0ßWe may include your Tree2Seq model instead of ours as a new baseline in the later version.ßdummyTurker
16339ß0.0ßIn fact, this experimental result also demonstrates that without using graph2seq model, one has to use well designed Tree2Seq or Seq2SeqßdummyTurker
16340ß0.0ßto achieve comparable performance on the graph structured input.ßdummyTurker
16341ß0.0ßQ7: In some cases, the constraints are like col_0 == val and col_1 == val.ßdummyTurker
16342ß0.0ß"For such cases, by creating the same constraint node (text attribute: ""== val""), the performance could be better."ßdummyTurker
16343ß0.0ßSuch graphs may have cycles.ßdummyTurker
16344ß0.0ßQ8: In the decode phase, we perform the dropout on the output vectors of lstm cell.ßdummyTurker
16345ß0.0ßQ9: In this case, each node has a text attribute that equals to an integer.ßdummyTurker
16346ß0.0ßAnd we randomly initialize the embeddings for these integers.ßdummyTurker
16347ß0.0ßQ10: Not yet.ßdummyTurker
16348ß0.0ßWe will include this approach as a new baseline in the later version.ßdummyTurker
16349ß0.0ßQ11: One main difference between our work and Basting is that, we utilize different node embedding generation method in the graph encoder.ßdummyTurker
16350ß0.0ßIn our approach, we aggregate the information of neighbor nodes within different hops using different aggregators.ßdummyTurker
16351ß0.0ßThis strategy explicitly encodes the graph distance into the node embedding, which helps our encoder to learn a hop-aware node embedding.ßdummyTurker
16352ß0.0ßHowever, Basting et al.ßdummyTurker
16353ß0.0ßneglects such important structure information.ßdummyTurker
16354ß0.0ßQ12: We want to point out that the graph embedding method is not perfect and could be further improved.ßdummyTurker
16355ß0.0ßOn the other hand, taking our experimental results into consideration, we naturally come to conclusion that the graph2seq model is better than seq2seq model in capturing the graph information.ßdummyTurker
16356ß0.0ßQ13: The BLEU scores in the toy experiment are the sum of sentence level BLEU scores.ßdummyTurker
16357ß0.0ßQ14: We tune the beam size parameter on the development set.ßdummyTurker
16358ß0.0ßQ15.ßdummyTurker
16359ß0.0ßThanks for pointing out this typo.ßdummyTurker
16360ß0.0ßWe will fix this in later version.ßdummyTurker
16361ß0.0ßThere has a lot of study of word association in the human brain.ßdummyTurker
16362ß0.0ßThis is part of the larger question of looking into how concepts are stored in our brain.ßdummyTurker
16363ß0.0ßIn particular, studies have shown that words in our brains have some sort of connections between conceptually similar words.ßdummyTurker
16364ß0.0ßIn addition, Deyne et al., 2016 (Predicting human similarity judgments with distributional models: The value of word associations [best paper of COLING]) have shown that using smaller amount of word association data can achieve state-of-the arts word representation.ßdummyTurker
16365ß0.0ßThanks for the suggestion and we will make it clearer in the next version.ßdummyTurker
16366ß0.0ßThere are four main differences: 1) they use manually constructed word attributes as textual and visual representation which is their main contribution.ßdummyTurker
16367ß0.0ßIn contrast, we use more effective textual, visual and auditory representations learned by the state-of-the-art deep neural network models.ßdummyTurker
16368ß0.0ß2) We use word association datasets as weak supervision to learn better multimodal word representation.ßdummyTurker
16369ß0.0ß3) We use gating mechanism to learn the importance weights of different input modality, which is possible because of the introduction of word association datasets.ßdummyTurker
16370ß0.0ß4) we have done experiments with more modality inputs.ßdummyTurker
16371ß0.0ßWe have made qualitative analysis on the most similar words by different models, and the gate values on different modalities.ßdummyTurker
16372ß0.0ßWe will add qualitative analysis and some examples if we have another page.ßdummyTurker
16373ß0.0ßThanks for the suggestion.ßdummyTurker
16374ß0.0ßWe will provide some qualitative analysis and check on more tasks in the next version.ßdummyTurker
16375ß0.0ßFor the AMA-MGval/vec question, the gating mechanism cannot work without word association dataset, because in unsupervised training condition, the objective of the word representation model is to reconstruct the input vectors, they cannot learn the importance weights of different input modality vectors.ßdummyTurker
16376ß0.0ßThank you for your feedback.ßdummyTurker
16377ß0.0ßWe think we have shown very clearly how our method generalizes between different datasets since we tested on many different corpora, but we see your point about packing many pieces together.ßdummyTurker
16378ß0.0ßWe will add some more discussion to help clarify what the key takeaways from our experiments are in the final version.ßdummyTurker
16379ß0.0ßThe model we used is a fair bit better then BiDaF on SQuAD, we think the design is a general improvement on BiDaF.ßdummyTurker
16380ß0.0ßWe do mention the SQuAD results in section 5.3 line 642: 72.139 F1 / 81.048 EM on the SQuAD test set, about four points better than the non-ensemble BiDaF.ßdummyTurker
16381ß0.0ßHowever we will look at moving that result into a table, or mentioning it in the discussion, so it won't be so easy to miss.ßdummyTurker
16382ß0.0ßWe think that the figures show the main point pretty clearly, as stated in the paper, “In general, we found the shared-norm approach to be the most effective way to resolve this problem,..”.ßdummyTurker
16383ß0.0ßIn the camera ready version we will add more detail about where is was more or less effective.ßdummyTurker
16384ß0.0ßThe merge method is competitive on TriviaQA Web/Wiki, but not on SQuAD and TriviaQA Unfiltered.ßdummyTurker
16385ß0.0ßThe merge approach trains the model on pairs of sampled paragraphs concatenated together.ßdummyTurker
16386ß0.0ßThis has the advantage of exposing the model to more irrelevant text during training.ßdummyTurker
16387ß0.0ßHowever, it still does not train the model to produce well calibrated confidence scores if it is run on many groups of paragraphs drawn from the same document, we think this is why it's performance sometimes degraded when more paragraphs were used.ßdummyTurker
16388ß0.0ßWe can clarify this more carefully in the discussion as well.ßdummyTurker
16389ß0.0ßWe will mention this in the camera ready version, we found that if there is only a single document using the linear model for paragraph selection was no better then using TF-IDF alone, so we used TF-IDF in those cases.ßdummyTurker
16390ß0.0ßWe are referring specifically to Table 3, in the “web” column.ßdummyTurker
16391ß0.0ßThanks for your feedback, we will update that in the camera ready versionßdummyTurker
16392ß0.0ßWe picked SQuAD because it is well known and was used by Chen et al.ßdummyTurker
16393ß0.0ßin “Reading Wikipedia to Answer Open-Domain Questions“, and we picked TriviaQA because it a large and has document-level input.ßdummyTurker
16394ß0.0ßQuasar-T would also have also been a good option, but was not available when we started this project.ßdummyTurker
16395ß0.0ßWe will describe this in the camera ready version.ßdummyTurker
16396ß0.0ßThanks for catching those, we will fix them in the camera ready versionßdummyTurker
16397ß0.0ßWe agree, and are happy to release the labelled questions as well.ßdummyTurker
16398ß0.0ß===== General Response =====ßdummyTurker
16399ß0.0ßWe express gratitude to the reviewers for their comments, which help us better understand how our work in being interpreted.ßdummyTurker
16400ß0.0ßWe wish to clarify the main contributions of this work, which can be more adequately communicated.ßdummyTurker
16401ß0.0ßIn this paper, we presented three concepts, as stated in the title, 1) densely-contextualized, 2) discriminative primitives, 3) compositions (n-grams).ßdummyTurker
16402ß0.0ßAs the reviewers pointed out in general, we now understand that our presentation on these concepts misled and blurred its original focus and contribution.ßdummyTurker
16403ß0.0ßThe main focus is to build a neural model that (1) equips an interpretability and (2) shows competitive performance against previous works.ßdummyTurker
16404ß0.0ßFor the 1st goal, we designed the final output of the model to match with that of its local interaction between primitives.ßdummyTurker
16405ß0.0ß"As presented in section 4.3, our model is capable to explain its decision process in addition to words' alignment, e.g., ""inspects"" in premise and ""sleeping"" in hypothesis are the evidences why our model determines the sample as contradiction (shown in the 1st example in Table 3)."ßdummyTurker
16406ß0.0ßWe believe this to be one contribution against previous models.ßdummyTurker
16407ß0.0ß"""discriminative primitives"" in the title is due to this aspect."ßdummyTurker
16408ß0.0ß"To address the 2nd goal, we applied the idea of DenseNet (Huang et al., 2017) and designed a multilayered architecture to make each word to be modeled with its neighbors (mother sentence) through multiple layers (this leads to ""densely-contextualized"" in the title)."ßdummyTurker
16409ß0.0ßAs the result, our model shows competitive performance accomplishing the 2nd goal.ßdummyTurker
16410ß0.0ßWe believe the blurring point is in dealing with n-grams, originally just an additional content.ßdummyTurker
16411ß0.0ßOur multilayered architecture is already capable to model interaction among primitives, and n-grams are intended only for broadening its explanation.ßdummyTurker
16412ß0.0ß"As shown in Table 4, we thought that it would provide an additional explanation on semantic relationships among words, as ""cauliflower ears"" and ""deformed ears."""ßdummyTurker
16413ß0.0ßAs all reviewers identified that there is no contribution of n-grams in this paper, however, we will address this issue in our revised paper.ßdummyTurker
16414ß0.0ßAgain, we like to stress on that our contribution is of two novel structural designs as mentioned earlier: local interaction, and multilayered architecture.ßdummyTurker
16415ß0.0ßWe hope these clarifications resolve the previous miscommunications.ßdummyTurker
16416ß0.0ßOur revised paper will deal with the issue of n-grams just as an additional discussion item, and minify any claim on it.ßdummyTurker
16417ß0.0ß==============================ßdummyTurker
16418ß0.0ßWe will exclude n-gram interactions from the focus of this paper, and make it clear to state that n-grams don't help.ßdummyTurker
16419ß0.0ßThe title and the abstract of the paper will be revised.ßdummyTurker
16420ß0.0ßWe will explicitly state the type of primitives in advance.ßdummyTurker
16421ß0.0ßWe will revise the formula to discern the input and the output of biLSTMs.ßdummyTurker
16422ß0.0ßWe will redraw Figure 1 to be readable as commented.ßdummyTurker
16423ß0.0ßWe express gratitude to your comments.ßdummyTurker
16424ß0.0ßPlease check our general response in advance (in response to review #1)ßdummyTurker
16425ß0.0ßThe intention of incorporating n-grams is to expand the presentation of semantic relationships among words, not in performance improvement.ßdummyTurker
16426ß0.0ßAs the reviewer pointed out, however, inconclusive parts make the focus of the paper unclear and we agree to take away the parts from our paper.ßdummyTurker
16427ß0.0ß"""primitive"" in the title originally includes uni/bi/tri-grams, but it will be revised to mean unigrams (individual words) only, since there is not much benefits to include bi/tri-grams."ßdummyTurker
16428ß0.0ß"(1) For performance, our multilayered architecture (called ""dense contextualization"") contributes to improve accuracy as presented in Figure 2."ßdummyTurker
16429ß0.0ßIn addition, we will explicitly describe the specific differences of our approach against others in section 3.2-4.ßdummyTurker
16430ß0.0ßAdditional ablations on other parts of our model (e.g., features or local interaction operators) will be included in the revised paper.ßdummyTurker
16431ß0.0ß(2) For interpretability, the local interaction directly contributes to the final output.ßdummyTurker
16432ß0.0ßAs the reviewer commented, since we do not provide any quantitative evidence on that extra n-grams improve interpretability, we will exclude any claims on n-grams from our revised paper.ßdummyTurker
16433ß0.0ß1) We will remove high-level claims about language in general.ßdummyTurker
16434ß0.0ß"2) We will replace ""alphabet"" with ""character."""ßdummyTurker
16435ß0.0ß3) We will describe the details to improve the presentation of our model, as mentioned by the reviewer.ßdummyTurker
16436ß0.0ßWe observed a consistent improvement over SCL for 12 pairs of source and target (system-6 is better than system-5 for all pairs, Table-4, page-8).ßdummyTurker
16437ß0.0ßHowever, the improvement is not statistically significant as per t-test.ßdummyTurker
16438ß0.0ßThe objective of our work is to build a high accuracy classifier for the unlabeled target domain using a labeled source domain.ßdummyTurker
16439ß0.0ßTo initiate the training of classifier, we need a mechanism to label target domain instances.ßdummyTurker
16440ß0.0ßClassifier C_s (Algorithm 1, page-5), which is trained in the labeled source domain using SCP words as features is used to get pseudo labeled data in the target domain.ßdummyTurker
16441ß0.0ßHamilton et al used domain specific word embeddings to build a domain-specific list of polar words using a small set of seed words.ßdummyTurker
16442ß0.0ßThey do not explicitly measure significance of words in the domain.ßdummyTurker
16443ß0.0ßWe have also used domain-specific word embeddings to decide polarity of words in the unlabeled target domain to decide SCP set.ßdummyTurker
16444ß0.0ßHamilton et al., are focused to identify polarity of words in the domain, our focus is to identify transferable features from the source.ßdummyTurker
16445ß0.0ßHence, we have shown F-score for transferable features in Figure-1.ßdummyTurker
16446ß0.0ßTraining of the system is done at the document level, hence we have done evaluation at the document level.ßdummyTurker
16447ß0.0ßThis multi-domain document annotated dataset is popularly used for cross-domain (Bhatt et al.ßdummyTurker
16448ß0.0ß2015, Ji et al., 2011, Blitzer et al.ßdummyTurker
16449ß0.0ß2007 etc.ßdummyTurker
16450ß0.0ß), hence we considered it a more reliable dataset.ßdummyTurker
16451ß0.0ßPivot selection process is automated to the most extent, it requires manual observation of polarity orientation of a very few words (pls refer footnote-8, page 4).ßdummyTurker
16452ß0.0ßPreparation of a seed set for this task would require more manual efforts.ßdummyTurker
16453ß0.0ß- We will positively do the formatting and writing corrections pointed out by the reviewer.ßdummyTurker
16454ß0.0ßWe will certainly add the missing information as pointed out, one more page in the final version would help us to do so.ßdummyTurker
16455ß0.0ßThe frequent occurrence of the word in the corpus allows it to be  used in the context of other words frequently.ßdummyTurker
16456ß0.0ßSince we are using context vector of the input word and the pivot word to decide polarity orientation of the input word, we used top words as per their frequency of use in the corpus.ßdummyTurker
16457ß0.0ßWe need to set the first positive word and the first negative word as pivots starting from the top in the ordered list.ßdummyTurker
16458ß0.0ßWhether we consider top-10 or top-20, it is not going to affect the final results for the target domain sentiment classification.ßdummyTurker
16459ß0.0ßThis is a valid point, and we will include results using tuned alpha in the final version of the paper.ßdummyTurker
16460ß0.0ßYes, while this can be veiwed as suitable assumption that works well in practice, we agree that a numerical justification would be desirable.ßdummyTurker
16461ß0.0ßWhile a thorough study of the posterior and suitable parameterizations are probably outside of the scope of this article, we will introduce a justification in the final version.ßdummyTurker
16462ß0.0ßVery true, thanks for pointing this out.ßdummyTurker
16463ß0.0ß"There are good arguments and intuition for assuming that working with upper-bound variances and prior counts of zero (""being as conservative as possible"") is suitable, and we will make sure to expand on this in the final version."ßdummyTurker
16464ß0.0ßVery interesting question indeed!ßdummyTurker
16465ß0.0ßThe PMI objective of the mentioned models is, as far as we understand, still only a conjecture.ßdummyTurker
16466ß0.0ßWe suspect the CPMI measure is closer to the actual optimization objective of SGNS/CBOW/GloVe than standard PMI, but we cannot prove this in the current paper.ßdummyTurker
16467ß0.0ßWhile it is not easy for us to see exactly how to transform learned embeddings according to the conservative measure, it could indeed be possible to use e.g.ßdummyTurker
16468ß0.0ßdistributions over NN weights and perhaps dropout during inference to estimate uncertainty in embeddings - this is very intersting and should be tested.ßdummyTurker
16469ß0.0ßThanks for noticing, very helpful!ßdummyTurker
16470ß0.0ßWe will correct these mistakes in the final version.ßdummyTurker
16471ß0.0ßCorrect, we will note that in the text.ßdummyTurker
16472ß0.0ßCorrect.ßdummyTurker
16473ß0.0ßIf we understand your comment correctly, yes, there are similarities to statistical testing and the argument for pushing towards 0 reflects an assumption that lower absolute values are more conservative and better for this and similar applications.ßdummyTurker
16474ß0.0ßA good argument could be made for setting all estimates with an expectation less than zero to zero, reflecting the use of positive pointwise mutual information.ßdummyTurker
16475ß0.0ßThanks for the suggestion.ßdummyTurker
16476ß0.0ßThis is a fair point, and we will include such a comparison in the final version.ßdummyTurker
16477ß0.0ßWhile interesting, this (and other related) articles concern themselves with the Mutual Information not Pointwise Mutual Information for which derivations and results are different, and do not construct, motivate, or evaluate a conservative measure.ßdummyTurker
16478ß0.0ßWhile we feel they are not directly relevant for the work at hand, we are of course happy to reference and point to this line of work if the reviewers feel it is very important.ßdummyTurker
16479ß0.0ßComparing absolute evaluation scores with previously published experiments can be useful in cases when an incremental improvement of a previoulsy published method is proposed.ßdummyTurker
16480ß0.0ßHowever, such a comparison makes less sense in our case, since we present a previously unpublished method.ßdummyTurker
16481ß0.0ßThe relevant comparison in our case is with other state of the art methods, using the same data for all methods.ßdummyTurker
16482ß0.0ßThis is what we do in our experiments, using a standard benchmark corpus and a collection of standard bnchmark tests.ßdummyTurker
16483ß0.0ßThere are potentially very many previoulsy published experiments we could replicate, but it would not be possible to use all previoulsy published data sets and all previously published evaluation benchmakrs.ßdummyTurker
16484ß0.0ßWe agree that the Levy et al paper is nice, but it is only one of several papers that compare embedding methods, and it is not clear to use why the experimental setup in this paper should be more relevant to replicate than others.ßdummyTurker
16485ß0.0ßHaving said this, we can definitely include results using the same data as in Levy et al in the final version of the paper.ßdummyTurker
16486ß0.0ßThis is a misunderstanding, and we will make sure to modify the article if it gives that impression.ßdummyTurker
16487ß0.0ßWhat we claim is that when comparing the results using the CPMI model with the other models that use factorization, one needs to be aware that the CPMI model only relies on the raw correlations, and as such, it is a somewhat unfair comparison.ßdummyTurker
16488ß0.0ßCPMI is of course perfectly suitable to use with factorization (in fact more so than standard PMI), and we will include such results in the final version of the paper.ßdummyTurker
16489ß0.0ßPlease see reply to weakness argument 1!ßdummyTurker
16490ß0.0ßThanks for the suggestion.ßdummyTurker
16491ß0.0ßWe will include such results in the final version of the paper.ßdummyTurker
16492ß0.0ßGood question.ßdummyTurker
16493ß0.0ßWe will include such results in the final version, demonstrating that CPMI is suitable to use with factorization.ßdummyTurker
16494ß0.0ßWe thank the reviewer for the time and effort taken to review our paper.ßdummyTurker
16495ß0.0ßWe will clarify the points detailed below in the camera-ready version.ßdummyTurker
16496ß0.0ßLooking into what a specific neural architecture learned is an interesting idea, on which we have started working (for the Residual Encoders model), but we think it's out of scope for this paper.ßdummyTurker
16497ß0.0ßThe main point of this paper is that end-to-end neural models that don’t employ external resources are limited in their ability to learn lexical inferences.ßdummyTurker
16498ß0.0ßWe demonstrated that on three models which are representative of the various approaches for neural NLI modelling.ßdummyTurker
16499ß0.0ß"We will replace the examples in Table 1 with less controversial examples, however, we note that they are in agreement with SNLI, as the SNLI paper states: ""Issues of coreference are not completely solved, but greatly mitigated."ßdummyTurker
16500ß0.0ß"For example, with the premise sentence ‘A dog is lying in the grass’ , a worker could safely assume that the dog is the most prominent thing in the photo, and very likely the only dog, and build contradicting sentences assuming reference to the same dog."""ßdummyTurker
16501ß0.0ßSimilarly, if someone is holding both a guitar and a saxophone, it is expected to appear in the premise already.ßdummyTurker
16502ß0.0ßIn fact, the SNLI training set contains such contradicting sentence pairs, e.g.ßdummyTurker
16503ß0.0ßP: “A man with a tuba and music stand.”, H: “A man is holding a clarinet.”ßdummyTurker
16504ß0.0ßThe human agreement on the annotations also shows that given the event coreference assumption, most examples are not ambiguous.ßdummyTurker
16505ß0.0ßThis contribution of our paper is in providing empirical evidence to the concerns raised by many researchers about the difficulty of neural NLI models to generalize and the simplicity of SNLI.ßdummyTurker
16506ß0.0ßWe are working on ways to incorporate external knowledge into neural NLI models and hope to present a solution in the future.ßdummyTurker
16507ß0.0ßReleasing the test set would allow other researchers that work on the problem to evaluate their models on the ability to learn lexical inferences.ßdummyTurker
16508ß0.0ßWe thank the reviewer for the time and effort taken to review our paper.ßdummyTurker
16509ß0.0ßWe will clarify the points detailed below in the camera-ready version.ßdummyTurker
16510ß0.0ßThanks for pointing out the contribution of a new dataset for lexical inference in context!ßdummyTurker
16511ß0.0ßIndeed, there are only a few datasets for that task (detailed in http://aclweb.org/anthology/S/S16/S16-2013.pdf).ßdummyTurker
16512ß0.0ßIt would be interesting in the future to evaluate models for lexical inference in context on this dataset as well.ßdummyTurker
16513ß0.0ßWe agree that the dataset is skewed towards contradiction, which means it can’t be used for training, and that models can exploit this to achieve high performance.ßdummyTurker
16514ß0.0ßWe'll make it clear that this dataset can only be used as an *additional* test set for models trained on SNLI/MultiNLI/SciTail, to test models' ability to learn lexical inferences.ßdummyTurker
16515ß0.0ßWe used the default hyper-parameter settings.ßdummyTurker
16516ß0.0ßWe will add more detailed model descriptions, including links to the code and hyper-parameter specification.ßdummyTurker
16517ß0.0ßThanks for the pointers to the relevant papers by Gururangan et al.ßdummyTurker
16518ß0.0ß(2018) and McCarthy and Navigli (2009), we will refer to them.ßdummyTurker
16519ß0.0ß- We assumed that training on a completely different dataset (i.e.ßdummyTurker
16520ß0.0ßonly on MultiNLI) would perform worse, as it is somewhat different in nature from SNLI.ßdummyTurker
16521ß0.0ßWe will think of a better way to demonstrate whether MultiNLI contributes due to better training data or simply due to additional training data.ßdummyTurker
16522ß0.0ß- That’s an interesting question.ßdummyTurker
16523ß0.0ßWe will provide a simple baseline that predicts the label based on the WordNet relation between the original and replaced words.ßdummyTurker
16524ß0.0ßWe acknowledge that the individual components of our approach are not novel.ßdummyTurker
16525ß0.0ßIn assessing the contribution of our work, we hope the reviewer would consider the research questions we address (lines:074-079), whether these questions were addressed in prior work, and whether this work adds to the collective knowledge of the research community regarding what is known about co-training till date.ßdummyTurker
16526ß0.0ßWe acknowledge that 0.05 is the most commonly used level (though not only); hence we specifically mentioned the p-value, when 0.1, for transparency to the reader.ßdummyTurker
16527ß0.0ßConfusion was not intended, but we can remove this significance statement to avoid it.ßdummyTurker
16528ß0.0ßRegarding assessing the benefit of the FFT-based method, please note that macro average F1 of anger, sadness, and fear achieves statistically significant improvements over classic at p<0.05; it is p<0.1 mainly due to subpar performance with the Joy emotion (Figure-1).ßdummyTurker
16529ß0.0ßFor Joy, confidence threshold did not generalize well from observation on development data to test data (Figure-3 shows a threshold of 0.9 would have yielded better results).ßdummyTurker
16530ß0.0ßWhile macro average (including joy) F1 scores comparing classic and FFT-based co-training in Table-2 has p<0.1, the recall gain and precision drop in Table-2 between these two methods are both significant at p<0.05.ßdummyTurker
16531ß0.0ßRegarding other models using unlabeled data, we experimented with initializing fastText by embeddings learned from the same unlabeled data (did not improve).ßdummyTurker
16532ß0.0ßWe can add these results to Table-2.ßdummyTurker
16533ß0.0ßFor logistic regression, we kept the default values of the library we used (line:254).ßdummyTurker
16534ß0.0ßFor fastText, we used grid search with epochs ϵ{5,10,25,50,100,200} and learning rate ϵ{0.0005,0.001,0.05,0.1,0.25,0.5}.ßdummyTurker
16535ß0.0ßfastText with default values yielded poor results, so we needed to tune.ßdummyTurker
16536ß0.0ßReply-to-question-2: Figures-1-3 show performance on test data.ßdummyTurker
16537ß0.0ßReply-to-additional-comments: thank you for pointing out the naming issue.ßdummyTurker
16538ß0.0ßWe acknowledge that co-training or the general idea of the features are not new.ßdummyTurker
16539ß0.0ßHowever, as the reviewer mentioned in strength-argument-2, this has not been done for fine-grained emotion analysis before.ßdummyTurker
16540ß0.0ßWe hope the reviewer would also consider the novelty of the idea of exploring diversity in training data selection, never before considered for co-training in prior work, when assessing the overall novelty/originality of this work in a short paper scope.ßdummyTurker
16541ß0.0ßThank you for the references to automatic feature decomposition.ßdummyTurker
16542ß0.0ßAlthough we did not explore this route, we tried random feature separation in early experiments which did not improve performance with co-training.ßdummyTurker
16543ß0.0ßWe can add these results to the paper as additional baselines.ßdummyTurker
16544ß0.0ßReply-to-weakness-argument-3/question-1: We conducted paired bootstrap significance test following the pseudocode given inßdummyTurker
16545ß0.0ßBerg-Kirkpatrick et al.ßdummyTurker
16546ß0.0ß"(2012): ""An Empirical Investigation of Statistical Significance in NLP”, In Proceedings of the 2012 EMNLP."ßdummyTurker
16547ß0.0ßWe created 10^6 bootstrap sample datasets (sample with replacement) and compared the result differences of any two given methods for each sample dataset.ßdummyTurker
16548ß0.0ßP-value is estimated by dividing the number of times a result difference is twice more than an originally found difference, with 10^6.ßdummyTurker
16549ß0.0ßAt the end of co-training, the individual classifiers are somehow combined (e.g.ßdummyTurker
16550ß0.0ßjoint probability used by Blum and Mitchel (1998)).ßdummyTurker
16551ß0.0ßWe combined them at the feature space instead of at the prediction space.ßdummyTurker
16552ß0.0ßWe experimented with voting/intersection (resulted in high precision, low recall, overall low F1-score) and prediction union (low precision, high recall, overall low F1-score), but found that a re-trained model (with added data from co-training) on the combined feature views (1) performed better and (2) could keep the model comparable to the initial supervised model (which is superior to the individual classifiers; see Table-2).ßdummyTurker
16553ß0.0ßFor string kernels, we used an efficient implementation based on hash tables, which is available online at http://string-kernels.herokuapp.com.ßdummyTurker
16554ß0.0ßPopescu et al.ßdummyTurker
16555ß0.0ß(2017) show that their implementation is 4x faster than suffix trie.ßdummyTurker
16556ß0.0ßThe most computational intensive matrix to compute is for the Arabic dialect experiments, since there about 17,000 samples in the whole data set.ßdummyTurker
16557ß0.0ßThe corresponding kernel matrix takes about 14-15 mins to compute using a single core on laptop computer with Intel i7 2.5 GHz processor and 8 GM of RAM.ßdummyTurker
16558ß0.0ßTraining KRR takes less than 5 mins.ßdummyTurker
16559ß0.0ßWe believe that it is hard to call this computational time expensive.ßdummyTurker
16560ß0.0ßHowever, it is true that the time grows quadratically (^2) with the number of samples.ßdummyTurker
16561ß0.0ßThis is a very good point, and we intend to include such examples in the final version of the paper, since we will have an extra page.ßdummyTurker
16562ß0.0ßFor the moment, we can provide some statistics about the test samples selected for inclusion in training (valid for all the experiments): the accuracy on these samples is always greater than 93%, usually around 95-96%.ßdummyTurker
16563ß0.0ßThis means that we only add a maximum of 7% samples with noisy (wrong) labels.ßdummyTurker
16564ß0.0ßThe performance on the entire test sets is usually much lower, so the second round of training is able to close this performance gap.ßdummyTurker
16565ß0.0ßWe understand this point of view.ßdummyTurker
16566ß0.0ßWe would like to mention that the string kernel approach of (Ionescu and Butnaru, 2017) achieved the best performance in the Arabic dialect identification, surpassing other models including deep neural networks by a large margin (the second-best method is more than 4% behind, as shown in Table 3).ßdummyTurker
16567ß0.0ßThe baseline considered for the sentiment classification experiments (Gimenez-Perez et al., 2017) was recently published at EACL 2017.ßdummyTurker
16568ß0.0ßConsequently, we regard it as strong baseline published in a top-tier conference.ßdummyTurker
16569ß0.0ßAs long as the new test is from the same target domain as the initial test set, we can foresee three possible solutions: 1) use our transductive model as is (without retraining) on the new test set (as it is adapted to the target domain, it should work better than a classifier trained solely on the training data) or 2) retrain the transductive classifier or 3) extend the kernel matrix and compute the missing similarities between (train + initial test set) and new test set and reapply the entire framework.ßdummyTurker
16570ß0.0ßWe expect the latter approach to obtain the best performance.ßdummyTurker
16571ß0.0ßFor a fair comparison with the shared task participants, the results on the test set are reported by including the labeled samples from the development set in the training.ßdummyTurker
16572ß0.0ßThe development and the test set contain samples collected from the same domain, while the training samples are from a different domain.ßdummyTurker
16573ß0.0ßEven so, our approach can still improve the performance on the test set.ßdummyTurker
16574ß0.0ßPlease see comments to Weakness1 of Reviewer2 for details about time.ßdummyTurker
16575ß0.0ßWe did not try to tune sigma to avoid overfitting.ßdummyTurker
16576ß0.0ßPlease read comment to Weakness4 of Reviewer1 for why we use RBF.ßdummyTurker
16577ß0.0ßWe choose KRR, since in previous works (Ionescu et al., 2016; Ionescu and Butnaru, 2017) it was found to work better than l1-SVM in combination with string kernels.ßdummyTurker
16578ß0.0ßNevertheless, we tested the l1-SVM in the multi-source sentiment classification setting.ßdummyTurker
16579ß0.0ßWe obtained the following results using K_0/1:ßdummyTurker
16580ß0.0ßDEK→B: 81.6%;ßdummyTurker
16581ß0.0ßBEK→D: 81.9%;ßdummyTurker
16582ß0.0ßBDK→E: 83.5%;ßdummyTurker
16583ß0.0ßBDE→K: 85.2%.ßdummyTurker
16584ß0.0ßThe results are actually comparable to those of KRR presented in Table 1.ßdummyTurker
16585ß0.0ßTherefore, we conclude that at least in this setting, there is no difference between l1 and l2 regularization.ßdummyTurker
16586ß0.0ßThis was due to lack of space.ßdummyTurker
16587ß0.0ßThe definition is found in (Ionescu et al., 2016).ßdummyTurker
16588ß0.0ß1) We employ gradient boosted trees so that the models are invariant to scaling of inputs and can learn higher order interactions between features.ßdummyTurker
16589ß0.0ß2) The dropout noise is sampled from a Bernoulli distribution which is discrete.ßdummyTurker
16590ß0.0ßWe also use Gaussian noise to obtain contiguous perturbations.ßdummyTurker
16591ß0.0ßWe will add an example in Section 6 to further explain the overlap@K metric.ßdummyTurker
16592ß0.0ßWe will use pseudocode to show how to compute Eq.ßdummyTurker
16593ß0.0ß(6,7) in Section 4.1.ßdummyTurker
16594ß0.0ßWe use a gradient tree boosting model to compute s(q,a).ßdummyTurker
16595ß0.0ß"Please see Section 2 in ""[KDD16] XGBoost"ßdummyTurker
16596ß0.0ßFor confidence estimation, noise is injected at the sequence level, so we can compute the proposed metrics in batch mode on GPU.ßdummyTurker
16597ß0.0ßMoreover, we can optimize the metric computation process by avoiding duplicate forward passes.ßdummyTurker
16598ß0.0ßFor example, if noise injection is in the decoder, we can reuse the encoding results without running the encoder.ßdummyTurker
16599ß0.0ßAs described in Footnote 1 (page 8), the proposed evaluation of uncertainty interpretation injects noise to *one token at a time*, so the running time of creating a proxy gold standard increases linearly with the length of input sentence.ßdummyTurker
16600ß0.0ßIn contrast, the confidence classifier injects noise at the sequence level, rather than iterating over tokens one by one.ßdummyTurker
16601ß0.0ßThank you for the suggestion!ßdummyTurker
16602ß0.0ßWe will add this analysis.ßdummyTurker
16603ß0.0ßThe posterior is one of the metrics of model uncertainty.ßdummyTurker
16604ß0.0ßTable 2-4 and Figure 3 show that using the metrics of data uncertainty and input uncertainty also improves performance.ßdummyTurker
16605ß0.0ßIn practice, we can do feature selection and trade-off performance against speed.ßdummyTurker
16606ß0.0ßThe proposed framework can be applied to a variety of tasks employing sequence-to-sequence architectures.ßdummyTurker
16607ß0.0ßHowever, we leave this to future work.ßdummyTurker
16608ß0.0ßWe will explain how the gold standard for uncertainty interpretation is created with a specific example.ßdummyTurker
16609ß0.0ßModel uncertainty indicates that the model is less confident about the values of $p(a|q)$, e.g., large variance of $p(a|q, M')$ when using different model configurations $M'$ learnt from the training set.ßdummyTurker
16610ß0.0ßSo, not every  model whith high uncertainty results in high decoding entropy.ßdummyTurker
16611ß0.0ßIn contrast, input uncertainty indicates that the uncertainty is caused by the input itself, even if the current model can estimate $p(a|q)$ reliably.ßdummyTurker
16612ß0.0ßIf the input is more ambiguous, the decoding entropy (computed using reliable $p(a|q)$) always tends to increase.ßdummyTurker
16613ß0.0ßHyperparameters of the confidence estimation model were cross-validated on the held-out set.ßdummyTurker
16614ß0.0ßThis is a good point.ßdummyTurker
16615ß0.0ßWe focused on textual similarity tasks because they were the main focus of Wieting et al.ßdummyTurker
16616ß0.0ßand Arora et al., whose approaches were the ideological predecessors to our work.ßdummyTurker
16617ß0.0ßSince submitting the paper, we have tried our sentence vectors on three other tasksßdummyTurker
16618ß0.0ßThank you for the suggestion.ßdummyTurker
16619ß0.0ßWe will restructure our discussion of the baseline accordingly.ßdummyTurker
16620ß0.0ßIt is not possible to quantify the total extent to which the confounding effect of vector length in Arora et al.’s model harms the overall result.ßdummyTurker
16621ß0.0ßTo calculate the exact extent of harm, we would have to know the true posterior distribution p(s|c_s), which is unknown, and then compare it to the posterior distribution proposed by Arora et al.’s model.ßdummyTurker
16622ß0.0ßIn section 3.3, we have done the next best thing, which is to show that distorting the length of word vectors can in theory have an unbounded effect on the probability of that sentence being generated.ßdummyTurker
16623ß0.0ßIn reality, the extent of the distortion will depend on factors like how the word vectors are generated.ßdummyTurker
16624ß0.0ßPlease see our response to reviewer #1 regarding hyperparameters (argument #6).ßdummyTurker
16625ß0.0ßPlease see our response to reviewer #1 regarding calibration.ßdummyTurker
16626ß0.0ß(argument #4).ßdummyTurker
16627ß0.0ßThank you for the suggestion regarding the exposition.ßdummyTurker
16628ß0.0ßWe agree that some restructuring could make it clearer.ßdummyTurker
16629ß0.0ß"Thank you for the suggestion regarding ""interpolation""."ßdummyTurker
16630ß0.0ßWe will make note of this in the paper.ßdummyTurker
16631ß0.0ß">  ""What's not clear from the analysis is whether this is a problem with the model itself, or with the approximations built into the estimation procedure described in the preceding section."""ßdummyTurker
16632ß0.0ßThis problem is with the Arora et al.ßdummyTurker
16633ß0.0ßmodel itself.ßdummyTurker
16634ß0.0ßAs we note at the beginning and end of Section 3.3, it is the assumption of a log-linear word production model that causes the probability of a word being produced by a discourse vector to be distorted by word vector length.ßdummyTurker
16635ß0.0ßOur model uses a similar method as Arora et al.ßdummyTurker
16636ß0.0ßto approximate the maximum likelihood estimate, but we do not have the same problem because we do not make the assumption of a log-linear word production model.ßdummyTurker
16637ß0.0ßWe will make this point clearer in section 3.3 as well as the rest of the paper.ßdummyTurker
16638ß0.0ß"> ""I also think the use of the term ""calibrated"" is inappropriate here."ßdummyTurker
16639ß0.0ß...ßdummyTurker
16640ß0.0ß"These words have established meanings, please respect them."""ßdummyTurker
16641ß0.0ßWe are familiar with the definition of calibration and believe we have used it appropriately here.ßdummyTurker
16642ß0.0ßCall the true posterior distribution over sentences conditioned on the discourse vector p(s|c_s).ßdummyTurker
16643ß0.0ßCall the proposal posterior distribution defined by Arora et al.’s model q(s|c_s).ßdummyTurker
16644ß0.0ßArora et al.’s model would be well calibrated IFF q(s|c_s) is approximately the same as p(s|c_s) for any given discourse vector.ßdummyTurker
16645ß0.0ßWhat we show is that for any given discourse vector, and any given sentence, q(s|c_s) can be much greater than p(s|c_s) if the word vectors used to calculate q(s|c_s) have a sufficiently large length.ßdummyTurker
16646ß0.0ßSimilarly, q(s|c_s) can be much less than p(s|c_s) if the word vectors have a sufficiently small length.ßdummyTurker
16647ß0.0ßArora et al.’s model is not well calibrated because the extent to which the proposed posterior distribution q approximates p depends greatly on the length of the word vectors used.ßdummyTurker
16648ß0.0ßIn section 3.3, we chose to focus on the worst-case scenarios that can arise as a result of this, since we thought that would be more intuitive to a reader unfamiliar with the pitfalls of poorly calibrated models.ßdummyTurker
16649ß0.0ßIn that example, even if the true posterior probability did not conform to our intuition, the problem would still persist – we could simply show that decreasing word vector length, instead of increasing it, would yield the wrong outcome.ßdummyTurker
16650ß0.0ßWe will include this extended explanation in the revised version of this paper.ßdummyTurker
16651ß0.0ß"> ""What would happen if we leave the Arora estimator alone except for normalization, or adding more common discourse vectors?"""ßdummyTurker
16652ß0.0ßAs noted in the ablation study, our weighting scheme alone is responsible for a 4.4% improvement over Arora et al.’s.ßdummyTurker
16653ß0.0ßWe think it unlikely that using Arora’s approach with normalization and additional discourse vectors would yield a relative improvement beyond what they’ve contributed to our model.ßdummyTurker
16654ß0.0ßHowever, this is certainly something that we could try and include in the Appendix, for interested readers.ßdummyTurker
16655ß0.0ß"> ""The authors complain about the hyperparameters in Arora's method, but they have one, too (m)."""ßdummyTurker
16656ß0.0ßThere is a subtle but important distinction here.ßdummyTurker
16657ß0.0ßWe complain about the hyperparameters in Arora’s method because those hyperparameters need to be tuned on a validation set.ßdummyTurker
16658ß0.0ßWhen not tuned, the results from Arora’s approach can be very poor.ßdummyTurker
16659ß0.0ßThis makes Arora’s approach useless when there is no labeled data, which is a common problem.ßdummyTurker
16660ß0.0ßOur approach is certainly not hyperparameter free, but the key difference is that our hyperparameters do not need to be tuned.ßdummyTurker
16661ß0.0ßDecent estimates of our hyperparameters can be calculated directly, or in the case of m, be fixed.ßdummyTurker
16662ß0.0ßThis allows our approach to be used on unlabeled data, which is a major advantage.ßdummyTurker
16663ß0.0ßThank you very much for your great suggestions and kind words about our paper.ßdummyTurker
16664ß0.0ßIt’s a good idea to expand the training dataset by adding the Princeton WordNet Gloss Corpus.ßdummyTurker
16665ß0.0ßThe name of this gloss corpus may mislead us to think it as a kind of gloss knowledge like we use in the paper.ßdummyTurker
16666ß0.0ßEssentially, it's a special sense annotated corpus which is only available in the WordNet.ßdummyTurker
16667ß0.0ßBy comparison, the glosses knowledge we use in the paper exists in almost any lexical database.ßdummyTurker
16668ß0.0ßBut since we have used the WordNet in the paper, it’s worth adding its available tagged corpus into the training dataset to enhance the completeness of results.ßdummyTurker
16669ß0.0ßHowever, it’s not strictly necessary to add this gloss corpus, because it might be not a fair comparison over other systems for using more labeled data (all the systems are trained on the SemCor 3.0 in the original papers or in Raganato’s paper [1]).ßdummyTurker
16670ß0.0ßTherefore, we will do more experiments about supervised systems which make use of the gloss corpus, show the results and make further analysis about this in our final version.ßdummyTurker
16671ß0.0ßWe will do the significance test and show the F1 score on the dev set in the final version.ßdummyTurker
16672ß0.0ßThank you very much for the insightful suggestions.ßdummyTurker
16673ß0.0ß[1] Alessandro Raganato, Jose Camacho-Collados, and Roberto Navigli.ßdummyTurker
16674ß0.0ßWord sense disambiguation: A unified evaluation framework and empirical comparison.ßdummyTurker
16675ß0.0ßIn Proc.ßdummyTurker
16676ß0.0ßof EACL.ßdummyTurker
16677ß0.0ßpages 99–110.ßdummyTurker
16678ß0.0ßThank you very much for your insightful suggestions.ßdummyTurker
16679ß0.0ßComparison with dummy-gloss GAS_baselineßdummyTurker
16680ß0.0ßIt’s a constructive suggestion to add a GAS_baseline to show the impact of using glosses in our neural network.ßdummyTurker
16681ß0.0ßActually, we have done this experiment before the submission deadline and got a 68.3% F1-score on the concatenation of the four test datasets in Table 1.ßdummyTurker
16682ß0.0ßCompared with GAS_baseline, our proposed model greatly improves the WSD task by 2.3% F1-score with the help of glosses knowledge.ßdummyTurker
16683ß0.0ßThe main reason why we didn’t show the result of this baseline is that we think the Bi-LSTM is equivalent to our model if we remove the gloss knowledge and its corresponding module (the gloss module and memory module) of GAS, as illustrated in Section 4.3.3.ßdummyTurker
16684ß0.0ß"However, GAS_baseline which all the glosses are dummy can ""directly"" and ""purely"" show the influence of using glosses."ßdummyTurker
16685ß0.0ßTherefore, we will add this baseline in Table 1 and include a more detailed analysis in our final version.ßdummyTurker
16686ß0.0ßAbout the paper writingßdummyTurker
16687ß0.0ßSorry for the grammatical mistakes and citation spacing problems made in the paper, we will further improve the paper writing.ßdummyTurker
16688ß0.0ßThank you for taking the time and energy to help us improve the paper.ßdummyTurker
16689ß0.0ßIt might indeed seem intuitive that complex classifiers should be less sensitive to the choice of particular embedding models and hyperparameters, however there are multiple recent publications (as surveyed in Introduction) using complex classifiers for evaluation.ßdummyTurker
16690ß0.0ßOur intention was to systematically verify this intuition.ßdummyTurker
16691ß0.0ßThe paper by Li et al does not compare the sensitivity of different classifiers to the choice of word embeddings, using LR for evaluating different context representations.ßdummyTurker
16692ß0.0ßThis paper, in contrast, focuses on a classifier choice for more robust of different types of word embeddings and hyper-parameters.ßdummyTurker
16693ß0.0ßincluding context representation.ßdummyTurker
16694ß0.0ßThank you for pointing out this mistake, we have indeed confused “bound” and “unbound” words in line 315.ßdummyTurker
16695ß0.0ßWe show in out experiments that certain evaluation methods are less sensitive to the choice of word embedding models and hyperparameters.ßdummyTurker
16696ß0.0ßAt the same time these evaluation methods are used across the field to make statements about quality of particular embedding models.ßdummyTurker
16697ß0.0ßOur work proposes more consistent evaluation methods.ßdummyTurker
16698ß0.0ßWe will make use of the additional page in the final version to include those details.ßdummyTurker
16699ß0.0ßYes, this submission is a short paper presenting ongoing work, we are currently trying to generalized on text classification task.ßdummyTurker
16700ß0.0ßWe choose sequence labeling mainly because it's popularity as a extrinsic task for evaluating word embeddings.ßdummyTurker
16701ß0.0ßWe were trying to reuse datasets if possible.ßdummyTurker
16702ß0.0ßPOS and Chunking are actually different layers of annotation over same texts, but to the best of our knowledge there’s no dataset to provide annotation for all tasks.ßdummyTurker
16703ß0.0ßThis we had to use separate resource.ßdummyTurker
16704ß0.0ßOur focus is evaluation of different classifiers, but not the embeddings themselves.ßdummyTurker
16705ß0.0ßWe have used embeddings with and without dependency information as such embeddings are commonly used in NLP practice.ßdummyTurker
16706ß0.0ß1) Thank you for your comments.ßdummyTurker
16707ß0.0ßadaptive prediction strategy experiment:  We didn’t include it in our experiment because of the main disadvantage of Hit@K, the parameter K needs to set manually which may cause misleading results.ßdummyTurker
16708ß0.0ßFor example, if there are more/less than K real entities in the test set, then some entities will be missed/unnecessarily included.ßdummyTurker
16709ß0.0ßWe will address this in revision.ßdummyTurker
16710ß0.0ß2) time complexity: It is a challenge faced by any algorithm working on large knowledge bases.ßdummyTurker
16711ß0.0ßHowever, the proposed algorithm can be easily parallelized with great scalability, the practical time complexity is manageable.ßdummyTurker
16712ß0.0ßWe agree adding a subsection before the experiment to discuss the time complexity would increase the rigor of paper.ßdummyTurker
16713ß0.0ß3) discussions on experimental results: The “CF+TransR” achieved a F1-Score of 0.3869 instead of 0.244 in your comments.ßdummyTurker
16714ß0.0ßWe think you might refer to “CF+TransE” instead?ßdummyTurker
16715ß0.0ßIn that case, the result of “CF+TransE” is lower than “TransE” because the assumption of “TransE” ignores the multi-relation in DBPedia.ßdummyTurker
16716ß0.0ßCombining CF with this assumption resulted even worse result.ßdummyTurker
16717ß0.0ßOn the other hand, the assumption of “TransR” does consider multi-relation, therefore CF managed to further improve the performance of “TransR”.ßdummyTurker
16718ß0.0ßRegarding your second question on missing results in Table 1, “CF” is only applicable to Conceptual Taxonomy (Probase), but not Factual Knowledge Graph (DBpedia).ßdummyTurker
16719ß0.0ßTherefore, these results are missing.ßdummyTurker
16720ß0.0ßSimilar reason for “TransE” and “TransR”.ßdummyTurker
16721ß0.0ß4) examples of generated results: We agree with this point.ßdummyTurker
16722ß0.0ßIf we can find space in revised version, we will include a few discovered samples to demonstrate the capabilities of algorithms.ßdummyTurker
16723ß0.0ß5) English language correction: We didn’t get the proof reading feedback on time, we will endeavour language correctness in next version.ßdummyTurker
16724ß0.0ß1) technical novelty: Thank you for your comments.ßdummyTurker
16725ß0.0ßTo the best of our knowledge, this is the first work for cross completion of two knowledge bases with proper collaborative learning framework.ßdummyTurker
16726ß0.0ßStructure of our global maximum likelihood equation (Eq 4) is similar to some related works which use KG to improve the recommendation task result.ßdummyTurker
16727ß0.0ßHowever, related works only focus on improving the recommendation quality, but our model jointly completes two knowledge bases which requires a different training strategy.ßdummyTurker
16728ß0.0ß2) writing issue: We acknowledge the unsatisfactory writing issue due to short of time for proof reading, we will seek professional language service in revision.ßdummyTurker
16729ß0.0ß3) discussion on completion error: Two situations may lead to wrong predictions.ßdummyTurker
16730ß0.0ßFirst, if an entity only appears a few times in the training set, we can’t learn a robust representation for it.ßdummyTurker
16731ß0.0ßThe other situation is that two entities have same concepts or two entities share same object as the value of same predictor.ßdummyTurker
16732ß0.0ßIn such cases, the system may wrongly predict some unique feature of one entity to the other entity.ßdummyTurker
16733ß0.0ß4) confusing on evaluation: We realised the confusion caused by terms.ßdummyTurker
16734ß0.0ßTo make it clearer and more consistent, we will change the term to “test set” from “real set”.ßdummyTurker
16735ß0.0ßThe test set was drawn from full dataset after random shuffling.ßdummyTurker
16736ß0.0ßAlso, we didn’t predict on training set, the “predicted set” refers to the predictions of our model on test set.ßdummyTurker
16737ß0.0ßWe will revise our draft to make it clearer.ßdummyTurker
16738ß0.0ß5) text data involved baseline: Text information is an important aspect for knowledge base completion, however, it fits outside of the main theme of our research, leveraging relationships and structural information.ßdummyTurker
16739ß0.0ß"""The paper relies heavily on APES which is a submitted manuscript."""ßdummyTurker
16740ß0.0ßThe paper summarizes the key meaning of APES to the extent that it makes clear what metric we are trying to optimize: the number of key questions that can be answered given the summary.ßdummyTurker
16741ß0.0ßThe generated headlines are ungrammatical, so I think this will not advance the summarization techniques.ßdummyTurker
16742ß0.0ßGrammaticality is only one of the criteria used to judge summaries.ßdummyTurker
16743ß0.0ßInformativeness (the fact that key information is conveyed) is also one of the key quality criteria for summaries.ßdummyTurker
16744ß0.0ßOur paper focuses on this aspect.ßdummyTurker
16745ß0.0ßReply to comments-ßdummyTurker
16746ß0.0ß1:  As discussed in sec 4.3 (metrics), a catchy tagline is a tagline  which is: 1) easily readable 2) memorable 3) carries positive sentiment, and 4) is creative.ßdummyTurker
16747ß0.0ßthis list of measuring catchiness  is not-exhaustive (such as measuring influence).ßdummyTurker
16748ß0.0ß2:   We have updated the relevant data and references.ßdummyTurker
16749ß0.0ßFYI, please see: https://www.statista.com/statistics/236943/global-advertising-spending/ßdummyTurker
16750ß0.0ß3: We have added some references for the same.ßdummyTurker
16751ß0.0ßFYI,ßdummyTurker
16752ß0.0ß1.ßdummyTurker
16753ß0.0ßMary Lou Maher.ßdummyTurker
16754ß0.0ß2010.ßdummyTurker
16755ß0.0ßEvaluating creativity in humans, computers, and collectively intelligent systems.ßdummyTurker
16756ß0.0ßIn Proceedings of the 1st DESIRE Network Conference on Creativity and Innovation in Design, pages 22–28.ßdummyTurker
16757ß0.0ß4:  Creation of ontology is a contribution and it helps the system to be general, efficient and effective in generating taglines.ßdummyTurker
16758ß0.0ßWe can change it to “knowledge graph” in the camera-ready submission.ßdummyTurker
16759ß0.0ßIt’s populated by identifying different concepts in the product description by using the NLC tool (link in the paper).ßdummyTurker
16760ß0.0ßThere are 20 concepts and 15 relations as of now which can be extended according to the input.ßdummyTurker
16761ß0.0ß6: CopyLSTM is a modified version of seq2seq model which takes the input in the form of key-value pairs and outputs the sentence which is filled by the input.ßdummyTurker
16762ß0.0ßLSTM will utter a word based on the previously uttered words where LSTM would have learnt the values as just mere words rather than the class to which it belongs to.ßdummyTurker
16763ß0.0ßCopyLSTM will utter class/key rather than values which will be filled to their respective values from the input at decoder.ßdummyTurker
16764ß0.0ßThis helps in learning the relationship of words with their classes rather than only values.ßdummyTurker
16765ß0.0ßLSTM vocabulary will have the ‘values’ of the classes whereas CopyLSTM vocabulary will have the ‘classes’ rather than their values seen in the data.ßdummyTurker
16766ß0.0ßThis improves upon the generality of the model to produce tagline even for those products which are not seen in the training set.ßdummyTurker
16767ß0.0ß7: The reason behind using EUROPARL corpus is that the fashion corpus that we used is small in size (10K) and not enough to train the paraphrase model.ßdummyTurker
16768ß0.0ßLarge EUROPARL corpus helps in learning the language and Grammar while fashion corpus helps to learn the OOV words.ßdummyTurker
16769ß0.0ß8:  W assume that “product description” may serve as non-memorable corpus which is based on the intuition that product description is usually used in advertising but lack creativity such as catchiness.ßdummyTurker
16770ß0.0ßMemorable corpus is crawled from fashion sites such as Vogue which consists of popular quotes.ßdummyTurker
16771ß0.0ßThe size of the corpus is 1700 quotes.ßdummyTurker
16772ß0.0ß9: Many of the references available online recommend that a catchy tagline should be positive in sentiment such as:ßdummyTurker
16773ß0.0ßhttp://blog.visme.co/how-to-create-memorable-taglines-and-slogans/ßdummyTurker
16774ß0.0ßWe are taking the tagline as a whole that carries the positive sentiment rather than specific word-level.ßdummyTurker
16775ß0.0ß10: Perplexity is used to train the model and also used for the early stopping criterion.ßdummyTurker
16776ß0.0ß-Which achieve readability score of 60 and above.ßdummyTurker
16777ß0.0ß-Achieve higher likelihood on a memorable corpus than a non-memorable corpus.ßdummyTurker
16778ß0.0ß-That carries a positive sentiment.ßdummyTurker
16779ß0.0ßThe main purpose of using CopyLSTM is to make sure that the generated text is consistent with the input key-value pairs.ßdummyTurker
16780ß0.0ßThe “value” part of key-value pairs are copied to the output (at the places where “key” is uttered by the model).ßdummyTurker
16781ß0.0ßSo, there is not much of a need for human evaluation to verify this fact and the human evaluation was conducted for creativity, novelty, readability and memorability which are more challenging metrics for automation.ßdummyTurker
16782ß0.0ß2: We have tried to measure the quality and diversity of taglines generated, but since we don't generate a complete ad, evaluation of ad revenue is not in our scope.ßdummyTurker
16783ß0.0ßThe last two paragraphs in section 4.4.2 discuss the correlation between automatic evaluation metrics and human evaluation.ßdummyTurker
16784ß0.0ß3: Novelty is measured at the human evaluation where humans got an input and outputs from the generated model as well as the baseline methods as in Table-2.ßdummyTurker
16785ß0.0ßHuman give a novelty score for the tagline and is shown in Figure 6.ßdummyTurker
16786ß0.0ß4: Concatenation of the vectors do not enable the network to understand that the value is an instance of the key and there exists a relationship between the key and the value.ßdummyTurker
16787ß0.0ßThus, we didn't do this.ßdummyTurker
16788ß0.0ß5:  This could indeed be a baseline and we initially did experiment with this before realizing the need of a relation gate to learn the relation between key and values.ßdummyTurker
16789ß0.0ßWith encoding “key:value” as an input, the model learns to place the key along with the value in the generated sentence, which is not the desired output (For ex: “look dapper in a Color:blue Product:shirt”, this looks odd due to presence of keys- color and product).ßdummyTurker
16790ß0.0ßThis model also generated a lot of repetitions and thus was not reported as a baseline, but we can surely report it in the final paper.ßdummyTurker
16791ß0.0ß6: The beam gives us the diverse sentences in terms of the content and diversity of tone is captured by using the paraphrase model integrated into the system.ßdummyTurker
16792ß0.0ß8: The x-axis represents different metrics and their values.ßdummyTurker
16793ß0.0ßY-axis represents the percentage of people rating the metric values.ßdummyTurker
16794ß0.0ßThe error bar shows the minimum and the maximum values scored in each class.ßdummyTurker
16795ß0.0ßChosen paraphrase model is based on translation via a pivot language (Mallinson et al., 2017) and reason for doing so is that 1) we do not have parallel corpus in the fashion domain 2) at the time of experiment, chosen model was the best model for translation as per the author’s claim.ßdummyTurker
16796ß0.0ßFor more details on NMT model for paraphrase, please see Sennrich et al.ßdummyTurker
16797ß0.0ß2016.ßdummyTurker
16798ß0.0ß2: The nodes in the knowledge graph are predefined according to the domain and the nodes are then populated according to whatever data is available in the domain.ßdummyTurker
16799ß0.0ßHence, some amount of data from each domain is expected.ßdummyTurker
16800ß0.0ßWe clarify our contribution for the irony detection task and technical contribution for improving the attention network structure.ßdummyTurker
16801ß0.0ßThe lexicon used by (Joshi et al., 2015) is only about the polarity of words, while the lexicon used in our work (VADER) has both polarity and intensity of words so that attention-based model can spread attentions according to the intensity of sentiment.ßdummyTurker
16802ß0.0ßMoreover, we use both VADER and an emoji corpus to cover more sentiment elements in Twitter than what (Joshi et al., 2015) did.ßdummyTurker
16803ß0.0ß"(Ghosh et al, 2017), ""The Role of Conversation Context for Sarcasm Detection”, is focused on irony detection in social conversations while our work is focused on irony detection in utterances (individual tweets)."ßdummyTurker
16804ß0.0ßIn conversational contexts, the context incongruence is distributed in context and response, while for individual sarcastic tweets, the context incongruence is in itself.ßdummyTurker
16805ß0.0ßThe reason why we explicitly use a sentiment lexicon is the standard attention-based model frequently fails to attend on some sentiment parts in tweets, which is possibly affected by data size.ßdummyTurker
16806ß0.0ßWhen a dataset is not big enough, the embedding of uncommon sentiment words, emoticons/emoji or hashtags are not being sufficiently tuned in model training, which could affect the model to learn the context incongruence.ßdummyTurker
16807ß0.0ßSo, we proposed using sentiment attention as a guide and supervision to help the model pay more attention to explicit incongruence.ßdummyTurker
16808ß0.0ßOur major technical contribution is using sentiment strength to create extra attention that helps attention-based model, which is also the major difference from previous work which mainly used the sentiment polarity of words.ßdummyTurker
16809ß0.0ßWe will cite and discuss the work in (Ghosh et al., 2017).ßdummyTurker
16810ß0.0ßThe sentiment lexicon is only used to bias the attention mechanism to the sentiment part of the attention vectors.ßdummyTurker
16811ß0.0ßThe sentiment-biased attention vectors will enhance the inference of contrast between the sentiment word and the implicit negative situation.ßdummyTurker
16812ß0.0ßWe clarify that the reason for inferencing attention based only on sentiment strength but not polarity is that sentiment words, whether positive or negative, shall receive an equal amount of attention.ßdummyTurker
16813ß0.0ßOur experiments show that the standard attention mechanism does not generate much attention on the sentiment words.ßdummyTurker
16814ß0.0ßThe polarity contrast, not only relying on sentiment words, is inferenced using the network structure and patterns in the textual vectors rather than the attention mechanism.ßdummyTurker
16815ß0.0ßWe will add more analysis on attention scores and on the mistakes made by models.ßdummyTurker
16816ß0.0ßWe agree and will include more attention and error analysis to strengthen the conclusion.ßdummyTurker
16817ß0.0ßIn Fig.ßdummyTurker
16818ß0.0ß2 the attention scores for SAABi-LSTM are not normalized and on a different scale from that for ABi-LSTM.ßdummyTurker
16819ß0.0ßWe computed the square of sentiment attention and then summarization.ßdummyTurker
16820ß0.0ßIn our experiments, the sum of final attention is around 1.014 on average and the 0.014 overflow spreads to sentiment words.ßdummyTurker
16821ß0.0ßYes, the correct cosine similarity definition with square (^2) for each component is used and the current formula contains a typesetting error that we will fix.ßdummyTurker
16822ß0.0ßThe reason why we did not include the direct comparison with (Reyes et al., 2013) and (Ptacek et al.ßdummyTurker
16823ß0.0ß2014) are:  (1)It is reported in (Sukis et al.ßdummyTurker
16824ß0.0ß2016) that their model, SVM(Sulis et al., 2016) in Table 1, performs better than (Reyes et al, 2013).ßdummyTurker
16825ß0.0ßSo, we use (Sukis et al.ßdummyTurker
16826ß0.0ß2016) as the baseline for the state-of-art classical feature-based method.ßdummyTurker
16827ß0.0ß(2) We used the same datasets in (Sulis et al., 2016) for experiments, which include a dataset slightly different from the original dataset in (Ptacek et al., 2014) -- 18889 sarcastic and 48890 non-sarcastic tweets in (Sukis et al., 2016) versus 25000 sarcastic and 75000 non-sarcastic tweets in (Ptacek et al, 2014).ßdummyTurker
16828ß0.0ßHowever, we are doing experiments to include the results for (Reyes et al., 2013) and (Ptacek et al., 2014).ßdummyTurker
16829ß0.0ßThe reported results in (Reyes et al, 2013) for the Education, Humour and Politics datasets are 0.70, 0.76, and 0.73, lower than the (Sulis et al., 2016) baseline.ßdummyTurker
16830ß0.0ßWe will include discussions in relation to (Riloff et al, 2013), which use a conventional model.ßdummyTurker
16831ß0.0ßWe used (Sukis et al., 2016) as the baseline, with a better result on the (Riloff et al., 2013) dataset and so we did not include the (Riloff et al., 2013) baseline.ßdummyTurker
16832ß0.0ßIndeed on the (Riloff et al., 2013) dataset, our models achieved 0.75 and 0.73 respectively, whereas (Riloff et al., 2013) and (Sukis et al., 2016) obtained 0.51, and 0.73.ßdummyTurker
16833ß0.0ßRecent work on deep learning approaches to irony detection can be classified into three types of problems – for given utterance and given words (Ghosh 2015),  in social dialogues (Ghosh 2017; Oraby 2015) and directly for utterances (Ghosh 2016, Felbo 2017).ßdummyTurker
16834ß0.0ßOur irony detection task belongs to the last class of problems and is closely related to (Ghosh 2016).ßdummyTurker
16835ß0.0ßAs their code (Ghosh 2016) is not readily available, we implemented their model in Keras with same hyper-parameter settings and obtained the F1 score of 0.71 on the (Riloff 2013) dataset.ßdummyTurker
16836ß0.0ßWe are experimenting to include results for (B.Felbo et al.ßdummyTurker
16837ß0.0ß2017).ßdummyTurker
16838ß0.0ßWe will carefully proof-read the paper and fix all spelling and grammar issues.ßdummyTurker
16839ß0.0ßWe clarify details for obtaining the sentiment strength:  We obtain the sentiment strength for words and emoticons using VADER (Gilbert, 2014), which is designed for sentiment analysis on social media data, specifically Twitter.ßdummyTurker
16840ß0.0ßVADER builds its corpus from several existing linguistic resources as well as social media datasets including symbols like emoticons.ßdummyTurker
16841ß0.0ßFor emojis, we used an existing emoji corpus provided by (Novak et al., 2015).ßdummyTurker
16842ß0.0ßIt is shown that the VADER lexicon performs remarkably better than other lexicons such as LIWC, GI, ANEW, SentiWordNet, SenticNet on social media data (Gilbert, 2014).ßdummyTurker
16843ß0.0ßAutomatic entity linking attempts to do entity deduplication however the result is often imperfect.ßdummyTurker
16844ß0.0ßThese errors that arise in a long NLP pipeline is what we mean by noise in a KG.ßdummyTurker
16845ß0.0ßWe describe the typical NLP pipeline in lines 46-49 and figure 1.ßdummyTurker
16846ß0.0ßFigure 1 also does double-duty by establishing where our system lies wrt to a typical NLP Pipeline for KBPßdummyTurker
16847ß0.0ßCrowdsourcing for system comparison is a well established methodology and the use of Amazon Mechanical Turk for gathering annotations has been validated many times in areas ranging from linguistics [1] to IR [2].ßdummyTurker
16848ß0.0ßAs we explain in section 7.3 we did a side-by-side comparison of our system with the baselines over 480 queries which is much larger than the 20 queries used by Shen et al.ßdummyTurker
16849ß0.0ßFurthermore Shen et al.’s queries solely contained the top 2000 most frequent entities whereas we experiment with entities that appear in only a single sentence.ßdummyTurker
16850ß0.0ß[1] “A validation of Amazon Mechanical Turk for the collection of acceptability judgements in linguistic theory”, Jan Sprouse, J. Behav Res (2011)ßdummyTurker
16851ß0.0ß[2] “Crowdsourcing systems on the World-Wide Web”.ßdummyTurker
16852ß0.0ßAnhai Doan, Raghu Ramakrishnan, and Alon Y. Halevy, Commun.ßdummyTurker
16853ß0.0ßACM 54, (2011)ßdummyTurker
16854ß0.0ßSuggested metrics such as Precision@Rank or MAP require relevance judgements per result.ßdummyTurker
16855ß0.0ßUnfortunately there is no annotated data for ESE on automatically built, noisy KBs.ßdummyTurker
16856ß0.0ßCollecting such data would be much more expensive than what was described in this article, which would be cost prohibitive.ßdummyTurker
16857ß0.0ßFurther, we are not confident such data could be collected at high accuracy on AMT.ßdummyTurker
16858ß0.0ßTypically such data is collected during shared tasks by dedicated annotators.ßdummyTurker
16859ß0.0ßAgain, the evaluation laid out in this article is not atypical, it follows from established practice in system comparison.ßdummyTurker
16860ß0.0ßThe un-interpretability of recommendation systems and neural networks is a well known problem.ßdummyTurker
16861ß0.0ßAnd we also cited the survey by Mitra and Craswell, (2017) who concluded (on Page 39) that  “... we prefer neural models that, if not interpretable, then at least are amenable to analysis and interrogation.” We used the NYTimes article as an additional reference only.ßdummyTurker
16862ß0.0ßWhich distributions?ßdummyTurker
16863ß0.0ßHow does this truly differ?ßdummyTurker
16864ß0.0ß...ßdummyTurker
16865ß0.0ßWe did not mean to “overstate” our contribution, we were only trying to describe our method.ßdummyTurker
16866ß0.0ßAs we describe in Section 5, line 375 we infer a posterior distribution over concepts from the queried entities which we then compare to distributions produced by other entities.ßdummyTurker
16867ß0.0ßBecause our posterior approximations are gaussian therefore this comparison between distributions can be reduced to a simple distance computation (line 421).ßdummyTurker
16868ß0.0ßWe thank you for pointing out an alternative description of what our method is doing as keyword expansion which may be more intuitive and approachable for some readers.ßdummyTurker
16869ß0.0ßWe will make sure to include it.ßdummyTurker
16870ß0.0ßThanks for your comments.ßdummyTurker
16871ß0.0ßOur model hyper-parameters are selected according to preliminary experiments and previous studies.ßdummyTurker
16872ß0.0ßE.g.ßdummyTurker
16873ß0.0ßWe set different GRU units∈{100,200,300} and CNN feature sizes∈{(2,2), (3,3),(4,4)}.ßdummyTurker
16874ß0.0ßFor our baseline models, we reproduce the models following the original same highly-optimized setting.ßdummyTurker
16875ß0.0ßLet’s take the current state-of-the-art baseline SMN for comparison.ßdummyTurker
16876ß0.0ßSMN has about 15709K parameters and our DUA has about 16619K, with only 5.4% increase.ßdummyTurker
16877ß0.0ßThe training time for SMN is 8 hours in a single GTX 1080Ti and ours is about 9.5 hours with the same setting.ßdummyTurker
16878ß0.0ßThe SMN takes about 8.1G graphic memory and 23G memory meanwhile ours takes about 9.3G graphic memory and 25G memory.ßdummyTurker
16879ß0.0ßWe are happy to summarize that our model only requires moderate computational cost increase compared to the strong baseline, which will be added accordingly following your suggestion.ßdummyTurker
16880ß0.0ßAs stated in line 541, word embedding is individually trained using the training set of each dataset.ßdummyTurker
16881ß0.0ßYes, we’ll add these metrics once the condition allows.ßdummyTurker
16882ß0.0ßThese metrics actually did not appear in most previous works, which comes for a reason.ßdummyTurker
16883ß0.0ßDuring evaluation, each conversation in Douban is assigned with more than one positive response, so MAP, MRR and P@1 are necessary.ßdummyTurker
16884ß0.0ßHowever, for Ubuntu and ECD, each conversation is paired with only one positive response, so P@1 is the same as R@1, MAP is equal to MRR, both of which show consistent changes with R@1.ßdummyTurker
16885ß0.0ßYes, we have also tried to re-run the experiments on Ubuntu and Douban corpus.ßdummyTurker
16886ß0.0ßThough the performance of previous baselines varies slightly (basically +/-0.5%), it is consistent with the results in previous literatures.ßdummyTurker
16887ß0.0ßThus, to keep fair comparisons with existing work, we simply copy their reported numbers.ßdummyTurker
16888ß0.0ßExactly, we report the average results of 3 runs.ßdummyTurker
16889ß0.0ßAs the common practice, our aim here is to find the optimal model setting in EACH RUN (with different EPOCHS) as the performance changes during training (Line 572-574).ßdummyTurker
16890ß0.0ßEmpirically, we don’t observe any increase after 5 epochs.ßdummyTurker
16891ß0.0ßWe are sorry the current statement of Lines.ßdummyTurker
16892ß0.0ß572-574 causes the misunderstanding, we will make a better clarification later.ßdummyTurker
16893ß0.0ßThanks for pointing out this typo, the reference to Table 6 is supposed to be in Line 686 (Ablation Study).ßdummyTurker
16894ß0.0ßThanks for your comments.ßdummyTurker
16895ß0.0ßFrom the overall framework, this work is neither a reproduction + extension nor a follow-up study of Wu et al (2017).ßdummyTurker
16896ß0.0ßOur proposed architecture is novel and effective for multi-turn conversation modeling where we only follow the SMN part for the final response matching as clearly stated in line 339-343 and we follow some commonly-used terminologies to help readers better understand.ßdummyTurker
16897ß0.0ßBoth the turns-aware aggregation and matching attention flow are the first attempts for the concerned task, which have nothing with Wu et al (2017) or any previous work in the dialogue domain to our best knowledge.ßdummyTurker
16898ß0.0ßPlease note that the matching mechanism with attentive CNN is inspired by several recent literatures (like Multi-view) and we did not highlight this contribution as ours, but instead only a basic part of the architecture.ßdummyTurker
16899ß0.0ßWe will make a careful proofreading and necessary re-paraphrasing in the later version accordingly.ßdummyTurker
16900ß0.0ßFollowing your suggestion, we will take a concise formulation writing later.ßdummyTurker
16901ß0.0ßWe will make the concerned part more clear later.ßdummyTurker
16902ß0.0ßIn the three datasets, each conversation is paired with various candidates, which are labeled as the correct (positive) and wrong (negative) responses.ßdummyTurker
16903ß0.0ßTake Douban for example, given a conversation with n utterances, the authors extract the top 5 key words from the n-1 utterances based on TF-IDF scores and expand the n-th utterance (last utterance) with the keywords to retrieve response candidates.ßdummyTurker
16904ß0.0ßThe candidate list is further processed by human labeler to annotate the positive responses, which they think to be proper in the dialogue as positive responses.ßdummyTurker
16905ß0.0ßThere might exists more than one positive responses in candidates.ßdummyTurker
16906ß0.0ßThe rest responses are the negative ones.ßdummyTurker
16907ß0.0ßWe sincerely thank you for carefully reviewing our paper.ßdummyTurker
16908ß0.0ßThe focus of this work is not a comparison of retrieval-based methods with generation-based methods, but how to mitigate the lack of labeled data problem in learning of a matching model.ßdummyTurker
16909ß0.0ßIn a customer assistant agent (e.g., AliMe Assist), responses come from editorial and the task is to select a proper one for a user’s query.ßdummyTurker
16910ß0.0ßIn this case, a matching model (i.e., retrieval-based methods) is more useful than a generative model.ßdummyTurker
16911ß0.0ßWe can include a comparison between matching models and a generative model in the final version.ßdummyTurker
16912ß0.0ßWe presented our motivation (as well as the assumption) behind the loss function in line 175-179 and line 202-206.ßdummyTurker
16913ß0.0ßPlease let us know the mistakes and thus we can correct them in the final version.ßdummyTurker
16914ß0.0ßWe do not make any claim that the trained retrieval-based chatbot is better than a generation-based chatbot, nor this is the focus of the work.ßdummyTurker
16915ß0.0ßWe are curious about if signals from a generative model estimated from one data source can be generally used to improve matching models on multiple data sets.ßdummyTurker
16916ß0.0ßWe have somewhat proved that in the paperßdummyTurker
16917ß0.0ßBroadly speaking, a teacher-student framework means learning one architecture (aka., a teacher network) to supervise the learning of another architecture (aka., a student network).ßdummyTurker
16918ß0.0ßYes, the method in the paper is quite different from the one in the work of Hinton et al., but in a broad sense, both methods fall in the teacher-student framework.ßdummyTurker
16919ß0.0ßWe can further clarify the point in the final version.ßdummyTurker
16920ß0.0ßEquation (2) is a standard form of a max-margin loss.ßdummyTurker
16921ß0.0ßThe max operation means if the matching score of a positive instance is larger than the matching score of a negative instance with a margin (defined by s’_{i,j}), then there is no loss in learning.ßdummyTurker
16922ß0.0ßThe objective tries to encourage a big gap between a positive instance and a negative instance, and thus results in a robust model.ßdummyTurker
16923ß0.0ßThe objective has been proven effective in classification, regression, and ranking (a representative method is the support vector machine (SVM)).ßdummyTurker
16924ß0.0ßs_{i,j}/s_{i,1} should be in that way because both of them are logarithm of probabilities, and thus <0.ßdummyTurker
16925ß0.0ßThen, ideally if a candidate is far from the true human response, its s_{i,j} < s_{i,1} and thus s_{i,j}/s_{i,1}>1 (both are negative), which means a large margin.ßdummyTurker
16926ß0.0ßIdeally, 422 posts should have 422 * 30 = 12660 responses.ßdummyTurker
16927ß0.0ßHowever, some posts only have 20+ responses in the released data set.ßdummyTurker
16928ß0.0ßWe do not know the exact reason behind that.ßdummyTurker
16929ß0.0ßTheir original paper only claims that ``We have labeled 422 posts and for each of them, ABOUT 30 candidate responses”.ßdummyTurker
16930ß0.0ßThanks for your comments!ßdummyTurker
16931ß0.0ßWe will follow that to improve the presentation.ßdummyTurker
16932ß0.0ßThank you for your valuable suggestion!ßdummyTurker
16933ß0.0ßWe will change our claim according to your suggestions.ßdummyTurker
16934ß0.0ßFirstly, we will remove the claim about the ``local optimum”.ßdummyTurker
16935ß0.0ßWhat we want to say is the negative sampling data is very easy for a neural network to classify, but the network performs not so well on labeled data.ßdummyTurker
16936ß0.0ßYes, the seq2seq only provides a score.ßdummyTurker
16937ß0.0ßWe will call it the seq2seq model instead of a generative model.ßdummyTurker
16938ß0.0ßSecondly, we will downgrade our claim of “a new learning framework”.ßdummyTurker
16939ß0.0ßThirdly, we agree with you on that policy-gradient is not a good way to improve a seq2seq model.ßdummyTurker
16940ß0.0ßThe only thing we can conclude here is that iteratively updating the seq2seq and a matching model does not help much on the matching model.ßdummyTurker
16941ß0.0ßWe will reply it in question 1.ßdummyTurker
16942ß0.0ßWe tried several parameters and algorithms, but we found that they areßdummyTurker
16943ß0.0ßnot sensitive to the final segmentations when comparing them with unigramßdummyTurker
16944ß0.0ßlikelihood (perplexity) Thus, we chose them rather heuristically in this work.ßdummyTurker
16945ß0.0ßOne interesting observation is that subword segmentations with theßdummyTurker
16946ß0.0ßunigram LM performs better than BPE with respect to the unigramßdummyTurker
16947ß0.0ßlikelihood on dev/test data.ßdummyTurker
16948ß0.0ßWhen taking a look at the segmented textßdummyTurker
16949ß0.0ßmanually, unigram LM outputs more reasonable and linguisticallyßdummyTurker
16950ß0.0ßmotivated segmentations.ßdummyTurker
16951ß0.0ßHowever, these differences did not help toßdummyTurker
16952ß0.0ßimprove the translation quality, i.e., the BLEU scores with BPE andßdummyTurker
16953ß0.0ßunigram language model (w/o regularization) are almost comparable,ßdummyTurker
16954ß0.0ßwhich is shown in Section 5.2.ßdummyTurker
16955ß0.0ß(line 540-544).ßdummyTurker
16956ß0.0ßThis result implies that we might not able to obtain furtherßdummyTurker
16957ß0.0ßimprovements on BLEU just by improving the segmentation itself withßdummyTurker
16958ß0.0ßbetter algorithms and/or hyperparameter tuning.ßdummyTurker
16959ß0.0ßThe improvementßdummyTurker
16960ß0.0ßof this paper would come not from the difference of primitiveßdummyTurker
16961ß0.0ßsegmentation algorithms (BPE vs unigram) but from the multiple segmentationsßdummyTurker
16962ß0.0ßand on-the-fly sampling.ßdummyTurker
16963ß0.0ßThank you for the suggestion.ßdummyTurker
16964ß0.0ßThough we have not analyzed theßdummyTurker
16965ß0.0ßtranslation results in detail, we found that the common NMT errors,ßdummyTurker
16966ß0.0ßincluding (1) word/phrase repetition and (2) coverage issue, areßdummyTurker
16967ß0.0ßresolved with the subword regularization.ßdummyTurker
16968ß0.0ßIn addition, we found thatßdummyTurker
16969ß0.0ßbaseline system easily copies the source words/phrases when they areßdummyTurker
16970ß0.0ßunknown.ßdummyTurker
16971ß0.0ßHowever, proposed system avoids such trivial copies.ßdummyTurker
16972ß0.0ßThisßdummyTurker
16973ß0.0ßimplies that subword regularization helps the model to capture the wordßdummyTurker
16974ß0.0ßcompositionality.ßdummyTurker
16975ß0.0ßYes, but we found that it is not straightforward to apply BayesianßdummyTurker
16976ß0.0ßapproach e.g., [Goldwater et al., 2006], as they basically assume anßdummyTurker
16977ß0.0ßinfinite vocabulary, which makes it hard to train the model under theßdummyTurker
16978ß0.0ßfixed vocabulary constraint.ßdummyTurker
16979ß0.0ßThese Bayesian approaches focus on determiningßdummyTurker
16980ß0.0ßword boundaries rather than finding the (fixed) vocabulary set.ßdummyTurker
16981ß0.0ßWordßdummyTurker
16982ß0.0ßboundaries are iteratively sampled during training and vocabulary setßdummyTurker
16983ß0.0ßis indirectly obtained after segmentation.ßdummyTurker
16984ß0.0ßAs we can notßdummyTurker
16985ß0.0ßcontrol the vocabulary set generation, unknown tokens (tokens whichßdummyTurker
16986ß0.0ßdid not appear in the training corpus) might be extracted duringßdummyTurker
16987ß0.0ßinference, which may badly affect translation quality.ßdummyTurker
16988ß0.0ßIn addition, Bayesian approaches are slow both in training and inference.ßdummyTurker
16989ß0.0ßIt would be hard to use them on large corpus settings (WMT).ßdummyTurker
16990ß0.0ßYes, possible.ßdummyTurker
16991ß0.0ßThank you for the suggestion.ßdummyTurker
16992ß0.0ßThe main motivation ofßdummyTurker
16993ß0.0ßthis work is not to find an oracle/optimal segmentation for NMT, but toßdummyTurker
16994ß0.0ßincorporate (noisy) multiple segmentations to improve the robustnessßdummyTurker
16995ß0.0ßof NMT.ßdummyTurker
16996ß0.0ßAs you suggested, we can mix multiple segmentations toßdummyTurker
16997ß0.0ßbetter enumerate noise.ßdummyTurker
16998ß0.0ßIn addition, it would be more useful if theseßdummyTurker
16999ß0.0ßsegmenters can provide multiple outputs with probabilities so we canßdummyTurker
17000ß0.0ßsample segmentations on-the-fly during NMT training.ßdummyTurker
17001ß0.0ßThank you for the suggestion.ßdummyTurker
17002ß0.0ßLet us explain the reasons why we selected these language pairs:ßdummyTurker
17003ß0.0ß(5.4 Comparison with other segmentation algorithms)=>ßdummyTurker
17004ß0.0ß[We 2016] already compared various segmentation algorithms.ßdummyTurker
17005ß0.0ßwe just wanted to reuse the results.ßdummyTurker
17006ß0.0ß(5.5 Impact of sampling hyperparameters.ßdummyTurker
17007ß0.0ß5.6 Results with single side regularization)=>ßdummyTurker
17008ß0.0ßWe used IWSLT corpus for experiments as IWSLT are relatively smaller than othersßdummyTurker
17009ß0.0ßand experiments finished much faster.ßdummyTurker
17010ß0.0ßIn addition, as our methodßdummyTurker
17011ß0.0ßshowed better quality improvements on smaller corpus, we thought thatßdummyTurker
17012ß0.0ßit is more useful to use IWSLT to analyze the model behaviors in detail.ßdummyTurker
17013ß0.0ß"""Additional analyses on the impact of hyperparameters and single side"ßdummyTurker
17014ß0.0ßregularization are also interesting and allow a better understandingßdummyTurker
17015ß0.0ßof the proposed method.ßdummyTurker
17016ß0.0ßHowever, I think it would be interesting toßdummyTurker
17017ß0.0ßdetail the effect of the proposed technique on training andßdummyTurker
17018ß0.0ßtranslation times.ßdummyTurker
17019ß0.0ßDoes the use of subword regularization have aßdummyTurker
17020ß0.0ßsignificant effect on training times?ßdummyTurker
17021ß0.0ßTo what extent does the use ofßdummyTurker
17022ß0.0ß"n-best segmentations slow down the translation process?"""ßdummyTurker
17023ß0.0ßOnly the difference between with and without subword regularization isßdummyTurker
17024ß0.0ßwhether we sample one segmentation or use the one-best segmentationßdummyTurker
17025ß0.0ßduring training.ßdummyTurker
17026ß0.0ßAs subword sampling is much faster than NMT'sßdummyTurker
17027ß0.0ßparameter update (22k sentences/sec, which is 10-20 times faster thanßdummyTurker
17028ß0.0ßNMT training) and can be executed on CPU asynchronously, subwordßdummyTurker
17029ß0.0ßsampling has almost no effect on training times.ßdummyTurker
17030ß0.0ßAs for the inference (decoding), we have proposed one-best decodingßdummyTurker
17031ß0.0ßand n-best decoding.ßdummyTurker
17032ß0.0ßOne-best decoding has no speed drop.ßdummyTurker
17033ß0.0ßAs n-bestßdummyTurker
17034ß0.0ßdecoding translates from n-best segmentations, the translation will beßdummyTurker
17035ß0.0ßn times slower than one-best decoding.ßdummyTurker
17036ß0.0ßTo maximize the translationßdummyTurker
17037ß0.0ßquality, we can use n-best decoding.ßdummyTurker
17038ß0.0ßHowever, it would be moreßdummyTurker
17039ß0.0ßpractical to apply one-best decoding as it still improves theßdummyTurker
17040ß0.0ßtranslation quality without any performance drops.ßdummyTurker
17041ß0.0ßIn fact, we plan to release all the experiment data to research community later (probably with the paper when it is officially published), including the data for learning the dialogue model and the human labeled data for dialogue act classification.ßdummyTurker
17042ß0.0ßWe will also release our trained dialogue act classifier.ßdummyTurker
17043ß0.0ßAs we claimed in the paper, there are no such data available for open domain dialogue research.ßdummyTurker
17044ß0.0ßMaking our data publicly available can facilitate the research on this area.ßdummyTurker
17045ß0.0ßBUT before that, we have to make sure there are no anonymity issues.ßdummyTurker
17046ß0.0ßThat is why we do not do that in the first submission.ßdummyTurker
17047ß0.0ßWe choose Baidu Tieba instead of OpenSubtitles, because it is easy for our labelers to annotate the dialogue acts as Chinese is their native language.ßdummyTurker
17048ß0.0ßMoreover, the data, once released, is going to be valuable for Chinese dialogue model research.ßdummyTurker
17049ß0.0ßThank you for your positive review.ßdummyTurker
17050ß0.0ß#W1: In our experiments, we found that finetuning the supervised method further on the full 200k unsupervised dataset yielded some gains for the baseline approach.ßdummyTurker
17051ß0.0ßTo allow for a fair comparison (as both our and the GAN method benefited from it), we included the pre-finetuning supervised performance as reported by Conneau et al in the relevant table, and then the post-finetuning performance, which we think was not reported by Conneau et al, in the post-finetuning table.ßdummyTurker
17052ß0.0ßThis was mentioned in line 533, but we will try to clarify it further in the paper.ßdummyTurker
17053ß0.0ß#W2: The reviewer is correct that we only reported k=1 accuracy.ßdummyTurker
17054ß0.0ßConneau et al.ßdummyTurker
17055ß0.0ßonly reported k=5,10 for En-It, but we are happy to do so (the conclusions will be unchanged) and will include it in the camera-ready.ßdummyTurker
17056ß0.0ß#W3: Thank you for the helpful feedback on clarity.ßdummyTurker
17057ß0.0ßWe will rebalance space between the motivation for PCA alignment, and the rest of the algorithm.ßdummyTurker
17058ß0.0ßOur method is quite simple and we will modify our presentation to reflect that.ßdummyTurker
17059ß0.0ß#Q1: The code and scripts used to perform the experiments will be released immediately after the anonymity period to allow full reproducibility.ßdummyTurker
17060ß0.0ßIt will be compatible with the MUSE framework, for easy comparison.ßdummyTurker
17061ß0.0ßThank you for the extensive feedback.ßdummyTurker
17062ß0.0ß#W1:  The core of our method, as that of [1] is the pre-finetuning method: MBC-ICP or GAN.ßdummyTurker
17063ß0.0ßMBC-ICP is essentially different from GANs, since it operates at the instance level rather than distribution level, and it is novel in the landscape of unsupervised domain translation, which is heavily dominated by GANs.ßdummyTurker
17064ß0.0ßIt is clear that MBC-ICP is superior to GAN in terms of accuracy and runtime.ßdummyTurker
17065ß0.0ßThe finetuning step indeed brings all methods much closer (although we still perform better), but this is orthogonal to the actual proposed methods.ßdummyTurker
17066ß0.0ßThe only reason we present post-finetuning performance is for comparison to previous methods that use it.ßdummyTurker
17067ß0.0ßWe do not anticipate users of MBC-ICP will use finetuning, the extra 1-2% do not justify the extra 200k words and slow training.ßdummyTurker
17068ß0.0ßOur method has many advantages in addition to much better performance:ßdummyTurker
17069ß0.0ßi ) It can be implemented in a few lines of basic numpy code, while the method of [1] is considerably more sophisticated and builds upon heavier machinery.ßdummyTurker
17070ß0.0ßii) Its parameters are easy to interpret (e.g.ßdummyTurker
17071ß0.0ßnumber of reruns, PCA dimension) as opposed to many carefully tuned and semantically meaningless tradeoff parameters, learning rates, weight initializations, label smoothing, architecture selection etc.ßdummyTurker
17072ß0.0ßiii) In our experience, our method is very stable with regards to its parameters while the method of [1] is not.ßdummyTurker
17073ß0.0ßFor example, any of our parameters can be multiplied or divided by at least 2 with little or no influence on performance for almost all language pairs, while the method of [1] breaks down by even modifying the label smoothing parameter by 15%.ßdummyTurker
17074ß0.0ßiv)  Our method is general and will apply with minimal modification to other tasks.ßdummyTurker
17075ß0.0ßEarly experiments showed success on unsupervised image matching whereas [1] would require significant tuning to succeed in new settings.ßdummyTurker
17076ß0.0ßv) We view the randomized nature of our algorithm as an advantage.ßdummyTurker
17077ß0.0ßRandomized algorithms are a potent and elegant tool in the face of uncertainty and often easily parallelizable.ßdummyTurker
17078ß0.0ßThey are also amenable to theoretical analysis which we anticipate in future work.ßdummyTurker
17079ß0.0ßvi) Preprocessing the input of [1] with PCA does not seem to help performance and is often detrimental.ßdummyTurker
17080ß0.0ßWe show in Table.3 that PCA is important for our method.ßdummyTurker
17081ß0.0ß[1] is using an orthogonality constraint, which as detailed in lines 181-185 of our manuscript, is related to the cycle constraint.ßdummyTurker
17082ß0.0ßvii) Using three stages yielded better performance, but if greater simplicity is desired, the whole algorithm can be composed of only two stages and no finetuning a) PCA-MBC-ICP followed by run selection b) A single run of MBC-ICP on raw reciprocal pairs from stage (a).ßdummyTurker
17083ß0.0ßThis simple algorithm will be very competitive with all other methods.ßdummyTurker
17084ß0.0ßviii) Please see comment by Reviewer #2 on the implication to opening this research field to researchers without access to GPU.ßdummyTurker
17085ß0.0ß#W2: We are extremely grateful for the very detailed feedback regarding the writeup.ßdummyTurker
17086ß0.0ßAll requests for elucidations would be fully addressed.ßdummyTurker
17087ß0.0ß#W3: Thank you for pointing out the Esperanto and Chinese comparisons.ßdummyTurker
17088ß0.0ßThe Esperanto ground truth data is not included in the MUSE data release (at this time), preventing evaluation.ßdummyTurker
17089ß0.0ßChinese data is available, but running the authors’ released code on this dataset yields different accuracies from those reported (0% after finetuning).ßdummyTurker
17090ß0.0ßAs the authors’ code replicated results for other languages quite faithfully, we think there is an issue with this dataset, and therefore did not include it in comparison.ßdummyTurker
17091ß0.0ßInstead, we included Arabic as another non-European language, for which the authors’ code and dataset worked (albeit only in one direction).ßdummyTurker
17092ß0.0ßWe will contact the authors and will resolve the Chinese and Esperanto dataset issues for the camera-ready.ßdummyTurker
17093ß0.0ß[1] Conneau et al, arXiv 2017ßdummyTurker
17094ß0.0ß"The Experiments section does not state clear enough that the last row, ""SVM TK for  <EDT-Q,EDT-A>"" is SVM learning for imaginary discourse trees, is the best."ßdummyTurker
17095ß0.0ß"It should be written as ""SVM TK for <EDT-Q ∩ EDT-A+EDT-Aimgi>."ßdummyTurker
17096ß0.0ßSo this is a typo / omission due to the lack of space .ßdummyTurker
17097ß0.0ßSo the bottom row implements SVM TK learning and the sixth (row above) implements maximal common subgraph nearest neighbor learningßdummyTurker
17098ß0.0ßWe will rewrite / improve readabilityßdummyTurker
17099ß0.0ß"The Experiments section does not state clear enough that the last row, ""SVM TK for  <EDT-Q,EDT-A>"" is SVM learning for imaginary discourse trees, is the best."ßdummyTurker
17100ß0.0ß"It should be written as ""SVM TK for <EDT-Q ∩ EDT-A+EDT-Aimgi>."ßdummyTurker
17101ß0.0ßSo this is a typo / omission due to the lack of space .ßdummyTurker
17102ß0.0ßSo the bottom row implements SVM TK learning and the sixth (row above) implements maximal common subgraph nearest neighbor learningßdummyTurker
17103ß0.0ßsee the aboveßdummyTurker
17104ß0.0ßsee the aboveßdummyTurker
17105ß0.0ßThe F1 for the bottom cases are : 77.2 +-3.4 |  72.0+-2.9ßdummyTurker
17106ß0.0ß82.8 +- 3.6 | 79.6+-3.2.ßdummyTurker
17107ß0.0ßHence the conclusion that the bottom row produces the best results is validßdummyTurker
17108ß0.0ßWe used (Surdeanu et al 2015) RST parser for evaluation and compared it with (Joty et al 2016) parser to make sure it performs reasonably well.ßdummyTurker
17109ß0.0ß"We added some rules to substitute ""Elaboration"" relation with more specific in our previous studies."ßdummyTurker
17110ß0.0ßWe can improve Figure 1 and make it occupy less space.ßdummyTurker
17111ß0.0ßWe will  highlight the E-Q and E-A mentioned in the sample Q and sample A, respectively.ßdummyTurker
17112ß0.0ßEDT is a DT with labels for entities - will add this definition.ßdummyTurker
17113ß0.0ßWe will introduce an example of subtree as recommended.ßdummyTurker
17114ß0.0ßWe relied on discourse parser (Surdeanu et al 2015) to obtain a DT and then built EDR ourselves by extracting entities.ßdummyTurker
17115ß0.0ßthe final model SVM TK for (EDT-Q, EDT-A), listed in the last row in Table 1, DOES take the information of the imaginary DTs into account.ßdummyTurker
17116ß0.0ßWe will add extra row for results of both SVM TK for (EDT-Q, EDT-A) and SVM TK for (EDT-Q, EDT-A+EDT-A_img)ßdummyTurker
17117ß0.0ßWe will fix the format and also add a document describing the format of the datasetßdummyTurker
17118ß0.0ßWe are sorry about the confusion.ßdummyTurker
17119ß0.0ßThe baselines in this paper directly correspond to the baselines in the MedNLI paper.ßdummyTurker
17120ß0.0ßUsing the techniques proposed here we beat these baselines and establish new state of the art.ßdummyTurker
17121ß0.0ßWe will make it more clear in the final version of the paper.ßdummyTurker
17122ß0.0ßThe accuracies on MedNLI is much lower than on SNLI, suggesting that MedNLI is a harder dataset.ßdummyTurker
17123ß0.0ßMoreover, the gain from transfer learning indicates that the domain significantly differs from SNLI, providing opportunities for developing better transfer learning techniques that can deal with such problems, and this paper presents necessarily first steps in this direction.ßdummyTurker
17124ß0.0ßBoth weakness 1 and 2 will go away once the MedNLI paper is published.ßdummyTurker
17125ß0.0ßThe gain from domain-specific word embeddings is more than the gain from integrating explicit knowledge.ßdummyTurker
17126ß0.0ßThis suggests that the implicit knowledge learned in word embeddings helps more than integration of human hand-crafted explicit knowledge.ßdummyTurker
17127ß0.0ßThis conclusion, of course, concerns only the way of feeding explicit knowledge we propose in this paper.ßdummyTurker
17128ß0.0ßThis requires further research and we hope that our paper will facilitate it.ßdummyTurker
17129ß0.0ßIn light of the above clarifications, can you please reconsider your assessment?ßdummyTurker
17130ß0.0ßThanks for the comments.ßdummyTurker
17131ß0.0ßWe will improve the presentation by more analysis and clear detailsßdummyTurker
17132ß0.0ßYes, we use max for testing as well.ßdummyTurker
17133ß0.0ßMore details will be presented in next versionßdummyTurker
17134ß0.0ßVisualizing entity and relation embeddings is really a good idea.ßdummyTurker
17135ß0.0ßWe will definitely consider this, given more space then.ßdummyTurker
17136ß0.0ßWe really feel excited about this suggestion for inverse relation modeling.ßdummyTurker
17137ß0.0ßWe will try our best to investigate this.ßdummyTurker
17138ß0.0ßThank you for the encouraging and insightful review!ßdummyTurker
17139ß0.0ßW1.ßdummyTurker
17140ß0.0ßWe understand your concern regarding the number of parameters and agree it is good to have baselines with the same number of parameters as mTreeLSTM (by, e.g., increasing their hidden sizes).ßdummyTurker
17141ß0.0ßIt is maybe worth to point out that rather than blindingly adding the *number* of parameters, we proposed a *new type* of parameters to model relation information.ßdummyTurker
17142ß0.0ßMoreover, due to the overfitting effect, an increasing number of parameters doesn't warrant a better predictive performance, as observed in Table 5 and 6.ßdummyTurker
17143ß0.0ßIn fact, when designing mTreeLSTM we purposely use matrix factorization to reduce the number of parameters (Eq.ßdummyTurker
17144ß0.0ß(3)).ßdummyTurker
17145ß0.0ßW2.ßdummyTurker
17146ß0.0ßThank you for providing the link to the constituency parser.ßdummyTurker
17147ß0.0ßWe are happy to test it out!ßdummyTurker
17148ß0.0ßFor parsing, we have used Stanford CoreNLP (v3.8.0) pre-trained models.ßdummyTurker
17149ß0.0ßAs reported on its website, the difference in F1 is small between models, e.g., PCFG vs. Shift-Reduce (2013).ßdummyTurker
17150ß0.0ß(The difference is significant on speed and memory efficiency: https://nlp.stanford.edu/software/srparser.html)ßdummyTurker
17151ß0.0ßW3.ßdummyTurker
17152ß0.0ßFor the NLI task, as observed on smaller SICK dataset, mTreeLSTM with dependency trees yields the best performance.ßdummyTurker
17153ß0.0ßIt is therefore sensible to use mTreeLSTM (D) to compare against state-of-the-arts on the larger SNLI dataset.ßdummyTurker
17154ß0.0ßMoreover, the computation time for parsing all SNLI sentence pairs (>500K) to AMR trees is prohibitively high.ßdummyTurker
17155ß0.0ßWe can comment more in the final version.ßdummyTurker
17156ß0.0ßQ.ßdummyTurker
17157ß0.0ßThe feature vectors used for the two sentence pair tasks in Section 5.2 and 5.3 are different, because in the first task the problem is symmetric, while in the second task it is not.ßdummyTurker
17158ß0.0ßThat is, if we switch the sentence pair, we will always get the same labeling in the semantic relatedness task but not so in the inference task.ßdummyTurker
17159ß0.0ßAs we focused on learning representations of sentences, the feature vectors used in the classification layers for the two sentence pair tasks were kept the same as in previous work.ßdummyTurker
17160ß0.0ßIn particular, in Section 5.2 we followed Tai et al.ßdummyTurker
17161ß0.0ß2015, and in Section 5.3 we used the same setting as Bowman et al.ßdummyTurker
17162ß0.0ß2016.ßdummyTurker
17163ß0.0ßOthers.ßdummyTurker
17164ß0.0ßWe will correct Figure 2 and the typo in the final version.ßdummyTurker
17165ß0.0ßThank you for the constructive and insightful review!ßdummyTurker
17166ß0.0ßW1&W2.ßdummyTurker
17167ß0.0ß"By ""Table 1 also shows mTreeLSTM consistently outperform TreeLSTM on the same tree structures in both settings"" we meant the following: Whenever a tree structure is applicable to both mTreeLSTM and TreeLSTM, the performance of mTreeLSTM with that tree structure is better."ßdummyTurker
17168ß0.0ßThat is, in phrase-level setting, mTreeLSTM (D) > TreeLSTM (D); in root-level setting, mTreeLSTM (D) > TreeLSTM (D) and mTreeLSTM (C) > TreeLSTM (C).ßdummyTurker
17169ß0.0ß(Note that constituency tree is not applicable to mTreeLSTM.)ßdummyTurker
17170ß0.0ßWe do agree with you that on this dataset, TreeLSTM with constituency parsers is the best bet across conditions, as also noted in the paper.ßdummyTurker
17171ß0.0ßW3.ßdummyTurker
17172ß0.0ßRest assured, we have no intention to manipulate you :) (After all, in Table 4, we did report a result from Choi et al.ßdummyTurker
17173ß0.0ß2018 that is better than all our models for this task.)ßdummyTurker
17174ß0.0ßWhen constructing Table 4, we aimed to compare models with the same hidden size.ßdummyTurker
17175ß0.0ß"At the end, 100D is the ""common denominator"" that all results in Table 4 are based on."ßdummyTurker
17176ß0.0ßAlternatively, we could report best numbers from individual papers, but then we had to make sure the authors have tuned the hidden size to reach the best result.ßdummyTurker
17177ß0.0ß(Some used a fixed number.)ßdummyTurker
17178ß0.0ßW4.ßdummyTurker
17179ß0.0ßThank you for pointing out the data creation detail!ßdummyTurker
17180ß0.0ßWe can comment more in the final version.ßdummyTurker
17181ß0.0ßAlthough some of the original SICK data were artificially constructed, we don't see it significantly alters our message in Section 6.4.ßdummyTurker
17182ß0.0ßQ1.ßdummyTurker
17183ß0.0ß"We don't follow the question ""For the entailment examples, why so many differences between the two data sets?"""ßdummyTurker
17184ß0.0ßWhat differences are you referring to?ßdummyTurker
17185ß0.0ßFor the NLI task, the analysis on composition matrices is done with the SICK data, not SNLI.ßdummyTurker
17186ß0.0ßWhen looking at the composition matrices, we directly present what the models have learned.ßdummyTurker
17187ß0.0ßIt is difficult to pinpoint, but for sure some observations are due to noises/randomness.ßdummyTurker
17188ß0.0ßWe are happy to dive deeper into this in the final version, as we have more space.ßdummyTurker
17189ß0.0ßWe believe the reviewer has fundamentally misunderstood our approach: We consider active transfer learning, not typical active learning within a domain.ßdummyTurker
17190ß0.0ßKoh and Liang considered neither active nor transfer learning, and they use a fundamentally different method.ßdummyTurker
17191ß0.0ßKoh and Liang used logistic regression on the email corpus, not LSTM as in our case.ßdummyTurker
17192ß0.0ßThe influence function assumes the model is convex (e.g., logistic regression).ßdummyTurker
17193ß0.0ßFor non-convex (like neural) models, the influence function approximates the influence.ßdummyTurker
17194ß0.0ßKoh and Liang do not report any results for neural models on NLP tasks.ßdummyTurker
17195ß0.0ßWe report the first application of the influence function for neural models for text classification and NER.ßdummyTurker
17196ß0.0ßAgain, our task, scenario, and proposed method are completely different from Koh and Liang’s.ßdummyTurker
17197ß0.0ßFirst, we actively select target points for labeling to expedite transfer learning from source to target, whereas they detect mislabels on a training set without any active learning or transfer learning.ßdummyTurker
17198ß0.0ßTheir figure 6 is not a learning curve.ßdummyTurker
17199ß0.0ßSecond, we use the influence function in a completely different way, by using it to identify “harmful” pseudo-labeled target points, w.r.t the performance realized on the source set.ßdummyTurker
17200ß0.0ßThe intuition is that these points are most mismatched between source and target domains.ßdummyTurker
17201ß0.0ßIn contrast, Koh and Liang calculate the “self-influence” of each training point w.r.t itself, to measure how possible this training point is correctly labeled or not.ßdummyTurker
17202ß0.0ßThey don't have the notion of source and target domain.ßdummyTurker
17203ß0.0ßWe also note that their “self-influence” method isn’t practical for general neural models in our setting, because it requires calculating the inverse Hessian vector product for each training point, which is computationally infeasible.ßdummyTurker
17204ß0.0ßOur approach calculates inverse Hessian vector product only once for the whole source set in Eq.ßdummyTurker
17205ß0.0ß(5).ßdummyTurker
17206ß0.0ßThe “DC-entropy” baseline is mostly widely used for active transfer learning.ßdummyTurker
17207ß0.0ßWe show the effectiveness of our proposed method across both text classification and sequence labeling (NER) tasks.ßdummyTurker
17208ß0.0ßExamples in Tables 5-8 and 9 in the supplementary material illustrate differences between our sub-sequence influence function and the original influence function.ßdummyTurker
17209ß0.0ßWhile the original influence function can only give the influential full training sequence w.r.t.ßdummyTurker
17210ß0.0ßa certain full test sequence, our sub-sequence method provides a more fine-grained explanation, showing which training named entity is most influential w.r.t.ßdummyTurker
17211ß0.0ßhow a given test named entity is labeled.ßdummyTurker
17212ß0.0ßWe feel the formulation is justified intuitively, but will attempt to clarify this in the final write up.ßdummyTurker
17213ß0.0ßWe feel the empirical results are relatively compelling as is.ßdummyTurker
17214ß0.0ßWe observe the same pattern on other points too, and will state this and show a subset.ßdummyTurker
17215ß0.0ßOur analysis and observations are similar to what Koh and Percy found regarding CNNs vs. SVMs in their original IF paper (they also used a single test point to illustrate this).ßdummyTurker
17216ß0.0ßIF could be potentially applied in adversarial training, but is tangential to active transfer learning.ßdummyTurker
17217ß0.0ßThe most harmful training points identified by IF may or may not be mis-labeled.ßdummyTurker
17218ß0.0ßIn some cases, the IF can identify mis-labeled training points (Table 4).ßdummyTurker
17219ß0.0ßIn other cases, the harmful training points can be correctly labeled yet still be misleading (Table 2).ßdummyTurker
17220ß0.0ßThe intuition is that the source model already contains enough information on the source domain, so training on both the source domain and the target domain may not have a big advantage over simple fine-tuning, as shown in [Mou et al., EMNLP-16].ßdummyTurker
17221ß0.0ßThe result in Figures 3 and 4 are averaged over 5 runs and show results over different datasets and tasks, suggesting that the improvement is robust.ßdummyTurker
17222ß0.0ßThe improvement is not stable because 1) in this setting, the target dataset is very small, so there exists variance; 2) conjugate gradient only gives the approximate IF for neural models, not the exact IF.ßdummyTurker
17223ß0.0ßWe’ll add analysis.ßdummyTurker
17224ß0.0ßWe will release our source code and make corresponding edits following the suggestions.ßdummyTurker
17225ß0.0ßWe compare our approach with “DC-entropy” because it is the most widely used just as “entropy” in vanilla active learning.ßdummyTurker
17226ß0.0ßWu’s paper is quite new, and is only for sentiment analysis with linear classifier constraining word feature weight via prior knowledge.ßdummyTurker
17227ß0.0ßIt may not be directly used on neural model or sequence labeling.ßdummyTurker
17228ß0.0ßLi’s method is based solely on the version space of SVM and so tied to that model.ßdummyTurker
17229ß0.0ßIt’s not clear what a better baseline (apply to neural model) is for active transfer learning.ßdummyTurker
17230ß0.0ßWe'll report result on more domain pairs.ßdummyTurker
17231ß0.0ßThe most expensive part in calculating influence function via conjugate gradient is inverse Hessian vector product (reviewer2) which scales as (# training points) x (# parameters).ßdummyTurker
17232ß0.0ßIn practice, it requires many fewer iterations than in theory.ßdummyTurker
17233ß0.0ßExamples identified by the influence function are not always enlightening (nor were they in [Koh and Liang]).ßdummyTurker
17234ß0.0ßIt needs future study on what neural model exactly does at each layer.ßdummyTurker
17235ß0.0ßWe’ll show top k influential sentences.ßdummyTurker
17236ß0.0ßAs noted earlier (reviewer2), the influence function assumes the model is convex (e.g., logistic regression).ßdummyTurker
17237ß0.0ßFor non-convex models (e.g., neural), the influence function approximates the true influence.ßdummyTurker
17238ß0.0ßLogistic regression thus works better than LSTM for the target model because it’s more accurate.ßdummyTurker
17239ß0.0ßTable 6 reports the “areas under the curve” for curves in Figure 3, not the AUC of a certain classifier.ßdummyTurker
17240ß0.0ßFor “book->electronics” in Figure 3, DC-entropy is worse than random sampling.ßdummyTurker
17241ß0.0ßIn Table 6, DC-entropy achieved 0.921 vs. random sampling’s 0.943.ßdummyTurker
17242ß0.0ßFor the other two pairs, we do not see much difference between DC-classifier and random in Figure 3, consistent with Table 6.ßdummyTurker
17243ß0.0ßDC-entropy or entropy are often informative but do not always reliably indicate which target points will expedite transfer learning.ßdummyTurker
17244ß0.0ßWe appreciate the suggestions and will make the corresponding edits.ßdummyTurker
17245ß0.0ßDear reviewer #1ßdummyTurker
17246ß0.0ßThanks for your kind review.ßdummyTurker
17247ß0.0ßIn the final submission, the readability issue will be thoroughly examined.ßdummyTurker
17248ß0.0ßBest regards,ßdummyTurker
17249ß0.0ßIn our manuscript, we mainly compare our proposed model with other feature combination methods (i.e.ßdummyTurker
17250ß0.0ßpooling and self-attention) in the same setting of Full-Shared Multi-Task Learning.ßdummyTurker
17251ß0.0ßThis work currently a project in progress.ßdummyTurker
17252ß0.0ßAfter the initial implementation, good results are obtained and result analysis shows that our idea is feasible.We will consider your suggestions in subsequent in-depth studies, introducing our idea into other sentence encoding method (e.g.ßdummyTurker
17253ß0.0ßCNN) and other multi-task learning frameworks (e.g.ßdummyTurker
17254ß0.0ßshared-private models).ßdummyTurker
17255ß0.0ßThat is because we conduct our experiments based on LSTM models.ßdummyTurker
17256ß0.0ßWe introduce the Learnable Guider Vector (LGV) to distinguish feature combination process of different tasks.ßdummyTurker
17257ß0.0ßAttention mechanism is a great method to combine hidden features in the LSTM models.ßdummyTurker
17258ß0.0ßOf course, we can combine MT with other text encoders.ßdummyTurker
17259ß0.0ßEspecially for CNN and our proposed LGVs, we can first apply CNN to extract local features, then utilize LGVs to guide the feature combination process.ßdummyTurker
17260ß0.0ßActually, taking LGVs into other encoding methods (e.g.ßdummyTurker
17261ß0.0ßCNN) is the work in our near future.ßdummyTurker
17262ß0.0ßMoreover, we will add discussion to this section in our revision.ßdummyTurker
17263ß0.0ßWe can consider the guider vector as Private Model, as they differ for tasks.ßdummyTurker
17264ß0.0ßAnd what we emphasize is that we only introduce small amount of private components (i.e.ßdummyTurker
17265ß0.0ßguider vector).ßdummyTurker
17266ß0.0ßFinally, thanks for your constructive and valuable comments!ßdummyTurker
17267ß0.0ßCompared to the single-task training, our proposed model improves the accuracy in all tasks by a considerable margin (i.e.ßdummyTurker
17268ß0.0ß1.38% in average).ßdummyTurker
17269ß0.0ßOur basic model BiLSTM is not a the-state-of-art model for sentence encoding.ßdummyTurker
17270ß0.0ßAs we can see, CNN performs much better in the single-task learning on CR and TREC, than BiLSTM.ßdummyTurker
17271ß0.0ßHowever, our proposed model gets a better result in CR and a slightly lower accuracy in TREC, in multi-task learning.ßdummyTurker
17272ß0.0ßWe will conduct more detailed analysis of our motivation in the next revision.ßdummyTurker
17273ß0.0ßSimply, in the stage of feature combination process, different tasks usually value different aspects of sentence contents.ßdummyTurker
17274ß0.0ßWe want to use a guider information to distinguish them.ßdummyTurker
17275ß0.0ßSo, we conduct preliminary experiments, combining guider vector with LSTM hidden states.ßdummyTurker
17276ß0.0ßFor future work, we are going to introduce it into other sentence encoders, like CNN.ßdummyTurker
17277ß0.0ßEspecially for CNN and our proposed guider vector, we can first apply CNN to extract local features with the form of sequence, then utilize guider vector to guide the feature combination process.ßdummyTurker
17278ß0.0ßAs long as there is a feature combination process, our idea is feasible to some extent.ßdummyTurker
17279ß0.0ßActually, we are now extending the experiment setting from full-shared to shared-private setup.ßdummyTurker
17280ß0.0ßIt is interesting to explore the effectiveness of our proposed framework in the field of shared-private.ßdummyTurker
17281ß0.0ßIn our experiments, we really ignore this problem.ßdummyTurker
17282ß0.0ßWe will take this into consideration in the next revision.ßdummyTurker
17283ß0.0ßYes, the “average” means the average accuracy in all multi-task training.ßdummyTurker
17284ß0.0ßIn this section, we’d like to show a more intuitive comparison with competitor models.ßdummyTurker
17285ß0.0ßActually, in the field of text classification, researchers generally only compare the accuracy with other models.ßdummyTurker
17286ß0.0ßMore specifically, we follow the work of [1].ßdummyTurker
17287ß0.0ß[1] Recurrent Neural Network for Text Classification with Multi-Task Learning https://arxiv.org/abs/1605.05101ßdummyTurker
17288ß0.0ßAs we choose Bi-LSTM as our feature extraction method, the input needs to be a sequence.ßdummyTurker
17289ß0.0ßSo in our current manuscript, the case is just like you said.ßdummyTurker
17290ß0.0ßWhat I want to point out is the extensibility of our proposed model.ßdummyTurker
17291ß0.0ßFor instance, if the inputs are images, we can first utilize CNN to extract local features and for feature combination process, guider vectors can be applied on that.ßdummyTurker
17292ß0.0ßYes,refer to argument 2 and 3.ßdummyTurker
17293ß0.0ßWe can divide the training process into two stages.ßdummyTurker
17294ß0.0ßFirst, update all parameters including shared and private ones.ßdummyTurker
17295ß0.0ßThen, fixing the shared parameters, we update the private parameters (i.e.ßdummyTurker
17296ß0.0ßprivate guider vectors and task-specific output layer) for each tasks.ßdummyTurker
17297ß0.0ßIn the full-shared models, the feature extraction and combination process show no difference for all tasks.ßdummyTurker
17298ß0.0ßWhen the correlation between tasks are weak, there are little common knowledge to be shared or to be used to improve other tasks.ßdummyTurker
17299ß0.0ßNo specific reason.ßdummyTurker
17300ß0.0ßWe have discussed this in the response to weakness argument 6.ßdummyTurker
17301ß0.0ßFinally, thanks for your constructive and valuable comments!ßdummyTurker
17302ß0.0ßAt first, we thank the reviewer for giving us valuable advice.ßdummyTurker
17303ß0.0ßThe sematic labelling information for each user utterance, which is needed during training, are generated from the user simulator based on templates.ßdummyTurker
17304ß0.0ßTherefore, employing the templates can avoid requiring huge amount of manually labeled data.ßdummyTurker
17305ß0.0ßActually, we aim to compare with the fully observable Markov Decision Process dialogue management rather than Q-learning algorithm itself.ßdummyTurker
17306ß0.0ßIn our setting, the Q-learning provides MDP a competitive resolution by computing precise value function.ßdummyTurker
17307ß0.0ßThrough the comparison, we show that our proposed POMDP dialogue management outperforms the individual dialogue management which is fully observable.ßdummyTurker
17308ß0.0ßOn the real user experiments, we want to show that our model can deal with the real task scenario.ßdummyTurker
17309ß0.0ßThank you for reminding us the real user experiments should also implemented to any other algorithm.ßdummyTurker
17310ß0.0ßWe will supplement this experiment in our work to make the conclusion more solid and sound.ßdummyTurker
17311ß0.0ßAlso we will find testers having the same language habits and understanding of our models.ßdummyTurker
17312ß0.0ßWe have made a good-faith effort to fix all typos in the text and the references.ßdummyTurker
17313ß0.0ßIf any remain, we apologize, and we will certainly be glad to correct them.ßdummyTurker
17314ß0.0ßOverall, we thank you very much for valid and helpful suggestions.ßdummyTurker
17315ß0.0ßwe already revised these two sections.ßdummyTurker
17316ß0.0ßFig.3 is magnified and more detailed descriptions on it are added.ßdummyTurker
17317ß0.0ßAs mentioned above, we choose to compare our model with Q-learning since Q-learning indeed outperforms several other more elaborate algorithms in our MDP settings.ßdummyTurker
17318ß0.0ßWhile in the same settings using the belief state provided by the simulated user as input, we concentrate on comparing performances of different model structures (i.e., Zhao and Eskenazi, 2016) by using AC algorithm.ßdummyTurker
17319ß0.0ßThe proposed model work in master action space rather than summary action space.ßdummyTurker
17320ß0.0ßE.g., in five-slot task, the actions include ask_time, ask_location, ask_attendance, ask_duration, ask_budget, confirm_time, confirm_location, confirm_attendance, confirm_duration, confirm_budget and bye.ßdummyTurker
17321ß0.0ßThank you for your smart idea.ßdummyTurker
17322ß0.0ßWe would like to consider comparing our model with these works on sentence classification problem in the further.ßdummyTurker
17323ß0.0ßWe will refer these studies in the related work of our paper.ßdummyTurker
17324ß0.0ßThank you for your valuable comments and suggestions.ßdummyTurker
17325ß0.0ßWe will give more detailed explanations on why CRNN+Types outperforms CRNN+Entity+Types and also further analyze the experimental results in the experiment partßdummyTurker
17326ß0.0ßThanks for your good suggestion.ßdummyTurker
17327ß0.0ßWe will modify the symbol W used for windows and eliminate the ambiguous representation.ßdummyTurker
17328ß0.0ßThank you for your insightful suggestions.ßdummyTurker
17329ß0.0ßIt is an honor that we have the same ideas with you.ßdummyTurker
17330ß0.0ßIn the future works, we will try our CRNN to other tasks such as QA, comments sentiment classification, sentence and document representation.ßdummyTurker
17331ß0.0ßThanks for your insightful comments.ßdummyTurker
17332ß0.0ßRegarding the model architecture, it is indeed not novel at all, so we did not refer it as a contribution of our work, but we will adopt your advice to cite some related works that will make the paper more self-consistent.ßdummyTurker
17333ß0.0ßWe have analyzed the influence of additional data in Section 4.6, which is evident that the extra data could improve the performance, we will add this result in Table 1 to make it clearer.ßdummyTurker
17334ß0.0ßThe variation w.r.t.ßdummyTurker
17335ß0.0ßrandom seeds are very small, because we have adopted three methods to generate the negative sentence (line 479-484), and combine them together as the negative sentences.ßdummyTurker
17336ß0.0ßAs our model achieves a significant improvement (nearly 7 points) we do not report the statistical significance test.ßdummyTurker
17337ß0.0ßThe number in Table 4 is the accuracy which is similar to previous evaluation, and the BLUE score was computed on the test set.ßdummyTurker
17338ß0.0ßAs the additional data is only used to strengthen the performance of our model, so we did not use the individual sample to train our generative model, but your advice is really insightful and we will make further analysis of this problem in the future works.ßdummyTurker
17339ß0.0ßthanks for your additional comments, we have compared our model with a more competitive discriminative method where the negative example is generated by GANs rather than random sampling (CGANs in section 4.3).ßdummyTurker
17340ß0.0ßBut our model still behaves better (about 5 points.)ßdummyTurker
17341ß0.0ßThanks for your valuable comments!ßdummyTurker
17342ß0.0ßWe have mentioned this in the footnote on page 2, the previous methods, especially for those that behave very good, use the small labeled validation set for training, which is, from our point of view, not reasonable.ßdummyTurker
17343ß0.0ßWhile our model only uses the unlabeled training set for training and report the result on both test and validation set.ßdummyTurker
17344ß0.0ßSo we think it is unfair for our model to compare with their models.ßdummyTurker
17345ß0.0ßBut we will rephrase the <state-of-the-arts> statement in the paper to make it clearer.ßdummyTurker
17346ß0.0ßThe SCT dataset was originally proposed to evaluate the script learning (Mostafazadeh et al., 2016).ßdummyTurker
17347ß0.0ßConcretely, we can regard the 4 sentences in the context as the plot and the target sentence as the ending, so they form a script of a story.ßdummyTurker
17348ß0.0ßThe baseline Narrative-Chains AP|S models used in (Mostafazadeh et al., 2016) is an off-the-shelf model that is trained on Associate Press datasets, and they only report the result but do not make any analysis.ßdummyTurker
17349ß0.0ßAs our model is so closely related to script learning, we do a throughout experiment to apply the traditional script-learning methods on SCT, and analysis why the traditional script-learning methods are not effective, we believe this will make our paper that employing the generative architecture more self-consistent.ßdummyTurker
17350ß0.0ßThere indeed exists some typos because of our carelessness.ßdummyTurker
17351ß0.0ßWe will make revisions to them carefully and invite a native English speaker to polish the whole paper.ßdummyTurker
17352ß0.0ßFor the limit of the space, we have not plotted the conceptual architecture of our model, but we will illustrate it in the final version.ßdummyTurker
17353ß0.0ßThanks for your insightful comments.ßdummyTurker
17354ß0.0ßThe generative model used in our paper is not accord to the generative model in machine learning, it just a conceptual reference, where our goal is not to discriminate the target but generate it during training (we even italic the word “generated”in line 115), so we think this description used in our paper is very clear to the reader which should not be regarded as a “major issue”.ßdummyTurker
17355ß0.0ßBut we will make some modification to the introduction, such as formulate our model as an unsupervised method or generative ARCHITECTURE.ßdummyTurker
17356ß0.0ßAs we do not use the small dev set for training, so the comparison of our model with these supervised methods seems to be unfair and confusing.ßdummyTurker
17357ß0.0ßBut we will report their result to make this paper more self-consistent.ßdummyTurker
17358ß0.0ßFurthermore, in addition to the three random sampling methods, we have experiment with a more competitive negative sample generation scheme (Section 4.3 CGANs), where they use the GANs to generate the negative sentence, but our model still outperforms to their model significantly (around 5 points.).ßdummyTurker
17359ß0.0ßBut we will cite some related works in this shared task.ßdummyTurker
17360ß0.0ßYour mention about “the encoder-decoder models perform much worse when the context is longer” may conclude w.r.t.ßdummyTurker
17361ß0.0ßthe quality of the generated sentence.ßdummyTurker
17362ß0.0ßHowever, generate the high-quality sentence is not our goal, we want to increase the inference probability of the target given the context, which we believe the extra <context-target> pairs would helps a lot.ßdummyTurker
17363ß0.0ßFor the formulation part, we think it is again an “empirical misunderstanding”.ßdummyTurker
17364ß0.0ßThe formulation in Equation (1) means that our model use the forward and backward LSTM and the hidden state is updated recurrently, so we omit the inner detail of LSTM and only use a denotation for simplicity.ßdummyTurker
17365ß0.0ßIt is a convention formulation that is widely used in previous works.ßdummyTurker
17366ß0.0ß"(Equation 9 in ""Stacked attention networks for image question answering."""ßdummyTurker
17367ß0.0ß"Or Equation 2 in ""Multiplicative LSTM for sequence modeling"", etc."ßdummyTurker
17368ß0.0ß).ßdummyTurker
17369ß0.0ßIn equation 2, we average the representation which serves as the context representation.ßdummyTurker
17370ß0.0ßIt is just a commonly used strategy.ßdummyTurker
17371ß0.0ßFor example, we can use the last state of an RNN as the sentence representation, or the average each state of the RNN as the sentence representation.ßdummyTurker
17372ß0.0ßThe PMI score was used as a benchmark for inference in script learning, see the fundamental works of script learning (Chambers and Jurafsky, 2008,ßdummyTurker
17373ß0.0ß2009)ßdummyTurker
17374ß0.0ßThanks for pointing out the missing references.ßdummyTurker
17375ß0.0ßWe will add them into our related work comparison.ßdummyTurker
17376ß0.0ßWe will try our best to do more thorough literature survey and add related papers on caption generation for online news images.ßdummyTurker
17377ß0.0ßThanks for the advice.ßdummyTurker
17378ß0.0ßWe will add this baseline and another baseline based on tags and recognized objects from images into the final version, with detailed comparison and analysis.ßdummyTurker
17379ß0.0ßWe agree that ‘a new task’ is not accurate and a little overstated given the similarity of task definition with (Tran et al.ßdummyTurker
17380ß0.0ß), we will revise the description.ßdummyTurker
17381ß0.0ßWe collected training data from news agencies in order to train a template generator using largely available resources.ßdummyTurker
17382ß0.0ßThe size of news data we were able to collect for the selected domains is much larger than the Flickr data.ßdummyTurker
17383ß0.0ßAlso the captions of news images are usually better aligned with the contents in images and thus can be more easily generalized into templates, compared with Flickr captions.ßdummyTurker
17384ß0.0ßBut news images are not suitable for testing our framework, because they don’t have associated tags and cannot mimic the image-centric social media environment.ßdummyTurker
17385ß0.0ßTherefore we collected test data from Flickr.ßdummyTurker
17386ß0.0ßAlso we chose the Flickr images with human captions in news-style for test, as to make the caption style of test data consistent with training data.ßdummyTurker
17387ß0.0ßWe will elaborate this motivation in the paper.ßdummyTurker
17388ß0.0ßThanks for the suggestion.ßdummyTurker
17389ß0.0ßWe will include the evaluation for template generation by manually replacing the named entities in the human annotations with entity-types, and compute the BLEU/METEOR scores on template generation.ßdummyTurker
17390ß0.0ßThanks for pointing out the inaccurate description about the citation.ßdummyTurker
17391ß0.0ßWe’ll correct it in the final version.ßdummyTurker
17392ß0.0ßThanks for your insightful comments!ßdummyTurker
17393ß0.0ßDue to space limitations we could only address the major points, but we’ll make sure to reflect all your advice in the final version.ßdummyTurker
17394ß0.0ßQ1: Are the tags and dependency trees optimized jointly or are the tags kept fixed?ßdummyTurker
17395ß0.0ßThe tags are not fixed during training and they are optimized jointly with dependency trees.ßdummyTurker
17396ß0.0ßMore clearly, we are not initializing our model with **tags**, instead we initialize the multinomial parameters of dependency trees in our model with the parameters from a pretrained TAG-DMV, then we optimize the multinomial parameters and emission parameters jointly.ßdummyTurker
17397ß0.0ßWe apologize for the confusion and will clarify this in future revisions.ßdummyTurker
17398ß0.0ßQ2: Incomplete comparison for unsupervised dependency parsing.ßdummyTurker
17399ß0.0ßWe apologize for a major miscommunication!ßdummyTurker
17400ß0.0ßOur primary intent was to compare our grammar induction method only against other methods that are trained **without** gold POS tags.ßdummyTurker
17401ß0.0ßAfter seeing the reviewer comments we have realized that the current draft may misleadingly convey a stronger claim.ßdummyTurker
17402ß0.0ßWe will absolutely correct this in the abstract, intro, and the body of the paper by making it clear that our approach does not outperform state-of-the-art grammar induction systems trained **with** gold POS.ßdummyTurker
17403ß0.0ßOur only claim of state-of-the-art performance on grammar induction is for training without gold POS.ßdummyTurker
17404ß0.0ßOverall, comparison with models trained on gold POS is not our focus.ßdummyTurker
17405ß0.0ßAgain, apologies for this oversight!ßdummyTurker
17406ß0.0ßWe believe it is easily fixed.ßdummyTurker
17407ß0.0ßMore details about this can be found in our response to Q1 of Reviewer-#1.ßdummyTurker
17408ß0.0ßFurther, we will be sure to include references to the papers you mentioned in future revisions.ßdummyTurker
17409ß0.0ßThank you for pointing them out.ßdummyTurker
17410ß0.0ßQ3: Relation to previously proposed model is not clear.ßdummyTurker
17411ß0.0ßThank you for pointing out this issue with clarity!ßdummyTurker
17412ß0.0ßThe Gaussian HMM refers to the system of Lin et al 2015.ßdummyTurker
17413ß0.0ßWe will be sure to cite this work early in the Gaussian HMM section and will explicitly cite it again in Table 1.ßdummyTurker
17414ß0.0ßFurther, we will also be careful to more explicitly summarize our contributions at the end of introduction, forward citing the fact that our projection architecture method is based on Dinh et al 2014.ßdummyTurker
17415ß0.0ßQ4: Evaluation on the full WSJ section 23.ßdummyTurker
17416ß0.0ßThanks for emphasizing this evaluation.ßdummyTurker
17417ß0.0ßWe agree that results on longer sentences may be of interest.ßdummyTurker
17418ß0.0ßWe are currently running this experiment (it requires substantially more computation and is quite expensive because we decode from words instead of gold tags) and will include results in future revisions.ßdummyTurker
17419ß0.0ßThanks for your insightful comments!ßdummyTurker
17420ß0.0ßDue to space limitations we could only address the major points, but we’ll make sure to reflect all your advice in the final version.ßdummyTurker
17421ß0.0ßQ1: Grammar induction comparisons / literature survey.ßdummyTurker
17422ß0.0ßFirst, thank you for pointing out additional work on unsupervised dependency parsing.ßdummyTurker
17423ß0.0ßWe will certainly include these in future revisions.ßdummyTurker
17424ß0.0ßSecond, we sincerely apologize for the lack of clarity and major miscommunication surrounding the comparison between our model and models trained on gold POS tags.ßdummyTurker
17425ß0.0ßOur intended comparison was between our approach and systems trained **without** gold POS and our intended claim of state-of-the-art performance was only about the **without-gold-POS** condition.ßdummyTurker
17426ß0.0ßWe see now that as written the paper may mistakenly convey that our system outperforms even state-of-the-art systems trained with gold POS.ßdummyTurker
17427ß0.0ßWe did not intend this statement and will be absolutely sure to correct it in all future revisions.ßdummyTurker
17428ß0.0ßSpecifically, we will modify the abstract and intro to reflect that our performance claims are only about the without-gold-POS condition.ßdummyTurker
17429ß0.0ßFurther, we will remove our misworded claim later on in the paper  (“reports superior results over gold tags in this standard experimental setting”) with which we intended something weaker: that our approach only achieved superior results to **the basic DMV** model trained on gold POS.ßdummyTurker
17430ß0.0ßOverall, comparison with models trained on gold POS is not our focus.ßdummyTurker
17431ß0.0ßIn the next revision we will remove all claims about gains over models trained on gold tags, and instead include in Table 2 the work with gold POS tags mentioned by the reviewer for reference to the readers, explicitly showing that our approach underperforms some models trained on gold POS.ßdummyTurker
17432ß0.0ßAgain, apologies for this major oversight!ßdummyTurker
17433ß0.0ßWe believe it is easily corrected.ßdummyTurker
17434ß0.0ßQ2: Comparison of POS induction result with CRF autoencoder.ßdummyTurker
17435ß0.0ßWe agree that a comparison with the CRF autoencoder would be very useful and will certainly add it in future drafts.ßdummyTurker
17436ß0.0ßQ3: DMV extensions test.ßdummyTurker
17437ß0.0ßThanks for this suggestion!ßdummyTurker
17438ß0.0ßWe agree that exploring more sophisticated grammar induction models (like extended DMV) in conjunction with our projection method may be interesting.ßdummyTurker
17439ß0.0ßWe will consider including this in future revisions, and will definitely explore this direction in future work.ßdummyTurker
17440ß0.0ßQ4: How would the training corpus size influence parsing accuracy?ßdummyTurker
17441ß0.0ßThis is an interesting point.ßdummyTurker
17442ß0.0ßOur model does not have drastically more parameters than the standard DMV with gold POS tags since our projection function does not have explicitly lexicalized parameters -- however, we do expect our model is somewhat higher-capacity.ßdummyTurker
17443ß0.0ßFurther, we expect the relationship between training size and the distribution of local optima may be different for our model, which would also be interesting to explore.ßdummyTurker
17444ß0.0ßQ5: Is the invertibility requirement really that important?ßdummyTurker
17445ß0.0ßThis is another great point.ßdummyTurker
17446ß0.0ßWe suspect that numerical stability of the Jacobian term and its gradient may be a limiting factor  -- something our proposed method avoids.ßdummyTurker
17447ß0.0ßFurther, computation of the determinant of Jacobian and its gradient is potentially quite costly.ßdummyTurker
17448ß0.0ßHowever, what the reviewer proposed definitely deserves empirical evaluation.ßdummyTurker
17449ß0.0ßWe will take a look at this in future experiments.ßdummyTurker
17450ß0.0ßThank you for your useful and encouraging comments.ßdummyTurker
17451ß0.0ß"-- The outliers refer to ""borderline"" sentences (in terms of complexity): we did an in-depth study on human agreement and it resulted that annotators have a higher agreement on the two opposite poles, i.e."ßdummyTurker
17452ß0.0ßvery simple and very complex sentences.ßdummyTurker
17453ß0.0ß-- In this first study we aimed to investigate the layman intuition of sentence complexity.ßdummyTurker
17454ß0.0ßNow, we are currently studying the correlation between these data and behavioral ones.ßdummyTurker
17455ß0.0ßYes, you are right, we will discuss the correlation scores more clearly.ßdummyTurker
17456ß0.0ßWe chose to use newspaper corpora for two main reasons: we did not intend to investigate sentences containing only a predefined set of linguistic complexity phenomena.ßdummyTurker
17457ß0.0ßIn addition, we used this typology of data since they have gold (manually revised) morpho-syntactic and syntactic annotation, and they allow to mitigate the textual genre interference since we have sentences taken from newspapers for both languages.ßdummyTurker
17458ß0.0ßWe agree with you that the paper contains too many data, both in terms of languages and linguistic complexity parameters.ßdummyTurker
17459ß0.0ßHowever, this choice was intentional since we aimed to investigate whether there are linguistic complexity parameters shared by typologically different languages adopting a bottom-up approach (without any a-priori selection of features of sentence complexity) for which a lot of parameters are needed.ßdummyTurker
17460ß0.0ßThank for you comments and suggestions in particular about the two papers you mentioned.ßdummyTurker
17461ß0.0ßWe selected these two parsers because they are representative of two different parsing strategies.ßdummyTurker
17462ß0.0ßThey have for example different approaches to the processing of long distance dependencies.ßdummyTurker
17463ß0.0ßIn addition, the Shift--Reduce parser follows a parsing strategy that is more similar to the human processing since it processes a sentence in one direction (usually from left to right) and it relies on local information with respect to the analysed token and on the history of all taken actions.ßdummyTurker
17464ß0.0ßIndeed there is a broad spectrum of domains, which is interesting to investigate, but they lack publicly available benchmarks for evaluation.ßdummyTurker
17465ß0.0ßCreation of such benchmark is associated with the need of large amounts of data and human resources for annotation.ßdummyTurker
17466ß0.0ßThe latter could be very challenging if we don`t restrict to a certain domain.ßdummyTurker
17467ß0.0ßWithin current experiment, crowdsourcer were tested to have understanding of the CS domain(Sec.3.6).ßdummyTurker
17468ß0.0ßWe imagine, that labeling a large number of thematically inconsistent topics at the same time, can be difficult to control/regulate and can lead to a low inter-annotator agreement rate and thus, noisy results.ßdummyTurker
17469ß0.0ß- DOI/keywords/research topics aren`t enough to evaluate an (anti)trend detection system.ßdummyTurker
17470ß0.0ßFor this, one needs a benchmark with gold data.ßdummyTurker
17471ß0.0ßThe system aims to predict its own (anti-)trend candidates (label of a cluster; paper; etc.)ßdummyTurker
17472ß0.0ßand then compares them with the ground truth to compute an accuracy score.ßdummyTurker
17473ß0.0ßThis score determines, whether the system performs well or not.ßdummyTurker
17474ß0.0ß- DOI/keywords/research topic don`t provide an information on whether the paper is a trend or anti-trend.ßdummyTurker
17475ß0.0ßOur benchmark provides this and additional information, which allows to train/evaluate a trend prediction system(Sec.3.9).ßdummyTurker
17476ß0.0ß- We compiled a list of CS trends based on the twelve publications (most popular/highly ranked) on the web.ßdummyTurker
17477ß0.0ßWe took only these trends, which were coherent (seen multiple times) across all publication.ßdummyTurker
17478ß0.0ßDue to variation as to the name of a specific trend, we create a standard name for each.ßdummyTurker
17479ß0.0ß- To our knowledge, SemanticScholar in one of the best resources for detecting (anti-)trends in CS.ßdummyTurker
17480ß0.0ßIt covers publications related to CS/biomedicine; indexes content from PubMed/ArXiv/Springer Nature, etc.ßdummyTurker
17481ß0.0ßCorpus includes more than 40mio papers from CS/biomedicine.ßdummyTurker
17482ß0.0ß- Results should be reproducible, which means we needed a large collection of publicly available documents for download.ßdummyTurker
17483ß0.0ßSemantic Scholar provides universities/organisations access to their data.ßdummyTurker
17484ß0.0ß- The exclusion of some (anti-)trends from the gold standard was caused by the content of a subset for research purposes, not by Semantic Scholar itself.ßdummyTurker
17485ß0.0ßEx., SemanticScholar provides about 1,750 publications containing word 'cryptocurrency' (results from the search on Semantic Scholar's page).ßdummyTurker
17486ß0.0ß- We conducted experiments on determining the best number of top clusters (150 top by the slope).ßdummyTurker
17487ß0.0ßDue to the paper`s space constraints, we excluded a detailed description.ßdummyTurker
17488ß0.0ßAccording to our experiments, after 150 top clusters the number of newly founded trends didn`t increase a lot/stayed stable (dependent on the abstract or full-text collection).ßdummyTurker
17489ß0.0ßFor a fair comparison, we have compared our model only with the participants from the shared task that had used the same amount of artificial data (500K triplets).ßdummyTurker
17490ß0.0ßThe winner of WMT17 APE had instead used 4M artificial data for training.ßdummyTurker
17491ß0.0ßOn the other hand, we thank the reviewer for pointing out the similarity of our attention to the Flat Attention of (Libovickỳ and Helcl, 2017).ßdummyTurker
17492ß0.0ßWe regret we were not aware of it.ßdummyTurker
17493ß0.0ßThe only difference between their attention and ours is that we do not include the learnable U matrices that project the hidden vectors onto a common space.ßdummyTurker
17494ß0.0ßHowever, their paper neither reports nor discusses any attention weight plots.ßdummyTurker
17495ß0.0ßWe will state this clearly in our paper.ßdummyTurker
17496ß0.0ßThe main goal of our experiments is simply to show that our model is as good as the best system trained with the 500K artificial data, while adding more insight around the decisions it makes thanks to its shared attention.ßdummyTurker
17497ß0.0ßTo extend the analysis of the results we have compiled additional statistics of the test sentences that in turn rely more on the src input, the mt input, or both, and we will add them to the paper using the extra page.ßdummyTurker
17498ß0.0ßIn this way, we will show which input is in focus at each time step.ßdummyTurker
17499ß0.0ßTo this aim, we have used the following thresholds:ßdummyTurker
17500ß0.0ß-	If sum_attn_src > 0.6: The model focuses more on the srcßdummyTurker
17501ß0.0ß-	If 0.4 <= sum_attn_src <= 0.6: Both inputs are significantly usedßdummyTurker
17502ß0.0ß-	If sum_attn_src < 0.4: The model focuses more on the mtßdummyTurker
17503ß0.0ßOverall, over the test set we have recorded 23% focus on the src, 31% focus on both and 45% focus on mt.ßdummyTurker
17504ß0.0ßIt is to be expected that for the majority of the sentences the model would focus on the mt if they are of sufficient quality.ßdummyTurker
17505ß0.0ßHowever, the percentage of focus on the src input is significant.ßdummyTurker
17506ß0.0ßFor the experiments, we have used the official datasets of WMT16 and WMT17.ßdummyTurker
17507ß0.0ßWhile not very extensive, these are commonly accepted benchmarks for the field.ßdummyTurker
17508ß0.0ßOur model uses a single, shared attention mechanism over the concatenated hidden vectors of the two encoded sentences to create a single context vector.ßdummyTurker
17509ß0.0ßThe two main relevant references are (Chatterjee et al., 2017) (the winner of WMT17 APE) and (Libovickỳ and Helcl, 2017).ßdummyTurker
17510ß0.0ß(Chatterjee et al., 2017) uses two separate attention mechanisms for the two inputs, and then it concatenates their respective context vectors using additional, learnable parameters.ßdummyTurker
17511ß0.0ßIn that sense, our model is simpler, yet different.ßdummyTurker
17512ß0.0ßOur main goal is to have an attention mechanism that can shed light on whether the model relies more on the src or the mt inputs at each decoding step.ßdummyTurker
17513ß0.0ßConversely, (Libovickỳ and Helcl, 2017) have proposed a very similar attention mechanism to ours which they have called the Flat Attention.ßdummyTurker
17514ß0.0ßThe only difference between their attention and ours is that we do not include the U matrices that project the hidden vectors onto a common space.ßdummyTurker
17515ß0.0ßMoreover, their paper does not report or discuss the attention weight plots.ßdummyTurker
17516ß0.0ßWe will state this more clearly in our paper.ßdummyTurker
17517ß0.0ßIndeed, our encoders do use the Bidirectional LSTM.ßdummyTurker
17518ß0.0ßIn Section 3 we generically say that the encoders are based on LSTMs.ßdummyTurker
17519ß0.0ßHowever, in the supplementary material we have included all the specifications of the model (due to the lack of space in the short paper).ßdummyTurker
17520ß0.0ßWe will certainly add the “bidirectional” attribute in the main paper to prevent misunderstandings.ßdummyTurker
17521ß0.0ßIn our paper we have not argued that our results are significantly higher than those of (Bérard et al., 2017).ßdummyTurker
17522ß0.0ßOur point is simply to show that our model is as good as the best system trained with the 500K artificial data, while adding more insight around the decisions it makes thanks to its shared attention.ßdummyTurker
17523ß0.0ßThe extra page in the final version will allow more discussion of the results.ßdummyTurker
17524ß0.0ßThe extra page in the final version will allow more discussion of the errors.ßdummyTurker
17525ß0.0ßThe focus here has been on the methods.ßdummyTurker
17526ß0.0ßThe discussion of mixed language texts was removed to focus on the main issue.ßdummyTurker
17527ß0.0ßHowever, two points are relevant hereßdummyTurker
17528ß0.0ßThe idea is to find the smallest number of characters that allows accurate predictions for a large number of languages.ßdummyTurker
17529ß0.0ßWe are trying to minimize sample size and 50 is the best we could do.ßdummyTurker
17530ß0.0ßAs states in (https://console.bluemix.net/docs/services/personality-insights/science.html#researchPrecise) the service results place the service at the cutting edge of personality inference from textual data with Average MAE 0.115 for all traits.ßdummyTurker
17531ß0.0ßUnderstanding individuals' personal values from social media word use - CSCW 2014 (https://dl.acm.org/citation.cfm?doid=2531602.2531608)ßdummyTurker
17532ß0.0ß25 Tweets to Know You: A New Model to Predict Personality with Social Media - ICWSM 2017 (https://aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15681/14833)ßdummyTurker
17533ß0.0ßWe will add those references in the Supplementary materialßdummyTurker
17534ß0.0ßThe work of Habernal and Gurevych is focused on ranking persuasive arguments where they used pairs of arguments, and aimed at ranking which argument is more persuasive.ßdummyTurker
17535ß0.0ßIn addition, they used the crowd to explain the reasons.ßdummyTurker
17536ß0.0ßIn our setting, we are looking on detecting persuasive arguments given the personality of the author and reader.ßdummyTurker
17537ß0.0ßWe will make these points clearer in the camera ready version.ßdummyTurker
17538ß0.0ß"We believe that this setting of detecting persuasive in stand-alone argument (message) is relevant and interesting for example in marketing, law, or any other application that you have a ""one time"" chance."ßdummyTurker
17539ß0.0ßHowever, we agree that study persuasiveness in a discussion (multiple arguments) is also interesting, and as we state we plan this as future work.ßdummyTurker
17540ß0.0ßWe agree and plan to add an example for the camera ready version.ßdummyTurker
17541ß0.0ßIn Change My View, the delta award is not limited to the opinion poster, and any user can actually give a delta award.ßdummyTurker
17542ß0.0ßIn our setting, we don't limit ourselves to the opinion posters (notice, that in Tan et al.ßdummyTurker
17543ß0.0ßthey do limit to OPs only).ßdummyTurker
17544ß0.0ßIt should be made clear that the random personalities were chosen from the 56K argument authors (i.e., users that wrote enough arguments so we could generate a personality for - the Personality Insights service requires around 3000 words of input (https://console.bluemix.net/docs/services/personality-insights/input.html#sufficient)).ßdummyTurker
17545ß0.0ßUsers in the system can be authors or readers (or both), and thus, we believe that randomly selecting a user from the large set of users is more representative then limit ourselves to opinion posters only.ßdummyTurker
17546ß0.0ßTan et al.ßdummyTurker
17547ß0.0ßsetting is different, as they only considered delta awards by opinion posters.ßdummyTurker
17548ß0.0ß"In addition, they also considered non ""direct arguments""."ßdummyTurker
17549ß0.0ßOur baseline model is implemented based on the relevant features from Tan’s paper using their setting as a guideline.ßdummyTurker
17550ß0.0ßWe agree with this and we will add an example for the camera ready version.ßdummyTurker
17551ß0.0ßWe experimented with different settings including the one suggested in here.ßdummyTurker
17552ß0.0ßHowever, due to the limited number of pages we decided to include the most interesting results.ßdummyTurker
17553ß0.0ßFor the camera ready version we will add this result.ßdummyTurker
17554ß0.0ßAs described in the IBM personality insights webpage, the service requires around 3000 words of input (https://console.bluemix.net/docs/services/personality-insights/input.html#sufficient).ßdummyTurker
17555ß0.0ßIn Tan et.ßdummyTurker
17556ß0.0ßal.ßdummyTurker
17557ß0.0ßthey considered only deltas awarded by opinion posters, and also considered all arguments.ßdummyTurker
17558ß0.0ßWhile in our setting, we allow any user to give a delta award, and we considered only direct arguments as we look on persuasiveness of a stand-alone argument.ßdummyTurker
17559ß0.0ßYes, the balancing of the data was done by adding direct arguments that were not awarded a delta.ßdummyTurker
17560ß0.0ßAs any user can give a delta award, there is no reason to limit ourselves to posters only, but rather to randomly extract a personality of a user which is an arguments author, and by that better represent a user in the Change My View forum (who could also be a delta giver/opinion poster).ßdummyTurker
17561ß0.0ßNote: in the following content, W=Weakness, Q=Question, AC=Additional CommentßdummyTurker
17562ß0.0ßW1: We can promise to release the code.ßdummyTurker
17563ß0.0ßIn fact, we are cleaning the code and it will be ready soon.ßdummyTurker
17564ß0.0ßW2: All the experiments are conducted on one NVIDIA GTX 1080 and we implement the model using Theano.ßdummyTurker
17565ß0.0ß"AC1: In the original setting of SemEval-ABSA, the ""gold"" aspects (or opinion targets) have been given."ßdummyTurker
17566ß0.0ßWe follow this setting in our paper.ßdummyTurker
17567ß0.0ß"AC2: Taking the sentence ""great food but the service is dreadful"" as an example (targets are ""food"" and ""service"" and the gold standard sentiments are positive and negative respectively)."ßdummyTurker
17568ß0.0ßSince LSTM-CNN and CNN cannot fully explore the target information, they tend to output same sentiments for both targets.ßdummyTurker
17569ß0.0ß"Thus, we add CPT layer to explicitly associate the target with each word in the sentence, e.g., ""food""-great and ""service""-dreadful."ßdummyTurker
17570ß0.0ßIncorporating target information into word representations may be still not enough because some general opinions will confuse the classifier.ßdummyTurker
17571ß0.0ß"For example, ""great"" will also obtain high relevance score with respect to ""service""."ßdummyTurker
17572ß0.0ßTo overcome this issue, we design a proximity-based function to filter the noises from the general opinion words and predict target sentiment more accurately.ßdummyTurker
17573ß0.0ßAC3: w^t is a single opinion target and it can contain multiple words.ßdummyTurker
17574ß0.0ßAC4: In Figure 1, different shade of the red color refers to different dimensions of the feature vector and the colors do not represent the specific feature values.ßdummyTurker
17575ß0.0ßAC5: We wish to clarify the computation in TST in Figure 2.ßdummyTurker
17576ß0.0ßAs shown in Eq (3) and Eq (2), we firstly measure the relatedness between h^t_i and h^{l}_i (i.e., the downward arrows), then we combine the h^t_i's according to the relation score for obtaining the r^t_i (i.e., the upward arrows).ßdummyTurker
17577ß0.0ß"AC6: It is shown in ""Recurrent Convolutional Neural Networks for Text Classification"" (AAAI 2015) that introducing context information from the recurrent structure can enhance the performance of CNN for text classification."ßdummyTurker
17578ß0.0ßHowever, after the processing of TST component (Eq (5)), the context information will get lost (please check the W3 & Q2 response of reviewer#3).ßdummyTurker
17579ß0.0ßTo incorporate the target features into the word representations while keeping the context information, we propose context-preserving mechanisms to combine the original features h^i and the transformed features ~h^l_i.ßdummyTurker
17580ß0.0ßAC7: Refer to the last row of Table 2 for C's setting.ßdummyTurker
17581ß0.0ßNote: in the following content, W=Weakness, Q=Question, AC=Additional CommentßdummyTurker
17582ß0.0ßW1 & W2: The suggestions for verifying the properties of different components are good and meaningful.ßdummyTurker
17583ß0.0ßWe also intend to do it.ßdummyTurker
17584ß0.0ßWe would like to mention that some manual resources are needed to find the appropriate cases for each examined property.ßdummyTurker
17585ß0.0ßW4: Thanks, we will explore them.ßdummyTurker
17586ß0.0ßTheir ideas on improving the internal structures of CNN and RNN could be helpful for us to further refine our model.ßdummyTurker
17587ß0.0ßQ1: We perform pairwise t-test between our model and the compared model.ßdummyTurker
17588ß0.0ßThe significance level is 0.05.ßdummyTurker
17589ß0.0ßWe use an approximate randomization test for significance testing (Noreen (1989).ßdummyTurker
17590ß0.0ßComputer Intensive Methods for Testing Hypotheses.ßdummyTurker
17591ß0.0ßAn Introduction)ßdummyTurker
17592ß0.0ßWe focused on the OSM domain so far but we have ideas how this can be extended for example to Freebase QA corpora in the future.ßdummyTurker
17593ß0.0ßThank you for this observation, we will add some additional information in the final paper.ßdummyTurker
17594ß0.0ßThank you for your detailed comments, we will work them into the final version.ßdummyTurker
17595ß0.0ßUsing a neural network, it truly is impossible to use DPM+R.ßdummyTurker
17596ß0.0ßA toy set small enough to work (<100 examples), would not provide any realistic insight.ßdummyTurker
17597ß0.0ßThe comparison would have to be done on a linear model that implements both DPM+R and DPM+OSL.ßdummyTurker
17598ß0.0ßSuch a system is not available to us at this time.ßdummyTurker
17599ß0.0ßWhether a certain field should exist for a query is determined by the structure of the query and certain trigger tokens (e.g.ßdummyTurker
17600ß0.0ß"""area"" for the field ""Town"")."ßdummyTurker
17601ß0.0ßThe value is taken from the arguments of the trigger tokens (e.g.ßdummyTurker
17602ß0.0ß"""area(keyval(name,'Edinburgh')) becomes ""Town"ßdummyTurker
17603ß0.0ßWe will add a paragraph to discuss this issue.ßdummyTurker
17604ß0.0ßFor example, our work could be generalized to FreebaseßdummyTurker
17605ß0.0ßWe use approximate randomization for significance testing.ßdummyTurker
17606ß0.0ßAs described, e.g., in Riezler & Maxwell (2005).ßdummyTurker
17607ß0.0ß"On some pitfalls in Automatic Evaluation and Significance Testing for MT, the so-called ""multiplicity problem"" of at least once incorrectly rejecting the null hypothesis in multiple pairwise comparisons can be counteracted by reducing the per-comparison p-value in order to achieve a small experiment-wise p-value."ßdummyTurker
17608ß0.0ßLuckily, in our case, pairwise p-values were mostly in the range of p<0.0001.ßdummyTurker
17609ß0.0ßWe used expert trees to estimate transition rate matrices.ßdummyTurker
17610ß0.0ßWe did infer the states and dates of the internal nodes, but we see them as by-products.ßdummyTurker
17611ß0.0ßWe made this explicit throughout the paper (e.g., in Figure 2).ßdummyTurker
17612ß0.0ßDetailed analysis of inferred dates and states is interesting, but no one can put everything in a single 8-page conference paper.ßdummyTurker
17613ß0.0ßIn the course of developing our framework, we did, of course, investigate the model behavior.ßdummyTurker
17614ß0.0ßWe are personally familiar with several past languages.ßdummyTurker
17615ß0.0ßWe only mentioned one case where we expected the model to fail (i.e., Old Chinese tone), but we confirmed that the model generally went well.ßdummyTurker
17616ß0.0ßThe one-page bonus would allow us to discuss this.ßdummyTurker
17617ß0.0ßAt this point, we cannot go beyond small-scale manual inspection.ßdummyTurker
17618ß0.0ßAs we discussed in Section 6, we need a collection of typological profiles of past languages.ßdummyTurker
17619ß0.0ßMarsico et al.ßdummyTurker
17620ß0.0ßwill present a database of phonological inventories from ancient and reconstructed languages in LREC 2018. http://lrec2018.lrec-conf.org/en/conference-programme/accepted-papers/ What we need is exactly a typological equivalent of this database.ßdummyTurker
17621ß0.0ßYou can see that such a database requires a huge amount of work by experts and deserves a separate paper.ßdummyTurker
17622ß0.0ßWe cannot keep our work unpublished until linguists make that happen, probably after a decade of work (WALS is a result of much longer work).ßdummyTurker
17623ß0.0ßWorse, we need to begin by letting them recognize the need of such a database.ßdummyTurker
17624ß0.0ßTo our knowledge, our paper is the first to make that explicit.ßdummyTurker
17625ß0.0ßOur paper should be placed in line of research summarized in Table 1.ßdummyTurker
17626ß0.0ßThe use of latent representations is completely new, in relation to the four previous studies.ßdummyTurker
17627ß0.0ßEven if you stick with the comparison with Murawaki (2017), statistical phylogenetic comparative methods were absent from it and related papers, as you recognize.ßdummyTurker
17628ß0.0ßThe notion of time is first introduced in our paper.ßdummyTurker
17629ß0.0ßAlso, Murawaki and Yamauchi (2018) didn't use latent representations at all.ßdummyTurker
17630ß0.0ßWe didn't cite Murawaki (2015) because our work is built on top of Murawaki (2017), which explicitly superseded Murawaki (2015).ßdummyTurker
17631ß0.0ßWe probably need to credit Murawaki (2015) with the introduction of latent representations to this topic.ßdummyTurker
17632ß0.0ßNote that Murawaki (2015) created trees, but they are nothing more than a weighted variant of clustering.ßdummyTurker
17633ß0.0ßThe notion of time is needed to do things the right way.ßdummyTurker
17634ß0.0ßRe contribution 1: see our response to Review #1 (Summary).ßdummyTurker
17635ß0.0ßWe see inferred dates as by-products.ßdummyTurker
17636ß0.0ßBut the one-page bonus would allow us to discuss them.ßdummyTurker
17637ß0.0ßWe will make our code publicly available upon acceptance.ßdummyTurker
17638ß0.0ßWe implemented statistical phylogenetic comparative methods by ourselves because no off-the-shelf software package fulfills our need.ßdummyTurker
17639ß0.0ßWe don't intend to claim their novelty.ßdummyTurker
17640ß0.0ßAnd the one-page bonus is not enough for detailed description.ßdummyTurker
17641ß0.0ßWe are considering supplementary materials as an option.ßdummyTurker
17642ß0.0ßAs for block sampling, we accumulate probability masses from bottom to top (forward filtering) and stochastically select states from top to bottom (backward sampling).ßdummyTurker
17643ß0.0ßThe first part is essentially Felsenstein's pruning algorithm.ßdummyTurker
17644ß0.0ßWe used the strict-clock model and didn't mention this because it is the default choice.ßdummyTurker
17645ß0.0ßAs you see, we don't have enough data for a relaxed-clock model to demonstrate its powers.ßdummyTurker
17646ß0.0ßYes.ßdummyTurker
17647ß0.0ßThe proposal is asymmetric and has a correction term accordingly.ßdummyTurker
17648ß0.0ßThank you for the suggestion.ßdummyTurker
17649ß0.0ßOther operators are worth trying indeed.ßdummyTurker
17650ß0.0ßIf I understand correctly, MrBayes doesn't allow a set of transition rate matrices to be tied to **a set of trees** rather than to a single tree.ßdummyTurker
17651ß0.0ßLevy and Daumé III (2011) suggested building a single world tree, but this surely introduces additional uncertainty.ßdummyTurker
17652ß0.0ßMaurits and Griffiths (2014) is similar to ours in this respect but they didn't disclose implementation details.ßdummyTurker
17653ß0.0ßWe do analyze the results, e.g.ßdummyTurker
17654ß0.0ßexamining the number of observations and actions required for localization in Section 3.2.1,ßdummyTurker
17655ß0.0ßbut we realize that the analysis is not very prominent in the current form of the paper.ßdummyTurker
17656ß0.0ßWe will split the Experiments section into separate sections for Tasks, Results, and Discussion.ßdummyTurker
17657ß0.0ßThis should emphasize the analysis better.ßdummyTurker
17658ß0.0ßThe mentioned discrepancy comes from using gold standard features, which we introduced because we found that perceptual grounding (landmark classification) was difficult.ßdummyTurker
17659ß0.0ßWe made this modeling assumption to better study other aspects of the dataset (such as localization), and we emphasize that the full task and dataset do not have this discrepancy.ßdummyTurker
17660ß0.0ßWe will incorporate the missing references into the related work section, thanks.ßdummyTurker
17661ß0.0ß- Perhaps the reviewer missed it, but we do define the landmark classes (Section 2.1), x_init (Section 3.2), and gold standard features (Section 3.2).ßdummyTurker
17662ß0.0ß- Perhaps monolithic is not the right name for this model, but one can think of it as a tourist and guide that communicate through continuous vectors.ßdummyTurker
17663ß0.0ßIt differs from the learned communication model in that 1) the latter needs to infer the order of observations from the messages and 2) it is trained end-to-end with backprop, while learned communication relies on policy gradients (which is much harder).ßdummyTurker
17664ß0.0ßThe full task brings together various problems, from landmark recognition in raw images to localization using natural language, to planning and generating language instructions.ßdummyTurker
17665ß0.0ßIn preliminary experiments, we discovered that the full navigation task is difficult for state of the art RL methods (using natural or synthetic language).ßdummyTurker
17666ß0.0ßHence, in the current work, we think that the complexity of the introduced task justifies an initial thorough study of the sub-problems in isolation, so as to facilitate exploration on the full task with a better understanding of the sub-problems.ßdummyTurker
17667ß0.0ßNote that we do report results (in Table 4) on the full navigation task by using localization models with random walks.ßdummyTurker
17668ß0.0ßThe fact that current state-of-the-art methods fail to learn on the full navigation task (but show promise on subtasks) highlights that our task is challenging and interesting to the community.ßdummyTurker
17669ß0.0ßWe will make this point more explicit.ßdummyTurker
17670ß0.0ßWe believe R1 is comparing the random baseline from Table 3 with our localization model from Table 4.ßdummyTurker
17671ß0.0ßThese two are not comparable because the results are for different tasks (full task vs localization).ßdummyTurker
17672ß0.0ßIn fact, the random performance for tourist localization is 6.25%, which is much lower than the 18.75% random performance for the full task (in the full task, agents get three attempts to reach the target location).ßdummyTurker
17673ß0.0ßWe have added this number in Table 4 now to make it clearer.ßdummyTurker
17674ß0.0ßOur localization model from human generated utterances achieves 19.32% accuracy and thus significantly outperforms the random baseline.ßdummyTurker
17675ß0.0ßWe hope that you can revise your scores in light of this observation.ßdummyTurker
17676ß0.0ßWe agree that the current 3x3 convolutions work very well for 2D grids, but we believe that this approach can easily be extended to non-uniformly spaced grids through the application of graph convolutions.ßdummyTurker
17677ß0.0ßSee our response above: random performance is 6.25%; our model achieves 19.32%.ßdummyTurker
17678ß0.0ßThis is a great observation.ßdummyTurker
17679ß0.0ßWe speculate that the number of steps taken by humans and random walk models is close because 1) annotators were not penalized for the number of steps taken and 2) the random walk model is very good at localization because it has access to gold standard features, while humans use raw perceptual input with noisy natural language communication.ßdummyTurker
17680ß0.0ßThis actually highlights the challenging nature of our dataset, as it shows that there is a lot of room for improvement on the modeling side (with non-random walks, and when taking into account perceptual input and noisy communication).ßdummyTurker
17681ß0.0ßWe agree that the biggest contribution of our work is the collection of the first large-scale dialogue dataset grounded in action and perception.ßdummyTurker
17682ß0.0ßTo our knowledge, convolutional attention masking is also novel, and our experiments (in Section 3.2.3-4) demonstrate that this mechanism improves localization performance for synthetic and natural language by respectively ~18 and ~1.5 accuracy points.ßdummyTurker
17683ß0.0ßAlthough some of our experimental results are on synthetic communication, we also show (Section 3.2.4) that our convolutional grounding mechanism increases localization performance on *human generated* utterances by ~1.5 accuracy points.ßdummyTurker
17684ß0.0ßAs mentioned to R1, we found that even state-of-the-art models for the full navigation task with natural language were difficult to train, and thus elected to focus first on studying the sub-problems in depth.ßdummyTurker
17685ß0.0ßWe will re-organize the experimental section to better decouple the main results from the supplementary material.ßdummyTurker
17686ß0.0ßWe think this is an interesting and important question.ßdummyTurker
17687ß0.0ßDuring the course of the work, we did train models with RL using task completion as reward.ßdummyTurker
17688ß0.0ßHowever, we found that these models didn't easily converge.ßdummyTurker
17689ß0.0ßWe provided additional experiments for perceptual grounding in Appendix C. Specifically, we show that a linear classifier on top of ResNet features or text recognition performs only slightly better than random (in terms of F1 score) in a multi-label classification problem.ßdummyTurker
17690ß0.0ßSome of the difficulty comes from the limited number of labels, but we also believe that a better text-recognizer could significantly improve performance.ßdummyTurker
17691ß0.0ßThe localization performance for synthetic communication is strong (achieving > 70% accuracy for T=3 actions) and much better than localization from human generated dialogues (<20%).ßdummyTurker
17692ß0.0ßThese models always communicate their observed landmarks as well as their actions, while humans do not tend to do so.ßdummyTurker
17693ß0.0ßThis makes localization harder for humans, but they might cope with this uncertainty through other mechanisms (e.g.ßdummyTurker
17694ß0.0ßbetter guidance, bias towards taking particular paths, etc).ßdummyTurker
17695ß0.0ßAlso note that the synthetic models operate under the assumption of perfect perception and thus use gold standard landmark features.ßdummyTurker
17696ß0.0ßIn addition to the other proposed changes, we aim to use the extra page in the camera ready for a detailed discussion of the differences between natural and synthetic language experiments.ßdummyTurker
17697ß0.0ßWe thank the reviewer for the encouraging comments as well as the points raised in the detailed review.ßdummyTurker
17698ß0.0ßIn addition to the simple method for emotionizing text documents to aid improved fake news identification, we believe that quantifying the utility of affective information across popular classification and clustering algorithms working over vector embeddings of documents would also add some value towards literature in the area (albeit, we admit, that the latter is not CS/ML-ish in flavor).ßdummyTurker
17699ß0.0ßWe were aware of this work, but kept it out of our radar due to its target being that of telling apart satirical news and true news - we now realize, given the reviewer comment, that it would have been useful not to.ßdummyTurker
17700ß0.0ßAs the authors point out in the introduction, satirical news is often called out to be so (so, is implicitly meant for users who read it as satire), and is expected to be less malicious (and potentially more humurous, as pointed out later on) than the kind of fake news (e.g., news about new fat-loss pill) we deal with, in our work.ßdummyTurker
17701ß0.0ßSome of the features in their work rely intimately to the nature of satirical news (e.g., absurdity and humor quantified using the nature of the last line of the article) and their only affect-related feature is observed to provide limited improvement (1 percentage point in F-score over base features, Table 2 in their paper).ßdummyTurker
17702ß0.0ßOn the other hand, our work puts affect in the centrestage and our emotionizing method is designed to be general purpose (motivated partly by a larger diversity of news sources in our fake news set) and we, in contrast to them, observe much higher classification accuracy gains with introduction of affective information, indicative of the different nature of the domains across our work and theirs.ßdummyTurker
17703ß0.0ßWe will utilize the additional page (5th page) in the accepted version to provide a detailed discussion on the work pointed out.ßdummyTurker
17704ß0.0ß"Further, as the reviewer points out, we will change the ""for the first time"" claim to generously reflect the existence of work in satirical news identification such as the paper that was pointed out."ßdummyTurker
17705ß0.0ßDue to our limited bandwidth, we had to restrict our data collection efforts to crawling and curating one dataset.ßdummyTurker
17706ß0.0ßWe have crawled some data and are considering using using crowdsourcing for assessing fakeness in order to have more datasets for future research; we acknowledge our limitation in this regard wrt this paper, but hope this could be seen as a first step nevertheless.ßdummyTurker
17707ß0.0ßFirst, we wish to point out that our heuristic to filter out all-but-one entry per word was motivated by the downstream data representation learning procedure.ßdummyTurker
17708ß0.0ßThe doc2vec model uses lexical neighborhoods to learn semantic representations, and each introduction of a lexicon word disturbs the lexical neighborhood in the document (the word gets spaced out from its actual subsequent work by the introduced label that comes in between).ßdummyTurker
17709ß0.0ßWe, towards keeping the focus of the paper on the emotion-improves-fake-news-identification and ensure that our emotionized documents could be fed into any text processing pipeline, chose not to adapt the doc2vec learner to render it robust to such neighborhood modifications.ßdummyTurker
17710ß0.0ßUnder such a situation, we chose to use just one label instead of inserting multiple labels, since that would be a good first step towards disturbing the lexical flow of the document minimally to embed affective information.ßdummyTurker
17711ß0.0ßSecond, at the threshold of 0.6 (that being where our method performs best), out of 1923 word sense entries that satisfy the threshold, our filter-out-non-best heuristic filtered out 424 entries (i.e., 22%); thus, only slightly more than one-fifth of entries were affected.ßdummyTurker
17712ß0.0ßIn the extra page in the expanded version, we will include filtering percentages for all thresholds and also include results without the filtering - we observed that removing filtering reduced the gains achieved by Acc(V') and Pur(V') slightly - we believe that is explained by the large neighborhood modifications which induces ramifications in doc2vec learning.ßdummyTurker
17713ß0.0ßYes, we will publish the dataset with free public access to aid future research in fake news detection (RefßdummyTurker
17714ß0.0ßWe will try to use another proper term for “dimension”.ßdummyTurker
17715ß0.0ßWe will add the proportion of ironic/non-ironic tweets for evaluation.ßdummyTurker
17716ß0.0ßWe will also provide the inter-annotator agreement and details of the annotation guidelines.ßdummyTurker
17717ß0.0ßWe did not compare the proposed method against other irony detection methods because our setting is a real-valued score prediction but not binary prediction of irony or not.ßdummyTurker
17718ß0.0ßWe will cite the background work in sarcasm/irony detection.ßdummyTurker
17719ß0.0ßWe will provided the details of the combined mode.ßdummyTurker
17720ß0.0ßYes, we will provide the results of significance test.ßdummyTurker
17721ß0.0ßWe will provide some references for this statement.ßdummyTurker
17722ß0.0ß"We will try to find prior work for ""dimension relation layer""."ßdummyTurker
17723ß0.0ßWe will cite your mentioned work.ßdummyTurker
17724ß0.0ßThe proposed method will be compared with dictionary-based and non-neural network methods.ßdummyTurker
17725ß0.0ßOther datasets (e.g., EmoBank) will be included for comparison.ßdummyTurker
17726ß0.0ßError/Success analysis will be provided to explain the internal and external modes in more details.ßdummyTurker
17727ß0.0ßWe will compare the proposed method with non-neural network models and using other datasets (e.g., EmoBank).ßdummyTurker
17728ß0.0ßYes, we will provide the results of significance test.ßdummyTurker
17729ß0.0ßYes, the data contains the scores from annotators with regard to the three dimensions.ßdummyTurker
17730ß0.0ßYes, your understanding is correct.ßdummyTurker
17731ß0.0ßWe will provided the details of the internal mode.ßdummyTurker
17732ß0.0ßYes, your understanding is correct.ßdummyTurker
17733ß0.0ßThe dimensionality is the same as the embeddings' dimensionality.ßdummyTurker
17734ß0.0ßThere is no previous work on VAI prediction.ßdummyTurker
17735ß0.0ßPlease see Reply to weakness argument 2.ßdummyTurker
17736ß0.0ßThe valence score is low because each sentence is an ironic text.ßdummyTurker
17737ß0.0ßThe internal mode can model the relations between dimensions before the predicted score are output, but the external cannot.ßdummyTurker
17738ß0.0ßThat is, the internal mode can feedback useful information in the process of embeddings construction.ßdummyTurker
17739ß0.0ßWe will report the MAE for the annotators.ßdummyTurker
17740ß0.0ßThe details of the annotation guidelines will also be provided.ßdummyTurker
17741ß0.0ßThanks for your review.ßdummyTurker
17742ß0.0ßWe will clarify our paper about achieving state-of-the-art performance for GEC (among classification approaches), and modify our conclusions accordingly.ßdummyTurker
17743ß0.0ßWe will also compare our models with other latest approaches (e.g., Yannakoudakis et al.ßdummyTurker
17744ß0.0ß2017; Chollampatt and Ng 2017, 2018; and others).ßdummyTurker
17745ß0.0ßIn fact we are going to make our codes public after we clean them up.ßdummyTurker
17746ß0.0ßFinally, as for Ji et al.ßdummyTurker
17747ß0.0ß2017, we are able to only find the F0.5 score of that method, and the performance numbers are not listed in that paper.ßdummyTurker
17748ß0.0ßThere are some examples that our neural models can correct but CUUI cannot.ßdummyTurker
17749ß0.0ßWe can add an example table in Appendix in the final version.ßdummyTurker
17750ß0.0ßWe will go through our paper carefully and fix all the problems.ßdummyTurker
17751ß0.0ßWe have improved perofrmance for five common error types in this paper.ßdummyTurker
17752ß0.0ßThe same methodology can be employed to other less frequent types, such as incorrect adjective/ adverb order (e.g., personally I → I personally).ßdummyTurker
17753ß0.0ßOf course, we need to first locate the target phrase correctly.ßdummyTurker
17754ß0.0ßWe expect this is feasible, again using the Stanford Corenlp toolkit.ßdummyTurker
17755ß0.0ßWe regard it as future work, and we believe our method is promising for other types.ßdummyTurker
17756ß0.0ßThanks for your review.ßdummyTurker
17757ß0.0ßWhen training our neural models, we do not use human annotated data, but generate data for training from large amount of regular English text -- we identify the target word for different types (using the Stanford Corenlp toolkit), and create the reference labels accordingly based on the word info.ßdummyTurker
17758ß0.0ßIn the validation and test process, we use the public dataset (CoNLL-2014) to evaluate our models.ßdummyTurker
17759ß0.0ßLines 84-86 refer to the training process of our neural models only.ßdummyTurker
17760ß0.0ßWe will modify it to be more clear.ßdummyTurker
17761ß0.0ßWe will add the comparison between our neural models and the classification system of Rozovskaya and Roth 2016 (43.11 F_0.5), and compare our ensemble model with other latest approaches (e.g., Yannakoudakis et al.ßdummyTurker
17762ß0.0ß2017; Chollampatt and Ng 2017, 2018).ßdummyTurker
17763ß0.0ßThe deep context model uses only the wiki dump data, and the base form of the target noun word as extra context information (see 3.4 Lemma in 'deep context model' paper).ßdummyTurker
17764ß0.0ßOur neural model is different in that it has an unified input format for all the error types.ßdummyTurker
17765ß0.0ßTable 3 shows type-specific results for three classifiers.ßdummyTurker
17766ß0.0ßThey differ in model structure and data used.ßdummyTurker
17767ß0.0ßWe think the results are comparable.ßdummyTurker
17768ß0.0ßWe have shown that replacing the classifier method in (Rozovskaya and Roth, 2016), the ensemble model can achieve a high F_0.5 score (50.16).ßdummyTurker
17769ß0.0ßWe can also combine our neural models with the SMT from Junczys-Dowmunt and Grundkiewicz 2016 to show the results the system can achieve.ßdummyTurker
17770ß0.0ßNo, our training data is sampled from both the wiki dump and COCA.ßdummyTurker
17771ß0.0ßWe can do additional experiments using just the wiki dump.ßdummyTurker
17772ß0.0ßWe got the script from the author of 'deep context model' to calculate the scores of type-specific results, and checked its correctness, since there is no official script and we want to use the same metric.ßdummyTurker
17773ß0.0ßThe numbers in Table 2 and Table 3 are calculated using this script by inputting the corrected file of our models, the CUUI system and deep context model.ßdummyTurker
17774ß0.0ßWe are also going to make our codes and scripts public after we clean them up.ßdummyTurker
17775ß0.0ßThe Stanford Corenlp toolkit can output the base form of the words in an input sentence, we just use that.ßdummyTurker
17776ß0.0ßWe would like to thank the reviewer for the extensive comments.ßdummyTurker
17777ß0.0ßWe find them extremely helpfulßdummyTurker
17778ß0.0ßThe Joint CRF and BiLSTM model was used by Lample et al.ßdummyTurker
17779ß0.0ß(2016).ßdummyTurker
17780ß0.0ß274 to 276 summarizes the techniques used.ßdummyTurker
17781ß0.0ßLines 280 onwards details the various parts of the architectures either adopted or newly tested in the experiment with the data.ßdummyTurker
17782ß0.0ßThe hyperparameters were tuned using all the 5 folds by examining the different accuracies of the models and using bias-variance trade off to get the best value to generalize the model.ßdummyTurker
17783ß0.0ßActually, per Twitter’s agreement, sets of 50k tweets can be distributed.ßdummyTurker
17784ß0.0ßFor bigger sets, you have to use the tweet IDs.ßdummyTurker
17785ß0.0ßWe are well below 50k tweets.ßdummyTurker
17786ß0.0ßThe input to the LSTM is the concatenation of the character-based representation, clitics and the features as one vector.ßdummyTurker
17787ß0.0ßThe BiLSTM works in both direction a feedforward and backward.ßdummyTurker
17788ß0.0ßThe lines indicate that the network state is context-aware and therefore there is a condition on the output given the outputs it produces at any given time.ßdummyTurker
17789ß0.0ßEldesouki et al.ßdummyTurker
17790ß0.0ßwere kind enough to provide us with the segmented dataset.ßdummyTurker
17791ß0.0ßAddressed in argument 2ßdummyTurker
17792ß0.0ßIt does have the limit that all tokens used must appear in the training data in some combination.ßdummyTurker
17793ß0.0ßMultimodal and multi-token are unrelated.ßdummyTurker
17794ß0.0ßMultimodal refers to the distribution in HSV space.ßdummyTurker
17795ß0.0ßHaving two or more peaks (or “modes”).ßdummyTurker
17796ß0.0ß“Green-ish blue” is multi-token (3 tokens in fact).ßdummyTurker
17797ß0.0ß“Green-ish” is multimodal – a peak to the left and the right of green.ßdummyTurker
17798ß0.0ß(“Green-ish” is also multi-token)ßdummyTurker
17799ß0.0ßThe reverse task is covered by existing works discussed in section 2.ßdummyTurker
17800ß0.0ßIn particular McMahan and Stone (2015) and Kawakami et al.ßdummyTurker
17801ß0.0ß(2016).ßdummyTurker
17802ß0.0ßHSV was chosen because of its channels coming close to meeting the conditional independence assumption.ßdummyTurker
17803ß0.0ßThis is (indirectly) discussed in section 3.1ßdummyTurker
17804ß0.0ßIndeed, we model the fact that differences exist within the population, but do not attempt to model individuals within the population.ßdummyTurker
17805ß0.0ßThat would be very interest future work, using demographic information.ßdummyTurker
17806ß0.0ßThis is standard usage of RNN for NLP.ßdummyTurker
17807ß0.0ßIt would indeed be good to include additional figures showing each term in insolation.ßdummyTurker
17808ß0.0ßThat would better show compositional effects.ßdummyTurker
17809ß0.0ßWe chose these figures to highlight the multimodal nature of some colors.ßdummyTurker
17810ß0.0ßDiscretising a kernel density estimate, rather than blurring the bins would be reasonable.ßdummyTurker
17811ß0.0ßThough a Von Mises kernel rather than Gaussian kernel would be needed for hue.ßdummyTurker
17812ß0.0ßThis is a good idea with simply did not occur to us.ßdummyTurker
17813ß0.0ßPerplexity on the validation set was used when selecting Ω.ßdummyTurker
17814ß0.0ßFrom a theoretical stand point, this is correct.ßdummyTurker
17815ß0.0ßAll bens should contain some nonzero amount of probability mass.ßdummyTurker
17816ß0.0ßHowever, for bins sufficiently far from any observations, that value is smaller than can be represented on a 32bit float.ßdummyTurker
17817ß0.0ß(The paper fails to mention this, thank you for bring it to our attention)ßdummyTurker
17818ß0.0ßYes, it should.ßdummyTurker
17819ß0.0ß(For reference it is the TensorFlow.jl default, Gaussian, mean 0, std dev 0.01)ßdummyTurker
17820ß0.0ßThank you additionally for your close reading of the text and your additional comments.ßdummyTurker
17821ß0.0ßThanks a lot for your comments.ßdummyTurker
17822ß0.0ßReply-to-weakness-argument-1: To the best of our knowledge, this work is the first one which focuses on the training efficiency of NMT from the data sampling aspect.ßdummyTurker
17823ß0.0ßAll of the existing similar methods, including Zhang et al., mainly focus on the BLEU performance.ßdummyTurker
17824ß0.0ßBesides the differences you mentioned, the method proposed by Zhang et al.ßdummyTurker
17825ß0.0ßdid not include any sampling strategy and just remove the sentences with high training cost.ßdummyTurker
17826ß0.0ßTheir method is only evaluated on an in-house corpus.ßdummyTurker
17827ß0.0ßIn addition, the empirical results in Figure 1 and Table 1 also indicate that our method significantly outperforms theirs in both training efficiency and BLEU.ßdummyTurker
17828ß0.0ßReply-to-weakness-argument-2: As the other reviewers mentioned, our paper belongs to the empirical/method type.ßdummyTurker
17829ß0.0ßOur target is to train NMT in a straightforward and efficient way.ßdummyTurker
17830ß0.0ßComplicated theories and algorithms will result in high training cost; therefore, we did not adopt them.ßdummyTurker
17831ß0.0ßReply-to-weakness-argument-3: As the other reviewers mentioned, we have compared with nearly all of the existing related baselines.ßdummyTurker
17832ß0.0ßEmpirical results on two typical datasets (WMT EN-DE and NIST ZH-EN) showed that the proposed method significantly outperformed the existing methods.ßdummyTurker
17833ß0.0ßThanks a lot for your comments.ßdummyTurker
17834ß0.0ßAs a small focused work, we only present the most important content in the submission.ßdummyTurker
17835ß0.0ßWe agreed with your insightful comments and would include the response to our final version.ßdummyTurker
17836ß0.0ßReply-to-weakness-argument-1: For the large corpus, we have tested the WMT EN-FR task, which contains 12 million (M) sentences.ßdummyTurker
17837ß0.0ßAfter two epochs training (300K batches), the BLEU on dev is approximately 33.5; after 4 epochs training (600K batches), the BLEU on dev is approximately 34.5; after 6 epochs training (1M batches), the BLEU on dev is approximately 35.0.ßdummyTurker
17838ß0.0ßBy using the proposed dynamic sentence sampling method, the BLEU is 35.2 and 35.6 after 600K/1M batches.ßdummyTurker
17839ß0.0ßThese results indicate that large-scale corpus still gain slight BLEU improvement after several-epoch training and the proposed method still can gain improvement over the baseline.ßdummyTurker
17840ß0.0ßWe would include it in the final version.ßdummyTurker
17841ß0.0ßReply-to-weakness-argument-2: Similarly, for the WMT EN-DE and NIST ZH-EN, if we keep training for more than 1M batches which takes 2-3 weeks, the BLEU would increase by 1.0-1.5 and differences between baseline and the proposed method would slightly decrease by 0.5-0.7 BLEU.ßdummyTurker
17842ß0.0ßBecause 7-10 days is a reasonable time for NMT training, we reported 500K batches training results in this paper.ßdummyTurker
17843ß0.0ßReply-to-question-1: For the web-scale corpora, in our opinion, if a sentence pair is not well-learned enough, it is necessary to learn it once more.ßdummyTurker
17844ß0.0ßAs you mentioned, to accelerate this judging processing, we can adopt the sentence similarities between the untrained sentences and small-sized trained sentences as the criteria for sentence sampling.ßdummyTurker
17845ß0.0ßReply-to-question-2: For the low-resource task, the training efficiency may not be the key.ßdummyTurker
17846ß0.0ßIn comparison, how to generate more available corpus is more important.ßdummyTurker
17847ß0.0ßWe would try to use the proposed method to extract some sentence pairs from comparable corpora.ßdummyTurker
17848ß0.0ßIf the training cost of some sentence pairs keeps decreasing, it would indicate that it may be potential aligned sentences.ßdummyTurker
17849ß0.0ßWe thank the reviewer for the supporting review.ßdummyTurker
17850ß0.0ßWe provide some clarifications here:ßdummyTurker
17851ß0.0ß*We agree that integrating our model with a sequence model (e.g., LSTM) for sentence representation may yield improved results.ßdummyTurker
17852ß0.0ßThis is an important direction of future work (see Lines 796-797), and we will mention it more explicitly in the paper.ßdummyTurker
17853ß0.0ß*All CCA-baselines (including ours) don't directly optimize the objective for word similarity or retrieval.ßdummyTurker
17854ß0.0ß"The ""contrastive loss"" mentioned by the reviewer was in fact used by the strong non-CCA baseline of Gella et al."ßdummyTurker
17855ß0.0ßand served as a key part of their algorithm.ßdummyTurker
17856ß0.0ßWe will add this point of comparison into the analysis.ßdummyTurker
17857ß0.0ß*We will use the extra page in the camera-ready to also emphasize the previous work that shows that visual context helps in translation tasks.ßdummyTurker
17858ß0.0ßIn fact, while we wrote this paragraph for the submission, we removed it for brevity.ßdummyTurker
17859ß0.0ßWe will return it to the paper following this suggestion.ßdummyTurker
17860ß0.0ß*The dimensionality of the visual features after PCA was set to the same dimensionality of the textual representations: 500 (retrieval) and 300 (word similarity).ßdummyTurker
17861ß0.0ßThis information was omitted from the text accidentally.ßdummyTurker
17862ß0.0ßVery constructive suggestions.ßdummyTurker
17863ß0.0ßWe for sure add more discussion along that line.ßdummyTurker
17864ß0.0ßRefer to our related response below as well.ßdummyTurker
17865ß0.0ßThank you!ßdummyTurker
17866ß0.0ßThe model in (Ahn et al.ßdummyTurker
17867ß0.0ß2016) used external knowledge for language modeling, a different task.ßdummyTurker
17868ß0.0ßWe will make this clear in revision.ßdummyTurker
17869ß0.0ßNeural nets-based NLI models need to leverage large training data, and have achieved top performance on such datasets, e.g., NLI and MultiNLI.ßdummyTurker
17870ß0.0ßSuch models may not work well by nature (as the requirement of fitting a large number of parameter) on small dataset, which is often used by conventional models.ßdummyTurker
17871ß0.0ßOn the other hand, if comparing on large datasets, no conventional models have been tested in achieving the state of the art on such dataset.ßdummyTurker
17872ß0.0ßThis is a very interesting question.ßdummyTurker
17873ß0.0ßWe will add some discussion along the line.ßdummyTurker
17874ß0.0ßThank you!ßdummyTurker
17875ß0.0ßWe concatenate the TransE relation embeddings with lexical semantic relations.ßdummyTurker
17876ß0.0ßWe will make it clearer in our revision.ßdummyTurker
17877ß0.0ßWe will add the report results in our revision.ßdummyTurker
17878ß0.0ßFor 15 semantic relation features, the result on development set of SNLI is 88.7%.ßdummyTurker
17879ß0.0ßFor concatenating TransE relation embedding with the semantic relation vectors, the accuracy is 88.6% and 88.5% on development set for two kinds of TransE relation embedding.ßdummyTurker
17880ß0.0ßYes, “Different ratios of external knowledge” means we randomly keep different percentages of whole lexical semantic relations as a knowledge vector r_ij .ßdummyTurker
17881ß0.0ßWe will address the additional comments in our revision.ßdummyTurker
17882ß0.0ß“Get the output of each task sequentially“ means that the hidden state of the RNN will be transferred from one task to next task as described in Eq.7.ßdummyTurker
17883ß0.0ßSorry for the confusion, and we will give a clearer explanation and conduct a careful proof-reading in the revision.ßdummyTurker
17884ß0.0ßAs there are no publicly available datasets released by previous works, we crawl 600,000 legal documents and construct our own dataset, which will be released for further research.ßdummyTurker
17885ß0.0ßWe will verify our model on other datasets to provide richer and more credible results.ßdummyTurker
17886ß0.0ßWe believe that the novelty of our model is on taking the logical dependencies between tasks into consideration during decoding and prediction, rather than get efficient representations for different tasks in the encoder part, which is quite common in most recent multi-task models.ßdummyTurker
17887ß0.0ßWe agree that employing more sophisticated encoder introduced by existing multi-task learning methods will be helpful, which will be explored in our future work.ßdummyTurker
17888ß0.0ßOur investigation shows that there are some courts in China that have already begun to use intelligent systems to provide reference for judgments.ßdummyTurker
17889ß0.0ßOf course, the ethical relationship between automatic judgments and real judgments is something that needs to be considered thoroughly, but we still believe that automatic judgments is a problem worthy to be studied.ßdummyTurker
17890ß0.0ßSorry for the grammatical mistakes.ßdummyTurker
17891ß0.0ßWe will conduct a careful proof-reading and correct the mistakes in our final version.ßdummyTurker
17892ß0.0ßPlease refer to R1.ßdummyTurker
17893ß0.0ßAs defined in the Formalization part(line 280) in section 3.2, t_i refers to the i-th subtask.ßdummyTurker
17894ß0.0ßIn the penalty prediction, the results are different for various charges.ßdummyTurker
17895ß0.0ßThe primary reason for the bad performance is that the data is unbalanced.ßdummyTurker
17896ß0.0ßFor some charges, there are over 100,000 related documents, while for a few others, there are only around 1,000 documents.ßdummyTurker
17897ß0.0ßMeanwhile, there are some confusing charges that are similar in nature(e.g.ßdummyTurker
17898ß0.0ßtheft and robbery) and hard to distinguish.ßdummyTurker
17899ß0.0ßThe prediction of few-shot and confusing charges will be our major future work.ßdummyTurker
17900ß0.0ßLJP makes the predictions based on the fact descriptions, without any other constrains.ßdummyTurker
17901ß0.0ßThe selected subtasks(relevant articles, charges, and terms of penalty) are three representative judgment results in the legal documents, which can be found in most criminal cases.ßdummyTurker
17902ß0.0ßBesides, we didn’t restrict the scope of subtasks and the forms of the dependencies among them.ßdummyTurker
17903ß0.0ßThus, we argue that LJP and the proposed model is not task-specific or dataset-specific.ßdummyTurker
17904ß0.0ßFor the penalty, we select it as a subtask, because it’s an important part of the judgment and involved in 75.6% convicted criminal cases, though existing models performs poor in this subtask.ßdummyTurker
17905ß0.0ßThe unsatisfactory performance may be due to the lack of certain crucial information in the legal documents.ßdummyTurker
17906ß0.0ßFor example, 47.0% criminal cases contain the circumstances of surrendering, which are not expressed in the facts.ßdummyTurker
17907ß0.0ßAs far as we know, current methods are not able to handle this situation well, and leave it a challenge for further exploration.ßdummyTurker
17908ß0.0ßWe mentioned 15 relevant papers in the related works of judgment prediction that dealt with legal text, including representative works from both NLP and legal fields.ßdummyTurker
17909ß0.0ßHowever, it is possible that some less relevant or too early works were not included.ßdummyTurker
17910ß0.0ßWe will revise it to include more recent works in revision.ßdummyTurker
17911ß0.0ßLJP consists of several subtasks, and the results from the subtasks form the whole result of LJP.ßdummyTurker
17912ß0.0ßThe joint prediction of multiple subtasks makes LJP a typical MTL task.ßdummyTurker
17913ß0.0ßCompared to conventional MTL models, our model considers the explicit logical dependencies among subtasks, and is capable of handling any form of DAG dependencies.ßdummyTurker
17914ß0.0ßPlease refer to R1.ßdummyTurker
17915ß0.0ßIn fact, we have conducted several experiments by treating penalty prediction as a regression or subsection regression problem.ßdummyTurker
17916ß0.0ßHowever, the results were very unsatisfactory, so we did not include them in our paper.ßdummyTurker
17917ß0.0ßWe agree that how to model the task of penalty prediction in a reasonable manner is challenging and will be explored in our future work.ßdummyTurker
17918ß0.0ßWe have conducted some experiments with pipeline methods(e.g., predict the law articles, charges, and penalty step by step, and feed the results of previous subtasks into later subtasks), but these methods perform worse than typical MTL methods.ßdummyTurker
17919ß0.0ßDue to space limitation, we didn’t show the detailed settings and results of these methods.ßdummyTurker
17920ß0.0ßWe will show these experiments in revision.ßdummyTurker
17921ß0.0ßThe main focus of our paper is how the logical dependencies between tasks will affect the performance.ßdummyTurker
17922ß0.0ßAs shown in Fig.3, our models use the same text representation vectors rather than task-specific representations.ßdummyTurker
17923ß0.0ß"Our findings indicate that the improved performance is mainly credited towards combining the hidden states of dependent tasks through ""task cell initialization""."ßdummyTurker
17924ß0.0ßSorry for the confusion, we will revise our paper to describe our settings more clearly.ßdummyTurker
17925ß0.0ßAs our model follows the end-to-end setting, it can also be trained with the loss of the last subtask, by removing the other classification objectives.ßdummyTurker
17926ß0.0ßThanks for your valuable suggestion.ßdummyTurker
17927ß0.0ßWe will cite these works and discuss them in revision.ßdummyTurker
17928ß0.0ßWeakness argument 1, 2ßdummyTurker
17929ß0.0ßThe Seq2Seq model does have a regular LSTM sequential neural language model, it is used implicitly, rather than explicitly, done as with our Seq2DRNN+LM.ßdummyTurker
17930ß0.0ßWe have conducted experiments on three extra pairs of dataset: CS-EN, RU-EN from News Commentary V8, and JP-EN from ASPEC corpus.ßdummyTurker
17931ß0.0ßOur Seq2DRNN+LM model is able to achieve better results than Seq2Seq and we beat Eriguchi et al on CS-EN and RU-EN.ßdummyTurker
17932ß0.0ßWe will add these to the final version.ßdummyTurker
17933ß0.0ßWeakness argument 3ßdummyTurker
17934ß0.0ßWe will include more samples in the appendix.ßdummyTurker
17935ß0.0ßQuestion 1ßdummyTurker
17936ß0.0ßThe original DRNN model was only applied to a limited NMT task, distinguishes between left and right children which we found less optimal than our model.ßdummyTurker
17937ß0.0ßAlso it operates on word level.ßdummyTurker
17938ß0.0ßAs pointed out elsewhere, our SynLM is tied into the DRNN and is entirely novel.ßdummyTurker
17939ß0.0ßThe original DRNN also does not have attention.ßdummyTurker
17940ß0.0ßOur model is more comprehensive, has less parameters while being NMT focused, has attention over tree structures (which is missing from previous syntactic NMT models), and has SynLM.ßdummyTurker
17941ß0.0ßQuestion 2ßdummyTurker
17942ß0.0ßSection 3.6 is attempting to provide more details into the training of our models.ßdummyTurker
17943ß0.0ßQuestion 3ßdummyTurker
17944ß0.0ßWe are planning to conduct research on beam search algorithms on tree structures.ßdummyTurker
17945ß0.0ßIn this paper, we only implemented beam search for our Seq2Seq baseline.ßdummyTurker
17946ß0.0ßQuestion 4ßdummyTurker
17947ß0.0ßDRNN is good at reconstructing tree structures, but human language with internal tree structures, are still used sequentially (spoken for example).ßdummyTurker
17948ß0.0ßSeq2DRNN is better at parsing because the parser only pays attention to the reconstruction of trees.ßdummyTurker
17949ß0.0ßSeq2DRNN+LM thanks to SynLM pays more attention to the sequential properties.ßdummyTurker
17950ß0.0ßAlso, the parser evaluation uses the output of Stanford parser, which is a noisy reference.ßdummyTurker
17951ß0.0ß1.We've mentioned these methods in paper.ßdummyTurker
17952ß0.0ßReinforcement learning (Ranzato et al.ßdummyTurker
17953ß0.0ß2016) and minimum risk training (Shen et al.ßdummyTurker
17954ß0.0ß2016) aim to minimize the gap between evaluation metrics and training loss.ßdummyTurker
17955ß0.0ßOur method tries to smooth the probabilities in one output sentence.ßdummyTurker
17956ß0.0ßThey are complementary between each other and not competitive.ßdummyTurker
17957ß0.0ßIn other words, our method can be applied to MLE loss as well as other training objectives, e.g., RL loss.ßdummyTurker
17958ß0.0ß2.You're right.ßdummyTurker
17959ß0.0ßSome curriculum learning methods meet the wooden barrel principle by reweighting tokens, and label smoothing meets the prediction saturation principle by neglecting tokens with large enough probability.ßdummyTurker
17960ß0.0ßOur method outperforms curriculum learning as shown in §4.ßdummyTurker
17961ß0.0ßDue to the limitation of space, we remove detailed analysis and discussion of these methods but simply mention them in §3.1.ßdummyTurker
17962ß0.0ß3.As shown in Figure 2, our method can greatly reduce the ratio of small probability while only reduce a small part of extremely big probability.ßdummyTurker
17963ß0.0ßOverall, this will result in lower MLE loss.ßdummyTurker
17964ß0.0ßAnd we will study it from the theoretical aspect in the future work.ßdummyTurker
17965ß0.0ßIn our paper, we choose TREC and 20 Newsgroups to illustrate our model works for traditional short text categorization tasks as well.ßdummyTurker
17966ß0.0ßThe performance of CNN on TREC is adopted from Kim’s work.ßdummyTurker
17967ß0.0ßAs for the 20NG, only subjects of the documents are used so the values are lower than those in other works.ßdummyTurker
17968ß0.0ßWe will add more recent work in comparison.ßdummyTurker
17969ß0.0ßAttention-free CNN performs a bit better than our model according to the F1 values but our model produces less false positives (this will be added in next version).ßdummyTurker
17970ß0.0ßHowever, attention-free CNN can not locate the position of the malicious codes.ßdummyTurker
17971ß0.0ßSince the lengths of URL vary from 5 to more 1000, it is important to know where the attack actually happens.ßdummyTurker
17972ß0.0ßYes this is one weakness that we cannot deny.ßdummyTurker
17973ß0.0ßThese URLs may contain private information so we cannot release them directly.ßdummyTurker
17974ß0.0ßWe will seek some way to release a portion of the dataset.ßdummyTurker
17975ß0.0ßThe word2vec is trained on the training samples.ßdummyTurker
17976ß0.0ßWe don’t want to predict the next words.ßdummyTurker
17977ß0.0ßWhat we want is the probability of the next words and then use them to smooth the attention values.ßdummyTurker
17978ß0.0ßStatistical significance tests will be conducted.ßdummyTurker
17979ß0.0ßIn the Keras API, there have two examples consist of CNN and LSTM.ßdummyTurker
17980ß0.0ßThe first one is used to predict the next frame of an artificially generated movie which contains moving squares.ßdummyTurker
17981ß0.0ßThis model is completely different from our model.ßdummyTurker
17982ß0.0ßThe second one is used on the IMDB sentiment classification task.ßdummyTurker
17983ß0.0ßThe model is composed a CNN and LSTM in order and trained together.ßdummyTurker
17984ß0.0ßIn our model CNN and LSTM are trained separately and the LSTM is used to generate the probability of word sequence to smooth the attention values.ßdummyTurker
17985ß0.0ßOnly train set is used for the training of LSTM.ßdummyTurker
17986ß0.0ßAs for the size of train set, We find that 8000 is enough for the detection of anomaly URLs and increasing training samples brings very little performance gain.ßdummyTurker
17987ß0.0ßIn our paper, we choose TREC and 20 Newsgroups to illustrate our model works for traditional short text categorization tasks as well.ßdummyTurker
17988ß0.0ßThe performance of CNN on TREC is adopted from Kim’s work.ßdummyTurker
17989ß0.0ßAs for the 20NG, only subjects of the documents are used so the values are lower than those in other works.ßdummyTurker
17990ß0.0ßWe will add more recent work in comparison.ßdummyTurker
17991ß0.0ßYes false negatives are more dangerous than false positives.ßdummyTurker
17992ß0.0ßWe didn’t report the details of FN and FP but our model achieves a lower FN than CNN but with higher FP.ßdummyTurker
17993ß0.0ßThe FN in our model is 179, but in CNN is 209.ßdummyTurker
17994ß0.0ßWe will add these details in next version.ßdummyTurker
17995ß0.0ßThe model we propose if actually a short text categorization algorithm.ßdummyTurker
17996ß0.0ßIt works on TREC and 20NG.ßdummyTurker
17997ß0.0ßThe attention thing is useful for the detection and visualization in sentiment analysis too.ßdummyTurker
17998ß0.0ßThe CNN and LSTM are trained separately in our work.ßdummyTurker
17999ß0.0ßIn fact you can using a Markov model to replace LSTM when smooth the attention values.ßdummyTurker
18000ß0.0ßWe thank you for the thorough reading and great suggestions.ßdummyTurker
18001ß0.0ßIndeed it is a challenge conveying the different levels of presentations; we will try to address this using a diagram in the final version.ßdummyTurker
18002ß0.0ßThank you for the 4-level ideaßdummyTurker
18003ß0.0ß"We agree that the use of the term ""gold standard"" might be misleading, as there is a whole space of valid corrections."ßdummyTurker
18004ß0.0ßWe will clarify this.ßdummyTurker
18005ß0.0ßRegarding the underlying assumption behind this framework: we do not make the claim that human rankings are necessarily less reliable than human corrections, but rather that while grammatical error correction is fairly well-defined (even if there are many valid corrections), most corrections are not clearly comparable with each other (e.g., because they address different errors, or correct one error and introduce another).ßdummyTurker
18006ß0.0ßSource: John like all bookßdummyTurker
18007ß0.0ßSystem output1: John likes all bookßdummyTurker
18008ß0.0ßSystem output2: John like all booksßdummyTurker
18009ß0.0ßWhich of the two outputs is better?ßdummyTurker
18010ß0.0ßWe argue that they are not comparable.ßdummyTurker
18011ß0.0ßWe also argue that ranking system outputs may introduce a different bias, as outputs may neglect to represent many types of errors or of valid corrections (because the systems don't produce them).ßdummyTurker
18012ß0.0ßUsing MAEGE we address both of these issues as we can compare metric rankings for sentences that are clearly comparable (one contains a sub-set of the errors of the other), and systematically traverse a space of corrections which is representative of the errors and corrections humans make.ßdummyTurker
18013ß0.0ßThanks for the suggestion; we will include additional examples in the final version.ßdummyTurker
18014ß0.0ßWe assume we are in the setting of independent corrections as this is a common assumption in GEC (and often holds in practice).ßdummyTurker
18015ß0.0ßMoreover, the gold standard edits we use to generate our corrections were compiled so as to be independent of each other as much as possible.ßdummyTurker
18016ß0.0ßIn section 7.1 we only compare scores of sentences which differ in exactly one edit.ßdummyTurker
18017ß0.0ßHuman judgment sometime judge two different outputs to be of the same quality.ßdummyTurker
18018ß0.0ßReply to the question about running I-measure on a small dataset:ßdummyTurker
18019ß0.0ßRegarding CPU time, we did run a small-scale experiment, but the estimate we got was found to be off by much (~12 CPU days).ßdummyTurker
18020ß0.0ßWe thought that our estimation would not constitute an acceptable proof if we didn't actually try all we could.ßdummyTurker
18021ß0.0ßAs there is reason to assume that the data collected by both benchmarks is invalid, using as many human judgments as possible would result with the most reliable estimation of the way a human would rank the different systems.ßdummyTurker
18022ß0.0ßBetween the different tests possible (ones using different amounts of the human judgments acquired), if both have the same significance level, this is the test that has the highest power (by the statistical sense of the word).ßdummyTurker
18023ß0.0ßWe will clarify this.ßdummyTurker
18024ß0.0ßWe had the theory that paraphrase detection is easier on the Vroniplag dataset then on the MSR Paraphrase Corpus because the training example of sentence pairs not being paraphrases are semantically very different in Vroniplag and semantically more similar in MSR Paraphrase Corpus.ßdummyTurker
18025ß0.0ßTherefore we trained a classifier to support our theory.ßdummyTurker
18026ß0.0ßThe higher evaluation scores on the Vroniplag dataset does not proof our theory but it gives the results we expected so it is at least an indication that our theory was right.ßdummyTurker
18027ß0.0ßWhen we edit the paper we will state the purpose of the evaluation more clearly.ßdummyTurker
18028ß0.0ß"Maybe ""Exploration"" is a better heading then ""Evaluation"" for this part of the paper."ßdummyTurker
18029ß0.0ßYes this is right.ßdummyTurker
18030ß0.0ßUnfortunately we cannot do better then this because we did not have the resources to annotate a significant amount of paraphrase pairs manually.ßdummyTurker
18031ß0.0ßThis argument holds for all automatically constructed datasets.ßdummyTurker
18032ß0.0ßIn the edited version we will add more examples from Vroniplag and make a manual evaluation on a small subset to better examine the suitability of the Vroniplag corpus for paraphrase detection.ßdummyTurker
18033ß0.0ßIn the edited version we can add a discussion about repurposing the data from plagiarism to paraphrases.ßdummyTurker
18034ß0.0ßWe cannot promise that we can do a cross dataset evaluation by training on vroniplag and testing on MSRPar.ßdummyTurker
18035ß0.0ßYes this is right.ßdummyTurker
18036ß0.0ßUnfortunately we cannot do better then this because we did not have the resources to annotate a significant amount of paraphrase pairs manually.ßdummyTurker
18037ß0.0ßThis argument holds for all automatically constructed datasets.ßdummyTurker
18038ß0.0ßIn the edited version we will add more examples from Vroniplag and make a manual evaluation on a small subset to better examine the suitability of the Vroniplag corpus for paraphrase detection.ßdummyTurker
18039ß0.0ßPDDB contains much more data, but only phrases.ßdummyTurker
18040ß0.0ßVroniplag contains whole sentences which is more useful in some scenarios.ßdummyTurker
18041ß0.0ßWe thought that a single classifier is enough to support our theory that Vroniplag has weaker negative examples than MSRPar.ßdummyTurker
18042ß0.0ßBut a relative frequency model would have been a good idea.ßdummyTurker
18043ß0.0ßWhen we do the manual evaluation we will add more examples.ßdummyTurker
18044ß0.0ßThe data comes from real world paraphrases so there are many types of paraphrases in the data.ßdummyTurker
18045ß0.0ßThank you for your comments and proofreading to our paper.ßdummyTurker
18046ß0.0ßReviewer 3's comments best characterize our contributionsßdummyTurker
18047ß0.0ßWe would not call our approach as an interpolation of Dai et.al.2017 and Ranzato et.al.2016 since it is derived from a clear objective function to be optimized.ßdummyTurker
18048ß0.0ßWe have shown in our supplementary material that the optimization is strictly following the objective function.ßdummyTurker
18049ß0.0ßIn general, for any machine learning algorithms, with more training data, we would expect better results.ßdummyTurker
18050ß0.0ßHowever, yet the goal of this paper is not to optimize a particular metric.ßdummyTurker
18051ß0.0ßIn general, for any sentence, if we would like to improve the CIDEr score, we can add a bad ending to it.ßdummyTurker
18052ß0.0ßThis is an essential issue of the evaluation metric itself but not a problem of the scale of training data.ßdummyTurker
18053ß0.0ßThis is why we did not think evaluation over scales of data would help us validate our motivation.ßdummyTurker
18054ß0.0ßThank you for your positive and constructive comments.ßdummyTurker
18055ß0.0ßDifferent from Ho and Ermon 2016, the regularization of policy in our framework is composed of some text generation priors like CIDEr or BLEU instead of entropy.ßdummyTurker
18056ß0.0ßWe consider our work as a first attempt using RL to regularize IM.ßdummyTurker
18057ß0.0ßThe human evaluation is done by about ten students in the lab who were informed with very detailed and concrete guidelines.ßdummyTurker
18058ß0.0ßWe will add more detailed information about this.ßdummyTurker
18059ß0.0ßDoing simple IRL may also help.ßdummyTurker
18060ß0.0ßHowever, the features used in simple IRL depend on human's knowledge about the task.ßdummyTurker
18061ß0.0ßUnfortunately, it is also difficult for human to tell why a sentence makes more sense for a lot of cases.ßdummyTurker
18062ß0.0ßAs we derived from the beginning, we use priors shown in RL as a regularization for IM.ßdummyTurker
18063ß0.0ßThus, there will be no entropy regularization.ßdummyTurker
18064ß0.0ßKL/JS indeed enjoys some simpler learning setting.ßdummyTurker
18065ß0.0ßWhereas Wasserstein may improve the stability of the learning process.ßdummyTurker
18066ß0.0ßAlthough by carefully tuning hyperparameters, different kinds of divergence could lead to similar results, Wasserstein saved us a lot of time to tune hyperparameters.ßdummyTurker
18067ß0.0ßThanks for your insightful suggestions for improving our paper.ßdummyTurker
18068ß0.0ß"The ""bad ending"" is an essential problem of RL optimization for CIDEr."ßdummyTurker
18069ß0.0ßConsider the computation in CIDEr which involves a cosine similarity.ßdummyTurker
18070ß0.0ß"Each cosine is normalized by the caption's ""length""."ßdummyTurker
18071ß0.0ßSo if a caption is longer and the additional words are common/frequent words(like 'a'), it can definitely improve the overall score.ßdummyTurker
18072ß0.0ßThis does not depend on what particular network we use.ßdummyTurker
18073ß0.0ßFor example, we also checked out a re-implementation of Rennie et.ßdummyTurker
18074ß0.0ßal.ßdummyTurker
18075ß0.0ß's approach at https://github.com/ruotianluo/self-critical.pytorch which was developed with the help of the authors.ßdummyTurker
18076ß0.0ß"We ran their code and also found that the frequent ""Bad-Ends"" phenomenon of the results."ßdummyTurker
18077ß0.0ßWe are also surprised that no one has pointed out this problem but just optimizing the scores on the leaderboard.ßdummyTurker
18078ß0.0ß"For the MLE part, we have also tried adding MLE loss to help RL (which is a widely used idea) and it improved the ""Bad-Ends"" phenomenon a little bit."ßdummyTurker
18079ß0.0ß"But we found that this suffers from the trade-off between ""Compounding Error"" and ""Bad-Ends""."ßdummyTurker
18080ß0.0ßMLE, in general, would produce worse results on different metrics than RL for image captioning.ßdummyTurker
18081ß0.0ßIn practice, as shown in (Ranzato et.al.2016), the mixed training strategy will be annealed to REINFORCE algorithm itself.ßdummyTurker
18082ß0.0ß"In this case, adding ""Bad-Ends"" would again help improving CIDEr."ßdummyTurker
18083ß0.0ß"We also tried a very small learning rate for RL-CIDEr and found that ""Bad-Ends"" problem still exists."ßdummyTurker
18084ß0.0ßFinally, rather than trying different existing networks, we believe we have carefully tuned our algorithm for RL and IM so that under the current network structure, we can produce the best results.ßdummyTurker
18085ß0.0ßDifferent from GGAN that uses a Gumbel temperature which is naturally prone to a larger vocabulary but helpless to other metrics, ours lambda=0 has a small vocabulary because our gnet doesn't use the Gumbel temperature.ßdummyTurker
18086ß0.0ßWe can compare incorporating different networks into GANs as what CGAN and GGAN did.ßdummyTurker
18087ß0.0ßEssentially they will produce similar trends when changing lambda.ßdummyTurker
18088ß0.0ßHowever, it is not the focus of this paper, and by using the simpler network Google NIC network, we can conduct extensive experiments to verify the general idea.ßdummyTurker
18089ß0.0ßAlthough the final formulation Eq.ßdummyTurker
18090ß0.0ß(11) is similar to GAN+RL, we have a solid mathematical formulation motivated by partially unknown reward.ßdummyTurker
18091ß0.0ßSimilarly, as an example, the first semi-supervised kmeans algorithm, which was published at ICML'02, was just using seed to initialize Kmeans centers.ßdummyTurker
18092ß0.0ßHowever, since it was the first paper that incorporates prior knowledge in kmeans clustering, it has been widely accepted as a good piece of work since the idea can be applied to many clustering algorithms with different metrics.ßdummyTurker
18093ß0.0ßIn our framework, we have proposed a principled way to incorporate known and unknown reward, which would inspire a lot of future work.ßdummyTurker
18094ß0.0ß"SPICE metric focuses more on object recognition and directly optimizing SPICE leads to a new phenomenon of repeated object nouns like ""man skateborad man street"" which can get high SPICE scores."ßdummyTurker
18095ß0.0ßBy combining the two, in principle, we can hack the overall metric by hacking either one of them.ßdummyTurker
18096ß0.0ßIn this paper, we only choose CIDEr because it is easier for us to use one metric as an illustration and build a consistent story of the paper.ßdummyTurker
18097ß0.0ßWe agree that both image and language are very important.ßdummyTurker
18098ß0.0ßWe will also add some related work and future work to reflect the comments.ßdummyTurker
18099ß0.0ßThank you very much for the reviews and for the very usefulßdummyTurker
18100ß0.0ßcomments.ßdummyTurker
18101ß0.0ßIn case of acceptance, we will implement your suggestions.ßdummyTurker
18102ß0.0ßAlthough, we did not experiment withßdummyTurker
18103ß0.0ßlanguages with different character sets we assume our methods wouldßdummyTurker
18104ß0.0ßwork for any language pair.ßdummyTurker
18105ß0.0ßOur proposed methods are language-pairßdummyTurker
18106ß0.0ßindependent because it does not rely on any language-specificßdummyTurker
18107ß0.0ßknowledge.ßdummyTurker
18108ß0.0ßIn the case of our delightfully simple approach MWEs in aßdummyTurker
18109ß0.0ßgiven domain can be trained for any language using monolingual corporaßdummyTurker
18110ß0.0ßand it was shown in earlier works that post-hoc mapping is alsoßdummyTurker
18111ß0.0ßlanguage independent.ßdummyTurker
18112ß0.0ßIn addition, we showed in the paper that ourßdummyTurker
18113ß0.0ßbroadly applicable method only needs good quality adapted BWEs inßdummyTurker
18114ß0.0ßorder to exploit unlabeled samples well.ßdummyTurker
18115ß0.0ßThe size of the seed lexicon has anßdummyTurker
18116ß0.0ßimpact on the quality of the mapped BWEs only.ßdummyTurker
18117ß0.0ßTheßdummyTurker
18118ß0.0ßdelightfully simple approach can exploit both general andßdummyTurker
18119ß0.0ßdomain-specific semantics in case of a smaller lexicon butßdummyTurker
18120ß0.0ßthe quality of the mapping would decrease.ßdummyTurker
18121ß0.0ßWe did not focusßdummyTurker
18122ß0.0ßon this aspect due to space limitations but based on ourßdummyTurker
18123ß0.0ßearly experiments for the sentiment task using only 1000ßdummyTurker
18124ß0.0ßword pairs results in a small performance drop (around 0.5%)ßdummyTurker
18125ß0.0ßwhile using only 100 results in a drop of about 4%.ßdummyTurker
18126ß0.0ßBy comparable we meant texts that areßdummyTurker
18127ß0.0ßfrom a similar source.ßdummyTurker
18128ß0.0ßFor example, we refer to the RepLabßdummyTurker
18129ß0.0ß"corpus as ""comparable"" because it contains English and"ßdummyTurker
18130ß0.0ßSpanish tweets that were acquired using similar query words,ßdummyTurker
18131ß0.0ßbut other then this they are unrelated.ßdummyTurker
18132ß0.0ßWe built our setupßdummyTurker
18133ß0.0ßthis way because we feel that this is close to a real wordßdummyTurker
18134ß0.0ßscenario, i.e., one would build a bilingual system usingßdummyTurker
18135ß0.0ß"source texts that are ""comparable"" to the target texts which it"ßdummyTurker
18136ß0.0ßwill be applied to.ßdummyTurker
18137ß0.0ßUsing source/target texts from a differentßdummyTurker
18138ß0.0ßsource would not break the system if the domain remains theßdummyTurker
18139ß0.0ßsame but would result in slightly lower results.ßdummyTurker
18140ß0.0ßThank you very much for the reviews and for the very usefulßdummyTurker
18141ß0.0ßcomments.ßdummyTurker
18142ß0.0ßIn case of acceptance, we will implement your suggestions.ßdummyTurker
18143ß0.0ßWe have chosen Subtitles and EuroparlßdummyTurker
18144ß0.0ßfor the two tasks respectively because we felt that althoughßdummyTurker
18145ß0.0ßthey are out-of-domain they are somewhat closer to theßdummyTurker
18146ß0.0ßtarget-domain, and thus serve as a stronger baseline for ourßdummyTurker
18147ß0.0ßexperiments compared to other corpora.ßdummyTurker
18148ß0.0ßFor example,ßdummyTurker
18149ß0.0ßSubtitles contains some slang words similar to the Twitter domain.ßdummyTurker
18150ß0.0ßUsingßdummyTurker
18151ß0.0ßEuroparl for the Twitter sentiment task resulted in lower baseline resultsßdummyTurker
18152ß0.0ßand we had higher improvements with our methods comparing to it,ßdummyTurker
18153ß0.0ßbut it would not had been a fair comparison in our opinion.ßdummyTurker
18154ß0.0ßWe kept the lexicons separate becauseßdummyTurker
18155ß0.0ßthey have a small common subset of source words which haveßdummyTurker
18156ß0.0ßdifferent target words, thus having a negative effect on theßdummyTurker
18157ß0.0ßmapping.ßdummyTurker
18158ß0.0ßWe also did not want to modify them because they were taken from someone else's work.ßdummyTurker
18159ß0.0ßIn the context of multi-lingual (or dialectal) embeddings, an intrinsic evaluation would by definition have to be multi-lingual/dialectal.ßdummyTurker
18160ß0.0ßHence, dictionary induction is our intrinsic evaluation.ßdummyTurker
18161ß0.0ßFurthermore, the meaningfulness of any intrinsic evaluation--be it dictionary induction or Mikolovian analogy constructions--depends on the downstream task.ßdummyTurker
18162ß0.0ßWe agree that downstream applications are ultimately most important and we can cite a recent publication in the camera-ready version in which we use these embeddings for morphological analysis, though we feel that discussing an additional task falls outside the scope of a short paper.ßdummyTurker
18163ß0.0ßIn fact, some of the best known related works which were much longer papers do not discuss any downstream applications, i.e.ßdummyTurker
18164ß0.0ßArtetxe et al.ßdummyTurker
18165ß0.0ß(2016, 2017), Conneau et al.ßdummyTurker
18166ß0.0ß(2017), and many of Mikolov's works.ßdummyTurker
18167ß0.0ßBoth the Extended and PP+Extended are actually models we propose, and they beat all baselines five of six times according to AnyRec.ßdummyTurker
18168ß0.0ßFurthermore, the Rec and wRec metrics demonstrate that there is in fact consistency in our results, as the PP+Extended model is the best in six of six and five of six pairs, respectively.ßdummyTurker
18169ß0.0ßThus, it seems far more likely that AnyRec is obscuring the consistency in our performance rather than Rec and wRec identifying consistent trends by chance, especially considering the flaws we identify with AnyRec:ßdummyTurker
18170ß0.0ß1) AnyRec does not evaluate how well multiple meanings of a single word can be recalled even though many words are polysemous (Habash et al 2012 finds that standard Arabic has 2.7 unique lemmas per type)ßdummyTurker
18171ß0.0ß2) AnyRec is type-based, and since words in natural language corpora exist in a Zipfian distribution, type-based evaluations disproportionally reflect accuracy over infrequent words.ßdummyTurker
18172ß0.0ßRec addresses (1) and wRec addresses (2) by being token-based.ßdummyTurker
18173ß0.0ßConsidering both Rec and wRec allows us to analyze how different techniques affect high versus low frequency words.ßdummyTurker
18174ß0.0ßFurthermore, our Rec metric is actually recall@k=5, so we are not neglecting k-best predictions.ßdummyTurker
18175ß0.0ßThat being said, most recent works unfortunately do neglect k-best predictions, namely the ones we compare to, Artetxe et al.ßdummyTurker
18176ß0.0ß(2016, 2017) and Conneau et al.ßdummyTurker
18177ß0.0ß(2017).ßdummyTurker
18178ß0.0ßWe agree with you that this is detrimental to fully understanding the results.ßdummyTurker
18179ß0.0ßThere is certainly unigram overlap in the seed and evaluation dictionary and, in section 6, we discuss how the different models perform on in-seed vs. out-seed forms.ßdummyTurker
18180ß0.0ßSince Arabic is highly ambiguous, using the seed dictionary to translate the in-seed evaluation forms does not come close to 100% accuracy, meaning that in-seed forms are non-trivial.ßdummyTurker
18181ß0.0ßThis is actually why we use recall-based metrics instead of precision@k: the evaluation dictionary cannot be expected to contain all possible valid translations of any given source word.ßdummyTurker
18182ß0.0ßWe agree that the unigram overlap in seed and evaluation dictionaries might be confusing, so we will clarify what we mentioned above in the camera-ready version and report the number of instances in the seed/evaluation dictionaries in section 3.ßdummyTurker
18183ß0.0ßYou are correct, dialect identification is integrally related to this work, but there are many solutions in the literature and, for a short paper, we felt that discussing them fell outside the scope.ßdummyTurker
18184ß0.0ßWe did explore approaches using parallel texts instead of seeds but found them to either not perform as well or require more parallel data than we had.ßdummyTurker
18185ß0.0ßWe should have mentioned this and we will be sure to do so in the camera-ready version.ßdummyTurker
18186ß0.0ßWhile you are correct, we do use and combine existing technologies, we would like to mention that our method of probabilistically identifying phrases and concatenating vectors learned using different parameters are entirely novel and lead to large gains in multi-dialectal modeling of Arabic, which had not even been attempted before.ßdummyTurker
18187ß0.0ßOur justification for the proposed evaluation metrics was brief, but section 5 demonstrates that these metrics address two shortcomings of recent works which only evaluate via AnyRec:ßdummyTurker
18188ß0.0ß1) AnyRec does not evaluate how well multiple meanings of a single word can be recalled even though many words are polysemous (Habash et al.ßdummyTurker
18189ß0.0ß(2012) finds that standard Arabic has 2.7 unique lemmas per type),ßdummyTurker
18190ß0.0ß2) AnyRec is type-based, and since words in natural language corpora exist in a Zipfian distribution, type-based evaluations disproportionally reflect accuracy over infrequent words.ßdummyTurker
18191ß0.0ßRec addresses (1) and wRec addresses (2) by being token-based.ßdummyTurker
18192ß0.0ßFurthermore, considering both Rec and wRec allows us to analyze how different techniques affect high versus low frequency words, as we mentioned in the paper.ßdummyTurker
18193ß0.0ßThis will be clarified in the camera-ready version, but we were not annotating gold data, only evaluating silver data.ßdummyTurker
18194ß0.0ßA three-person team jointly evaluated each entry to get the 89% accuracy.ßdummyTurker
18195ß0.0ßThe fact that we could identify consistent trends in our results however, is the real proof that this evaluation dictionary is meaningful.ßdummyTurker
18196ß0.0ßWe did not do any tuning on the test set.ßdummyTurker
18197ß0.0ßWe will clarify this in the camera-ready version.ßdummyTurker
18198ß0.0ßThe evaluation dictionaries are extracted from 2000 sentences, yielding about 3000 word pairs, which is actually twice as many word pairs than Artetxe et al.ßdummyTurker
18199ß0.0ß(2016) use in their test set.ßdummyTurker
18200ß0.0ßWe will have room to include the sizes of seed and evaluation dictionaries in a table in the camera-ready version.ßdummyTurker
18201ß0.0ßIn section 6 we actually do discuss all possible combinations of embedding models with the four mapping techniques (supervised and unsupervised VECMAP and MUSE).ßdummyTurker
18202ß0.0ßThe table itself displays results for both the best supervised (VECMAP) and the best unsupervised (MUSE) mapping techniques.ßdummyTurker
18203ß0.0ßNote that in contrast to SNLI, the feature based system performing keyword matching, performs poorly on MedNLI.ßdummyTurker
18204ß0.0ßMoreover, augmenting the keyword matching features with UMLS features slightly increases the score, suggesting that integrating domain-specific knowledge helps the task.ßdummyTurker
18205ß0.0ßIt is reasonable to assume that a feature based system would perform well.ßdummyTurker
18206ß0.0ßHowever, this is exactly what we do not observe in this work (see Table 3).ßdummyTurker
18207ß0.0ßWe have a detailed error analysis but couldn’t fit into the space constraints.ßdummyTurker
18208ß0.0ßGiven an additional page in the camera-ready version, we will add more analysis.ßdummyTurker
18209ß0.0ßThank you, we will fix itßdummyTurker
18210ß0.0ßCould you please provide a citation?ßdummyTurker
18211ß0.0ßGoogle Scholar yields no results.ßdummyTurker
18212ß0.0ßIf providing additional details requested in the review can address the concerns, can you please consider changing your assessment?ßdummyTurker
18213ß0.0ßWe strongly disagree that this submission is just marginally appropriate for ACL.ßdummyTurker
18214ß0.0ßThe call for papers has a special section “Resources and Evaluations” and ACL even has a “Best Resource Paper” award.ßdummyTurker
18215ß0.0ßThe goal of the paper is to present a new dataset in clinical domain and provide a set of strong state-of-the-art baselines.ßdummyTurker
18216ß0.0ßWe do not apply them to prove the quality of the corpus but to provide a baselines accuracies of a newly released dataset.ßdummyTurker
18217ß0.0ßAiming for a perfect agreement is reasonable when the task is simple (e.g.ßdummyTurker
18218ß0.0ßnamed entity recognition or even sentiment analysis).ßdummyTurker
18219ß0.0ßIn contrast, textual entailment or NLI is a task involving pragmatics and implicature, that too in a domain as nuanced as medicine.ßdummyTurker
18220ß0.0ßAs you noted, (Artsein and Poesio) by themselves argue that it is hardly possible to provide a single threshold of IIA for all possible situations.ßdummyTurker
18221ß0.0ßThus, a kappa of 0.78 indicates a good agreement ((Landis and Koch) indicate 0.61-0.8 as a substantial agreement) considering the complexity of the task of the annotation.ßdummyTurker
18222ß0.0ßOne can achieve better agreement by having multiple annotators working on the task.ßdummyTurker
18223ß0.0ßUnfortunately, we don’t have that luxury since hiring doctors for annotations is very expensive.ßdummyTurker
18224ß0.0ßMedNLI is comparable to SNLI in a sense that it poses the task of Natural Language Inference.ßdummyTurker
18225ß0.0ßIt is not, and was not meant to be, comparable to SNLI in a sense of the target domain.ßdummyTurker
18226ß0.0ßAs we shown, in the medical domain the task of NLI is much harder and we hope that this dataset facilitates new research on domain adaptation and knowledge integration.ßdummyTurker
18227ß0.0ßMIMIC-III is de-identified, so it is safely being used for research as is.ßdummyTurker
18228ß0.0ßWe further tried to minimize patients’ privacy by selecting only deceased patients.ßdummyTurker
18229ß0.0ßWe fail to see why this is a weakness.ßdummyTurker
18230ß0.0ßMedNLI drastically differs from such papers.ßdummyTurker
18231ß0.0ßThis paper specifically presents a new dataset for the CL community and makes it available for all researchers around the world.ßdummyTurker
18232ß0.0ßAs stated in the paper, MedNLI was certified by MIMIC-III to publish their “MIMIC-III derived datasets” portal.ßdummyTurker
18233ß0.0ßWe make sure that the same premise will end up in the same set.ßdummyTurker
18234ß0.0ßThe very way the dataset is constructed, it guarantees that the sets will be balanced.ßdummyTurker
18235ß0.0ßIt is possible that two premises from a medical note from one patient will end up in different sets.ßdummyTurker
18236ß0.0ßHowever, the annotators wrote three sentence without knowing any other details than the sentence itself.ßdummyTurker
18237ß0.0ßLabeling a sentence pair as entailment or neutral was the most common cause of disagreement.ßdummyTurker
18238ß0.0ßThis was mainly because once physician would think of the sentence pair as an obvious entailment while the other would think of a rare corner case.ßdummyTurker
18239ß0.0ßWe should have added this in the paper.ßdummyTurker
18240ß0.0ßWe will do so in the camera ready.ßdummyTurker
18241ß0.0ßThank you, we will fix those in the final version of the paper.ßdummyTurker
18242ß0.0ßIn light of the above arguments, we believe our work and the efforts by our clinicians deserve a much better assessment.ßdummyTurker
18243ß0.0ßCan you please consider changing your assessment?ßdummyTurker
18244ß0.0ßThe MIMIC-III database, and, as a consequence, MedNLI, contains sensitive clinical data.ßdummyTurker
18245ß0.0ßIt would be irresponsible to just publish it through unrestricted open access.ßdummyTurker
18246ß0.0ßMedNLI is available at no cost to any individual that passes the CITI training, which is not hard and can be completed in a few hours - this is the best any research group can do.ßdummyTurker
18247ß0.0ßWe apologize for misunderstanding: MetaMap is not used in any of the neural network based models.ßdummyTurker
18248ß0.0ßMetaMap-derived features are used only in the feature-based system.ßdummyTurker
18249ß0.0ßWe will make it more clear in the camera-ready version.ßdummyTurker
18250ß0.0ßWe do not claim that all feature-based systems are outperformed by neural network based systems.ßdummyTurker
18251ß0.0ßAll our conclusions concern only our specific case.ßdummyTurker
18252ß0.0ßNeural network based system do not use any of the features of the feature-based system.ßdummyTurker
18253ß0.0ßWe will additionally clarify this.ßdummyTurker
18254ß0.0ßAgain, we apologize for misunderstanding.ßdummyTurker
18255ß0.0ßNeither ESIM or InferSent use MetaMap.ßdummyTurker
18256ß0.0ßIt’s only used in the feature-based system.ßdummyTurker
18257ß0.0ßInferSent and ESIM are compared on the same dataset using the same input (two cases: GloVe embedding and MIMIC embeddings), allowing us to draw conclusions.ßdummyTurker
18258ß0.0ßWe see now that the structure of the paper might not have been clear enough: we will restructure it.ßdummyTurker
18259ß0.0ßWe will add these in the final version of the paper.ßdummyTurker
18260ß0.0ßWe say what does “0” mean only in the caption of the figure (namely, 0 means they contain the same concept) - we will add in to the text itself.ßdummyTurker
18261ß0.0ßGiven that, it should be clear now from the figure 4 that if there’s a path, it’s more likely to be longer than 1, which supports our claims.ßdummyTurker
18262ß0.0ßOf the 9 arXiv citations 7 are published in peer reviewed conferences such as ICLR and ACL.ßdummyTurker
18263ß0.0ßWe cited the arxiv versions since they are easily accessible.ßdummyTurker
18264ß0.0ßWe will also look at the papers you mentioned and cite them wherever appropriate.ßdummyTurker
18265ß0.0ßThanks for your comments.ßdummyTurker
18266ß0.0ßThe sentence function classification task is challenging in Chinese.ßdummyTurker
18267ß0.0ßAccording to our observations on the Chinese dialogue dataset, declarative and imperative responses have less explicit patterns than interrogative ones, sometimes misclassified even by human.ßdummyTurker
18268ß0.0ßWe manually investigated the error cases (62 samples) of the classifier and 48% of the errors are caused by the confusion between declarative and imperative responses.ßdummyTurker
18269ß0.0ßKindly note, that large-scale data with noisy labels are sufficient to train a response generation model in practice.ßdummyTurker
18270ß0.0ßFor instance, previous work on controllable response generation, such as (Zhou et al.ßdummyTurker
18271ß0.0ß2018, Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory.ßdummyTurker
18272ß0.0ßAAAI 2018), also annotated the large-scale dataset automatically using a classifier with ~62.3% accuracy.ßdummyTurker
18273ß0.0ßBoth automatic and manual evaluations showed that our model trained on the noisy data can achieve better performance in controlling the sentence function of generated results than the baselines.ßdummyTurker
18274ß0.0ßThe novelty of our approach mainly falls into two aspects:ßdummyTurker
18275ß0.0ß1) We use a continuous latent variable to capture the function information of responses and realize the global control in language generation.ßdummyTurker
18276ß0.0ß2) We devise a type controller to lexicalize the sentence function and the content, which explicitly addresses the compatibility issue of controlling the two aspects.ßdummyTurker
18277ß0.0ßTo the best of our knowledge, type controller is never used before.ßdummyTurker
18278ß0.0ßThe two techniques are motivated to address the global attribute of sentence function, and the compatibility of both controllable sentence functions and informative content.ßdummyTurker
18279ß0.0ßWe believe that our model is a well-integrated whole for this controlling task in dialogues, rather than just a mixture or combination of existing techniques.ßdummyTurker
18280ß0.0ßIn addition, dealing with the global control of dialogue generation is novel in comparison to other local controlling tasks.ßdummyTurker
18281ß0.0ß1:Hu, et al, 2014 focuses on sentences matching instead of documents.ßdummyTurker
18282ß0.0ßText matching problem is actually a suite of NLP problems.ßdummyTurker
18283ß0.0ßOur problem is document semantics comparison problem which can be categorized into the document level matching; however, to be clarified, semantics, in our paper, means the base level meaning of a document, which is not topic nor sentiment.ßdummyTurker
18284ß0.0ßWord2vec is not a method that we used for comparison.ßdummyTurker
18285ß0.0ßInstead, it is the fundamental of Doc2vec.ßdummyTurker
18286ß0.0ßDoc2vec is an explicit document semantics comparison method, and one of the state of the art methods for this problem utilizing neural network.ßdummyTurker
18287ß0.0ßSemantics comparison is an effective approach to examine the effectiveness of a representation.ßdummyTurker
18288ß0.0ß2:The key motivation of our method is as follows.ßdummyTurker
18289ß0.0ßIf two documents have a similar semantics, then they must have many expressions that share similar sub-semantics.ßdummyTurker
18290ß0.0ßAt the word level, word meanings contribute to semantics, while, at the sentence level, logic relationships do.ßdummyTurker
18291ß0.0ßThe graph proposed in our paper exactly captures these relationships.ßdummyTurker
18292ß0.0ßTopological persistence has been proved as an effective method to extract the key features of topological spaces, and graphs are also topological spaces.ßdummyTurker
18293ß0.0ßThen it is natural to apply topological persistence to extract features of semantics expressed by our graphs.ßdummyTurker
18294ß0.0ß3:This 50 documents dataset is the only proper dataset that we can find by this point.ßdummyTurker
18295ß0.0ßNonetheless, it is one of the most authoritative datasets used for document semantics comparison problem, e.g.ßdummyTurker
18296ß0.0ßit is one of the official datasets provided by Gensim.ßdummyTurker
18297ß0.0ßThe experimental results shows that our method can beat other state of the art methods, and performs as well as tf-idf.ßdummyTurker
18298ß0.0ßThe reason that our method did not outperform tf-idf is that WordNet+LIN is not perfect.ßdummyTurker
18299ß0.0ßThe correctness rate of LIN is 36.00%.ßdummyTurker
18300ß0.0ßMoreover, one of the key reasons that LIN did not give good performance is that the semantic web provided by WordNet is limited.ßdummyTurker
18301ß0.0ßWe have replaced WordNet+LIN with another method, and our new experimental results show that our method can outperform tf-idf significantly.ßdummyTurker
18302ß0.0ß4:Algorithm 2 is actually one of our work in progress.ßdummyTurker
18303ß0.0ßWe consider it as a future work in this paper.ßdummyTurker
18304ß0.0ßDue to the limit of time, we did not provide many details about this algorithm.ßdummyTurker
18305ß0.0ßWe will add more explanation on it in the camera ready version if our paper would be accepted.ßdummyTurker
18306ß0.0ß5:We sincerely apologize about the poor quality of writing.ßdummyTurker
18307ß0.0ßWe will fix all writing issues then.ßdummyTurker
18308ß0.0ßThanks for your incisive comments.ßdummyTurker
18309ß0.0ßYes, we totally agree that when all entity pairs have multiple instances, we only need Strategy 2.ßdummyTurker
18310ß0.0ßHowever, in Table 2 we can see that Strategy 1 performs better on single-instance entity pairs in terms of ONE, which indicates Strategy 1 is partially necessary.ßdummyTurker
18311ß0.0ßWe are also working on further investigation on the separate comparison of two strategies on single-instance and multi-instance entity pairs, respectively.ßdummyTurker
18312ß0.0ßInitial results show that Strategy 1 has significant advantage on single-instance pairs.ßdummyTurker
18313ß0.0ßWe infer that this is because for a mixed corpus, the 2-D sentence-level attention is not necessary if there is only one instance.ßdummyTurker
18314ß0.0ßAll these experimental results and analysis will be reported in the final paper.ßdummyTurker
18315ß0.0ßAdditionally, in the application of our model, we expect that it could be a hybrid model, i.e.ßdummyTurker
18316ß0.0ßStrategy 1 processes single-instance entity pairs, and Strategy 2 processes multi-instance entity pairs.ßdummyTurker
18317ß0.0ßIn this case, we might achieve better performance.ßdummyTurker
18318ß0.0ßThis will be another avenue for future work.ßdummyTurker
18319ß0.0ßWe followed some recent work using NYT as default for distance supervised RE task for a fair comparison.ßdummyTurker
18320ß0.0ßAs suggested by Reviewer, we’ve downloaded 2010/2011 KBP dataset (from 2012-11-27 and Data from EMNLP 2014) from https://nlp.stanford.edu/software/mimlre.shtml.ßdummyTurker
18321ß0.0ßDue to the time issue, we only quickly ran an experiment on a small-scale dataset extracted from “mimlre-2014-07-17-data” to compare baseline BGRU+2ATT and our MLMS-ATT-2.ßdummyTurker
18322ß0.0ßInitial results show that MLMS-ATT-2 outperforms BGRU+2ATT in terms of PR curves.ßdummyTurker
18323ß0.0ßWe have confidence that our method is robust on other distance supervised RE datasets, and the conclusion will be consistent.ßdummyTurker
18324ß0.0ßBut we will carry out extensive experiments on other datasets and we will  include these results in the final paper.ßdummyTurker
18325ß0.0ßThe focus of the reported study is on interpretation (as demonstrated through conjunction insertion).ßdummyTurker
18326ß0.0ßWe are saying that listeners get evidence for the possibly multiple concurrent relations they take to hold from both explicit connectives and inference.ßdummyTurker
18327ß0.0ßThey don't stop when they notice explicit evidence such as explicit connectives.ßdummyTurker
18328ß0.0ßAs for the NONE option, we will be clear that it is there for TWO reasonsßdummyTurker
18329ß0.0ßWe will show several ways in which the presense of concurrent relations has affected the annotation of the PDTB, and the ways in which this either CAN affect the performance of discourse parsers trained and tested on the PDTB or HAVE affected their performance.ßdummyTurker
18330ß0.0ß(As to the latter, we are aware of only one discourse parser [Pyatkin, 2016] which has NOT ignored the PDTB's annotation of concurrent relations.ßdummyTurker
18331ß0.0ß"Other discourse parsers have instead attended to only the ""first"" in the list of relations.)"ßdummyTurker
18332ß0.0ßWe agree with the reviewer that the issue of multiple discourse relations has been discussed before, with different researchers using the term to refer to different things.ßdummyTurker
18333ß0.0ßWe are also aware that the Penn Discourse TreeBank (PDTB 2.0) has been annotated sporadically with multiple discourse relations, with them annotated to a greater extent in the PDTB 3.0 [Webber et al, 2016].ßdummyTurker
18334ß0.0ß"We will add a ""Related Work"" section in which we discuss other research related to multiple discourse relations."ßdummyTurker
18335ß0.0ß"We are not sure why the reviewer does not believe we have answered the question raised at the beginning, about whether several relations can hold simultaneously, but we will re-iterate this in both ""Results"" sections in which it is relevant (i.e., Results for the ""otherwise"" and ""instead"")."ßdummyTurker
18336ß0.0ß"Although the ""after all"" and ""in other words"" parts of the study did not address multiple relations, we can show where it is visible in the participants' responses."ßdummyTurker
18337ß0.0ß"Reviewer 3 also asks whether, when we mention the ""availability"" of multiple relations, it should be"ßdummyTurker
18338ß0.0ßinterpreted as co-occurrence in the eventual Discourse Structure, or only an option available atßdummyTurker
18339ß0.0ßsome point during the processing.ßdummyTurker
18340ß0.0ß"By ""availability"", we mean that when participants (as hearers)"ßdummyTurker
18341ß0.0ßare asked to express how the two segments of the passage relate to one another, they give evidenceßdummyTurker
18342ß0.0ßfor more than one relation holding.ßdummyTurker
18343ß0.0ß"We described this as more than one relation being ""available"","ßdummyTurker
18344ß0.0ßbut perhaps there is a better, equally concise way of expressing this.ßdummyTurker
18345ß0.0ß“I could never find an explicit definition of the supervised method in this paper.ßdummyTurker
18346ß0.0ßMoreover, I found it hard to understand why the eigenvector similarity measure should work.”ßdummyTurker
18347ß0.0ßThank you for the suggestions.ßdummyTurker
18348ß0.0ßWe will add an explicit definition of the supervised method for the camera-ready version and provide more explanation about the eigenvector similarity measure.ßdummyTurker
18349ß0.0ßFor more background, you can also refer to Merris (1998), Theorem 2.1.ßdummyTurker
18350ß0.0ßWe will use the additional page to flesh this out.ßdummyTurker
18351ß0.0ßMerris, R. (1998).ßdummyTurker
18352ß0.0ßLaplacian graph eigenvectors.ßdummyTurker
18353ß0.0ßLinear algebra and its applications, 278(1-3), 221-236.ßdummyTurker
18354ß0.0ßThe main purpose of this paper is to provide favorable embeddings of both cross-lingual words and entities in a unified space, rather than to beat other EL models as in Tsai and Roth (2016) and Sil et al.ßdummyTurker
18355ß0.0ß(2018).ßdummyTurker
18356ß0.0ßThey only learn cross-lingual word representations without considering entities relying on the existing CCA-based method in Faruqui and Dyer (2014).ßdummyTurker
18357ß0.0ßBesides, a simple classifier GBRT using our embeddings achieves comparable results with Tsai and Roth (2016) and Sil et al.ßdummyTurker
18358ß0.0ß(2018) that demonstrates the effectiveness of our method in measuring similarity among cross-lingual words and entities, which is the goal of the TAC2015 experiment.ßdummyTurker
18359ß0.0ßWe highlight BWESG is the most similar baseline for us, which also uses comparable data as cross-lingual supervision, and our method achieves significant improvements.ßdummyTurker
18360ß0.0ßWe wish to report the performance of 200 dim in (Zhang et al.ßdummyTurker
18361ß0.0ß2017) but there are no exact numbers in Figure 5.ßdummyTurker
18362ß0.0ßFollow your suggestions, we shall mark it later.ßdummyTurker
18363ß0.0ßFor fairly comparison, we trained our method using the same parameters as in BWESG and Bilbowa, which are suggested in their original papers (Section 6.3), and evaluate the embeddings shared by all tasks.ßdummyTurker
18364ß0.0ßThe development set is unnecessary since we already set epoch=1 (Section 6.1) to ensure convergence, which is commonly used in Skip-gram based models (Mikolov 2013).ßdummyTurker
18365ß0.0ßWe take multilingual Wikipedia as the example KB, because it is regarded as a large enough resource in many papers, such as BWESG.ßdummyTurker
18366ß0.0ßOur method could be applicable at least in 60 languages that have the same size as Chinese and Turkish in Wikipedia (https://meta.wikimedia.org/wiki/List_of_Wikipedias).ßdummyTurker
18367ß0.0ßThis is an infrequent situation between languages.ßdummyTurker
18368ß0.0ßUsing shared representations shall be unnecessary and leads to confusion.ßdummyTurker
18369ß0.0ßwe didn't have Ja-En.ßdummyTurker
18370ß0.0ßBILBOWA, BWESG and our method have less performance reduction between large and small vocab because we all focus on overall textual semantics instead of overfitting on top-n performance.ßdummyTurker
18371ß0.0ß"""joint inference"" means the similarity measurement between words and entities that come from texts and KB, respectively."ßdummyTurker
18372ß0.0ßThat's the main goal of the TAC2015 experiment.ßdummyTurker
18373ß0.0ßToken under entity column denotes the total number of entities occurring training corpus.ßdummyTurker
18374ß0.0ßWe will make it clear.ßdummyTurker
18375ß0.0ßEnglish Wikipedia has much richer resources than other languages, which leads to fewer improvements from cross-lingual settings than those of low-resource language from English.ßdummyTurker
18376ß0.0ßWe will make it clear.ßdummyTurker
18377ß0.0ßWe greatly appreciate the helpful comments made by the reviewers regarding this paper.ßdummyTurker
18378ß0.0ßSome machine translation systems use the mixed method (word embedding + syllable embedding) for reducing the encoding and decoding burden caused by a large number of proper nouns.ßdummyTurker
18379ß0.0ßIn the chat domain, we found that the mixed method can effectively reduce the decoding burden.ßdummyTurker
18380ß0.0ßThe chat model using only word embedding failed to decode proper nouns in many times, and the chat model using only syllable embedding returned more general nouns instead of proper nouns, as follows.ßdummyTurker
18381ß0.0ß[Translated from Korean to English]ßdummyTurker
18382ß0.0ßUser’s query: Do you know Seoul?ßdummyTurker
18383ß0.0ßSystem’s response: (Word-Only) No, I did not eat it (Syllable-Only) Really?ßdummyTurker
18384ß0.0ß(Mixture) Yes, I like Seoul.ßdummyTurker
18385ß0.0ßThe dialogue corpus consists of multi-turn dialogue collected from online chat rooms.ßdummyTurker
18386ß0.0ßFor training our system, we just split the dialogue corpus into a set of single-turn dialogues.ßdummyTurker
18387ß0.0ßFor example, the dialogue sequence ‘Usser1:A - User2:B - User1:C - User2:D’ was split into ‘User1:A – User2:B’, ‘User1:C – User2:D’.ßdummyTurker
18388ß0.0ßIf we have a chance, we will add this explanation in detail.ßdummyTurker
18389ß0.0ßThank you for your good comment.ßdummyTurker
18390ß0.0ßIf we have a chance, we will add some examples with more analysis, as follows.ßdummyTurker
18391ß0.0ß[Translated from Korean to English]ßdummyTurker
18392ß0.0ßUser’s query: Do you know Seoul?ßdummyTurker
18393ß0.0ßSystem’s response: (Word-Only) No, I did not eat it (Syllable-Only) Really?ßdummyTurker
18394ß0.0ß(Mixture) Yes, I like Seoul.ßdummyTurker
18395ß0.0ßUser’s query: Talk back again?ßdummyTurker
18396ß0.0ßSystem’s response: (Single-Step) Non-grammatical sentence (LM) Yes, I do (Two-Step) I hate to say that.ßdummyTurker
18397ß0.0ßThank you for your valuable comment.ßdummyTurker
18398ß0.0ßTo justify auto evaluations, we trained all systems one more, and the proposed system outperformed the comparison systems in the additional experiment.ßdummyTurker
18399ß0.0ßThe followings show average performances in two times of experiments.ßdummyTurker
18400ß0.0ß[Performances of Content words]ßdummyTurker
18401ß0.0ßBLUE: (Word-Only) 0.465 (Syllable-Only) 0.411 (Mixture) 0.488ßdummyTurker
18402ß0.0ßROUGE-1: (Word-Only) 0.365 (Syllable-Only) 0.402 (Mixture) 0.425ßdummyTurker
18403ß0.0ßROUGE-L: (Word-Only) 0.386 (Syllable-Only) 0.384 (Mixture) 0.418ßdummyTurker
18404ß0.0ßTo justify human evaluations, we calculated Fleiss’ Kappa values, and the values were 0.3954 for syntactic score and 0.3774 for semantic score that indicate fair agreement, although it is by no means universally accepted.ßdummyTurker
18405ß0.0ßWe will add this Kappa value with some explanations.ßdummyTurker
18406ß0.0ßAs responded in argument 3, we will add it.ßdummyTurker
18407ß0.0ßPease refer to the response of argument 1.ßdummyTurker
18408ß0.0ßLi et.al.ßdummyTurker
18409ß0.0ß(2009, 2010a, 2010b) had investigated the historical term alignment problem since 2009.ßdummyTurker
18410ß0.0ßTheir work mainly explored the co-occurrence information to extract term translation pairs.ßdummyTurker
18411ß0.0ßWe only cited their representative work (Li et.ßdummyTurker
18412ß0.0ßal., 2010a) and did not report their performance on our dataset due to the length constraint.ßdummyTurker
18413ß0.0ßWe both ran the experiment on Shiji.ßdummyTurker
18414ß0.0ßHowever, our test corpora are not exactly the same since we removed some incorrect sentence pairs and added some sentence pairs into the parallel corpora.ßdummyTurker
18415ß0.0ßWe can draw a comparison not strictly.ßdummyTurker
18416ß0.0ßThe accuracy of their methods is higher but the number of extracted term translation pairs is far smaller than our method.ßdummyTurker
18417ß0.0ßWe will implement their method on our corpora and make some comparison with our method in the next version of our manuscript.ßdummyTurker
18418ß0.0ßXiuying Li, Chao Che, Limin Han and Xiaoxia Liu.ßdummyTurker
18419ß0.0ßExtracting Historical Terms Based on Aligned Chinese-English Parallel Corpora [C].ßdummyTurker
18420ß0.0ßProceedings of International conference on nature language processing and knowledge engineering.ßdummyTurker
18421ß0.0ßDalian:IEEE, 2009.ßdummyTurker
18422ß0.0ß978-982.ßdummyTurker
18423ß0.0ßXiuying Li, Chao Che, Hongfei Lin, Rongpei Wang.ßdummyTurker
18424ß0.0ß2010a.ßdummyTurker
18425ß0.0ßTerm Translation Pair Alignment Based on a Parallel Corpus of Chinese Historical Classics and Their English Translations.ßdummyTurker
18426ß0.0ßJournal of Information & Computational Science 7: 9: 1967–1986ßdummyTurker
18427ß0.0ßXiuying Li, Chao Che, Xiaoxia Liu, Hongfei Lin, Rongpei Wang.ßdummyTurker
18428ß0.0ß2010b.ßdummyTurker
18429ß0.0ßCorpus-based extraction of Chinese historical term translation equivalents.ßdummyTurker
18430ß0.0ßInternational Journal of Asian Language Processing, 20(2):63-74.ßdummyTurker
18431ß0.0ßIn our original manuscript, we actually listed some comparative results on the abbreviation issue in a figure.ßdummyTurker
18432ß0.0ßBecause of its big size and overlap with table 2, we removed it from the final version.ßdummyTurker
18433ß0.0ßI also believe that adding some examples of misaligned abbreviation can help understand our idea more clearly.ßdummyTurker
18434ß0.0ßMaybe, we can give some examples in the form of a table, which does not take up much space.ßdummyTurker
18435ß0.0ßApologized for the bad writing of section 4.3.ßdummyTurker
18436ß0.0ßIt is wired to list the results for term alignment and word segmentation in Table 3 and Line 372.ßdummyTurker
18437ß0.0ßI agree with the reviewer that this is not our contribution to the paper.ßdummyTurker
18438ß0.0ßMaybe this section could be removed to leave enough space to illustrate the main contributions of the paper.ßdummyTurker
18439ß0.0ßThe low recall is compared to the precision of the word segmentation.ßdummyTurker
18440ß0.0ßThe low recall is the direct reason for the low recall in term alignment.ßdummyTurker
18441ß0.0ßIf the word is not segmented correctly, we cannot obtain its correct translation.ßdummyTurker
18442ß0.0ßWe have employed Jieba and Stanford word segmentation method on our corpora, whose F1 scores are 61.9% and 66.1%, respectively.ßdummyTurker
18443ß0.0ßThis is much lower than the 74.8% of WSWA method.ßdummyTurker
18444ß0.0ßHence, they were not employed in term alignment experiments.ßdummyTurker
18445ß0.0ßThank you for your thoughtful comments.ßdummyTurker
18446ß0.0ßThank you very much for bringing D-NOMINATE to our attention.ßdummyTurker
18447ß0.0ßWe will describe that work and how our work relates to it.ßdummyTurker
18448ß0.0ßargument 2: We agree that D-NOMINATE is a good reference point.ßdummyTurker
18449ß0.0ßHowever, our scenario is different from congressional debate where politicians might vote according to their party membership.ßdummyTurker
18450ß0.0ßIn section 5.2 we discuss that STS methods result in very low correlation coefficients, while the trained models obtain much better coefficients.ßdummyTurker
18451ß0.0ßTo better understand what textual properties of two assertions cause judgment similarity, it is necessary to annotate and experimentally control typed relationships (e.g.ßdummyTurker
18452ß0.0ßparaphrases, entailment) of pairs of assertions.ßdummyTurker
18453ß0.0ßWe plan to do that in future work.ßdummyTurker
18454ß0.0ßIn the first phase, 69 people participated and generated the 2,243 assertions on 16 issues.ßdummyTurker
18455ß0.0ßIn the second phase, 230 subjects participated and judged the previously collected assertions.ßdummyTurker
18456ß0.0ßThese participants were free to judge as many assertions on as many issues as they want.ßdummyTurker
18457ß0.0ßOn average each assertion is judged by about 45 persons and each participant judged over 400 assertions.ßdummyTurker
18458ß0.0ßConsequently, there are a number of missing values in the matrix.ßdummyTurker
18459ß0.0ßWe encode them with zeroes.ßdummyTurker
18460ß0.0ßHowever, we did some experiments with other similarity metrics that only consider the overlap of the vectors.ßdummyTurker
18461ß0.0ßThe results were highly correlated, which is why we went for the more standard cosine metric.ßdummyTurker
18462ß0.0ßWe will describe these and additional data details in the paper.ßdummyTurker
18463ß0.0ßIf given the chance, we will discuss our results in the light of roll call voting prediction, a task that is indeed very related.ßdummyTurker
18464ß0.0ßHowever, there are also clear differences because for politicians there seems to be an incentive to vote like their party colleagues.ßdummyTurker
18465ß0.0ßPoliticians often forge inner-party accounts and so it is not surprising that -- especially in a two-party system such as the US -- there are only few latent factors that determine the voting behavior.ßdummyTurker
18466ß0.0ßsee weakness argument 3ßdummyTurker
18467ß0.0ßWe agree that statistical hypothesis testing allows to make stronger claims based on the findings and we would be happy to include it in the final version.ßdummyTurker
18468ß0.0ßHowever, considering that we have collected over 100,000 judgments, it is likely that slight differences already result in a significant p-value.ßdummyTurker
18469ß0.0ßThanks for pointing out this literature.ßdummyTurker
18470ß0.0ßWe cut related work on target-dependent sentiment analysis, stance detection and argument mining due to space constraints, but will include this in the final version of the paper if accepted (as we would have another extra page).ßdummyTurker
18471ß0.0ßAs Pearson’s r is defined in a probabilistic space it cannot be averaged directly.ßdummyTurker
18472ß0.0ßTherefore, we first z-transform the scores, average them and then transform them back into the original space.ßdummyTurker
18473ß0.0ßFor the SVM we used a linear kernel with C=100.ßdummyTurker
18474ß0.0ßIterative experiments showed that this configuration gave the most stable results across all issues.ßdummyTurker
18475ß0.0ß"For the direct regression we used nu-SVR (""traditional"" approach) and a CNN with additional dense layers and a single node with linear activation at the end (the neural approach)."ßdummyTurker
18476ß0.0ßWe will include these details  if accepted.ßdummyTurker
18477ß0.0ßWe have provided the issue-specific results in the additional material.ßdummyTurker
18478ß0.0ßWe provided the same baseline for each issue to be able to compare the obtained scores.ßdummyTurker
18479ß0.0ßWe will explore issue-specific methods in future work.ßdummyTurker
18480ß0.0ßWe agree.ßdummyTurker
18481ß0.0ß"In fact, we state this point in the future work section ""we want to explore how robust the learned models are by examining whether they can be transferred between issues.""."ßdummyTurker
18482ß0.0ßWe will rephrase it to make it clearer.ßdummyTurker
18483ß0.0ßThank you for your time and insightful comments.ßdummyTurker
18484ß0.0ßQ1:  bilinear term...ßdummyTurker
18485ß0.0ßRESCAL and other Knowledgebase embedding methods learn word/entity embeddings as well as relational embeddings for the relations that exist between two words/entities simultaneously such that some (bilinear or non-bilinear) score function is optimised.ßdummyTurker
18486ß0.0ßOn the other hand, the settings that we consider in the paper are relational representation methods that do not learn relation-specific embeddings but use pre-trained word embeddings via some operator (bilinear) to compose relational representations.ßdummyTurker
18487ß0.0ßWord embeddings are trained using co-occurrence statistics (global or local), ignoring relational embeddings.ßdummyTurker
18488ß0.0ßTherefore, implications of our analysis do not entail methods that learn relational embeddings, such as RESCAL, DistMult, etc.ßdummyTurker
18489ß0.0ßQ2: Validity of relational independenceßdummyTurker
18490ß0.0ßRelational independence is assuming that the relationship between two words is independent of that between the different pair of words.ßdummyTurker
18491ß0.0ßFor example, (ostrich, is-a-large, bird) is independent of (Trump, president-of, USA).ßdummyTurker
18492ß0.0ßThis assumption is similar to the i.i.d.ßdummyTurker
18493ß0.0ßassumption commonly used in machine learning, and enables us to decompose the total loss (expectation) over that of individual relational instances.ßdummyTurker
18494ß0.0ßQ3: On E_{p+}[h_i t_i] = 1ßdummyTurker
18495ß0.0ßPlease note that the expectation is taken over relational instances (word pairs) for a particular (fixed) dimension (i-th dimension in this case).ßdummyTurker
18496ß0.0ßThe self-correlation corr(h_i, t_i) = 1 because the correlation of the i-th dimension with itself will be always 1 (irrespective of any of the assumptions).ßdummyTurker
18497ß0.0ßThen from Eq.ßdummyTurker
18498ß0.0ß(6) it follows that E_{p+}[h_i t_i] = 1 because E_{p+}[h_i] = E_{p+}[t_i] = 0 from the standerdidastion assumption.ßdummyTurker
18499ß0.0ßA similar argument can be made using second-order covariances to compute E_{p+}[h_i^2 t_i^2] and that the overall loss is independent of the tensor A.ßdummyTurker
18500ß0.0ßDue to the limited availability of space, we had to omit few steps in the proof.ßdummyTurker
18501ß0.0ßHowever, with the extra page in the final version, we will make these steps more explicit.ßdummyTurker
18502ß0.0ßQ4: Regularisation of P and Q...ßdummyTurker
18503ß0.0ßIn our experiments, we tried both strict parametrisation (P = pI, Q = qI) as well as regularisation (||P-pI||, ||Q-qI||) but the empirical performance was similar.ßdummyTurker
18504ß0.0ßFor simplicity sake, we stated the former in the paper.ßdummyTurker
18505ß0.0ßUpon acceptance, we will release code/data such that anyone can try different embeddings and regularisation schemes to empirically test the theoretical claims we make in the paper.ßdummyTurker
18506ß0.0ßExperiment design is biased towards claims.ßdummyTurker
18507ß0.0ßWe respectfully disagree.ßdummyTurker
18508ß0.0ßTheorem 1 does not require A, P, Q to be regularised and only make a claim about the non-dependence on tensor A. Regularisation is introduced in Sec 3.1 to show a connection to PairDiff.ßdummyTurker
18509ß0.0ßIn our experiments, we use regularisation only to avoid overfitting due to the large number of parameters (a tensor cubic in the embedding dimensionality plus two matrices).ßdummyTurker
18510ß0.0ßIn fact, the regularisation parameters are set to 1 in all of our experiments and no tuning is done.ßdummyTurker
18511ß0.0ßConsidering the magnitude of the losses involved this turns out to be a very mild form of regularisation.ßdummyTurker
18512ß0.0ßWe will add these additional details and show the effect of not performing regularisation in the final version of the paper.ßdummyTurker
18513ß0.0ßSee response to Q1.ßdummyTurker
18514ß0.0ßmodelling relations as vectors and measuring relational strength using Euclidean distance.ßdummyTurker
18515ß0.0ßWe fully agree with your concern here.ßdummyTurker
18516ß0.0ßWe are aware that there are many other relation representation methods and relational strength measurement methods besides what we have considered in the paper.ßdummyTurker
18517ß0.0ßOur goal in this paper is to focus on one of the many (yet arguably the most popular) settings.ßdummyTurker
18518ß0.0ßRepresenting relations using matrices or tensors are less popular due to their parameter scalability issues.ßdummyTurker
18519ß0.0ßOur analysis still holds with l1 distance (Euclidean loss is replaced with hinge loss).ßdummyTurker
18520ß0.0ßWe can show this analysis in an appendix of the final version or an extended journal version of this work.ßdummyTurker
18521ß0.0ßBut more importantly, we sincerely hope our “noble” motivation will be taken seriously by the NLP community and similar analysis will be conducted in follow-up work for different types of relation representations and strength measures.ßdummyTurker
18522ß0.0ßQ: Is LSA by design uncorrelated cross-dimensionally?ßdummyTurker
18523ß0.0ßIf the top singular values are all distinct, then this would be the case.ßdummyTurker
18524ß0.0ßHowever, pathological cases could exist with rank deficit settings or LSA is computed by raising singular values to some power as done in practice.ßdummyTurker
18525ß0.0ßHowever, we are unaware of such an analysis of LSA and this is indeed an interesting point for further analysis.ßdummyTurker
18526ß0.0ßDoing such an analysis on other types of word embeddings (SGNS, CBOW, GloVe etc.)ßdummyTurker
18527ß0.0ßwould also be interesting (but challenging compared to LSA).ßdummyTurker
18528ß0.0ßQ: Just how close to 0 are the norms of A at the solution and how did you tune the regularization strength on A?ßdummyTurker
18529ß0.0ßNo tuning of the regularisation coefficient is done.ßdummyTurker
18530ß0.0ßSee the response to the “Weakness Argument 1” above.ßdummyTurker
18531ß0.0ßQ: If I understand correctly, (8) says E[h_i h_j] is 0, but this doesn't mean we can't have correlations between h_i and h'_j for different vectors (conditioned on p+), right?ßdummyTurker
18532ß0.0ßIf i and j are not equal, then E_{p+}[h_i h’_j] will also be zero from the uncorrelation assumption because the expectations are computed over relational instances (word-pairs created by sampling from the same vocabulary of words).ßdummyTurker
18533ß0.0ßPlease see also the response to Q3 above.ßdummyTurker
18534ß0.0ßTypos: l_{1,2} norm.ßdummyTurker
18535ß0.0ßWe meant l_1 or l_2 norm here.ßdummyTurker
18536ß0.0ßWe are not convinced of your review, because you ignored a very important contribution of our paper.ßdummyTurker
18537ß0.0ß"That is our paper belongs to ""replication studies,"" which is one of the ""other contributions"" in the"ßdummyTurker
18538ß0.0ßreview form.ßdummyTurker
18539ß0.0ßIn particular, we successfully replicate previously proposed domain adaptation techniquesßdummyTurker
18540ß0.0ßon the Transformer, also with a multilingual and multi-domain extension.ßdummyTurker
18541ß0.0ßSorry for this unclarity due to the lack of space, but we followed the settings in Chu et al., 2017.ßdummyTurker
18542ß0.0ß"""Regarding development data, for fine tuning, an out-of-domain development set is first used for training"ßdummyTurker
18543ß0.0ßthe out-of-domain NMT model, then an in-domain development set is used for fine tuning; For multi-domain,ßdummyTurker
18544ß0.0ßa mix of in-domain and out-of-domain development sets are used; For mixed fine tuning, an out-of-domainßdummyTurker
18545ß0.0ßdevelopment set is first used for training the out-of-domain NMT model, then a mix of in-domain andßdummyTurker
18546ß0.0ß"out-of-domain development sets are used for mixed fine tuning."""ßdummyTurker
18547ß0.0ßThe difference is that there are multiple and multilingual out-of-domain development sets in our paper,ßdummyTurker
18548ß0.0ßand thus we simply mixed them for tuning.ßdummyTurker
18549ß0.0ßDid you mean simply concatenate the in- and out-of-domain data?ßdummyTurker
18550ß0.0ßThe multi-domain method in our paperßdummyTurker
18551ß0.0ßis essentially a more sophisticated version of the simple concatenation, but with oversampling andßdummyTurker
18552ß0.0ßlanguage and domain tags.ßdummyTurker
18553ß0.0ßAccording to our preliminary experiments, multi-domain works better thanßdummyTurker
18554ß0.0ßsimple concatenation.ßdummyTurker
18555ß0.0ßBut sure, we can add this baseline into our paper.ßdummyTurker
18556ß0.0ßThis is due to the fact that fine tuning was conducted on the in-domain data only.ßdummyTurker
18557ß0.0ßUsing this in-domainßdummyTurker
18558ß0.0ßmodel to translate out-of-domain data gives significantly low performance.ßdummyTurker
18559ß0.0ßIssue #1 + Comments: Could the authors also present results for 100 tokensßdummyTurker
18560ß0.0ßThe reduction factor between types and tokens for most languages means that 500 tokens is often very close to 100 types, so empirically we do not expect a big discrepancy between the reported numbers for that scenario (this is part of why we choose 500/1k/5k token splits, as they loosely corresponded to typical type-level partitions).ßdummyTurker
18561ß0.0ßBut we agree that it would be good to have those numbers published, and we will add an analogous type-level version of our results table in the appendix for the camera-ready version.ßdummyTurker
18562ß0.0ß(We aggregate some concerns listed multiple times or by multiple reviews, and reference these replies at later points.ßdummyTurker
18563ß0.0ßWe appreciate the comments and the feedback on parts of the paper you’ve found unclear has been very helpful.)ßdummyTurker
18564ß0.0ßIssue #1 & 3.ßdummyTurker
18565ß0.0ß“Figure 1 is not clear or connected to the text” / “The paper would benefit from an overview of the system architecture.”ßdummyTurker
18566ß0.0ßWe feel these concerns are fundamentally the same, as we intended with Figure 1 to provide such an overview of the system architecture.ßdummyTurker
18567ß0.0ßOur model is not an end-to-end differentiable or otherwise modular architecture that often has an overview diagram: it is a generative model over sequences, and so it is presented visually in the standard and appropriate graphical model notation that is found in countless HMM/CRF papers.ßdummyTurker
18568ß0.0ßIn a generative model, the factorization of the joint probability distribution, as defined in Eq.ßdummyTurker
18569ß0.0ß1, *is* the system architecture, and Fig.ßdummyTurker
18570ß0.0ß1. is a graphic depiction of the same.ßdummyTurker
18571ß0.0ßBoth are linked to the text (section 4.2 specifically) using colors (and alternatively, circled numbers).ßdummyTurker
18572ß0.0ßWe feel strongly that this is the correct way to provide such an overview -- it would be breaking with established tradition to use anything else.ßdummyTurker
18573ß0.0ßBut in light of the confusion, we do agree it could have been labeled better, and grounded to an actual example of inflection, so we plan to make these changes accordingly.ßdummyTurker
18574ß0.0ßIssue #2.ßdummyTurker
18575ß0.0ßThe problem statement is not clear.ßdummyTurker
18576ß0.0ß"Succinctly, the problem statement is: ""Given a limited set of disambiguated morphological analyses (lemma/morph-tags/form) in sentential context, and a comparatively larger set of unlabeled raw text sequences, can we make use of this unlabeled data to improve the accuracy of a model of morphological inflection."""ßdummyTurker
18577ß0.0ßThis is discussed in Section 2.2, where both the general task of morphological inflection, and the particular semi-supervised use-case we focus on in this paper are established, but we will try to establish this as clearly as written above, earlier in the paper, for the camera-ready.ßdummyTurker
18578ß0.0ßWe will revisit the equation descriptions to help ensure those that are not experts can follow easier.ßdummyTurker
18579ß0.0ßOnly a paragraph has been given to explaining *why* the model presented is a structured VAE.ßdummyTurker
18580ß0.0ßHowever, almost the entire paper (especially sections 3-5) are towards defining the model, a model which IS a VAE.ßdummyTurker
18581ß0.0ßOnce described, it doesn’t take much to make the connection, but it also doesn’t warrant further discussion.ßdummyTurker
18582ß0.0ßWe believe it is more accurate to call it a Structured VAE in the title, given that it is what it is, than the too vague “A generative model of…”, regardless of how much of the paper discusses it.ßdummyTurker
18583ß0.0ßWe agree.ßdummyTurker
18584ß0.0ßWe include some hallucinations of our model and other analyses in the appendix, but would like to include further qualitative examples in camera-ready.ßdummyTurker
18585ß0.0ßAgreed, but we simply didn’t have any space to do so.ßdummyTurker
18586ß0.0ßRegarding the Faruqui paper, Yes, this is a good point.ßdummyTurker
18587ß0.0ßWe do not claim to be the first, in fact we point out Zhou & Neubig as previous work in this line.ßdummyTurker
18588ß0.0ßBut yes, thought we cite it earlier in the paper, it should also be included in the related work.ßdummyTurker
18589ß0.0ßOur work still differs from both these approaches by using text in context (i.e., token-level information, where order is important), where these preceding approaches are type-level.ßdummyTurker
18590ß0.0ßThank you for your thoughtful comments and recommendations.ßdummyTurker
18591ß0.0ß“Events are essentially headwords and some fixed surrounding context, enriched with features derived from resouces like Wordnet, and subjected to a standard embedding.ßdummyTurker
18592ß0.0ßConsequently, the outputs of this system are not immediately useful, but would have to be combined with components that muster the arguments and construct fully populated event frames.”ßdummyTurker
18593ß0.0ßWe would like to clarify that we intentionally perform this event coreference task without constructing event frames fully.ßdummyTurker
18594ß0.0ßThis is important because extracting event arguments, semantic roles and other components of an event is still an error prone task.ßdummyTurker
18595ß0.0ß“I was a little puzzled by the approach taken with respect to data balancing.ßdummyTurker
18596ß0.0ßFirst, is the model trained twice or just once (and evaluated twice on different data samples)?”ßdummyTurker
18597ß0.0ßWe have two classifiers (WD and CD) each of which is trained once, both on balanced data.ßdummyTurker
18598ß0.0ßThe rationale for training on balanced data was to ensure that any statistical bias in the distribution of positive and negative data is not picked up by the system.ßdummyTurker
18599ß0.0ßEach of these two classifiers is then evaluated on the entire test set (no balancing is done here) for both WD and CD coreference to be able to compare our results to previous work for a total of 4 results reported in the final two rows of Table 4 and Table 5.ßdummyTurker
18600ß0.0ßFor each classifier we use both gold standard event mentions as well as event mentions extracted by an event detector.ßdummyTurker
18601ß0.0ßThe gold results are a reasonable upper bound of the performance of our approach if we had perfect event detection.ßdummyTurker
18602ß0.0ß“For example, do we only train on gold and just evaluate the model on detect?”ßdummyTurker
18603ß0.0ßWe train both the classifier (CD and WD) only once; on gold data to avoid training on noisy data.ßdummyTurker
18604ß0.0ßWA 1: As far as we know, this is the first work that proposes a computational implementation of EC theory and use it to help elevate the data scarcity problem of Code-mixing.ßdummyTurker
18605ß0.0ßWe will try to emphasize this fact in the final version as well as expand this section with more algorithmic details and examples.ßdummyTurker
18606ß0.0ßWA 2:  (i) As described in the paper in lines 273-277,  “We use the Stanford Parser (Klein and Manning, 2003) to parse the English sentence.ßdummyTurker
18607ß0.0ßUsing the alignments, the parse tree of the English sentence is then projected onto the Spanish sentence”.ßdummyTurker
18608ß0.0ßWe cannot use two different parsers because the EC theory assumes the use of not only the same grammar formalism but also the symmetry of its constituents - which may not exist for a pair of languages.ßdummyTurker
18609ß0.0ßSo the best/practical thing is to project.ßdummyTurker
18610ß0.0ßWe chose to use English parse, since the performance of existing Spanish parsers are relatively poor on Spanish tweet data.ßdummyTurker
18611ß0.0ß(ii) By language distribution, we meant the proportions of minor and major language tokens.ßdummyTurker
18612ß0.0ßCMI is a comprehensive measure of code-mixing and is widely used in the community.ßdummyTurker
18613ß0.0ßThere are other measures but CMI is the only single measure to cover various aspects of code-mixing.ßdummyTurker
18614ß0.0ß(iii) Test-14 has human annotated word-level language labels.ßdummyTurker
18615ß0.0ßFor other datasets, we use an English-Spanish LID to language tag the words.ßdummyTurker
18616ß0.0ßWe will try to add more details in the final version.ßdummyTurker
18617ß0.0ß(iv) The edit distance computed is based on Wagner and Fisher (1974).ßdummyTurker
18618ß0.0ßWe will add the reference in the final version.ßdummyTurker
18619ß0.0ß(v) The models 4(a)-\up and 4(b)-\down maintain the continuity in the CMI values.ßdummyTurker
18620ß0.0ßOther curricula are conceivable but will break this continuity by abruptly changing the CMI while training.ßdummyTurker
18621ß0.0ßThere are many variants possible but given the training times of our language model, we only chose to experiment these two variants.ßdummyTurker
18622ß0.0ßWA 3: The model tries to predict the word given the context (the previous words).ßdummyTurker
18623ß0.0ßSo, the perplexity we report on is for the word we are going to predict next.ßdummyTurker
18624ß0.0ßThe perplexities at SP are computed in a similar way but to predict the word right after the switch points.ßdummyTurker
18625ß0.0ßWA 4: Code/data will be released subject to the approval of our institution’s release review board.ßdummyTurker
18626ß0.0ßWA  5:  Due to lack of space, we could not mention the strengths, drawbacks and conclusions from the prior work.ßdummyTurker
18627ß0.0ßIn the final version, with one extra page, we will try to incorporate these.ßdummyTurker
18628ß0.0ßWA 1: We use random phrases and not parallel tweets, but keep the real SPF distribution.ßdummyTurker
18629ß0.0ßWe see the point (and in fact, thought about it) that using aligned pairs of tweets/phrases might lead to stronger baselines.ßdummyTurker
18630ß0.0ßDue to the word order similarity of English-Spanish, the word-aligned or phrase based SMT merging is expected work better.ßdummyTurker
18631ß0.0ßThe number of possible sentences will be in the similar order of magnitude, except that EC based generation will reject few variants.ßdummyTurker
18632ß0.0ßBut, when there is a bigger divergence between languages like in English-Japanese or English-Hindi, this might not be the case.ßdummyTurker
18633ß0.0ßExploring the similarities between these approaches of merging for various language pairs is something we plan to explore in the future.ßdummyTurker
18634ß0.0ßWA 2: Yes, we agree that its resource intensive.ßdummyTurker
18635ß0.0ßBut we only use monolingual resources, and these are readily available for most of the common languages.ßdummyTurker
18636ß0.0ßThis is much easier to obtain than to collect large CM datasets for language modeling.ßdummyTurker
18637ß0.0ßYes, we could have used other closer domains for generating synthetic data but we believe that the language usage might be very different in the real CM data (from twitter) and the gCM data.ßdummyTurker
18638ß0.0ßWhen we incorporate our LM into a code-mixed ASR system, we will definitely explore data resources from other domains.ßdummyTurker
18639ß0.0ßOne advantage of using Twitter data is its abundance and ease of accessibility, and it might be difficult to get large amounts of publicly available parallel data from other domains for many languages.ßdummyTurker
18640ß0.0ßWA 3: Yes, we could have experimented with different amounts of monolingual data.ßdummyTurker
18641ß0.0ßBut, given the training times of our language model, we couldn’t run these experiments.ßdummyTurker
18642ß0.0ßWA 4: We agree and we plan to use the LMs in code-mixed speech recognition system to see the effect on WER.ßdummyTurker
18643ß0.0ßQ 1: see the reply to weakness argument 1ßdummyTurker
18644ß0.0ß"-We never claimed ""need""."ßdummyTurker
18645ß0.0ßSee the two reasons to propose CMM(L90).ßdummyTurker
18646ß0.0ß"We just used ""the-difficulty-of-training-programs""(L93) and ""a-feasible-way""(L101)."ßdummyTurker
18647ß0.0ßWe show 7-point NLVR promotion over FiLM, and 2~3-point promotion on number-related CLEVR-questions.ßdummyTurker
18648ß0.0ß"Couldn’t this comparison support that we reduced ""training-difficulties-of-programs"", since we use the same ResBlock-pipeline as FiLM?"ßdummyTurker
18649ß0.0ß"Why should we design experiments for ""need""?"ßdummyTurker
18650ß0.0ß"-For L112-L116, we claimed ""accurate"", not ""interpretable/human-like""."ßdummyTurker
18651ß0.0ß"I don’t think an explicit, symbolic program like NMN/PG+EE is the only way to support ""accurate-reasoning-procedure""."ßdummyTurker
18652ß0.0ßFor end-to-end models, reasoning process is conventionally shown by tracking attention shift[1].ßdummyTurker
18653ß0.0ßOur mutual-modulating-process is something like this.ßdummyTurker
18654ß0.0ßSee also Question-2-reply.ßdummyTurker
18655ß0.0ß"Besides, CLEVR(Johnson-2017a) claimed ""no-biases"": answer-word-accuracy on CLEVR provides strong evidences for ""accurate-reasoning-procedure""."ßdummyTurker
18656ß0.0ß"-We claimed ""lack-of-attention-on-language-logics"" for previous models because they never treated questions equally to images as we did."ßdummyTurker
18657ß0.0ß-We’ll add more visualizations in the additional page if accepted.ßdummyTurker
18658ß0.0ß-Visual reasoning has become an individual research field(L44): Most recent works(PG+EE-on-ICCV/RN-on-NIPS/FiLM-on-AAAI) didn't use real-world images even in computer-vision top-conferences; NLVR was also accepted as outstanding-paper in ACL2017.ßdummyTurker
18659ß0.0ßThus, not using VQA shouldn’t be weakness; we also show the best real-world-language-abilities in reasoning-models by testing on both NLVR&CLEVR-Humans.ßdummyTurker
18660ß0.0ß-We'll follow most of them.ßdummyTurker
18661ß0.0ß"-""Attention-on-tokens"" isn’t right."ßdummyTurker
18662ß0.0ß"Our bi-GRU combines context-words into ""phrases"" (FiLM uses single-direction-GRU); multi-step-attention focuses on different ""phrases""."ßdummyTurker
18663ß0.0ßThis partly models compositional nature.ßdummyTurker
18664ß0.0ß-As mentioned above, end-to-end models often reason with multi-step-attention.ßdummyTurker
18665ß0.0ßFor similar sayings, see textual-reasoning[2].ßdummyTurker
18666ß0.0ßFor CMM, it's mutual-modulation-process.ßdummyTurker
18667ß0.0ß"-For connections to PG+EE/N2NMN-modules, see figure-4: step1->""equal_shape/relocate()""; step2->""filter_color[brown]/filter()""; step3->""exist/exist""."ßdummyTurker
18668ß0.0ßCertainly, they are just a subset, but we have the visual pipeline interacting with them at each step to help complete the process.ßdummyTurker
18669ß0.0ß-NLVR is smaller and less compositional compared to CLEVR, and labels are only yes/no.ßdummyTurker
18670ß0.0ßThis makes it difficult to learn long attention sequences(L647).ßdummyTurker
18671ß0.0ß4-blocks in CLEVR is consistent with FiLM and helps locate our main contributions.ßdummyTurker
18672ß0.0ß"-Thanks for your opinions, generally helpful despite miss-understanding of ""reasoning-process""."ßdummyTurker
18673ß0.0ßBased on responses to Weakness-1&2, I think our Soundness, Substance, Comparison are seriously under-estimated.ßdummyTurker
18674ß0.0ßHoping for higher scores.ßdummyTurker
18675ß0.0ß"[1]""End-to-end-Memory-Networks"""ßdummyTurker
18676ß0.0ß"[2]""ReasoNet:Learning-to-Stop-Reading-in-Machine-Comprehension"""ßdummyTurker
18677ß0.0ßWe agree that the cognitive mechanisms need to be better explained.ßdummyTurker
18678ß0.0ßWe thank reviewer for providing other baselines.ßdummyTurker
18679ß0.0ßWe also appreciate it if reviewer would point to a reference for the deep learning based hybrids SOTA.ßdummyTurker
18680ß0.0ßtanh is used to keep the polarity scores roughly in a range of [-1, 1], other forms, such as sigmoid can also be used.ßdummyTurker
18681ß0.0ßWe agree the notation can be improved.ßdummyTurker
18682ß0.0ßActually V(T)_i is delta(x), we tried to also refer to the index in V(T)_i, namely i. i is the same index for word x.ßdummyTurker
18683ß0.0ßSo maybe it is not that appropriate to write x= V(T)_ißdummyTurker
18684ß0.0ßWe considered uni-gram bow only.ßdummyTurker
18685ß0.0ßThe specific accuracy values are not important.ßdummyTurker
18686ß0.0ßInstead, we focus on the improvement before/after adaptation.ßdummyTurker
18687ß0.0ßLike stated in the paper, the goal is not to create a SOTA classifier, but to generate a reliable domain-specific lexicon.ßdummyTurker
18688ß0.0ßDoing transfer learning usually does not provide with a sentiment lexicon.ßdummyTurker
18689ß0.0ßThank you for pointing out this problem.ßdummyTurker
18690ß0.0ßWe will add explanations to make the statement clearer.ßdummyTurker
18691ß0.0ßOn average, searching 30 top templates for a sentence takes about 0.1s.ßdummyTurker
18692ß0.0ßTherefore, it takes about 4 days to find templates for the whole 3.8M training data.ßdummyTurker
18693ß0.0ßHowever, the Retrieve module acts as a data preprocessing step.ßdummyTurker
18694ß0.0ßIt only needs to be executed ONCE before the training of the seq2seq model.ßdummyTurker
18695ß0.0ßThus, the total time cost of our experiments is Retrieve (4 days) + training of Re3Sum (2 days) +training of PIPELINE (2 days), not very long.ßdummyTurker
18696ß0.0ßDuring test, the time cost to generate a summary is 0.1s (Retrieve) + 0.07s (Rerank and Rewrite).ßdummyTurker
18697ß0.0ßIt is quite efficient.ßdummyTurker
18698ß0.0ßMoreover, we use single-threading mode in Retrieve for convenience.ßdummyTurker
18699ß0.0ßIn fact, Lucune supports multi-threading searching, which is much faster.ßdummyTurker
18700ß0.0ßROUGE has been used to measure the quality of a generated summary.ßdummyTurker
18701ß0.0ßHere we also use ROUGE to select important soft templates.ßdummyTurker
18702ß0.0ßThank you for this note.ßdummyTurker
18703ß0.0ßWe will change “informativeness” to a more general word “importance”.ßdummyTurker
18704ß0.0ßYes, we need golden summaries in the training data.ßdummyTurker
18705ß0.0ßIt is the basic requirement of all the seq2seq models.ßdummyTurker
18706ß0.0ßThe system performance with improper templates can be seen in Table 6.ßdummyTurker
18707ß0.0ßIn Line 626, we find our model is quite robust.ßdummyTurker
18708ß0.0ßWe conduct manual qualitative analysis in Table 7 and 8.ßdummyTurker
18709ß0.0ßAs mentioned in weakness 1, it only takes 0.17s to generate a summary using a single computer with a single thread.ßdummyTurker
18710ß0.0ßTherefore, the practicability is high.ßdummyTurker
18711ß0.0ßThe requirements of our model are the same as the general seq2seq models.ßdummyTurker
18712ß0.0ßThus, as long as the dataset is enough to train a general seq2seq model, we think we can find proper soft templates in it.ßdummyTurker
18713ß0.0ßPlease refer to weakness 3.ßdummyTurker
18714ß0.0ß[soft template performance]ßdummyTurker
18715ß0.0ßYes, in Table 5, we demonstrate although the ROUGE scores of soft templates are high (see Table 4), they are often unfaithful to the source sentences.ßdummyTurker
18716ß0.0ß[saliency]ßdummyTurker
18717ß0.0ßThis word stands for the quality of a soft template, which is measured by ROUGE scores in this paper.ßdummyTurker
18718ß0.0ß[LEN-DIF]ßdummyTurker
18719ß0.0ßIt is the length difference between generated summaries and golden summaries.ßdummyTurker
18720ß0.0ßWe use this metrics to measure whether the model can learn a proper compression ratio.ßdummyTurker
18721ß0.0ßIn Line 588, we use LESS_3 to reflect readability and informativeness.ßdummyTurker
18722ß0.0ß[COPY]ßdummyTurker
18723ß0.0ßCompressive summarization picks words in the source sentence to form the summary.ßdummyTurker
18724ß0.0ßThus we regard it as a type of copying.ßdummyTurker
18725ß0.0ßIn this paper, abstraction means rewriting.ßdummyTurker
18726ß0.0ß[Lower perplexity]ßdummyTurker
18727ß0.0ßIt is the learning object of a general seq2seq model (refer to Eq.ßdummyTurker
18728ß0.0ß6).ßdummyTurker
18729ß0.0ß[the copy ration of the actual summaries]ßdummyTurker
18730ß0.0ßYes, it does not exist in the table.ßdummyTurker
18731ß0.0ßWe will add this information in Table 1.ßdummyTurker
18732ß0.0ßWe will try to conduct some human evaluation on a subset of test data.ßdummyTurker
18733ß0.0ßWe will release all the generation results shown in Table 6.ßdummyTurker
18734ß0.0ßFor convenience, we use single-threading mode to search templates.ßdummyTurker
18735ß0.0ßHowever, Lucene can work in multi-threading mode, which is much faster.ßdummyTurker
18736ß0.0ßWe will specify it after Line 185.ßdummyTurker
18737ß0.0ßFor more details, please see our reply to weakness 1 to Reviewer #3.ßdummyTurker
18738ß0.0ßYesßdummyTurker
18739ß0.0ßIndex means building the <term, document> database for the whole 3M data.ßdummyTurker
18740ß0.0ßYour understanding of “retrieving candidate templates…” is right.ßdummyTurker
18741ß0.0ßAbout 2 days on a GPU of GTX 1080.ßdummyTurker
18742ß0.0ßBoth OpenNMT and our model run 13 epoches.ßdummyTurker
18743ß0.0ßThank you for indicating these writing problems.ßdummyTurker
18744ß0.0ßThank you for the useful advice.ßdummyTurker
18745ß0.0ßAs described in Section 5, we are testing our model on the CNN/Daily Mail dataset and short text conversation datasets.ßdummyTurker
18746ß0.0ßWe follow Rush’s work and the statement of OpenNMT to call this task sentence summarization.ßdummyTurker
18747ß0.0ßI agree it is more like text simplification.ßdummyTurker
18748ß0.0ßI will add reference papers in the related work.ßdummyTurker
18749ß0.0ßRetrieve acts as a necessary preprocessing step for the seq2seq model.ßdummyTurker
18750ß0.0ßSince we use the default setting of Lucene, we do not describe it in detail.ßdummyTurker
18751ß0.0ßRetrieve only needs to be executed ONCE before the training of the seq2seq model.ßdummyTurker
18752ß0.0ßDuring test, it only takes 0.17s to generate a summary using a single computer with a single thread.ßdummyTurker
18753ß0.0ßFor more details, please see our reply to weakness 1 to Reviewer #3.ßdummyTurker
18754ß0.0ßThank you for pointing out this problem.ßdummyTurker
18755ß0.0ßDropout is used in LSTM.ßdummyTurker
18756ß0.0ßNo.ßdummyTurker
18757ß0.0ßReferring to Line 423 and 427, we add two parameters (-replace_unk -alpha 1) for OpenNMT_I during generation.ßdummyTurker
18758ß0.0ßThank you for the note.ßdummyTurker
18759ß0.0ßThis result is implemented by the paperßdummyTurker
18760ß0.0ßAs described in Section 2.2, our model contains another encoder for templates and a Bilinear network to predict the importance of a template.ßdummyTurker
18761ß0.0ßReferring to Eq.ßdummyTurker
18762ß0.0ß7, the cost function of our model combines generation loss and reranking loss.ßdummyTurker
18763ß0.0ßThese are the difference between our model and OpenNMT.ßdummyTurker
18764ß0.0ßHowever, we can use the same setting as OpenNMT to run our model, which largely simplifies its implementation.ßdummyTurker
18765ß0.0ßYes, but the replacement makes this task quite unrealistic.ßdummyTurker
18766ß0.0ßWe have added a rule in beam search to block the generation of new named entities in soft templates.ßdummyTurker
18767ß0.0ß[punctual remarks]ßdummyTurker
18768ß0.0ß1.ßdummyTurker
18769ß0.0ßWe will add important formulas to make this section clearer.ßdummyTurker
18770ß0.0ß2.ßdummyTurker
18771ß0.0ßAs the reply to weakness argument 1, we are testing our model on other datasets.ßdummyTurker
18772ß0.0ß3.ßdummyTurker
18773ß0.0ßAs the reply to question 6, a simple rule can solve this problem.ßdummyTurker
18774ß0.0ß4.ßdummyTurker
18775ß0.0ßYes, it is a weakness since the learning goal is not concerning semantics.ßdummyTurker
18776ß0.0ßAlthough our improvements are only marginal, please notice that we have not changed the underlying model but only the decoding algorithm.ßdummyTurker
18777ß0.0ßAlso, our motivation is that many of the beam-search expansions are wasted on expanding similar states, which restricts the reachable space in some way.ßdummyTurker
18778ß0.0ßWe empirically show that even with RNN-based model, approximate merging strategies are reasonable.ßdummyTurker
18779ß0.0ßPlease also notice that on some datasets, the improvements are statistically significant as shown in the supplementary material.ßdummyTurker
18780ß0.0ßAbout introducing merging errors, this is an unavoidable risk with recombination on NMT.ßdummyTurker
18781ß0.0ßAbout the 15% lower scores, the differences are not quite large.ßdummyTurker
18782ß0.0ßAlthough searching errors are possible, we still think the main problem is imperfect NMT modeling.ßdummyTurker
18783ß0.0ßFirstly, previous work (Wu et al., 2016) and this work show that simple modifying the log scores considering the length can bring improvements.ßdummyTurker
18784ß0.0ßThis can be regarded as modifying the model itself, and when the modeling is not defined well, the searching will be strongly influenced.ßdummyTurker
18785ß0.0ßMoreover, in our preliminary experiments, we tried re-ranking the n-best list plus the reference sentence and the results showed that quite a lot references received low model scores which would not be reached by the beam searcher.ßdummyTurker
18786ß0.0ßAbout the running time, in Line 200 we denoted that merging can be effective with hash maps, and the overhead is only the matching in each step and final n-best extraction.ßdummyTurker
18787ß0.0ßIn fact, we observed **nearly no** runtime differences compared to ordinary beam searching, because their neural scoring parts are almost the same.ßdummyTurker
18788ß0.0ßWe implemented batched version of decoder on GPU and can get 2x to 3x speedup than un-batched version, both for beam searchers with or without recombination.ßdummyTurker
18789ß0.0ßThank you for pointing out these points, but we think there are slight misunderstandings.ßdummyTurker
18790ß0.0ß* Line 16, ``explorable space'' means the output space that the searcher reaches.ßdummyTurker
18791ß0.0ßOf course the whole output space is independent of hidden states, but the reachable or expanded space are restricted by RNN.ßdummyTurker
18792ß0.0ß* Line 17, in a typical s2s NMT model, the whole output features are encoded in RNN hidden layers and previous token.ßdummyTurker
18793ß0.0ßImagine this, given a hidden layer and previous token, we can totally restart the decoding process.ßdummyTurker
18794ß0.0ß* Line 45, yes, it is almost impossible to expand the whole output space.ßdummyTurker
18795ß0.0ßWe should say that expanding all similar states might be inefficient.ßdummyTurker
18796ß0.0ßYes, this means adding a reward \lambda to each token.ßdummyTurker
18797ß0.0ßAs we explained previously, we regard length reward as a modification on model.ßdummyTurker
18798ß0.0ßWe observed that the main differences are on the Brevity-Penalty.ßdummyTurker
18799ß0.0ßWe will modify this statement.ßdummyTurker
18800ß0.0ßWe thank you for appreciating the dataset and challenges involved in the proposed task.ßdummyTurker
18801ß0.0ßThe objective of this work was mainly to introduce this dataset, motivate the tasks involved, establish baselines, and encourage the research community to develop richer models towards these challenges.ßdummyTurker
18802ß0.0ßMore specifically, in Section 4 (Additional NLP pre-processing), we do employ NER and noun/verb phrase extraction to identify keywords (or entities) in the question and plot.ßdummyTurker
18803ß0.0ßWhile we agree, that like NER, there can be other NLP tools to help address some of the language understanding complexities, but, given the overall space of different complexities posed by this dataset, handling each of them would be beyond the scope of one paper.ßdummyTurker
18804ß0.0ßPrecisely for this, we intend to release the dataset so that larger community can analyze the various complexities in the task.ßdummyTurker
18805ß0.0ßNumber of crowd workers: Since the first phase of data collection involved the annotator creating the QA pair from the shorter plot, every annotator will mostly generate unique QA pairs.ßdummyTurker
18806ß0.0ßIn the second phase, the longer plot is split up into segments or paragraphs and every question obtained from the 1st phase of annotation is paired separately with each of the segments and each (question, segment) pair is posted as a crowdsource job.ßdummyTurker
18807ß0.0ßWith this, we essentially get multiple answers to the same question, if it was answerable from more than one segment, but on an average we get approximately 1 unique answer for each question.ßdummyTurker
18808ß0.0ßQuality Assessment: Overall, 62% of the questions have partial overlap between the answer obtained from the shorter and the longer plot, which is indicative of the fact that quality is reasonable.ßdummyTurker
18809ß0.0ßThe remaining 38% where there is no overlap can be attributed to non-answerablity of the question from the bigger plot or information gap or paraphrasing of information between the two plots.ßdummyTurker
18810ß0.0ßFor quality assessment purposes, various levels of manual and semi-automated inspections were done (see below), especially in the second phase of annotationßdummyTurker
18811ß0.0ßi) Weeding out annotators who mark a majority of answers as non-answerable, by taking into account their response time of reading the passage and answeringßdummyTurker
18812ß0.0ßii) Annotators for whom a high percentage of answers have no entity (or noun phrase) overlap with the entire passage were subjected to strict manual inspection and blacklisted if necessary.ßdummyTurker
18813ß0.0ßTable 1 comparison: We use NER and noun phrase/verb phrase extraction over the entire dataset to identify key entities in the question, plot and answer.ßdummyTurker
18814ß0.0ß"This is used to get metrics like ""Avg word/sentence distance"" and “Number of sentences for inferencing”."ßdummyTurker
18815ß0.0ßEffective wait period: Overall 2559 workers took part in the annotation phase1, 8021 workers in annotation phase2, out of which 703 workers were from the first phaseßdummyTurker
18816ß0.0ßIn the preprocessing, coreference resolution was used to alleviate the complexity of inferencing over multiple sentences.ßdummyTurker
18817ß0.0ßWe used Stanford Coref library to replace every coreference with the referent entity in the passage.ßdummyTurker
18818ß0.0ß"""the Glove and Skip-thought embeddings"": For both glove and skip-thought embedding, we first create approximate nearest neighbor indices based on the word embeddings and consider a word to be very close to another if it appears within the top 50 neighbors of the latter."ßdummyTurker
18819ß0.0ßWe will add the citation of 'Skip-thought' in the paper.ßdummyTurker
18820ß0.0ßnon-neural baseline: One possible non-neural baseline is an Information-retrieval based one.ßdummyTurker
18821ß0.0ßBut given the nature of the IR paradigm, it is hard to use it directly to get the answer of a question, since these are not factoid questions but usually require a short phrase as answer.ßdummyTurker
18822ß0.0ßOn the other hand, such IR systems are good at extracting relevant snippets of the passage for a given question, which is in turn given to the QA system for answer extraction/generation.ßdummyTurker
18823ß0.0ßThis two-phased model, with a non-neural extraction model followed by a state-of-the-art neural QA model is used as a baseline to benchmark our proposed task.ßdummyTurker
18824ß0.0ß1.ßdummyTurker
18825ß0.0ß"We thank the reviewer for noting the statement is not true: ""the best performance is obtained when both training and testing is done on SelfRC""."ßdummyTurker
18826ß0.0ßWe will correct thisßdummyTurker
18827ß0.0ß2.ßdummyTurker
18828ß0.0ßBLEU is an additional metricßdummyTurker
18829ß0.0ßWe thank the reviewer for appreciating the dataset and challenges involved in the proposed task.ßdummyTurker
18830ß0.0ßFor phase1 of the annotation, the instructions wereßdummyTurker
18831ß0.0ß1.ßdummyTurker
18832ß0.0ßCreate QA pairs, where the answer is preferably a short phraseßdummyTurker
18833ß0.0ß2.ßdummyTurker
18834ß0.0ßSubjective questions (asking for opinion) are discouraged.ßdummyTurker
18835ß0.0ßBecause of 1. and 2. the questions obtained were mostly factoid in nature.ßdummyTurker
18836ß0.0ß3.ßdummyTurker
18837ß0.0ßQuestions should be entirely answerable from the passage and not require any external knowledgeßdummyTurker
18838ß0.0ß4.ßdummyTurker
18839ß0.0ßQuestions should be well formed and grammatically correctßdummyTurker
18840ß0.0ßFor phase2, the instructions wereßdummyTurker
18841ß0.0ß1.ßdummyTurker
18842ß0.0ßAnswer with the key-phrase onlyßdummyTurker
18843ß0.0ß2.ßdummyTurker
18844ß0.0ßSome questions may have additional information not contained in this snippet, but by applying logic (or outside general knowledge), they would still be answerable from this passageßdummyTurker
18845ß0.0ß3.ßdummyTurker
18846ß0.0ßSome questions are unanswerableßdummyTurker
18847ß0.0ßour analysis of IMDB and Wikipedia plots of over 7.6K movies showed that the word-level jaccard between them (i.e.ßdummyTurker
18848ß0.0ßratio of intersection to union of the bag-of-words in the two plots) is 26%.ßdummyTurker
18849ß0.0ßThis indicates that not only one of the plots is usually longer and descriptive, the two plots are infact quite different in wording, even though the information content is very similar as they are based on the same movie.ßdummyTurker
18850ß0.0ßDataset would be made available on acceptance of the paper.ßdummyTurker
18851ß0.0ß""" Why 50% and 30%"": This was based on manual observation on a small sample set, which gave us an understanding of what is a decent threshold of word-overlap between the sentence and question, that has a reasonable precision and recall of the relevant snippet extraction step."ßdummyTurker
18852ß0.0ßSince this step was rule-based we could only employ such qualitative inspections to set these parameters.ßdummyTurker
18853ß0.0ßAnd since this step was targetted to be high recall, we relaxed the thresholds to 30% if no match was found.ßdummyTurker
18854ß0.0ßMost of the previous NLU paper reports ‘accuracy’ for intent classification and ‘F1’ for slot-filling task, so we just reported the metrics similarly.ßdummyTurker
18855ß0.0ßReranking NLU task is a selecting best semantic frame – the combination of intent and slots – among many semantic frames.ßdummyTurker
18856ß0.0ßTherefore, we need to measure how much the selected semantic frame has performance gain against baseline semantic frame in sense of both intent and slot-filling.ßdummyTurker
18857ß0.0ßThat is why we introduced the joint metric for reranking performance measuring.ßdummyTurker
18858ß0.0ßWe wanted to show that the framework can be successfully trained and applicable to re-rank with only training NLU corpus.ßdummyTurker
18859ß0.0ßOther supervised reranking approaches requires the actual prediction NLU results from the systems to train re-ranker, and we thought that it is another unfair comparison.ßdummyTurker
18860ß0.0ßWe think that this is strongly dependent to the dataset.ßdummyTurker
18861ß0.0ßATIS dataset has few intent-tags and many slot-tags.ßdummyTurker
18862ß0.0ßSo probably, the model tends to learn intent information intensively than slot information.ßdummyTurker
18863ß0.0ßActually, we found that the exactly same proposed method shows greatly improved performance on slot-filling task than intent-prediction task on another dataset.ßdummyTurker
18864ß0.0ßSince the dataset is quite private and not-English, we did not report to this paper.ßdummyTurker
18865ß0.0ßWe will add extra experiment results with the private dataset.ßdummyTurker
18866ß0.0ßWe will coach and refer the paper’s analysis and suggestions.ßdummyTurker
18867ß0.0ßIn current model, the intent-tag information is blended with the slot-tag and slot-value information in/after processing of ‘concat’ in the Figure 3.ßdummyTurker
18868ß0.0ßHowever, connecting v_{intent} to each step-inputs for slot-value reader(RNN) can be a simple extension to enforce joint.ßdummyTurker
18869ß0.0ßWe reported all the details of model hyperparameters in the manuscript with inline statements, however, readers might not catch the hyperparameters at a glance.ßdummyTurker
18870ß0.0ßIn the final submission, we will put a table to summary the hyperparameters.ßdummyTurker
18871ß0.0ßWe believe that it is ATIS 2 since we used the same dataset of Hakkani-Tur et al.ßdummyTurker
18872ß0.0ß2016.ßdummyTurker
18873ß0.0ßL_{slot} is a loss for measuring how the semantic frame generator successfully generate only slot-tags given semantic frame.ßdummyTurker
18874ß0.0ßIndeed, we conducted other experiments with another generator model to capture the slot-values generation loss, but it didn’t show good results in ATIS dataset.ßdummyTurker
18875ß0.0ßIt seems that the model tends to overfit to the slot-values of each semantic frame.ßdummyTurker
18876ß0.0ßWe will put the equation numbers later.ßdummyTurker
18877ß0.0ßWe think that this is strongly dependent to the dataset.ßdummyTurker
18878ß0.0ßATIS dataset has few intent-tags and many slot-tags.ßdummyTurker
18879ß0.0ßSo probably, the model tends to learn intent information intensively than slot information.ßdummyTurker
18880ß0.0ßActually, we found that the exactly same proposed model shows improved performance on slot-filling task than intent-prediction task on another dataset.ßdummyTurker
18881ß0.0ßSince the dataset is quite private and not-English, we did not report to this paper.ßdummyTurker
18882ß0.0ßWe will add extra experiment results with the private dataset.ßdummyTurker
18883ß0.0ßDue to the page limit, we didn’t put the detail of analysis, however, the analysis of ‘learned semantic frame vector’ is very important research topic.ßdummyTurker
18884ß0.0ßAlso, the application for another task and dataset are planned as future work.ßdummyTurker
18885ß0.0ßWithout L_task, L_dist rapidly goes to zero since the network generate v_t and v_s as zeros.ßdummyTurker
18886ß0.0ßIt mean that the network can’t learn ‘semantic’ information successfully.ßdummyTurker
18887ß0.0ßThat’s why we devised the concept of L_task in this study.ßdummyTurker
18888ß0.0ßIndeed, we conducted experiments with all the combinations L(L_dist, L_task), and it turned out that L=L_dist + L_task showed best performance.ßdummyTurker
18889ß0.0ßWe will add the analysis.ßdummyTurker
18890ß0.0ßWe will add commented analysis.ßdummyTurker
18891ß0.0ßWe answered it in ‘Weakness #1’.ßdummyTurker
18892ß0.0ßWhat we intend is that the supervised learning-based re-rankers including ensemble learning requires actual NLU results of the target NLU system to train the re-ranker model.ßdummyTurker
18893ß0.0ßHowever, our approach only requires the NLU training corpus.ßdummyTurker
18894ß0.0ßIt does not require any NLU results for training semantic frame learning.ßdummyTurker
18895ß0.0ßIt just uses the trained semantic frame learning model to calculate the distance between v_s and v_t for reranking the NLU results.ßdummyTurker
18896ß0.0ß-We will put the number of each equationsßdummyTurker
18897ß0.0ß-We will move the description of v to around line 268.ßdummyTurker
18898ß0.0ß-It is a typo.ßdummyTurker
18899ß0.0ßIt should be $E_slot $ has 200 dimensions, and it should be moved to the section 2.3ßdummyTurker
18900ß0.0ßWe think that this is strongly dependent to the dataset.ßdummyTurker
18901ß0.0ßATIS dataset has few intent-tags and many slot-tags.ßdummyTurker
18902ß0.0ßSo probably, the model tends to learn intent information than slot information.ßdummyTurker
18903ß0.0ßActually, we found that the exactly same proposed model shows greatly improved performance on slot-filling task than intent-prediction task on another dataset.ßdummyTurker
18904ß0.0ßSince the dataset is quite private and not-English, we did not report to this paper.ßdummyTurker
18905ß0.0ßWe will add extra experiment results with the private dataset in the final versionßdummyTurker
18906ß0.0ßWe awarded the discrepancy and tried to reproduce the same performance using the code from Github, but we didn’t get the same performance.ßdummyTurker
18907ß0.0ßWe will put some note.ßdummyTurker
18908ß0.0ßWe will make it smooth in final submission.ßdummyTurker
18909ß0.0ßLet’s take an example of E_{text}(x_s).ßdummyTurker
18910ß0.0ßE_{text} has a weight W which has dimension (VxH), V for number of tokens and H for embedding dimension, and x_s is a one-hot vector with dimension (1xV).ßdummyTurker
18911ß0.0ßE_{text}(x_s) is a just x_s * W which return a dense vector representation with dimension (1xH).ßdummyTurker
18912ß0.0ßWe train the weight parameter W as well as other model parameters.ßdummyTurker
18913ß0.0ßWe will put more explanation about this.ßdummyTurker
18914ß0.0ßIndeed, we investigated it.ßdummyTurker
18915ß0.0ßThe conclusion so far is that it is heavily depend on the dataset.ßdummyTurker
18916ß0.0ßThe proposed approach shows satisfactory result in slot-filling task on a different dataset without weighting.ßdummyTurker
18917ß0.0ßWe will improve the paper with the reviewer’s suggestion.ßdummyTurker
18918ß0.0ßDear Reviewer,ßdummyTurker
18919ß0.0ßSorry about the lack of clarity in the explanation.ßdummyTurker
18920ß0.0ßWe understand about anaphora and agreement and their interaction, but our explanation was brief due to the limited space afforded by short papers.ßdummyTurker
18921ß0.0ßWe'll make the explanation more complete.ßdummyTurker
18922ß0.0ßRegarding the qualitative example, we verified in the source/target language (talk and respective transcript: https://www.ted.com/talks/david_hoffman_shares_his_sputnik_mania/transcript?referrer=playlist-why_yes_actually_it_is_rocket).ßdummyTurker
18923ß0.0ßThe segmentation into separate sentences was kept in the french translation.ßdummyTurker
18924ß0.0ßWe agree to some extent that the relative pronoun “qui” could be used, but only for the second and third sentence.ßdummyTurker
18925ß0.0ßThis is present in the reference and should be more explicit in the paper.ßdummyTurker
18926ß0.0ßOur point with this example was to show that our proposed method handled pronoun agreement across sentences better than our baseline.ßdummyTurker
18927ß0.0ßWe would argue that “qui.. qui..” would be a better translation had the source been “which… which”, (ie.ßdummyTurker
18928ß0.0ßrelative pronoun to relative pronoun).ßdummyTurker
18929ß0.0ßHowever, as stated by the reviewer, this is an artifact of the annotation.ßdummyTurker
18930ß0.0ßSorry about the lack of clarity, we created our own dataset directly downloading from the TED site as this was more conducive to separating into documents with the appropriate annotations.ßdummyTurker
18931ß0.0ßWe will make this more clear in the revision.ßdummyTurker
18932ß0.0ßWe apologize for accidentally omitting the datasets sizes: our TED talks training data sizes are ~ 0.16M and 0.18M for EN-DE and EN-FR, respectively, and the KFTT 0.4M; and for testing we use ~ 4k sentences for both EN-DE and EN-FR, with ~1k sentences for EN-JA.ßdummyTurker
18933ß0.0ßWe understand the concerns about significance of the results with small test sets, so we measured the statistical significance of our methods across the multiple runs we performed using multeval (https://github.com/jhclark/multeval).ßdummyTurker
18934ß0.0ßWe measured the statistical significance of our methods across the multiple runs with multeval (https://github.com/jhclark/multeval).ßdummyTurker
18935ß0.0ßWith statistically significance and a p-value < 0.05: EN-JA both source side propagation and document attention; JA-EN all methods; EN-De source side and target side propagation, and EN-FR source side propagation.ßdummyTurker
18936ß0.0ßRegarding the experiments omitting sentences with UNKs, we do realize it’s uncommon.ßdummyTurker
18937ß0.0ßWe experiment as such with TED because there are many missing translations in the corpus (whole sentences or sentences with a very high percentage of UNKs).ßdummyTurker
18938ß0.0ßThis missing information can be regarded as a discontinuity of the document context, the missing sentences had context information.ßdummyTurker
18939ß0.0ßThus, our point was to remove those sentences to understand how our methods perform against a vanilla Seq2Seq.ßdummyTurker
18940ß0.0ßWe agree we may have pruned too aggressively and will address this for the camera ready version.ßdummyTurker
18941ß0.0ßWe apologize for the missing references, they were not intentional.ßdummyTurker
18942ß0.0ßFurthermore, regarding anaphoric pronoun translation and coherent translations, as already stated the explanations were brief due to the limited space afforded by short papers but we will try to expand.ßdummyTurker
18943ß0.0ßRegarding extended context, we will complete our related work with the work suggested (note Serban et al.’s intuition is already in Wang et al., 2017).ßdummyTurker
18944ß0.0ßSorry if we worded poorly and were misinterpreted: our premise is context does indeed help the translation process, we question whether it helps in all settings (for instance, language pairs, dataset sizes, among others) as Jean et al.ßdummyTurker
18945ß0.0ßshowed it’s not always beneficial.ßdummyTurker
18946ß0.0ßWe apologize if our wording comparing with existing literature seemed to inflate the importance of our work; it was not intentional, and we will make sure we clarify this in the revision.ßdummyTurker
18947ß0.0ßThe main intention of this statement was to stress that we do a more extensive empirical comparison of methods than the previous work, both across different types of methods and with regards to number of language pairs compared (which we believe is true).ßdummyTurker
18948ß0.0ßDear Reviewer,ßdummyTurker
18949ß0.0ßAlthough our approaches are similar to the work cited, there are two key differences: first, we use fewer parameters in the state propagation method; second, we consider the whole document structure with the document attention and the attention is performed at sentence level (represented with skipt-thoughts), while Jean et al.ßdummyTurker
18950ß0.0ßis at word level across sentences.ßdummyTurker
18951ß0.0ßSorry if this was not clear, we will use the extra page to make this part clear after revision.ßdummyTurker
18952ß0.0ß"Comment ""Section 3."ßdummyTurker
18953ß0.0ßInformation(...).ßdummyTurker
18954ß0.0ß""":"ßdummyTurker
18955ß0.0ßWe apologize for accidentally omitting the datasets sizes.ßdummyTurker
18956ß0.0ßSizes are: KFTT ~0.4M pairs, TED: 0.16M and 0.18M for EN-DE and EN-FR, respectively.ßdummyTurker
18957ß0.0ßAs for test sizes: ~ 4k sentences for both EN-DE and EN-FR, while ~1k sentences for EN-JA.ßdummyTurker
18958ß0.0ßWe measured the statistical significance of our methods across the multiple runs with multeval (https://github.com/jhclark/multeval).ßdummyTurker
18959ß0.0ßWith statistically significance and a p-value < 0.05: EN-JA both source side propagation and document attention; JA-EN all methods; EN-De source side and target side propagation, and EN-FR source side propagation.ßdummyTurker
18960ß0.0ßWe thank the reviewer for the constructive feedback and for the many thoughtful ideas for improving the presentation and clarity of the paper.ßdummyTurker
18961ß0.0ßWe will follow the suggestions in the camera-ready version.ßdummyTurker
18962ß0.0ßWe would like to emphasize that although the system contains several different steps and components, most of these rely on the notion of abstraction, and thus highlight the power of this approach.ßdummyTurker
18963ß0.0ßWe thank the reviewer for the useful feedback.ßdummyTurker
18964ß0.0ßWe performed hyper-parameter search for both epsilon (how random is the beam) and beta (how meritocratic are the updates) for a reward with 4 examples and found the best setup to be epsilon=0 (no randomization) and beta=0.5.ßdummyTurker
18965ß0.0ßHowever, even epsilon=0 and beta=1 (maximum marginal likelihood) performs reasonably well.ßdummyTurker
18966ß0.0ßFor OneExampleReward we epsilon=0, beta=0.5 and it performed very badly, as shown in Table 5, so this experiment indeed uses meritocratic updates and does not yield good results.ßdummyTurker
18967ß0.0ßThus, we believe it is safe to say that RANDOMER with a single reward does not solve spuriousness.ßdummyTurker
18968ß0.0ßWe will expand and clarify this important point in the camera-ready version.ßdummyTurker
18969ß0.0ßWe cast the problem in the standard semantic parsing approach where the KB is used to execute queries only and is not an input to the model.ßdummyTurker
18970ß0.0ßWhile it is possible to incorporate KB features into the sequence to sequence model, it is not immediate how to do this.ßdummyTurker
18971ß0.0ßFurthermore, the type of spuriousness we tackle is unlikely be solved by such an approach.ßdummyTurker
18972ß0.0ßIn any case, this is an interesting direction for future research that can provide additional signal to the model and can further improve performance.ßdummyTurker
18973ß0.0ßAlthough there are 74,460 sentence-image pairs, there are multiple images that correspond to the same KB.ßdummyTurker
18974ß0.0ßThus, there are only 12,410 sentence-KB pairs, with up to 4 KBs per sentence.ßdummyTurker
18975ß0.0ßWe therefore used the maximum possible number of KBs per sentence.ßdummyTurker
18976ß0.0ßAs mentioned – FFP and SA are the best results.ßdummyTurker
18977ß0.0ßExperiments in the paper aim to evaluate the usefulness of the enriched embeddings for different kind of tasks.ßdummyTurker
18978ß0.0ß"In case of Personality Detection and Emo-Int, using mean/median to observe the combined effect may not be a fair comparison as the dimensions/tasks(""EXT"", ""NEU"" etc) are independent."ßdummyTurker
18979ß0.0ßAlso, we do not intend to propose a generalised best-performing embedding.ßdummyTurker
18980ß0.0ßOur experiments look for the improved performance due to affect addition for the individual tasks.ßdummyTurker
18981ß0.0ß5/8 is a measure from Table 2 (intrinsic evaluation), you probably mean to refer to table 3.ßdummyTurker
18982ß0.0ß"Apart from ""EXT"", ""CON"" and ""JOY"" there is a consistent improvement with affect enriched embeddings."ßdummyTurker
18983ß0.0ßThis rules out chance.ßdummyTurker
18984ß0.0ßWe will incorporate some error analysis and hypothesis testing in the final version.ßdummyTurker
18985ß0.0ßPicard (1997) introduces affective computing across various modalities.ßdummyTurker
18986ß0.0ßIt is a seminal paper that  discusses issues and aspects of this space.ßdummyTurker
18987ß0.0ßThe key relevant takeaway from the paper is the ability to model and interpret human moods, opinions, and reactions using different interactions and modalities.ßdummyTurker
18988ß0.0ßThe challenges and the opportunities including the potential to use text and language to model human reactions are presented.ßdummyTurker
18989ß0.0ßThorough motivation and their methods support our claim that human reactions or feelings can be interpreted from text communication.ßdummyTurker
18990ß0.0ßYes, we have a large set of experiments in this paper.ßdummyTurker
18991ß0.0ßThe idea was to show that the Aff2Vec embeddings are beneficial for multiple kinds of tasks and are not limited to the FFP prediction.ßdummyTurker
18992ß0.0ßWe will reorder the paper to make it easy to follow.ßdummyTurker
18993ß0.0ßYes, the ENRON – FFP dataset will be publicly available along with the details about the annotation task.ßdummyTurker
18994ß0.0ß"p. 1 line 81: WordNet is not a ""semantic word distribution"""ßdummyTurker
18995ß0.0ßAgreed, that is a typo.ßdummyTurker
18996ß0.0ßWordnet is a lexical database with semantic, lexical, and conceptual word relationships.ßdummyTurker
18997ß0.0ßWill edit that in the revised version.ßdummyTurker
18998ß0.0ß"p. 1 line 86: word embeddings are not ""word distributions"""ßdummyTurker
18999ß0.0ßMeant to say word representations as against distributions.ßdummyTurker
19000ß0.0ßThanks for pointing out the mistake.ßdummyTurker
19001ß0.0ß"p. 3 line 222: Not clear what ""relational relationships"" are..."ßdummyTurker
19002ß0.0ßWould skip the word relational (typo).ßdummyTurker
19003ß0.0ß"p. 3 line 283: does ""regularized"" here mean the same as ""normalized"" above?"ßdummyTurker
19004ß0.0ßIf so, stick to consistent terminology.ßdummyTurker
19005ß0.0ß"Yes ""regularized"" means the same as ""normalized""."ßdummyTurker
19006ß0.0ßWill fix this in the final version.ßdummyTurker
19007ß0.0ß"p. 3 line 289: ""uniform distribution"" , and computation $\mu$ and $\sigma$."ßdummyTurker
19008ß0.0ßYes, it would be a standard normal distribution and not a uniform distribution.ßdummyTurker
19009ß0.0ßThe mean and standard deviation are calculated for each individual embedding.ßdummyTurker
19010ß0.0ßThank you for a detailed review, will fix the typos, citations, and the grammar in the revised paper.ßdummyTurker
19011ß0.0ßThank you for this suggestion.ßdummyTurker
19012ß0.0ßWe agree that the language barrier of the dataset will potentially challenge some researchers when they want to investigate this newly proposed topic.ßdummyTurker
19013ß0.0ßUnfortunately, because of the availability and cost of the crowdsourced annotations, currently, we can only provide Chinese dataset.ßdummyTurker
19014ß0.0ßIn the future, we will work on the English Q-A data set from Amazon while additional funding support will be applied for this work.ßdummyTurker
19015ß0.0ßBut, we agree that this can be an important issue.ßdummyTurker
19016ß0.0ßSo, we will tailor the dataset that will be released.ßdummyTurker
19017ß0.0ßIn addition to the original (Chinese) text data, we will also release the Q/A mapping files for each each Q/A and token ID pair.ßdummyTurker
19018ß0.0ßThen, scholars can reproduce the experiment results as well as explore new method without understanding the textual content.ßdummyTurker
19019ß0.0ßWe will add this to the paper.ßdummyTurker
19020ß0.0ßThanks for proposing this weakness.ßdummyTurker
19021ß0.0ßExperiment result in table 2 and table 3 tells the importance of 4-levels neural network, which outperforms than other approaches, especially in Conflict category.ßdummyTurker
19022ß0.0ßWe agree that the analysis part of this paper can be somehow weak.ßdummyTurker
19023ß0.0ßIf this paper is accepted, we will enhance this part, and we will provide additional evidence to tell the importance of the each module of the proposed model.ßdummyTurker
19024ß0.0ßThanks for your question.ßdummyTurker
19025ß0.0ßWe train these baseline models based on 4 classes (i.e., positive, negative, neutral and conflict).ßdummyTurker
19026ß0.0ßAs far as we know, the baseline Attention-LSTM (Wang et al., 2016) is also designed for a 4-class classification task (i.e., positive, negative, neutral and conflict).ßdummyTurker
19027ß0.0ßFrom this viewpoint, the experiment method and the baselines are comparable.ßdummyTurker
19028ß0.0ßThanks for your question.ßdummyTurker
19029ß0.0ßWe fed both the question and the answer to baseline models because the sentiment polarity of a Q-A pair can be potentially decided by both the question and the answer.ßdummyTurker
19030ß0.0ßAs E9 in Table 6 shows, we cannot observe the polarity information by only using the answer ‘No.’.ßdummyTurker
19031ß0.0ßInstead, question answer pair provides us a more comprehensive input for QA sentiment analysis.ßdummyTurker
19032ß0.0ßFor this reason, we provide Q+A for the baselines.ßdummyTurker
19033ß0.0ßIf this paper is accepted, we will add this to the paper.ßdummyTurker
19034ß0.0ßAlthough some works like (Meng et al., ACL 2017) have used seq2seq framework for key-phrase generation, they do not take advantage of reinforcement learning which is widely used in many other NLP tasks including Machine Translation, KB embedding and etc.ßdummyTurker
19035ß0.0ßWhen using the off_the_shelf  RL algorithms, e.g.ßdummyTurker
19036ß0.0ßPolicy Gradient, the key is to design a specific reward function for a task.ßdummyTurker
19037ß0.0ßFor this reason, we propose a new reward function that can handle the irrelevance of the order of extracted key-phrases.ßdummyTurker
19038ß0.0ßThanks for your writing advice.ßdummyTurker
19039ß0.0ßRL is used to handle the irrelevance of the order of the extracted key-phrases.ßdummyTurker
19040ß0.0ßUnlike the other tasks in NLP, e.g.ßdummyTurker
19041ß0.0ßMachine Translation and Text Summarization, there may be no context dependency among the key-phrases generated by the decoder.ßdummyTurker
19042ß0.0ßIf it ignores the order, the objective function is non-differentiable.ßdummyTurker
19043ß0.0ßThe vocabulary does contain the target key-phrases.ßdummyTurker
19044ß0.0ßThis work does not consider the situation that the keyphrases may not appear in the training.ßdummyTurker
19045ß0.0ßThis setting is definitely reasonable.ßdummyTurker
19046ß0.0ßTake the Seq2Seq based English-Chinese translation for example, all Chinese words constitute the vocabulary of the decoder.ßdummyTurker
19047ß0.0ßThe problem you mentioned is similar to the OOV problem in machine translation.ßdummyTurker
19048ß0.0ßIt is biased to say that this is not a correct problem setting.ßdummyTurker
19049ß0.0ßInstead, this is a good question for us to solve next.ßdummyTurker
19050ß0.0ßThanksßdummyTurker
19051ß0.0ßThanks for your writing advice in spite of that these methods are relatively weak baselines compared to RNN and CopyRNN.ßdummyTurker
19052ß0.0ßResponded in weakness argument 2ßdummyTurker
19053ß0.0ßThere is a pre-training procedure before the RL.ßdummyTurker
19054ß0.0ßIt is a vanilla Seq2Seq model.ßdummyTurker
19055ß0.0ßWe use Adam to optimize the MLE objective function and heuristically train for 100 epochs.ßdummyTurker
19056ß0.0ßResponded in weakness argument 3.ßdummyTurker
19057ß0.0ßThe vocabulary of decoder forms the action space.ßdummyTurker
19058ß0.0ßEach keyphrase is an action.ßdummyTurker
19059ß0.0ß“The movement of sampling a keyphrase from the softmax distribution” is the process of selecting actions.ßdummyTurker
